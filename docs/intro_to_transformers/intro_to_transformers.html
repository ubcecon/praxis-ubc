<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Praxis - Transformers : complething this …</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../media/praxis-badge.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="keywords" content="economics, econometrics, R, data, machine learning, UBC, COMET, geog 374, econ 325, econ 326, learning, teaching, learn r, r help, help, tutorial, r tutorial for beginners,learning statistics with r, learn r programming, learn statistics, linear regression, r machine learning, learn machine learning, university of british columbia, british columbia, r programming for beginners, r language tutorial, r tutorial for beginners, economic data, econometrics tutoring, economics help for students, economics homework help, oer resources for teachers, open educational resources for teachers, educational resource, oer project, oer materials, oer resources, learn economics online, learn econometrics, teach yourself economics, teach yourself econometrics, econometrics basics for beginners">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../media/praxis-badge-white.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Praxis</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-get-started" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Get Started</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-get-started">    
        <li>
    <a class="dropdown-item" href="../../pages/quickstart.html" rel="" target="">
 <span class="dropdown-text">Quickstart Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-courses" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Courses</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-courses">    
        <li>
    <a class="dropdown-item" href="../../pages/index/index_HIST-414.html" rel="" target="">
 <span class="dropdown-text">HIST-414</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/index/index_AMNE-376.html" rel="" target="">
 <span class="dropdown-text">AMNE-376</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/index/index_SOCI415.html" rel="" target="">
 <span class="dropdown-text">SOCI-415</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/index/index_SOCI280.html" rel="" target="">
 <span class="dropdown-text">SOCI-280</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/index/index_ECON227.html" rel="" target="">
 <span class="dropdown-text">ECON-227</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../pages/index/all.html" rel="" target="">
 <span class="dropdown-text">Browse All</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../pages/index/index_topical.html" rel="" target="">
 <span class="menu-text">Topics</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-teach-with-praxis" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Teach With prAxIs</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-teach-with-praxis">    
        <li>
    <a class="dropdown-item" href="../../pages/teaching_with_comet.html" rel="" target="">
 <span class="dropdown-text">Learn how to teach with prAxIs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/using_comet.html" rel="" target="">
 <span class="dropdown-text">Using prAxIs in the Classroom</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-launch-praxis" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
      <i class="bi bi-play" role="img">
</i> 
 <span class="menu-text">Launch prAxIs</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-launch-praxis">    
        <li>
    <a class="dropdown-item" href="https://open.jupyter.ubc.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-notebooks&amp;urlpath=lab%2Ftree%2Fcomet-notebooks%2F&amp;branch=main" rel="" target=""><i class="bi bi-cloud-check" role="img">
</i> 
 <span class="dropdown-text">Launch on JupyterOpen (with Data)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://open.jupyter.ubc.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-project&amp;urlpath=lab%2Ftree%2Fcomet-project%2F&amp;branch=main" rel="" target=""><i class="bi bi-cloud-check" role="img">
</i> 
 <span class="dropdown-text">Launch on JupyterOpen (lite)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://ubc.syzygy.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-project&amp;urlpath=lab%2Ftree%2Fcomet-project%2F&amp;branch=main" rel="" target=""><i class="bi bi-gear" role="img">
</i> 
 <span class="dropdown-text">Launch on Syzygy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://colab.research.google.com/github/ubcecon/comet-notebooks/blob/main/" rel="" target=""><i class="bi bi-google" role="img">
</i> 
 <span class="dropdown-text">Launch on Colab</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/comet-notebooks/archive/refs/heads/main.zip" rel="" target=""><i class="bi bi-cloud-download" role="img">
</i> 
 <span class="dropdown-text">Launch Locally</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/comet-open/archive/refs/heads/datasets.zip" rel="" target=""><i class="bi bi-clipboard-data" role="img">
</i> 
 <span class="dropdown-text">Project Datasets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/praxis-ubc" rel="" target="">
 <span class="dropdown-text">Github Repository</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../#" rel="" target="">
 <span class="menu-text">|</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-about" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">About</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-about">    
        <li>
    <a class="dropdown-item" href="../../pages/team.html" rel="" target="">
 <span class="dropdown-text">prAxIs Team</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/copyright.html" rel="" target="">
 <span class="dropdown-text">Copyright Information</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#prediction-game-where-will-the-ball-go" id="toc-prediction-game-where-will-the-ball-go" class="nav-link active" data-scroll-target="#prediction-game-where-will-the-ball-go">Prediction Game: Where Will the Ball Go?</a></li>
  <li><a href="#given-an-image-of-a-ball-can-you-predict-where-it-will-go-next" id="toc-given-an-image-of-a-ball-can-you-predict-where-it-will-go-next" class="nav-link" data-scroll-target="#given-an-image-of-a-ball-can-you-predict-where-it-will-go-next">Given an image of a ball can you predict where it will go next ?</a></li>
  <li><a href="#sequential-prediction-with-rnns" id="toc-sequential-prediction-with-rnns" class="nav-link" data-scroll-target="#sequential-prediction-with-rnns">Sequential Prediction with RNNs</a>
  <ul class="collapse">
  <li><a href="#how-it-works-step-by-step" id="toc-how-it-works-step-by-step" class="nav-link" data-scroll-target="#how-it-works-step-by-step">How It Works (Step-by-Step)</a></li>
  <li><a href="#why-rnns-can-be-tricky" id="toc-why-rnns-can-be-tricky" class="nav-link" data-scroll-target="#why-rnns-can-be-tricky">Why RNNs Can Be Tricky ?</a></li>
  </ul></li>
  <li><a href="#what-is-a-gpt-model" id="toc-what-is-a-gpt-model" class="nav-link" data-scroll-target="#what-is-a-gpt-model">What is a GPT model ?</a></li>
  <li><a href="#whats-a-transformer" id="toc-whats-a-transformer" class="nav-link" data-scroll-target="#whats-a-transformer">What’s a transformer ?</a></li>
  <li><a href="#how-does-a-transformer-work" id="toc-how-does-a-transformer-work" class="nav-link" data-scroll-target="#how-does-a-transformer-work">How does a transformer work ?</a></li>
  <li><a href="#transformer-applications" id="toc-transformer-applications" class="nav-link" data-scroll-target="#transformer-applications">Transformer Applications</a></li>
  <li><a href="#tokenization-this-takes-to-long-to-load-might-be-too-much-else-i-think-including-this-would-be-good" id="toc-tokenization-this-takes-to-long-to-load-might-be-too-much-else-i-think-including-this-would-be-good" class="nav-link" data-scroll-target="#tokenization-this-takes-to-long-to-load-might-be-too-much-else-i-think-including-this-would-be-good">Tokenization (this takes to long to load, might be too much, else I think including this would be good)</a></li>
  <li><a href="#word-prediction-demo" id="toc-word-prediction-demo" class="nav-link" data-scroll-target="#word-prediction-demo">Word Prediction Demo</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/ubcecon/praxis-ubc/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Transformers : complething this …</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>Author: <em>Krishaant Pathmanathan, PRAXIS UBC Team</em></p>
<p>Date: 2025-06</p>
<hr>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math, copy, time</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>matplotlib inline</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.animation <span class="im">as</span> animation</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="prediction-game-where-will-the-ball-go" class="level2">
<h2 class="anchored" data-anchor-id="prediction-game-where-will-the-ball-go">Prediction Game: Where Will the Ball Go?</h2>
<p>Transformers are all about <strong>making predictions from context</strong>. But what does that really mean? Before diving into transformers, lets start with something simple.</p>
<p><strong>Imagine this:</strong> You see a bouncing ball mid-air. Where will it go next? How do you know? You don’t need physics equations to guess. You use your intuition. You remember what came before — the ball’s past trajectory — and you predict what’s most likely to happen next. Just like us, models try to guess what comes next based on what they’ve already seen.</p>
</section>
<section id="given-an-image-of-a-ball-can-you-predict-where-it-will-go-next" class="level2">
<h2 class="anchored" data-anchor-id="given-an-image-of-a-ball-can-you-predict-where-it-will-go-next">Given an image of a ball can you predict where it will go next ?</h2>
<div style="display: flex; gap: 20px;">
<p><img src="data/static_ball2.png" alt="Static ball" width="300"> <img src="data/moving_ball3.png" alt="Moving ball" width="300"></p>
</div>
<p>Given a sequence, can you now tell ? This act of prediction based on prior observations is exactly what transformer models do with text, images, or sequences instead of balls.</p>
<p>Turns out there are many sequences in the world - words in a sentence, frames in a video, notes in a melody, steps in a recipe. The challenge isn’t just seeing them, it’s predicting what comes next. To understand or predict them, we need a model that doesn’t just look at things in isolation… It has to <strong>remember what came before</strong>.</p>
<!-- maybe insert some pictures of that here  -->
</section>
<section id="sequential-prediction-with-rnns" class="level2">
<h2 class="anchored" data-anchor-id="sequential-prediction-with-rnns">Sequential Prediction with RNNs</h2>
<p>Imagine reading one word at a time, keeping track of what came before… that’s what an RNN does. RNN stands for <strong>Recurrent Neural Network</strong>. It’s a type of model that learns from <strong>sequences</strong> — like sentences, music, or even time. It’s an important precursor to a <strong>transformer</strong></p>
<section id="how-it-works-step-by-step" class="level3">
<h3 class="anchored" data-anchor-id="how-it-works-step-by-step">How It Works (Step-by-Step)</h3>
<ol type="1">
<li><p><strong>Give it a sentence</strong> — for example:<br>
<code>"I love recurrent neural ____"</code></p></li>
<li><p><strong>Turn words into numbers</strong> (this is called <em>tokenizing</em>)<br>
&gt; Computers can’t understand words — they only understand numbers!</p></li>
<li><p><strong>Send the numbers into the RNN</strong> — one by one</p></li>
<li><p><strong>At each step</strong>, the RNN tries to <strong>remember what came before</strong><br>
&gt; It passes a little memory called a <strong>hidden state</strong> from word to word</p></li>
<li><p><strong>At the end</strong>, it <strong>guesses what comes next!</strong><br>
&gt; Like filling in the blank at the end of the sentence</p></li>
</ol>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated RNN function (takes a look at past to make prediction)</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> my_rnn(word, hidden_state):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Imagine the RNN does something with the word and memory</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    new_hidden_state <span class="op">=</span> [h <span class="op">+</span> <span class="dv">1</span> <span class="cf">for</span> h <span class="kw">in</span> hidden_state]  <span class="co"># update memory</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> <span class="st">"networks!"</span> <span class="cf">if</span> word <span class="op">==</span> <span class="st">"neural"</span> <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> prediction, new_hidden_state</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Initial hidden state</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>hidden_state <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Input sentence (tokenized)</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> [<span class="st">"I"</span>, <span class="st">"love"</span>, <span class="st">"recurrent"</span>, <span class="st">"neural"</span>]</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># RNN step-by-step loop</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> sentence:</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>    prediction, hidden_state <span class="op">=</span> my_rnn(word, hidden_state)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Final predicted word</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>next_word_prediction <span class="op">=</span> prediction</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Next word prediction:"</span>, next_word_prediction)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="why-rnns-can-be-tricky" class="level3">
<h3 class="anchored" data-anchor-id="why-rnns-can-be-tricky">Why RNNs Can Be Tricky ?</h3>
<ul>
<li>They <strong>read one word at a time</strong>, so it’s slow</li>
<li>They <strong>forget things</strong> after a while (just like people!)</li>
<li>They <strong>can’t look at everything at once</strong></li>
</ul>
<p>RNNs are like someone reading a story <strong>out loud, one word at a time</strong>. Transformers (like GPT) are like someone <strong>looking at the whole page at once</strong>.</p>
<p><strong>RNNs walked, so Transformers could fly.</strong></p>
</section>
</section>
<section id="what-is-a-gpt-model" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-gpt-model">What is a GPT model ?</h2>
<p>A GPT is a Generative Pre-Trained Transformer. The first two words are self-explanatory: generative means the model generates new text; pre-trained means the model was trained on large amounts of data. What we will focus on is the <strong>transformer</strong> aspect of the language model, the main proponent of the recent boom in AI.</p>
</section>
<section id="whats-a-transformer" class="level2">
<h2 class="anchored" data-anchor-id="whats-a-transformer">What’s a transformer ?</h2>
<p>A transformer is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence.It is the main component that underlies tools like ChatGPT. It can trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, then produce a prediction of what comes next, in the form of a probability distribution over all chunks of text that might follow.</p>
<p>*Note there are many other types of transformers (voice-to-text, text-to-image, etc.).</p>
</section>
<section id="how-does-a-transformer-work" class="level2">
<h2 class="anchored" data-anchor-id="how-does-a-transformer-work">How does a transformer work ?</h2>
<p><strong>1) Embeddings A.K.A Turning Words into Numbers:</strong></p>
<p>Before the model can understand anything it has to turn words into numbers. Not just any numbers, numbers that capture meaning. For instance the words “table” and “desk” might have numbers that are “simillar” (closer together in a vector space) whereas “table” and “apple” would be less simillar. Also, since the model doesn’t intuitively understand the order of words like you and I we give it <em>positional encodings</em>. It’s like telling the model this is the first, second, … word in the sentence.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a simple vocabulary</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'he'</span>: <span class="dv">0</span>, <span class="st">'deposited'</span>: <span class="dv">1</span>, <span class="st">'cash'</span>: <span class="dv">2</span>, <span class="st">'at'</span>: <span class="dv">3</span>, <span class="st">'the'</span>: <span class="dv">4</span>, <span class="st">'bank'</span>: <span class="dv">5</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'on'</span>: <span class="dv">6</span>, <span class="st">'friday'</span>: <span class="dv">7</span>, <span class="st">'river'</span>: <span class="dv">8</span>, <span class="st">'runs'</span>: <span class="dv">9</span>, <span class="st">'fast'</span>: <span class="dv">10</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">5</span>  <span class="co"># keep it small so students can visualize</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the embedding layer</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(num_embeddings<span class="op">=</span>vocab_size, embedding_dim<span class="op">=</span>embedding_dim)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Try editing this sentence! </span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>sentence <span class="op">=</span> <span class="st">"he deposited cash at the bank"</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> sentence.lower().split()</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>token_ids <span class="op">=</span> [vocab.get(word, <span class="dv">0</span>) <span class="cf">for</span> word <span class="kw">in</span> tokens]  <span class="co"># default to 0 if word is missing</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to tensor</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>input_tensor <span class="op">=</span> torch.tensor(token_ids).unsqueeze(<span class="dv">0</span>)  <span class="co"># shape: [1, seq_len]</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Get vector representations</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>embedded <span class="op">=</span> embedding(input_tensor).squeeze(<span class="dv">0</span>).detach()</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Display as a table</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(embedded.numpy(), index<span class="op">=</span>tokens, columns<span class="op">=</span>[<span class="ss">f"dim_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embedding_dim)])</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>display(df)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Optional: Visualize with a bar chart for each word</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>df.plot(kind<span class="op">=</span><span class="st">"bar"</span>, figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>), title<span class="op">=</span><span class="st">"Word Embeddings"</span>)</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Words"</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Embedding Values"</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>plt.xticks(rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Words that are more simillar like cash and bank, share some bars, whereas words that might represent grammar instead like “he” only has negative bars</p>
<p><strong>2) Attention : Who’s talking to Whom ?</strong></p>
<p>Imagine a dinner party conversation, You’re trying to follow what someone is saying, but also who they are talking to, and who said what before that. Transformers to the same thing, they look at every word in a sentence and ask “which words should I pay most attention to in order to understand this word ?” This process is called attention, and it helps the model understand context - like knowing the difference between a river bank and a money bank. “He deposited cash at the bank on Friday.” The attention mechanism would allow the model to place more emphasis on the words “deposited”, “cash”, and “Friday”, and less emphasis on function words like “the” — helping the model understand that “bank” refers to a financial institution, not a riverbank.</p>
<p>This idea of letting models decide what to pay “attention” to was first outlined in the landmark research paper “Attention is all you need” by Google in 2017</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate a short sentence</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [<span class="st">'he'</span>, <span class="st">'deposited'</span>, <span class="st">'cash'</span>, <span class="st">'at'</span>, <span class="st">'the'</span>, <span class="st">'bank'</span>]</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="bu">len</span>(tokens)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">8</span>  <span class="co"># Choose a small embedding size for demonstration</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: Generate positional encodings</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>position <span class="op">=</span> torch.arange(seq_len).unsqueeze(<span class="dv">1</span>)  <span class="co"># shape: [seq_len, 1]</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, embedding_dim, <span class="dv">2</span>) <span class="op">*</span> (<span class="op">-</span>np.log(<span class="fl">10000.0</span>) <span class="op">/</span> embedding_dim))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> torch.zeros(seq_len, embedding_dim)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>pe[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(position <span class="op">*</span> div_term)  <span class="co"># even indices</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>pe[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(position <span class="op">*</span> div_term)  <span class="co"># odd indices</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 2: Show in table</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(pe.numpy(), index<span class="op">=</span>tokens, columns<span class="op">=</span>[<span class="ss">f"dim_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embedding_dim)])</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>display(df)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 3: Visualize positional encodings</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(embedding_dim):</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    plt.plot(pe[:, i], label<span class="op">=</span><span class="ss">f"dim_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Positional Encoding Patterns Across Dimensions"</span>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Position in Sentence"</span>)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Encoding Value"</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>plt.legend(loc<span class="op">=</span><span class="st">"upper right"</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.15</span>, <span class="fl">1.0</span>))</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>plt.xticks(<span class="bu">range</span>(seq_len), tokens)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each coloured line helps the model tell the difference between words based on where they appear, through their encoding values. These patterns are carefully designed so that each position (1st word, 2nd word, 3rd word…) looks different to the model.</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Simple token list</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [<span class="st">'he'</span>, <span class="st">'deposited'</span>, <span class="st">'cash'</span>, <span class="st">'at'</span>, <span class="st">'the'</span>, <span class="st">'bank'</span>]</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>seq_len <span class="op">=</span> <span class="bu">len</span>(tokens)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate embeddings (normally from Step 1 + 2)</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.rand(seq_len, embed_dim)  <span class="co"># [seq_len, embed_dim]</span></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulate attention: Q, K, V are all the same</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> x</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>K <span class="op">=</span> x</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>V <span class="op">=</span> x</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Scaled dot-product attention</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> torch.matmul(Q, K.T) <span class="op">/</span> (embed_dim <span class="op">**</span> <span class="fl">0.5</span>)  <span class="co"># [seq_len, seq_len]</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># rows = which words each word attends to</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Display as heatmap</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame(attention_weights.numpy(), index<span class="op">=</span>tokens, columns<span class="op">=</span>tokens)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>sns.heatmap(df, annot<span class="op">=</span><span class="va">True</span>, cmap<span class="op">=</span><span class="st">"Blues"</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Self-Attention: Who's Listening to Whom?"</span>)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Query Word →"</span>)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Key Word →"</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>This maps shows who’s paying attention to whom in this sentence. For example the word “cash” pays most attention to itself (0.24) but also listens to “he” (0.11). “he” looks around but leans toward “cash” and “bank”, possibly tracking who’s doing what. and 0The values here show how much attention is paid to each word, higher numbers = stronger focus. Rows add up to 1 because each word is spreading 100% of its attention across all other words in the sentence. Again this is important because, if the attention scores for “bank” were highest on “cash” or “deposited”, that would help the model guess that “bank” means money bank, not river bank.</p>
<p><strong>3) Thinking Layers (a.k.a. Feedforward Networks):</strong></p>
<p>After deciding which words to pay “attention” to, the model passes each one through a thinking block (feed forward network) — basically a small decision-making system.It takes in a word’s current understanding and asks it a series of questions to refine its meaning. You can imagine it asking things like: “Is this word a noun? Does it seem important? What kind of word should come next?” Each of these layers updates the word’s internal representation based on what the model has learned from training.</p>
<p>Imagine the word “bank” in this sentence: “He deposited cash at the bank on Friday.”</p>
<p>The model already knows — thanks to attention — that “cash” and “deposited” are important. Now the feed-forward layers take over and might internally ask: “Does this word usually appear after the word cash?”, “Is this word often associated with financial transactions?”</p>
<p>Each question helps the model become more confident in its interpretation:</p>
<p>“Okay — this is probably a money bank, not a river bank.”</p>
<p>The model repeats this back-and-forth process:</p>
<p>Attention chooses what matters, Feed-forward layers analyze it</p>
<p>Then it loops again, with each layer getting a more refined picture of the sentence’s meaning</p>
<p>By the final layer, the model is ready to make a prediction — like guessing the next word, classifying a sentence, or answering a question. In text generation, this output takes the form of a probability distribution over all the words it knows.</p>
<p><img src="data/transformerpredict.png" alt="Transformer Prediction" width="800"> <!-- if we decide to use this I will recreate this image with a more social science specific example  --></p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated output from attention (Step 3)</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [<span class="st">'he'</span>, <span class="st">'deposited'</span>, <span class="st">'cash'</span>, <span class="st">'at'</span>, <span class="st">'the'</span>, <span class="st">'bank'</span>]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>attention_output <span class="op">=</span> torch.randn(<span class="bu">len</span>(tokens), <span class="dv">8</span>)  <span class="co"># 8-dim vectors per word</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Feed-Forward Network (same for all words)</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>ffn <span class="op">=</span> nn.Sequential(</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">8</span>, <span class="dv">16</span>),  <span class="co"># Expand</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">16</span>, <span class="dv">8</span>)   <span class="co"># Compress back</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>ffn_output <span class="op">=</span> ffn(attention_output)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Display before and after transformation</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>df_before <span class="op">=</span> pd.DataFrame(attention_output.detach().numpy(), index<span class="op">=</span>tokens, columns<span class="op">=</span>[<span class="ss">f"before_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>)])</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>df_after <span class="op">=</span> pd.DataFrame(ffn_output.detach().numpy(), index<span class="op">=</span>tokens, columns<span class="op">=</span>[<span class="ss">f"after_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">8</span>)])</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat([df_before, df_after], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>display(df)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize how the vector for one word changes (e.g., "bank")</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">4</span>))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>plt.plot(df.loc[<span class="st">'bank'</span>].values[:<span class="dv">8</span>], label<span class="op">=</span><span class="st">"Before FFN"</span>)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>plt.plot(df.loc[<span class="st">'bank'</span>].values[<span class="dv">8</span>:], label<span class="op">=</span><span class="st">"After FFN"</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Word: 'bank' — Vector Before &amp; After Feed-Forward Network"</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Vector Dimension"</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Value"</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The orange line shows you that the feed forward network makes the word “bank” the output value for the word bank more smooth, making it more sure of what kind of work it is.</p>
<p><strong>4) Unembedding matrix:</strong></p>
<p>Once the model has chewed through the whole sentence, it takes all that processed information and makes a guess: “Based on everything I’ve seen so far, what word is most likely to come next?”</p>
<p>It does this by comparing its internal numbers to a giant list of all the words it knows, and picking the one with the highest score — kind of like autocomplete when texting, but way smarter.</p>
<p><strong>5) Weights, Weight Matrices, and Fine-tuning:</strong></p>
<p><strong>Weights:</strong> Weights are parameters within a neural network that are learned during the training process. They determine the strength and direction of the connections in the network. Intially, weights are set randomly; during training, the weights are adjusted to minimize the error between the predicted output and the actual output, by minimizing a loss function. This process is known as <em>gradient descent</em>.</p>
<p><strong>Weight matrices</strong> are structured collections of weights arranged in matrix form. They represent the connections between layers in a neural network. The operation of passing inputs through the network involves matrix multiplication: the input vector is multiplied by the weight matrix to produce the output vector for the next layer.</p>
<p>In the attention mechanism, each word in the input sequence is transformed into three different vectors: the query vector (used to search for relevant information from other words in the sequence), the key vector (represents the words in the sequence and is used to match with query vectors), and the value vector (holds the actual information of the words in the sequence and is used to generate the output of the attention mechanism), using separate weight matrices <span class="math inline">\(^{[14]}\)</span>. For example, if the input is a sequence of words represented as vectors, the queries, keys, and values are computed as:</p>
<p><span class="math display">\[Q=W_{Q}(X), K=W_{K}(X), V=W_{V}(X)\]</span></p>
<p>where <span class="math inline">\(W_{Q}\)</span>​, <span class="math inline">\(W_{K}\)</span>​, and <span class="math inline">\(W_{V}\)</span>​ are weight matrices <span class="math inline">\(^{[14]}\)</span> <span class="math inline">\(^{[15]}\)</span>. These vectors are used to calculate attention scores, which determine how much focus each word should give to every other word in the sequence.</p>
<p><strong>Fine-tuning</strong> is the process of updating the key, query and value matrices to reflect new data <span class="math inline">\(^{[16]}\)</span>. Because the weight matrices contain both the original, general weights and the new adjustments from the fine-tuning process, fine-tuning allows the model to retain the broad, general knowledge from the pre-training phase while specializing in the a new task, such as sentiment analysis, customer feedback, etc.</p>
</section>
<section id="transformer-applications" class="level2">
<h2 class="anchored" data-anchor-id="transformer-applications">Transformer Applications</h2>
<p>Transformers aren’t just for chatbots ! There are plenty of uses for transformers outside of this, espescially in the arts and humanities. For instance : - Archival Text Restoration - filling in gaps in damaged ancient manuscripts - Symbol Classification - Automatically labelling ancient symbols and handwritting (eg. cuneiform tablets) - Culutral Analysis - Using LLMs to trace narrative across historical text</p>
</section>
<section id="tokenization-this-takes-to-long-to-load-might-be-too-much-else-i-think-including-this-would-be-good" class="level2">
<h2 class="anchored" data-anchor-id="tokenization-this-takes-to-long-to-load-might-be-too-much-else-i-think-including-this-would-be-good">Tokenization (this takes to long to load, might be too much, else I think including this would be good)</h2>
<p>Just like text is broken into tokens, images are broken into patches, and audio is split into time steps or spectrogram slices before being passed to transformer models like ViT or Wav2Vec2.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a BERT tokenizer</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">'bert-base-uncased'</span>)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize an example text</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> <span class="st">'The ball is round.'</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> tokenizer(text)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>tokens</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="word-prediction-demo" class="level2">
<h2 class="anchored" data-anchor-id="word-prediction-demo">Word Prediction Demo</h2>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">'The history of the world is'</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>output <span class="op">=</span> generator(prompt, max_length<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output[<span class="dv">0</span>][<span class="st">'generated_text'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ol type="1">
<li><p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems. arXiv:1706.03762</p></li>
<li><p>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … &amp; Amodei, D. (2020). Language Models are Few-Shot Learners. arXiv:2005.14165</p></li>
<li><p>Vasvammi. Visual Explanation of Transformers. https://vasvammi.github.io/transformer-visual</p></li>
<li><p>MIT Deep Learning: Introduction to Deep Learning (2024) https://introtodeeplearning.com/2024/index.html</p></li>
<li><p>3Blue1Brown: But What is a GPT? https://www.3blue1brown.com/lessons/gpt</p></li>
<li><p>Henry AI Labs: Transformers in a Nutshell (YouTube) https://www.youtube.com/watch?v=zxQyTK8quyY&amp;vl=en</p></li>
<li><p>Peter Bloem: Transformers Explained Visually and Conceptually https://peterbloem.nl/blog/transformers</p></li>
<li><p>Jay Alammar: The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/</p></li>
<li><p>COMET Docs: Fine-tuning LLMs with Ollama (UBC Arts) https://comet.arts.ubc.ca/docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#setting-up</p></li>
</ol>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplified code demo </span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 1: Word Embeddings ===</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> {<span class="st">'he'</span>: <span class="dv">0</span>, <span class="st">'deposited'</span>: <span class="dv">1</span>, <span class="st">'cash'</span>: <span class="dv">2</span>, <span class="st">'at'</span>: <span class="dv">3</span>, <span class="st">'the'</span>: <span class="dv">4</span>, <span class="st">'bank'</span>: <span class="dv">5</span>, <span class="st">'on'</span>: <span class="dv">6</span>, <span class="st">'friday'</span>: <span class="dv">7</span>}</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> [<span class="st">'he'</span>, <span class="st">'deposited'</span>, <span class="st">'cash'</span>, <span class="st">'at'</span>, <span class="st">'the'</span>, <span class="st">'bank'</span>, <span class="st">'on'</span>, <span class="st">'friday'</span>]</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>token_ids <span class="op">=</span> torch.tensor([vocab[word] <span class="cf">for</span> word <span class="kw">in</span> tokens]).unsqueeze(<span class="dv">0</span>)  <span class="co"># shape: [1, seq_len]</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>embedding_dim <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>embedding <span class="op">=</span> nn.Embedding(<span class="bu">len</span>(vocab), embedding_dim)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>embedded_tokens <span class="op">=</span> embedding(token_ids)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 2: Add Positional Encoding ===</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, embedded_tokens.size(<span class="dv">1</span>)).unsqueeze(<span class="dv">1</span>).<span class="bu">float</span>()</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>div_term <span class="op">=</span> torch.exp(torch.arange(<span class="dv">0</span>, embedding_dim, <span class="dv">2</span>).<span class="bu">float</span>() <span class="op">*</span> <span class="op">-</span>(torch.log(torch.tensor(<span class="fl">10000.0</span>)) <span class="op">/</span> embedding_dim))</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>pe <span class="op">=</span> torch.zeros_like(embedded_tokens)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>pe[:, :, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> torch.sin(pos <span class="op">*</span> div_term)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>pe[:, :, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> torch.cos(pos <span class="op">*</span> div_term)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>pos_embed <span class="op">=</span> embedded_tokens <span class="op">+</span> pe</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 3: Self-Attention ===</span></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> K <span class="op">=</span> V <span class="op">=</span> pos_embed</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>dk <span class="op">=</span> Q.size(<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>scores <span class="op">=</span> torch.matmul(Q, K.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">/</span> dk<span class="op">**</span><span class="fl">0.5</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>attention_weights <span class="op">=</span> F.softmax(scores, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>attention_output <span class="op">=</span> torch.matmul(attention_weights, V)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 4: Feed-Forward Network ===</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>ffn <span class="op">=</span> nn.Sequential(</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    nn.Linear(embedding_dim, <span class="dv">32</span>),</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    nn.ReLU(),</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    nn.Linear(<span class="dv">32</span>, embedding_dim)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>ffn_output <span class="op">=</span> ffn(attention_output)</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="co"># === Step 5: Unembedding and Final Output ===</span></span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>unembedding <span class="op">=</span> nn.Linear(embedding_dim, <span class="bu">len</span>(vocab))</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>logits <span class="op">=</span> unembedding(ffn_output)</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="co"># === Show Top Predictions for Each Token ===</span></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, token <span class="kw">in</span> <span class="bu">enumerate</span>(tokens):</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>    pred_id <span class="op">=</span> torch.argmax(probs[<span class="dv">0</span>, i]).item()</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>    pred_word <span class="op">=</span> <span class="bu">list</span>(vocab.keys())[pred_id]</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>token<span class="sc">}</span><span class="ss"> → Top prediction: </span><span class="sc">{</span>pred_word<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png"></a>.  <a rel="license" href="https://comet.arts.ubc.ca/pages/copyright.html">See details.</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 The prAxIs Project and UBC are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples.
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>