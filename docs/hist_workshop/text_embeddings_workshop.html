<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="prAxIs UBC Team   Kaiyan Zhang, Irene Berezin, Alex Ronczewski">
<meta name="dcterms.date" content="2025-08-04">
<meta name="description" content="This notebook aims to demonstrate how machine learning can assist with historical research. - Python - Word Embeddings - Sentiment Analysis">

<title>Praxis - Embedding Driven Text Analysis of Crease’s Stance Towards Chinese Immigrants</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<link href="../../media/praxis-badge.png" rel="icon" type="image/png">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="keywords" content="economics, econometrics, R, data, machine learning, UBC, COMET, geog 374, econ 325, econ 326, learning, teaching, learn r, r help, help, tutorial, r tutorial for beginners,learning statistics with r, learn r programming, learn statistics, linear regression, r machine learning, learn machine learning, university of british columbia, british columbia, r programming for beginners, r language tutorial, r tutorial for beginners, economic data, econometrics tutoring, economics help for students, economics homework help, oer resources for teachers, open educational resources for teachers, educational resource, oer project, oer materials, oer resources, learn economics online, learn econometrics, teach yourself economics, teach yourself econometrics, econometrics basics for beginners">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../media/praxis-badge-white.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Praxis</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-get-started" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Get Started</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-get-started">    
        <li>
    <a class="dropdown-item" href="../../pages/quickstart.html" rel="" target="">
 <span class="dropdown-text">Quickstart Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-courses" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Courses</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-courses">    
        <li>
    <a class="dropdown-item" href="../../pages/index/index_HIST-414.html" rel="" target="">
 <span class="dropdown-text">HIST-414</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/index/index_AMNE-376.html" rel="" target="">
 <span class="dropdown-text">AMNE-376</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/index/index_SOCI415.html" rel="" target="">
 <span class="dropdown-text">SOCI-415</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/index/index_SOCI280.html" rel="" target="">
 <span class="dropdown-text">SOCI-280</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/index/index_ECON227.html" rel="" target="">
 <span class="dropdown-text">ECON-227</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../pages/index/all.html" rel="" target="">
 <span class="dropdown-text">Browse All</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../pages/index/index_topical.html" rel="" target="">
 <span class="menu-text">Topics</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-teach-with-praxis" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Teach With prAxIs</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-teach-with-praxis">    
        <li>
    <a class="dropdown-item" href="../../pages/teaching_with_comet.html" rel="" target="">
 <span class="dropdown-text">Learn how to teach with prAxIs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/using_comet.html" rel="" target="">
 <span class="dropdown-text">Using prAxIs in the Classroom</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-launch-praxis" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
      <i class="bi bi-play" role="img">
</i> 
 <span class="menu-text">Launch prAxIs</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-launch-praxis">    
        <li>
    <a class="dropdown-item" href="https://open.jupyter.ubc.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-notebooks&amp;urlpath=lab%2Ftree%2Fcomet-notebooks%2F&amp;branch=main" rel="" target=""><i class="bi bi-cloud-check" role="img">
</i> 
 <span class="dropdown-text">Launch on JupyterOpen (with Data)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://open.jupyter.ubc.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-project&amp;urlpath=lab%2Ftree%2Fcomet-project%2F&amp;branch=main" rel="" target=""><i class="bi bi-cloud-check" role="img">
</i> 
 <span class="dropdown-text">Launch on JupyterOpen (lite)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://ubc.syzygy.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-project&amp;urlpath=lab%2Ftree%2Fcomet-project%2F&amp;branch=main" rel="" target=""><i class="bi bi-gear" role="img">
</i> 
 <span class="dropdown-text">Launch on Syzygy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://colab.research.google.com/github/ubcecon/comet-notebooks/blob/main/" rel="" target=""><i class="bi bi-google" role="img">
</i> 
 <span class="dropdown-text">Launch on Colab</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/comet-notebooks/archive/refs/heads/main.zip" rel="" target=""><i class="bi bi-cloud-download" role="img">
</i> 
 <span class="dropdown-text">Launch Locally</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/comet-open/archive/refs/heads/datasets.zip" rel="" target=""><i class="bi bi-clipboard-data" role="img">
</i> 
 <span class="dropdown-text">Project Datasets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/praxis-ubc" rel="" target="">
 <span class="dropdown-text">Github Repository</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../#" rel="" target="">
 <span class="menu-text">|</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-about" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">About</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-about">    
        <li>
    <a class="dropdown-item" href="../../pages/team.html" rel="" target="">
 <span class="dropdown-text">prAxIs Team</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../pages/copyright.html" rel="" target="">
 <span class="dropdown-text">Copyright Information</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#library-loading" id="toc-library-loading" class="nav-link active" data-scroll-target="#library-loading">Library Loading</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#how-computers-interpret-text" id="toc-how-computers-interpret-text" class="nav-link" data-scroll-target="#how-computers-interpret-text">How Computers Interpret Text?</a></li>
  <li><a href="#word-embeddings" id="toc-word-embeddings" class="nav-link" data-scroll-target="#word-embeddings">Word Embeddings</a></li>
  <li><a href="#embedding-driven-text-analysis" id="toc-embedding-driven-text-analysis" class="nav-link" data-scroll-target="#embedding-driven-text-analysis">Embedding Driven Text Analysis</a></li>
  <li><a href="#topic-modeling-and-alignment-analysis" id="toc-topic-modeling-and-alignment-analysis" class="nav-link" data-scroll-target="#topic-modeling-and-alignment-analysis">Topic Modeling and Alignment Analysis</a></li>
  <li><a href="#llm-and-zero-shot-classification" id="toc-llm-and-zero-shot-classification" class="nav-link" data-scroll-target="#llm-and-zero-shot-classification">LLM and Zero-Shot Classification</a></li>
  <li><a href="#putting-it-all-together" id="toc-putting-it-all-together" class="nav-link" data-scroll-target="#putting-it-all-together">Putting It All Together</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/ubcecon/praxis-ubc/issues/new" class="toc-action">Report an issue</a></p></div></div><div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="text_embeddings_workshop.ipynb" download="text_embeddings_workshop.ipynb"><i class="bi bi-journal-code"></i>Jupyter</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Embedding Driven Text Analysis of Crease’s Stance Towards Chinese Immigrants</h1>
</div>

<div>
  <div class="description">
    This notebook aims to demonstrate how machine learning can assist with historical research. - Python - Word Embeddings - Sentiment Analysis
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>prAxIs UBC Team <br> <em>Kaiyan Zhang, Irene Berezin, Alex Ronczewski</em> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">4 August 2025</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="library-loading" class="level3">
<h3 class="anchored" data-anchor-id="library-loading">Library Loading</h3>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This cell loads the necessary libraries for executing the notebook.</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> umap</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.patches <span class="im">import</span> Patch</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.graph_objects <span class="im">as</span> go</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> plotly.subplots <span class="im">import</span> make_subplots</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> sent_tokenize, word_tokenize</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.corpus <span class="im">import</span> stopwords</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> TfidfVectorizer</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cosine</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel, pipeline</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict, Counter</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Dict, Any, Union</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<section id="overview-of-historical-background" class="level4">
<h4 class="anchored" data-anchor-id="overview-of-historical-background">Overview of Historical Background</h4>
<p><strong><em>The 1884 Chinese Regulation Act</em></strong> in British Columbia is widely regarded as one of the most notorious discriminatory provincial laws targeting Chinese immigration, it was challenged and ultimately declared unconstitutional in the 1885 case of <strong><em>R v. Wing Chong</em></strong> by the judge Henry Pering Pellew Crease. The Justice Crease found the legislation to be unconstitutional on economic grounds; infringing on federal authority over immigration, trade, commerce, treaty-making, and taxation.</p>
<p>The central figure in the ruling, <em>Henry Pering Pellew Crease</em>, came from a wealthy English military family, and possessed a prestigious law background.</p>
<ul>
<li>His social identity was above-all English, and this was made clear in his politics.</li>
<li>He viewed Canada not as a new society to be built, but as an extension of the british empire.</li>
<li>He displayed mistrust towards Canadians, referring to them as “North American Chinamen”, afraid that they would “rule the country and job its offices” (Tina Loo).</li>
</ul>
<p>In previous years, students expressed interest Crease’s opinion on the 1884 Chinese regulation act, given that the regulation act was strongly condemned and ultimately struck down by Crease. However, this seems at odds with Crease’s position on Chinese immigrants.</p>
<p>This raises an interesting question: <strong>Did Judge Crease strike down the act because of genuine anti-racism concerns, or because he saw the Chinese immigrant labor force as a valuable asset for growing the Canadian economy?</strong></p>
</section>
<section id="objective" class="level4">
<h4 class="anchored" data-anchor-id="objective">Objective</h4>
<ul>
<li><p>We aim to explore this question by analyzing the language used by Justice Crease in his legal opinions and writings related to Chinese immigrants through <strong>Natural Language Processing (NLP)</strong> approaches. By examining the text, we hope to uncover insights into his stance.</p></li>
<li><p>The workshop is also to demonstrate how historians can use computational tools to <em>help</em> them answer such a research question, by showing each step in the research process.</p></li>
<li><p>In the end, we will be able to transform the text documents into something more intuitive and visually appealing, such as a 2D UMAP projection of the legal text embeddings by sentences. This can possibly help historians to better interpret the relationship between different texts.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="data/embedding_visualization.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">The 2D UMAP projection of legal text embeddings by sentences</figcaption>
</figure>
</div>
</section>
<section id="the-problem-legal-text-analysis" class="level4">
<h4 class="anchored" data-anchor-id="the-problem-legal-text-analysis">The Problem: Legal Text Analysis</h4>
<p>Legal text analysis is itself a complex task, as legal documents are often lengthy, dense, formal, and filled with specialized terminology. They are also often written in neutral or passive voice, making it difficult to discern the author’s personal opinions or biases, it poses unique challenges for historians and legal scholars alike, which also challenged the usual methods of natural language processing (NLP).</p>
<p>Mining insights from such texts requires sophisticated techniques to extract meaningful information and identify patterns. We need the technique to be able to: * <strong>Understand legal vocabulary</strong>: Legal texts often contain specialized terminology and complex sentence structures, the technique should be able to handle legal jargon and formal language. * <strong>Identify contextual semantics</strong>: Legal texts often involve nuanced meanings and interpretations, so the technique should be able to capture the context and semantics of legal language. * <strong>Handle ambiguity</strong>: Legal texts can be ambiguous, with multiple interpretations possible, the technique should be able to handle ambiguity and provide insights into different interpretations. * <strong>Extract relevant topics</strong>: Legal texts often cover multiple topics and issues, the technique should be able to extract relevant topics and themes from the text. * <strong>Analyze sentiment</strong>: Legal texts can convey different sentiments, such as positive, negative, or neutral, the technique should be able to analyze sentiment and provide insights into the author’s tone and attitude.</p>
</section>
<section id="research-approach" class="level4">
<h4 class="anchored" data-anchor-id="research-approach">Research Approach</h4>
<p>In this workshop, we will explore how to address these challenges using a comparison approach, that is, while we focus on the text of Justice Crease, we will compare it with other legal texts from the same period to gain a better understanding of the language used in legal documents at that time.</p>
<p>The first subject we will use for comparison is the <strong><em>1884 Chinese Regulation Act</em></strong>, which was the law that Crease struck down. The second subject we will use for comparison is <strong>Justice Matthew Baillie Begbie</strong>, who testified alongside Crease in the 1884 Royal Commission on Chinese Immigration.</p>
<ul>
<li>Unlike Crease, historical accounts describe Begbie as protective of marginalized peoples, particularly Indigenous communities and Chinese immigrants.</li>
<li>Similar to what Crease did to the Chinese Regulation Act, Begbie struck down discriminatory municipal by-laws in Victoria that targeted Chinese-owned businesses in the 1888 case of <strong><em>R v. Victoria</em></strong>.</li>
</ul>
<p>We use machine learning techniques, specifically text embeddings, to do the following:</p>
<ol type="1">
<li>Compile <strong>a corpus of legal cases and commission reports</strong> authored by contemporary judges concerning Chinese immigrants.</li>
<li>Apply <strong>Optical Character Recognition (OCR)</strong> to the reports in order to convert them to a machine-readable format.</li>
<li>Examine <strong>keywords</strong> in the texts, to compare the positions of different justices and regulations.</li>
<li>Use <strong>machine learning</strong> to assess the relative emphasis on economic versus social justice concerns.</li>
<li>Use <strong>sentiment analysis</strong> to evaluate the tone of the documents, focusing on whether they reflect positive, negative, or neutral sentiments, and compare the sentiments of writings by different authors to identify patterns.</li>
<li>Use <strong>zero-shot classification</strong> to evaluate whether the documents reflect pro-discrimination, neutral, or anti-discrimination positions.</li>
</ol>
<p>This approach demonstrates different techniques historians can use to identify patterns in documents for analysis.</p>
</section>
<section id="data-collection-and-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="data-collection-and-preprocessing">Data Collection and Preprocessing</h4>
<p>We plan to use 10 digitalized texts, they are:</p>
<ul>
<li>Legal Documents that address Chinese immigration in BC during the period:
<ul>
<li><em>R v. Wing Chong</em></li>
<li><em>Wong Hoy Woon v. Duncan</em></li>
<li><em>R v. Mee Wah, R v. Victoria</em></li>
<li><em>Chinese Regulation Act, 1884</em></li>
</ul></li>
<li>Reports authored by Crease and Begbie for the Royal Commission that show the judges’ personal perspectives.</li>
<li>The remaining documents enrich our corpus for analysis and supplement our study.</li>
</ul>
<p>A big issue with working with historical texts is the format they’re stored in: usually scans of varying quality from physical books, articles, etc. However, these are not machine-readable file formats (e.g., text files), so our first step will be using <strong>Optical Character Recognition (OCR)</strong> to convert the scanned images into machine-readable text. We chose this approach because:</p>
<ol type="1">
<li>It is a common technique for digitizing printed texts that is already widely used in legal case archives such as the CanLii database, and</li>
<li>There are many OCR tools available that vary in cost, effectiveness, and ease of use.</li>
</ol>
<p>Below is a brief overview of early and modern OCR techniques:</p>
<ul>
<li><p><strong>Early OCR (Pattern Matching):</strong></p>
<ul>
<li>Compared each character image to a library of fonts and shapes.</li>
<li>Worked well for clean, printed text.</li>
<li>Struggled with handwriting, unusual fonts, or degraded scans.</li>
</ul></li>
<li><p><strong>Modern OCR (Intelligent Recognition):</strong></p>
<ul>
<li>Uses AI to “read” text more like a human.</li>
<li>Analyzes shapes, context, and layout.</li>
<li>Handles messy, handwritten, or complex documents much better.</li>
</ul></li>
</ul>
<p>After testing several tools, we found that modern, AI-based OCR methods produced the most accurate results for our historical documents.</p>
</section>
<section id="data-overview" class="level4">
<h4 class="anchored" data-anchor-id="data-overview">Data Overview</h4>
<p>After OCR, we obtained a <code>.csv</code> file containing the text and metadata of the documents. Note that we removed the direct quotes of the <em>1884 Chinese Regulation Act</em> in Crease’s ruling, as they don’t reflect his own language. The structure of the data is as follows: | Column Name | Description | | —————————– | ——————————————————– | | filename | Name of the file containing the document text. | | author | Author of the document (e.g., “Crease”, “Begbie”). | | type | Document type (e.g., “case”, “report”). | | text | Full text of the document, which may include OCR errors. | | act_quote_sentences_removed | Number of quoted sentences removed from the full text. |</p>
<p>Here, we read the <code>.csv</code> file into a pandas DataFrame and display.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"data/metadata_cleaned.csv"</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We are also interested in the length of each document, as it can provide insights into the depth and complexity of the text. Therefore, we create a summary below quantifying the number of characters in each document.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Summary the distribution of document lengths</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame to store the document lengths</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>doc_lengths <span class="op">=</span> []</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Measure lengths of each document by number of characters</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> row <span class="kw">in</span> df.iterrows():</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    text_length <span class="op">=</span> <span class="bu">len</span>(row[<span class="dv">1</span>][<span class="st">'text'</span>])</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    doc_lengths.append({<span class="st">'Document'</span>: row[<span class="dv">1</span>][<span class="st">'filename'</span>], <span class="st">'Length'</span>: text_length})</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame and display</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>doc_lengths_df <span class="op">=</span> pd.DataFrame(doc_lengths)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>doc_lengths_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="how-computers-interpret-text" class="level3">
<h3 class="anchored" data-anchor-id="how-computers-interpret-text">How Computers Interpret Text?</h3>
<p>While computers can process text swiftly, they do not “understand” it in the human sense. Instead, they build mathematical models of language from statistical patterns and structural regularities. These models produce symbolic and continuous representations of words and passages that allow downstream algorithms to detect topics, relationships, and affective signals. However, these representations remain proxies for meaning rather than literal comprehension.</p>
<p>This process typically involves a sequence of steps:</p>
<ol type="1">
<li><p><strong>Tokenization</strong>: Breaking text into analyzable units (words, subwords, or sentences).</p></li>
<li><p><strong>Preprocessing</strong>: Cleaning and normalizing text (lowercasing, removing OCR noise, handling archaic spelling).</p></li>
<li><p><strong>Vectorization</strong>: Converting tokens or texts into numerical vectors.</p>
<ul>
<li>Simple count-based approaches (TF-IDF) capture term importance across documents.</li>
<li>Modern contextual methods (BERT, Legal‑BERT) produce dense embeddings that capture usage-dependent semantics.</li>
</ul></li>
<li><p><strong>Modeling and Comparison</strong>: Applying algorithms to those vectors. Examples include cosine similarity for semantic closeness, UMAP for visualization, and zero‑shot classification.</p></li>
<li><p><strong>Aggregation and Interpretation</strong>: Aggregating sentence- or snippet-level outputs to produce document- or author-level summaries (mean stance vectors, topic distributions), followed by careful human interpretation.</p></li>
</ol>
<p>Why this is helpful for social science and humanities research:</p>
<ul>
<li><strong>Scalability</strong>: Enables analysis of large corpora beyond human reading capacity.</li>
<li><strong>Pattern Discovery</strong>: Uncovers latent structures and relationships not easily seen by humans.</li>
<li><strong>Quantification</strong>: Provides numerical measures of abstract concepts (e.g., sentiment, stance).</li>
<li><strong>Reproducibility</strong>: Offers systematic, repeatable methods for text analysis.</li>
</ul>
<section id="count-approach-tf-idf" class="level4">
<h4 class="anchored" data-anchor-id="count-approach-tf-idf">Count Approach: TF-IDF</h4>
<p>The <strong>Term Frequency-Inverse Document Frequency (TF-IDF)</strong> is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents (corpus). It is one of the earliest and most widely used methods for text analysis. It is essentially a count-based approach that quantifies the importance of words in a document based on their frequency and distribution across multiple documents. TF-IDF works by calculating two components: 1. <strong>Term Frequency (TF)</strong>: Measures how frequently a term appears in a document. 2. <strong>Inverse Document Frequency (IDF)</strong>: Measures how important a term is across the entire corpus, by considering how many documents contain the term.</p>
<p>For our purpose, we can use TF-IDF to identify the most important words in each document, which can help us understand the key themes and topics discussed in the text. More details on what we are going to do:</p>
<ol type="1">
<li>Regroup the text data into 5 groups:
<ul>
<li>All writings</li>
<li>Crease’s writings</li>
<li>Begbie’s writings</li>
<li>Chinese Regulation Act</li>
<li>Other documents</li>
</ul></li>
<li>For each group, we will:
<ul>
<li>Create a TF-IDF vectorizer to convert the text into numerical vectors.</li>
<li>Remove common filler words (“the”, “and”, etc.).</li>
<li>Calculate the TF-IDF scores for each word in the documents.</li>
<li>Identify the most important words based on their TF-IDF scores.</li>
</ul></li>
<li>The most frequent remaining words can reveal the main topics of each case.</li>
</ol>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the function to preprocess text in a DataFrame column</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> preprocess_text(text_string):</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Cleans and preprocesses text by:</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    1. Converting to lowercase</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">    2. Removing punctuation and numbers</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    3. Tokenizing</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="co">    4. Removing English stop words </span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    5. Removing words with 4 or fewer characters</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Start with the standard English stop words</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    stop_words <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'english'</span>))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Add custom domain-specific stop words if needed</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    custom_additions <span class="op">=</span> {<span class="st">'would'</span>, <span class="st">'may'</span>, <span class="st">'act'</span>, <span class="st">'mr'</span>, <span class="st">'sir'</span>, <span class="st">'also'</span>, <span class="st">'upon'</span>, <span class="st">'shall'</span>}</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    stop_words.update(custom_additions)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Lowercase and remove non-alphabetic characters</span></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    processed_text <span class="op">=</span> text_string.lower()</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>    processed_text <span class="op">=</span> re.sub(<span class="vs">r'[^a-z\s]'</span>, <span class="st">''</span>, processed_text)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> processed_text.split()</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Filter out stop words AND short words in a single step</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    filtered_tokens <span class="op">=</span> [</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        word <span class="cf">for</span> word <span class="kw">in</span> tokens </span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> word <span class="kw">not</span> <span class="kw">in</span> stop_words <span class="kw">and</span> <span class="bu">len</span>(word) <span class="op">&gt;</span> <span class="dv">4</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Re-join the words into a single string</span></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">" "</span>.join(filtered_tokens)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply the function to create the 'processed_text' column</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'processed_text'</span>] <span class="op">=</span> df[<span class="st">'text'</span>].<span class="bu">apply</span>(preprocess_text)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows of the processed text</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'processed_text'</span>].head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform TF-IDF vectorization on the processed text</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Regrouping the DataFrame for better representation</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'group'</span>] <span class="op">=</span> <span class="st">'Other'</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>df.loc[df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Crease'</span>, <span class="st">'group'</span>] <span class="op">=</span> <span class="st">'Crease'</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>df.loc[df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Begbie'</span>, <span class="st">'group'</span>] <span class="op">=</span> <span class="st">'Begbie'</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>df.loc[df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Others'</span>, <span class="st">'group'</span>] <span class="op">=</span> <span class="st">'Regulation Act'</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the vectorizer and transform the processed text</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="co"># This calculates IDF based on word rarity across ALL individual texts.</span></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> TfidfVectorizer(max_features<span class="op">=</span><span class="dv">1000</span>, ngram_range<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>tfidf_matrix <span class="op">=</span> vectorizer.fit_transform(df[<span class="st">'processed_text'</span>])</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a new DataFrame with the TF-IDF scores</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> vectorizer.get_feature_names_out()</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>tfidf_df <span class="op">=</span> pd.DataFrame(tfidf_matrix.toarray(), columns<span class="op">=</span>feature_names)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the 'group' column to this TF-IDF DataFrame for aggregation</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>tfidf_df[<span class="st">'group'</span>] <span class="op">=</span> df[<span class="st">'group'</span>].values</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Group by author and calculate the MEAN TF-IDF score for each word</span></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>mean_tfidf_by_group <span class="op">=</span> tfidf_df.groupby(<span class="st">'group'</span>).mean()</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate TF-IDF for the combined corpus ("All") using the same vectorizer</span></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>processed_all <span class="op">=</span> <span class="st">" "</span>.join(df[<span class="st">'processed_text'</span>])</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>all_vec <span class="op">=</span> vectorizer.transform([processed_all]).toarray().ravel()</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>all_series <span class="op">=</span> pd.Series(all_vec, index<span class="op">=</span>feature_names, name<span class="op">=</span><span class="st">'All'</span>)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Add the "All" row to the grouped TF-IDF DataFrame</span></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>mean_tfidf_by_group <span class="op">=</span> pd.concat([all_series.to_frame().T, mean_tfidf_by_group], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Collect top words and arrange them into a side-by-side DataFrame</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>list_of_author_dfs <span class="op">=</span> []</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> group_name <span class="kw">in</span> [<span class="st">'All'</span>, <span class="st">'Crease'</span>, <span class="st">'Begbie'</span>, <span class="st">'Regulation Act'</span>, <span class="st">'Other'</span>]:</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> group_name <span class="kw">not</span> <span class="kw">in</span> mean_tfidf_by_group.index:</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If a group is missing, append an empty frame to keep column alignment</span></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>        empty_df <span class="op">=</span> pd.DataFrame({group_name: [], <span class="ss">f'</span><span class="sc">{</span>group_name<span class="sc">}</span><span class="ss">_score'</span>: []})</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>        list_of_author_dfs.append(empty_df)</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get the top 10 terms and scores for the current author/group</span></span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    top_words <span class="op">=</span> mean_tfidf_by_group.loc[group_name].sort_values(ascending<span class="op">=</span><span class="va">False</span>).head(<span class="dv">10</span>)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert the Series to a DataFrame</span></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    top_words_df <span class="op">=</span> top_words.reset_index()</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    top_words_df.columns <span class="op">=</span> [group_name, <span class="ss">f'</span><span class="sc">{</span>group_name<span class="sc">}</span><span class="ss">_score'</span>]</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    list_of_author_dfs.append(top_words_df)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Concatenate the list of DataFrames horizontally</span></span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>final_wide_df <span class="op">=</span> pd.concat(list_of_author_dfs, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the final combined DataFrame (includes "All")</span></span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>final_wide_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Undoubtedly, the TF-IDF practice on our corpus has identified some interesting patterns, such as:</p>
<ul>
<li>The emphasis on “Chinese” in all groups.</li>
<li>The emphasis on “labor” in Crease’s writings.</li>
<li>The emphasis on “license” in Begbie’s writings.</li>
<li>And the emphasis on “dollars” in the Chinese Regulation Act.</li>
<li>Other texts put “Canada” on the top of the list, and “legislation” right after “Chinese”.</li>
</ul>
<p>However, this approach has limitations, as it does not capture the semantic meaning of words or their relationships to each other. For example, it cannot distinguish between “Chinese” as a noun and “Chinese” as an adjective, or between “labor” as a noun and “labor” as a verb. It also does not consider the context in which words are used, which can lead to misinterpretation of their meaning.</p>
</section>
<section id="embedding-approach" class="level4">
<h4 class="anchored" data-anchor-id="embedding-approach">Embedding Approach</h4>
<p>With the advancement of machine learning, <strong>text embeddings</strong> emerged as a more powerful technique for text analysis. It represents words or phrases as dense vectors in a high-dimensional space, capturing semantic relationships between them. This allows for more nuanced understanding of text, enabling tasks like similarity measurement, clustering, and classification.</p>
<p>There are several popular text embedding models, including: - <strong>Word2Vec</strong>: A neural network-based model that learns word embeddings by predicting context words given a target word (or vice versa). - <strong>GloVe</strong>: A global vector representation model that learns word embeddings by factorizing the word co-occurrence matrix. - <strong>FastText</strong>: An extension of Word2Vec that represents words as bags of character n-grams, allowing it to handle out-of-vocabulary words and capture subword information. - <strong>BERT</strong>: A transformer-based model that generates contextualized embeddings by considering the entire sentence context, allowing it to capture word meanings based on their surrounding words.</p>
<p>In this workshop, we will use a BERT-based model to generate text embeddings for our corpus. <a href="https://huggingface.co/nlpaueb/legal-bert-base-uncased">nlpaueb/legal-bert-base-uncased</a> is a BERT model pre-trained on English legal texts, including legislation, law cases, and contracts. It is designed to capture the legal language and semantics, making it suitable for our analysis.</p>
<p>However, we must note that the model is not perfect and may still have limitations in understanding the nuances of legal language, especially in historical texts.</p>
</section>
</section>
<section id="word-embeddings" class="level3">
<h3 class="anchored" data-anchor-id="word-embeddings">Word Embeddings</h3>
<section id="creating-word-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="creating-word-embeddings">Creating Word Embeddings</h4>
<p>While the model itself has the ability to generate word embeddings that capture the semantic meaning of words, we still need to design our own strategy to extract these meanings from our corpus.</p>
<ul>
<li>Load LEGAL-BERT model and tokenizer.</li>
<li>Tokenize sentences into smaller subword units using a tokenizer.</li>
<li>Process each tokenized sentence through the model to extract hidden layer representations.</li>
<li>Combine subword embeddings to form a single vector for each word by averaging the embeddings of its subword components.</li>
<li>Aggregate embeddings for repeated words across sentences by averaging their vectors.</li>
<li>Return a dictionary mapping each word to its mean embedding, capturing its semantic meaning in the context of the text.</li>
</ul>
<p>In this way, we are not only able to generate word embeddings with contextual meanings over the whole corpus, but also be able to aggregate our corpus into different groups, and generate contextualized word embeddings for each group.</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We will use the Legal-BERT model for this task</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">'nlpaueb/legal-bert-base-uncased'</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">'nlpaueb/legal-bert-base-uncased'</span>).<span class="bu">eval</span>() <span class="co"># set the model to evaluation mode</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to embed words using the tokenizer and model</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> embed_words(sentences, tokenizer<span class="op">=</span>tokenizer, model<span class="op">=</span>model, target_words<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>                device<span class="op">=</span><span class="va">None</span>, max_length<span class="op">=</span><span class="dv">512</span>):</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns a dictionary {word: mean_embedding}.</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Only the mean embedding (float32 numpy array) per word is kept.</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> device <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> <span class="bu">next</span>(model.parameters()).device</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    device <span class="op">=</span> torch.device(device)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    model.to(device).<span class="bu">eval</span>()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    target_set <span class="op">=</span> <span class="va">None</span> <span class="cf">if</span> target_words <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> <span class="bu">set</span>(target_words)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    sums <span class="op">=</span> {}   <span class="co"># word -&gt; torch.Tensor sum of embeddings</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    counts <span class="op">=</span> {} <span class="co"># word -&gt; occurrence count</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sent <span class="kw">in</span> sentences:</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>            enc <span class="op">=</span> tokenizer(</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>                sent,</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>                return_tensors<span class="op">=</span><span class="st">"pt"</span>,</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>                truncation<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>                max_length<span class="op">=</span>max_length</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>            enc <span class="op">=</span> {k: v.to(device) <span class="cf">for</span> k, v <span class="kw">in</span> enc.items()}</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(<span class="op">**</span>enc)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>            hidden <span class="op">=</span> outputs.last_hidden_state.squeeze(<span class="dv">0</span>)  <span class="co"># (seq_len, hidden)</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>            tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(enc[<span class="st">"input_ids"</span>][<span class="dv">0</span>])</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>            i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">while</span> i <span class="op">&lt;</span> <span class="bu">len</span>(tokens):</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>                tok <span class="op">=</span> tokens[i]</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> tok <span class="kw">in</span> (<span class="st">"[CLS]"</span>, <span class="st">"[SEP]"</span>, <span class="st">"[PAD]"</span>):</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>                    i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>                <span class="co"># Gather wordpieces</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>                j <span class="op">=</span> i <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>                piece_embs <span class="op">=</span> [hidden[i]]</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>                word <span class="op">=</span> tok[<span class="dv">2</span>:] <span class="cf">if</span> tok.startswith(<span class="st">"##"</span>) <span class="cf">else</span> tok</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>                <span class="cf">while</span> j <span class="op">&lt;</span> <span class="bu">len</span>(tokens) <span class="kw">and</span> tokens[j].startswith(<span class="st">"##"</span>):</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>                    piece_embs.append(hidden[j])</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>                    word <span class="op">+=</span> tokens[j][<span class="dv">2</span>:]</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>                    j <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> target_set <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="kw">and</span> word <span class="kw">not</span> <span class="kw">in</span> target_set:</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>                    i <span class="op">=</span> j</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">continue</span></span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>                word_emb <span class="op">=</span> torch.stack(piece_embs, dim<span class="op">=</span><span class="dv">0</span>).mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> word <span class="kw">in</span> sums:</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>                    sums[word] <span class="op">+=</span> word_emb</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>                    counts[word] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a>                <span class="cf">else</span>:</span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>                    sums[word] <span class="op">=</span> word_emb.clone()</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>                    counts[word] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>                i <span class="op">=</span> j</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> {w: (sums[w] <span class="op">/</span> counts[w]).cpu().numpy() <span class="cf">for</span> w <span class="kw">in</span> sums}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to clean and preprocess text</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(text):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.lower()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'[^\w\s]'</span>, <span class="st">''</span>, text)  <span class="co"># Remove punctuation</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text.strip()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_sm"</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Group texts to form a single text per group</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>grouped_texts <span class="op">=</span> df.groupby(<span class="st">'group'</span>)[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="st">' '</span>.join(x)).reset_index()</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a row for the combined text of all groups</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>grouped_texts <span class="op">=</span> pd.concat(</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    [grouped_texts, pd.DataFrame([{<span class="st">'group'</span>: <span class="st">'All'</span>, <span class="st">'text'</span>: <span class="st">' '</span>.join(df[<span class="st">'text'</span>])}])],</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    ignore_index<span class="op">=</span><span class="va">True</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Create new columns for word and sentence tokens</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>grouped_texts[<span class="st">'word_tokens'</span>] <span class="op">=</span> grouped_texts[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: word_tokenize(clean_text(x)))</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Sentence tokenization using spaCy</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>grouped_texts[<span class="st">'sentence_tokens'</span>] <span class="op">=</span> grouped_texts[<span class="st">'text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: [sent.text <span class="cf">for</span> sent <span class="kw">in</span> nlp(x).sents])</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Apply clean_text to the sentence tokens</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>grouped_texts[<span class="st">'sentence_tokens'</span>] <span class="op">=</span> grouped_texts[<span class="st">'sentence_tokens'</span>].<span class="bu">apply</span>(</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> x: [clean_text(sent) <span class="cf">for</span> sent <span class="kw">in</span> x]</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Embed the words in each group</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>grouped_texts[<span class="st">'word_embeddings'</span>] <span class="op">=</span> grouped_texts[<span class="st">'sentence_tokens'</span>].<span class="bu">apply</span>(</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">lambda</span> x: embed_words(x)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the number of unique words in each group</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>grouped_texts[<span class="st">'num_unique_words'</span>] <span class="op">=</span> grouped_texts[<span class="st">'word_tokens'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: <span class="bu">len</span>(<span class="bu">set</span>(x)))</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>grouped_texts.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We created word embeddings of all tokens in each group, respectively. The word embeddings are stored in a dictionary format, where each key is a word and the value is its corresponding embedding vector.</p>
<p>It is clear that the word embeddings of the same word in different groups are different, which reflects the contextualized meaning of the word in each group.</p>
<ul>
<li>For example, the word “Chinese” has a different embedding in Crease’s writings compared to Begbie’s writings, indicating that the two authors used the word in different contexts and with different connotations.</li>
<li>However, since they were embedded using the same model, the word embeddings of the same word in different groups are still similar, which reflects the shared meaning of the word across different contexts.</li>
<li>The dimensionality of all word embeddings is 768, which is the size of the hidden layer in the LEGAL-BERT model we used.</li>
</ul>
<div class="cell" data-execution_count="11">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the word embedding of Chinese for the whole corpus</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>chinese_embedding <span class="op">=</span> grouped_texts[grouped_texts[<span class="st">'group'</span>] <span class="op">==</span> <span class="st">'All'</span>][<span class="st">'word_embeddings'</span>].values[<span class="dv">0</span>].get(<span class="st">'chinese'</span>)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first 20 dimensions for brevity</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 20 Dimensions of Word Embedding for 'Chinese' in the Full Corpus:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>chinese_embedding[:<span class="dv">20</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total Dimensions of Word Embedding for 'Chinese': </span><span class="sc">{</span><span class="bu">len</span>(chinese_embedding)<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the word embedding of Chinese in Crease's text</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>crease_embeddings <span class="op">=</span> grouped_texts[grouped_texts[<span class="st">'group'</span>] <span class="op">==</span> <span class="st">'Crease'</span>][<span class="st">'word_embeddings'</span>].values[<span class="dv">0</span>]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first 20 dimensions for brevity</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 20 Dimensions of Word Embeddings for 'Chinese' in Crease's Text:</span><span class="ch">\n</span><span class="sc">{</span>crease_embeddings<span class="sc">.</span>get(<span class="st">'chinese'</span>)[:<span class="dv">20</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>) </span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total Dimensions of Word Embeddings for 'Chinese' in Crease's Text: </span><span class="sc">{</span><span class="bu">len</span>(crease_embeddings.get(<span class="st">'chinese'</span>))<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>begbie_embeddings <span class="op">=</span> grouped_texts[grouped_texts[<span class="st">'group'</span>] <span class="op">==</span> <span class="st">'Begbie'</span>][<span class="st">'word_embeddings'</span>].values[<span class="dv">0</span>]</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Display first 20 dimensions for brevity</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 20 Dimensions of Word Embeddings for 'Chinese' in Begbie's Text:</span><span class="ch">\n</span><span class="sc">{</span>begbie_embeddings<span class="sc">.</span>get(<span class="st">'chinese'</span>)[:<span class="dv">20</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Total Dimensions of Word Embeddings for 'Chinese' in Begbie's Text: </span><span class="sc">{</span><span class="bu">len</span>(begbie_embeddings.get(<span class="st">'chinese'</span>))<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="measurement-of-similarity" class="level4">
<h4 class="anchored" data-anchor-id="measurement-of-similarity">Measurement of Similarity</h4>
<p>Another important aspect of word embeddings is the ability to measure the similarity between words based on their embeddings. This can be done using cosine similarity, which calculates the cosine of the angle between two vectors in the embedding space. The cosine similarity ranges from 0 to 1, where:</p>
<ul>
<li>0 indicates no similarity (orthogonal vectors)</li>
<li>1 indicates perfect similarity (identical vectors)</li>
<li>The closer the cosine similarity is to 1, the more similar the words are in meaning.</li>
</ul>
<p>This allows us to identify related words and concepts based on their embeddings, enabling us to explore the semantic relationships between words in our corpus. And more importantly, it doesn’t only allows us to measure the similarity between words, but also allows us to measure the similarity between sentences, paragraphs, and even entire documents, as long as they are represented as vectors in the same embedding space.</p>
<p>The math behind cosine similarity is as follows: <span class="math display">\[
\text{cosine similarity}(a, b) = \frac{a \cdot b}{||a|| \cdot ||b||}
\]</span> Where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are the embedding vectors of the two words, and <span class="math inline">\(||a||\)</span> and <span class="math inline">\(||b||\)</span> are their Pythagorean norms (lengths).</p>
<p>Focusing on the word “Chinese”, we can calculate its cosine similarity with other words in the same group to identify related terms. This can help us understand how the word is used in different contexts and how it relates to other concepts. Here, we will list out the top 10 most similar words to “Chinese” in each group, along with their cosine similarity scores.</p>
<p><strong>Note</strong>: All words are put into lowercase.</p>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute top-10 most similar words to target for EVERY group (including "All")</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> <span class="st">"chinese"</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>all_results <span class="op">=</span> []</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through each group and compute similarities</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, grp_row <span class="kw">in</span> grouped_texts.iterrows():</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    group <span class="op">=</span> grp_row[<span class="st">'group'</span>]</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    emb_dict <span class="op">=</span> grp_row[<span class="st">'word_embeddings'</span>]</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> target <span class="kw">not</span> <span class="kw">in</span> emb_dict:</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    target_vec <span class="op">=</span> emb_dict[target]</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    sims <span class="op">=</span> []</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w, vec <span class="kw">in</span> emb_dict.items():</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> w <span class="op">==</span> target:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            sim <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> cosine(target_vec, vec)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        sims.append((w, sim))</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>    sims_sorted <span class="op">=</span> <span class="bu">sorted</span>(sims, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)[:top_n]</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> rank, (w, sim) <span class="kw">in</span> <span class="bu">enumerate</span>(sims_sorted, <span class="dv">1</span>):</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>        all_results.append({<span class="st">'group'</span>: group, <span class="st">'rank'</span>: rank, <span class="st">'word'</span>: w, <span class="st">'similarity'</span>: sim})  <span class="co"># Use :4f for better readability</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>similar_words_df <span class="op">=</span> pd.DataFrame(all_results)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the first few rows of the DataFrame with similar words</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>sims_wide <span class="op">=</span> similar_words_df.pivot(index<span class="op">=</span><span class="st">'rank'</span>, columns<span class="op">=</span><span class="st">'group'</span>, values<span class="op">=</span><span class="st">'similarity'</span>)</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>words_wide <span class="op">=</span> similar_words_df.pivot(index<span class="op">=</span><span class="st">'rank'</span>, columns<span class="op">=</span><span class="st">'group'</span>, values<span class="op">=</span><span class="st">'word'</span>)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine with a tidy multi-level column index: </span></span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>wide_combined <span class="op">=</span> pd.concat({<span class="st">'word'</span>: words_wide, <span class="st">'similarity'</span>: sims_wide}, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a>wide_combined <span class="op">=</span> (</span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a>    wide_combined.swaplevel(<span class="dv">0</span>,<span class="dv">1</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a>                 .sort_index(axis<span class="op">=</span><span class="dv">1</span>, level<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a>wide_combined  <span class="co"># Display</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="embedding-driven-text-analysis" class="level3">
<h3 class="anchored" data-anchor-id="embedding-driven-text-analysis">Embedding Driven Text Analysis</h3>
<section id="creating-keyword-focused-stance-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="creating-keyword-focused-stance-embeddings">Creating Keyword-Focused Stance Embeddings</h4>
<p>In comparison to generating word embeddings, modeling stance of each text is more challenging, as it requires us to capture the author’s position on a specific issue or topic. Oftentimes, the stance is not explicitly stated in the text, but rather implied through the language used.</p>
<p>There is not a universal optimum for stance modeling, as it depends on the specific context and the author’s perspective. However, we can use a combination of techniques to create focused embeddings that capture the stance of each text. The strategy we used is as follows:</p>
<ol type="1">
<li>Tokenize the text into smaller units and identify the positions of specific keywords or phrases that are relevant to the stance being analyzed.</li>
<li>For each occurrence of the keywords, extract a surrounding “window” of text to capture the context in which the keywords are used.</li>
<li>Represent the text in the window as numerical vectors using a pre-trained language model, which encodes the meaning of the words and their relationships.</li>
<li>Combine the vectors within each window using a pooling method (e.g., averaging or selecting the maximum value) to create a single representation for the context around the keyword.</li>
<li>If multiple occurrences of the keywords are found, average their representations to create a unified vector that captures the overall stance in the text.</li>
<li>If no keywords are found, use a fallback representation based on the overall text.</li>
</ol>
<p>This approach thus allows us to create focused embeddings that capture the stance of each text focusing on specific keywords or phrases. The sentence is used as the basic unit of analysis here, but larger chunks of text can also be used if needed.</p>
<p>In the end, we will store the lists of embeddings in a dictionary format, where each key is the author and the value is a list of embeddings for each text authored by that author.</p>
<div class="cell" data-execution_count="15">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> embed_text(</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    text,</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    focus_token<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    window<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    pooling<span class="op">=</span><span class="st">"mean"</span>,  <span class="co"># "mean" (default), "max", or "min"</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model):</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get stopwords for filtering</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    stop_words <span class="op">=</span> <span class="bu">set</span>(stopwords.words(<span class="st">'english'</span>))</span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run the model once</span></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>    hidden <span class="op">=</span> outputs.last_hidden_state.squeeze(<span class="dv">0</span>) </span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> focus_token <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> hidden[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize to list</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a>    keywords <span class="op">=</span> (</span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>        [focus_token] <span class="cf">if</span> <span class="bu">isinstance</span>(focus_token, <span class="bu">str</span>)</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span> focus_token</span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pre-tokenize each keyword to its subtoken ids</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a>    kw_token_ids <span class="op">=</span> {</span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a>        kw: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(kw))</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> kw <span class="kw">in</span> keywords</span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> inputs[<span class="st">"input_ids"</span>].squeeze(<span class="dv">0</span>).tolist()</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> tokenizer.convert_ids_to_tokens(input_ids)</span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a>    spans <span class="op">=</span> []  <span class="co"># list of (start, end) index pairs</span></span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a>    <span class="co"># find every match of every keyword</span></span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> kw, sub_ids <span class="kw">in</span> kw_token_ids.items():</span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>        L <span class="op">=</span> <span class="bu">len</span>(sub_ids)</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(input_ids) <span class="op">-</span> L <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> input_ids[i:i<span class="op">+</span>L] <span class="op">==</span> sub_ids:</span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a>                spans.append((i, i<span class="op">+</span>L))</span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> spans:</span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fallback on CLS vector</span></span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> hidden[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For each span, grab the window around it</span></span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>    vecs <span class="op">=</span> []</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (start, end) <span class="kw">in</span> spans:</span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>        lo <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, start <span class="op">-</span> window)</span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a>        hi <span class="op">=</span> <span class="bu">min</span>(hidden.size(<span class="dv">0</span>), end <span class="op">+</span> window)</span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Filter out stopwords from the window</span></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a>        non_stop_indices <span class="op">=</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(lo, hi) </span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a>                           <span class="cf">if</span> tokens[i] <span class="kw">not</span> <span class="kw">in</span> stop_words <span class="kw">and</span> <span class="kw">not</span> tokens[i].startswith(<span class="st">'##'</span>)]</span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If all tokens are stopwords, use the original window</span></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> non_stop_indices:</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a>            span_vec <span class="op">=</span> hidden[lo:hi]</span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>            span_vec <span class="op">=</span> hidden[non_stop_indices]</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> pooling <span class="op">==</span> <span class="st">"mean"</span>:</span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>            pooled <span class="op">=</span> span_vec.mean(dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> pooling <span class="op">==</span> <span class="st">"max"</span>:</span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>            pooled <span class="op">=</span> span_vec.<span class="bu">max</span>(dim<span class="op">=</span><span class="dv">0</span>).values</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> pooling <span class="op">==</span> <span class="st">"min"</span>:</span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a>            pooled <span class="op">=</span> span_vec.<span class="bu">min</span>(dim<span class="op">=</span><span class="dv">0</span>).values</span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f"Unknown pooling method: </span><span class="sc">{</span>pooling<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>        vecs.append(pooled.cpu().numpy())</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average across all spans</span></span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(np.stack(vecs, axis<span class="op">=</span><span class="dv">0</span>), axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="16">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>crease_cases <span class="op">=</span> df[(df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Crease'</span>) <span class="op">&amp;</span> (df[<span class="st">'type'</span>] <span class="op">==</span> <span class="st">'case'</span>)][<span class="st">'text'</span>].tolist()</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>begbie_cases <span class="op">=</span> df[(df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Begbie'</span>) <span class="op">&amp;</span> (df[<span class="st">'type'</span>] <span class="op">==</span> <span class="st">'case'</span>)][<span class="st">'text'</span>].tolist()</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>act_1884 <span class="op">=</span> df[df[<span class="st">'type'</span>] <span class="op">==</span> <span class="st">'act'</span>][<span class="st">'text'</span>].tolist()</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>act_dict <span class="op">=</span> {</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Crease'</span>: crease_cases,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Begbie'</span>: begbie_cases,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Act 1884'</span>: act_1884}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>act_snippets <span class="op">=</span> {}</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>keywords <span class="op">=</span> [<span class="st">"Chinese"</span>, <span class="st">"China"</span>, <span class="st">"Chinaman"</span>, <span class="st">"Chinamen"</span>, </span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>            <span class="st">"immigrant"</span>, <span class="st">"immigrants"</span>, <span class="st">"alien"</span>, <span class="st">"aliens"</span>, </span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>            <span class="st">"immigration"</span>]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, texts <span class="kw">in</span> act_dict.items():</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    snippets <span class="op">=</span> []</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> txt <span class="kw">in</span> texts:</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Sentence tokenize using Spacy</span></span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>        sentence <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> nlp(txt).sents]</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sent <span class="kw">in</span> sentence:</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">any</span>(keyword <span class="kw">in</span> sent <span class="cf">for</span> keyword <span class="kw">in</span> keywords):</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>                snippets.append(sent)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    act_snippets[auth] <span class="op">=</span> snippets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="18">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Investigate the length of the snippets</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>n_snippet <span class="op">=</span> {auth: <span class="bu">len</span>(snippets) <span class="cf">for</span> auth, snippets <span class="kw">in</span> act_snippets.items()}</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Snippet size by author:"</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, num <span class="kw">in</span> n_snippet.items():</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>auth<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>num<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="19">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create embeddings</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>embeddings_dict <span class="op">=</span> {<span class="st">'Crease'</span>: [], <span class="st">'Begbie'</span>: [], <span class="st">'Act 1884'</span>: []}</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> act_snippets.items():</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snip <span class="kw">in</span> snippets:</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> embed_text(snip, focus_token<span class="op">=</span>keywords, window<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        embeddings_dict[auth].append(v) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="measuring-stance-similarity" class="level4">
<h4 class="anchored" data-anchor-id="measuring-stance-similarity">Measuring Stance Similarity</h4>
<p>Just like word embeddings, cosine similarity can also be used to measure the stance similarity between texts. The interpretation of cosine similarity in this context is similar to that of word embeddings, where a higher cosine similarity indicates a stronger alignment in stance between two texts.</p>
<p>With sentence being the basic unit of analysis, we can calculate the overall cosine similarity between each pair of authors’ texts in various ways, but here we will focus on two of them: 1. <strong>Mean Embeddings</strong>: We calculate the mean embedding for each author’s texts and then compute the cosine similarity between these mean embeddings. This gives us a single similarity score for each pair of authors, reflecting their overall stance alignment. 2. <strong>Pairwise Embeddings</strong>: We calculate the cosine similarity between each pair of texts authored by different authors, then average the scores to get a more comprehensive view of stance alignment across all texts.</p>
<p>Note that similarity scores are not deterministic, as they depend on the specific texts and the context in which the keywords are used. However, they can provide valuable insights into the stance of each author and how it relates to other authors’ positions. This reinforces the idea that <strong>stance is not a fixed attribute</strong>, but rather a dynamic and context-dependent aspect of language.</p>
<div class="cell" data-execution_count="20">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pairwise cosine similarity</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>mean_crease <span class="op">=</span> np.mean(embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>mean_begbie <span class="op">=</span> np.mean(embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>mean_act_1884 <span class="op">=</span> np.mean(embeddings_dict[<span class="st">"Act 1884"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie <span class="op">=</span> cosine_similarity(mean_crease, mean_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>sim_crease_act_1884 <span class="op">=</span> cosine_similarity(mean_crease, mean_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>sim_begbie_act_1884 <span class="op">=</span> cosine_similarity(mean_begbie, mean_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between mean Crease and mean Begbie: </span><span class="sc">{</span>sim_crease_begbie<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between mean Crease and mean Act 1884: </span><span class="sc">{</span>sim_crease_act_1884<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between mean Begbie and mean Act 1884: </span><span class="sc">{</span>sim_begbie_act_1884<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract embeddings for Crease, Begbie and the Act 1884</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>crease_embeddings <span class="op">=</span> embeddings_dict[<span class="st">"Crease"</span>]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>begbie_embeddings <span class="op">=</span> embeddings_dict[<span class="st">"Begbie"</span>]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>act_1884_embeddings <span class="op">=</span> embeddings_dict[<span class="st">"Act 1884"</span>]</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to compute mean cosine similarity</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mean_cosine_similarity(embeddings1, embeddings2):</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    similarities <span class="op">=</span> [</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="dv">1</span> <span class="op">-</span> cosine(e1, e2)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> e1 <span class="kw">in</span> embeddings1</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> e2 <span class="kw">in</span> embeddings2</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(similarities) <span class="op">/</span> <span class="bu">len</span>(similarities)</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Extract embeddings</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>crease_emb <span class="op">=</span> embeddings_dict[<span class="st">"Crease"</span>]</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>begbie_emb <span class="op">=</span> embeddings_dict[<span class="st">"Begbie"</span>]</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>act_1884_emb <span class="op">=</span> embeddings_dict[<span class="st">"Act 1884"</span>]</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute mean similarities</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>crease_begbie_sim <span class="op">=</span> mean_cosine_similarity(crease_emb, begbie_emb)</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>crease_act_sim <span class="op">=</span> mean_cosine_similarity(crease_emb, act_1884_emb)</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>begbie_act_sim <span class="op">=</span> mean_cosine_similarity(begbie_emb, act_1884_emb)</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Output</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean cosine similarity between Crease and Begbie embeddings: </span><span class="sc">{</span>crease_begbie_sim<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean cosine similarity between Crease and Act 1884 embeddings: </span><span class="sc">{</span>crease_act_sim<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Mean cosine similarity between Begbie and Act 1884 embeddings: </span><span class="sc">{</span>begbie_act_sim<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="visualizing-text-embeddings" class="level4">
<h4 class="anchored" data-anchor-id="visualizing-text-embeddings">Visualizing Text Embeddings</h4>
<p>While the embeddings themselves are high-dimensional vectors (in our case, 768-dimensional), we can visualize them in a lower-dimensional space (e.g., 2D or 3D) using <strong>dimensionality reduction</strong> techniques such as <strong>UMAP</strong> (Uniform Manifold Approximation and Projection).</p>
<p><strong>UMAP</strong> is a dimensionality reduction technique that projects high-dimensional embeddings into a 2D space while preserving local structure, making it ideal for visualizing our embeddings.</p>
<p>Using <strong>Plotly Express</strong>, we create an interactive scatter plot where each point represents a text snippet, colored by author, with hover functionality to display the corresponding sentence. This visualization highlights clusters and relationships between snippets, offering insights into semantic similarities across authors.</p>
<div class="cell" data-execution_count="22">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set seed for umap reproducibility</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>all_vecs <span class="op">=</span> np.vstack(embeddings_dict[<span class="st">"Crease"</span>] <span class="op">+</span> embeddings_dict[<span class="st">"Begbie"</span>] <span class="op">+</span> embeddings_dict[<span class="st">"Act 1884"</span>])</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>labels  <span class="op">=</span> ([<span class="st">"Crease"</span>] <span class="op">*</span> <span class="bu">len</span>(embeddings_dict[<span class="st">"Crease"</span>])) <span class="op">+</span> ([<span class="st">"Begbie"</span>] <span class="op">*</span> <span class="bu">len</span>(embeddings_dict[<span class="st">"Begbie"</span>])) <span class="op">+</span> ([<span class="st">'Act 1884'</span>] <span class="op">*</span> <span class="bu">len</span>(embeddings_dict[<span class="st">"Act 1884"</span>]))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> umap.UMAP(n_neighbors<span class="op">=</span><span class="dv">15</span>, min_dist<span class="op">=</span><span class="fl">0.1</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>proj <span class="op">=</span> reducer.fit_transform(all_vecs) </span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wrap_text(text, width<span class="op">=</span><span class="dv">60</span>):</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">'&lt;br&gt;'</span>.join(textwrap.wrap(text, width<span class="op">=</span>width))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>umap_df <span class="op">=</span> pd.DataFrame(proj, columns<span class="op">=</span>[<span class="st">'UMAP 1'</span>, <span class="st">'UMAP 2'</span>])</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Author'</span>] <span class="op">=</span> labels</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Text'</span>] <span class="op">=</span> [snip <span class="cf">for</span> auth <span class="kw">in</span> act_snippets <span class="cf">for</span> snip <span class="kw">in</span> act_snippets[auth]]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Text'</span>] <span class="op">=</span> umap_df[<span class="st">'Text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> t: wrap_text(t, width<span class="op">=</span><span class="dv">60</span>))</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(umap_df, x<span class="op">=</span><span class="st">'UMAP 1'</span>, y<span class="op">=</span><span class="st">'UMAP 2'</span>, </span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'Author'</span>, hover_data<span class="op">=</span>[<span class="st">'Text'</span>], </span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>                 width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">500</span> )</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>fig.update_traces(marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">5</span>))</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>fig.update_layout(title<span class="op">=</span><span class="st">'UMAP Projection of Stance Embeddings by Author'</span>)</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="investigating-texts" class="level4">
<h4 class="anchored" data-anchor-id="investigating-texts">Investigating Texts</h4>
<p>The stance embeddings ultimately serve as analytical tools to support our text analysis objectives.</p>
<ul>
<li>By calculating the “conceptual mean stance” for each author, we gain a quantitative basis for comparing the positions of different authors.</li>
<li>However, embeddings alone cannot fully capture the nuances of language or the complexity of an author’s stance. To truly understand the perspectives reflected in the texts, it is essential to investigate the sentences that are most similar to the conceptual average position of each author.</li>
</ul>
<p>Here, we will examine the top 10 sentences with the highest stance similarity to the mean stance of each author.</p>
<p>This approach allows us to delve deeper into the texts, uncovering how the language used aligns with the calculated average stance and providing richer insights into the authors’ positions on the issue of Chinese immigrants.</p>
<div class="cell" data-execution_count="24">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the 10 most similar embedding sentences to Crease's mean embedding</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>crease_similarity_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'Author'</span>, <span class="st">'Text'</span>, <span class="st">'Similarity Score'</span>])</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through the embeddings and their corresponding sentences</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> act_snippets.items():</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snippet, emb <span class="kw">in</span> <span class="bu">zip</span>(snippets, embeddings_dict[auth]):</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> cosine_similarity(emb.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), mean_crease)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>        crease_similarity_df.loc[<span class="bu">len</span>(crease_similarity_df)] <span class="op">=</span> [auth, snippet, similarity]</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by similarity score</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>crease_sorted_similarity <span class="op">=</span> crease_similarity_df.sort_values(by<span class="op">=</span><span class="st">'Similarity Score'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 10 most similar sentences to Crease's mean embedding:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> crease_sorted_similarity.head(<span class="dv">10</span>).iterrows():</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Sentence: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity Score: </span><span class="sc">{</span>row[<span class="st">'Similarity Score'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="25">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the 10 most similar embedding sentences to Begbie's mean embedding</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>begbie_similarity_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'Author'</span>, <span class="st">'Text'</span>, <span class="st">'Similarity Score'</span>])</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through the embeddings and their corresponding sentences</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> act_snippets.items():</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snippet, emb <span class="kw">in</span> <span class="bu">zip</span>(snippets, embeddings_dict[auth]):</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> cosine_similarity(emb.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), mean_begbie)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        begbie_similarity_df.loc[<span class="bu">len</span>(begbie_similarity_df)] <span class="op">=</span> [auth, snippet, similarity]</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by similarity score</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>begbie_sorted_similarity <span class="op">=</span> begbie_similarity_df.sort_values(by<span class="op">=</span><span class="st">'Similarity Score'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 10 most similar sentences to Begbie's mean embedding:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> begbie_sorted_similarity.head(<span class="dv">10</span>).iterrows():</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Sentence: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity Score: </span><span class="sc">{</span>row[<span class="st">'Similarity Score'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="26">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the 10 most similar embedding sentences to the Regulation Act's mean embedding</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>regulation_similarity_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'Author'</span>, <span class="st">'Text'</span>, <span class="st">'Similarity Score'</span>])</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through the embeddings and their corresponding sentences</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> act_snippets.items():</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snippet, emb <span class="kw">in</span> <span class="bu">zip</span>(snippets, embeddings_dict[auth]):</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> cosine_similarity(emb.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), mean_act_1884)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        regulation_similarity_df.loc[<span class="bu">len</span>(regulation_similarity_df)] <span class="op">=</span> [auth, snippet, similarity]</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by similarity score</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>regulation_sorted_similarity <span class="op">=</span> regulation_similarity_df.sort_values(by<span class="op">=</span><span class="st">'Similarity Score'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 10 most similar sentences to the Regulation Act's mean embedding:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> regulation_sorted_similarity.head(<span class="dv">10</span>).iterrows():</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Sentence: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity Score: </span><span class="sc">{</span>row[<span class="st">'Similarity Score'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="topic-modeling-and-alignment-analysis" class="level3">
<h3 class="anchored" data-anchor-id="topic-modeling-and-alignment-analysis">Topic Modeling and Alignment Analysis</h3>
<div class="cell" data-execution_count="27">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>final_wide_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In the TF-IDF analysis, we observed that the word “Chinese” was a prominent term across all groups, indicating its centrality to the discussions in all texts. Following the word “Chinese”, we also noticed that “labor”, “white”, “legislation”, and “taxation” were all significant terms reflected in all texts. This make us wonder: <strong>How do these topics relate to each other, and how do they align with the stances of different authors?</strong></p>
<p>In natural language processing, <strong>topic modeling</strong> is a technique used to identify and extract topics from a collection of documents. It involves analyzing the words and phrases in the text to identify patterns and themes that can be grouped together into topics. Oftentimes, topic modeling are performed using unsupervised learning algorithms such as Latent Dirichlet Allocation (LDA). However, these methods may not be suitable for our corpus, as they require a large amount of text data and may not capture the nuances of legal language.</p>
<p>Therefore, we will use a different approach to explore the topics in our corpus, by leveraging the word embeddings we have already generated. The strategy we will use is as follows:</p>
<ol type="1">
<li><strong>Identify Key Terms</strong>: We will focus on the key terms identified in the TF-IDF analysis, such as “Chinese”, “labor”, “white”, “legislation”, and “taxation”. These terms will serve as anchors for our topic analysis.</li>
<li><strong>Calculate Cosine Similarity</strong>: For each key term, we will calculate its cosine similarity with other words in the same group to identify related terms. This will help us understand how the key terms are used in different contexts and how they relate to other concepts.</li>
<li><strong>Aggregate Related Terms</strong>: We will aggregate the related terms for each key term to form a topic. This will allow us to identify the main topics discussed in each group and how they align with the stances of different authors.</li>
<li><strong>Analyze Topic Alignment</strong>: We will analyze the alignment of the identified topics with the stances of different authors. This will help us understand how the topics reflect the authors’ positions on the issue of Chinese immigrants.</li>
</ol>
<div class="cell" data-execution_count="28">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define our target "topics"</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>target_words <span class="op">=</span> [<span class="st">"labor"</span>, <span class="st">"legislation"</span>, <span class="st">"license"</span>, <span class="st">"taxation"</span>]</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Find most similar words to the keywords using the "All" group embeddings</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>all_emb <span class="op">=</span> grouped_texts.loc[grouped_texts[<span class="st">'group'</span>] <span class="op">==</span> <span class="st">'All'</span>, <span class="st">'word_embeddings'</span>].values</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">len</span>(all_emb) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"No 'All' group found in grouped_texts"</span>)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>all_emb <span class="op">=</span> all_emb[<span class="dv">0</span>]</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>top_n <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {}</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> target <span class="kw">in</span> target_words:</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>    target_vec <span class="op">=</span> all_emb.get(target)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> target_vec <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fill with NaN if target missing</span></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>        results[target] <span class="op">=</span> [np.nan] <span class="op">*</span> top_n</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">continue</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    sims <span class="op">=</span> []</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w, vec <span class="kw">in</span> all_emb.items():</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> w <span class="op">==</span> target:</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">try</span>:</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>            sim <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> cosine(target_vec, vec)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>        sims.append((w, sim))</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>    sims_sorted <span class="op">=</span> <span class="bu">sorted</span>(sims, key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)[:top_n]</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>    results[target] <span class="op">=</span> [w <span class="cf">for</span> w, _ <span class="kw">in</span> sims_sorted]</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Create DataFrame with targets as columns and ranks as rows</span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>similar_words_df <span class="op">=</span> pd.DataFrame(results)</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>similar_words_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="29">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create anchors for the topics</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_anchor(topic, similar_df<span class="op">=</span>similar_words_df, top_n<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> topic</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># collect words: topic + top_n similar words </span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    similar_words <span class="op">=</span> similar_df[t].astype(<span class="bu">str</span>).tolist()[:top_n]</span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    words <span class="op">=</span> [t] <span class="op">+</span> [w.lower() <span class="cf">for</span> w <span class="kw">in</span> similar_words]</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># deduplicate while preserving order</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    seen <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    uniq_words <span class="op">=</span> []</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w <span class="kw">in</span> words:</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> w <span class="kw">not</span> <span class="kw">in</span> seen:</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>            seen.add(w)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>            uniq_words.append(w)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># embed each word and average</span></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>    vecs <span class="op">=</span> []</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> w <span class="kw">in</span> uniq_words:</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>        emb <span class="op">=</span> embed_text(w)  </span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        vecs.append(emb)</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(np.stack(vecs, axis<span class="op">=</span><span class="dv">0</span>), axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create anchors for the topics</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>labor_anchor <span class="op">=</span> create_anchor(<span class="st">"labor"</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>legislation_anchor <span class="op">=</span> create_anchor(<span class="st">"legislation"</span>)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>license_anchor <span class="op">=</span> create_anchor(<span class="st">"license"</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>taxation_anchor <span class="op">=</span> create_anchor(<span class="st">"taxation"</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame to hold the anchors</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>anchors_df <span class="op">=</span> pd.DataFrame({</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Topic'</span>: [<span class="st">'labor'</span>, <span class="st">'legislation'</span>, <span class="st">'license'</span>, <span class="st">'taxation'</span>],</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Anchor Vector'</span>: [labor_anchor, legislation_anchor, license_anchor, taxation_anchor]</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>anchors_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="31">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the cosine similarity between each anchor and the mean embeddings of Crease, Begbie, and the Act 1884</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the results as a box plot</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calculate_similarity(anchor, embeddings):</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> cosine_similarity(anchor.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), embeddings).flatten()</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame to hold the similarity scores</span></span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>similarity_scores <span class="op">=</span> {</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Author'</span>: [],</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Topic'</span>: [],</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Text'</span>: [],</span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Similarity Score'</span>: []</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> anchors_df[<span class="st">'Topic'</span>]:</span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    anchor_vector <span class="op">=</span> anchors_df.loc[anchors_df[<span class="st">'Topic'</span>] <span class="op">==</span> topic, <span class="st">'Anchor Vector'</span>].values[<span class="dv">0</span>]</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> author <span class="kw">in</span> [<span class="st">'Crease'</span>, <span class="st">'Begbie'</span>, <span class="st">'Act 1884'</span>]:</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        emb_list <span class="op">=</span> embeddings_dict.get(author, [])</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>        texts <span class="op">=</span> act_snippets.get(author, [])</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(emb_list) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb31-23"><a href="#cb31-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-24"><a href="#cb31-24" aria-hidden="true" tabindex="-1"></a>        embeddings <span class="op">=</span> np.vstack(emb_list)</span>
<span id="cb31-25"><a href="#cb31-25" aria-hidden="true" tabindex="-1"></a>        sim_scores <span class="op">=</span> calculate_similarity(anchor_vector, embeddings)</span>
<span id="cb31-26"><a href="#cb31-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-27"><a href="#cb31-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> idx, score <span class="kw">in</span> <span class="bu">enumerate</span>(sim_scores):</span>
<span id="cb31-28"><a href="#cb31-28" aria-hidden="true" tabindex="-1"></a>            similarity_scores[<span class="st">'Author'</span>].append(author)</span>
<span id="cb31-29"><a href="#cb31-29" aria-hidden="true" tabindex="-1"></a>            similarity_scores[<span class="st">'Topic'</span>].append(topic)</span>
<span id="cb31-30"><a href="#cb31-30" aria-hidden="true" tabindex="-1"></a>            similarity_scores[<span class="st">'Text'</span>].append(texts[idx] <span class="cf">if</span> idx <span class="op">&lt;</span> <span class="bu">len</span>(texts) <span class="cf">else</span> <span class="st">""</span>)</span>
<span id="cb31-31"><a href="#cb31-31" aria-hidden="true" tabindex="-1"></a>            similarity_scores[<span class="st">'Similarity Score'</span>].append(<span class="bu">float</span>(score))</span>
<span id="cb31-32"><a href="#cb31-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-33"><a href="#cb31-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to DataFrame</span></span>
<span id="cb31-34"><a href="#cb31-34" aria-hidden="true" tabindex="-1"></a>similarity_df <span class="op">=</span> pd.DataFrame(similarity_scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="32">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare authors and topics</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>preferred_order <span class="op">=</span> [<span class="st">'Crease'</span>, <span class="st">'Begbie'</span>, <span class="st">'Act 1884'</span>]</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>authors <span class="op">=</span> [a <span class="cf">for</span> a <span class="kw">in</span> preferred_order <span class="cf">if</span> a <span class="kw">in</span> similarity_df[<span class="st">'Author'</span>].unique()]</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>topics <span class="op">=</span> <span class="bu">list</span>(similarity_df[<span class="st">'Topic'</span>].unique())[:<span class="dv">4</span>] </span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Color Blind friendly palette</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>author_palette <span class="op">=</span> sns.color_palette(<span class="st">"colorblind"</span>, n_colors<span class="op">=</span><span class="bu">len</span>(authors))</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>sns.<span class="bu">set</span>(style<span class="op">=</span><span class="st">"whitegrid"</span>, context<span class="op">=</span><span class="st">"notebook"</span>)</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>), sharey<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axes.flat[i]</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;</span> <span class="bu">len</span>(topics):</span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>        topic <span class="op">=</span> topics[i]</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>        df_t <span class="op">=</span> similarity_df[similarity_df[<span class="st">'Topic'</span>] <span class="op">==</span> topic]</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        <span class="co"># draw boxplot</span></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        sns.boxplot(</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>            data<span class="op">=</span>df_t,</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span><span class="st">'Author'</span>,</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="st">'Similarity Score'</span>,</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>            order<span class="op">=</span>authors,</span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>            palette<span class="op">=</span>author_palette,</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>            width<span class="op">=</span><span class="fl">0.6</span>,</span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>            fliersize<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a>            ax<span class="op">=</span>ax,</span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a>            boxprops<span class="op">=</span><span class="bu">dict</span>(linewidth<span class="op">=</span><span class="fl">0.9</span>),</span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a>            medianprops<span class="op">=</span><span class="bu">dict</span>(linewidth<span class="op">=</span><span class="fl">1.1</span>, color<span class="op">=</span><span class="st">'black'</span>),</span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>            whiskerprops<span class="op">=</span><span class="bu">dict</span>(linewidth<span class="op">=</span><span class="fl">0.9</span>),</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a>            capprops<span class="op">=</span><span class="bu">dict</span>(linewidth<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a>        <span class="co"># compute per-author means and overlay them</span></span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a>        means <span class="op">=</span> df_t.groupby(<span class="st">'Author'</span>)[<span class="st">'Similarity Score'</span>].mean().reindex(authors)</span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a>        x_positions <span class="op">=</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(authors)))</span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># plot white diamond with black edge so it stands out on colored boxes</span></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a>        ax.scatter(x_positions, means.values, marker<span class="op">=</span><span class="st">'D'</span>, s<span class="op">=</span><span class="dv">60</span>,</span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a>                   facecolors<span class="op">=</span><span class="st">'white'</span>, edgecolors<span class="op">=</span><span class="st">'black'</span>, zorder<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># robust y-limits </span></span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>        vals <span class="op">=</span> df_t[<span class="st">'Similarity Score'</span>].dropna()</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(vals) <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a>            ymin, ymax <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span>, <span class="fl">1.0</span></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb32-46"><a href="#cb32-46" aria-hidden="true" tabindex="-1"></a>            q1 <span class="op">=</span> vals.quantile(<span class="fl">0.25</span>)</span>
<span id="cb32-47"><a href="#cb32-47" aria-hidden="true" tabindex="-1"></a>            q3 <span class="op">=</span> vals.quantile(<span class="fl">0.75</span>)</span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a>            iqr <span class="op">=</span> q3 <span class="op">-</span> q1</span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> iqr <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a>                whisker_low <span class="op">=</span> <span class="bu">float</span>(vals.<span class="bu">min</span>())</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a>                whisker_high <span class="op">=</span> <span class="bu">float</span>(vals.<span class="bu">max</span>())</span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a>                whisker_low <span class="op">=</span> <span class="bu">float</span>(q1 <span class="op">-</span> <span class="fl">1.5</span> <span class="op">*</span> iqr)</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a>                whisker_high <span class="op">=</span> <span class="bu">float</span>(q3 <span class="op">+</span> <span class="fl">1.5</span> <span class="op">*</span> iqr)</span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a>            span <span class="op">=</span> <span class="bu">max</span>(whisker_high <span class="op">-</span> whisker_low, <span class="fl">1e-6</span>)</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a>            pad <span class="op">=</span> <span class="bu">max</span>(span <span class="op">*</span> <span class="fl">0.08</span>, <span class="fl">0.03</span>)</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a>            ymin <span class="op">=</span> <span class="bu">max</span>(<span class="op">-</span><span class="fl">1.0</span>, whisker_low <span class="op">-</span> pad)</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a>            ymax <span class="op">=</span> <span class="bu">min</span>(<span class="fl">1.0</span>, whisker_high <span class="op">+</span> pad)</span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> ymin <span class="op">&gt;=</span> ymax:</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a>                mid <span class="op">=</span> <span class="bu">float</span>(vals.median())</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a>                ymin <span class="op">=</span> <span class="bu">max</span>(<span class="op">-</span><span class="fl">1.0</span>, mid <span class="op">-</span> <span class="fl">0.05</span>)</span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a>                ymax <span class="op">=</span> <span class="bu">min</span>(<span class="fl">1.0</span>, mid <span class="op">+</span> <span class="fl">0.05</span>)</span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a>        ax.set_ylim(ymin, ymax)</span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="ss">f"Topic: </span><span class="sc">{</span>topic<span class="sc">}</span><span class="ss">"</span>, fontsize<span class="op">=</span><span class="dv">12</span>, weight<span class="op">=</span><span class="st">'semibold'</span>)</span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a>        ax.set_xlabel(<span class="st">''</span>)</span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a>        ax.set_ylabel(<span class="st">'Cosine Similarity'</span> <span class="cf">if</span> i <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span> <span class="cf">else</span> <span class="st">''</span>)</span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a>        ax.axhline(<span class="fl">0.0</span>, color<span class="op">=</span><span class="st">'grey'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="fl">0.8</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a>        ax.tick_params(axis<span class="op">=</span><span class="st">'x'</span>, rotation<span class="op">=</span><span class="dv">0</span>, labelsize<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a>        ax.tick_params(axis<span class="op">=</span><span class="st">'y'</span>, labelsize<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb32-73"><a href="#cb32-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> spine <span class="kw">in</span> ax.spines.values():</span>
<span id="cb32-74"><a href="#cb32-74" aria-hidden="true" tabindex="-1"></a>            spine.set_linewidth(<span class="fl">0.8</span>)</span>
<span id="cb32-75"><a href="#cb32-75" aria-hidden="true" tabindex="-1"></a>        sns.despine(ax<span class="op">=</span>ax, trim<span class="op">=</span><span class="va">True</span>, left<span class="op">=</span><span class="va">False</span>, bottom<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb32-76"><a href="#cb32-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb32-77"><a href="#cb32-77" aria-hidden="true" tabindex="-1"></a>        ax.set_visible(<span class="va">False</span>)</span>
<span id="cb32-78"><a href="#cb32-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-79"><a href="#cb32-79" aria-hidden="true" tabindex="-1"></a><span class="co"># Single legend for authors</span></span>
<span id="cb32-80"><a href="#cb32-80" aria-hidden="true" tabindex="-1"></a>legend_handles <span class="op">=</span> [Patch(facecolor<span class="op">=</span>author_palette[idx], label<span class="op">=</span>authors[idx]) <span class="cf">for</span> idx <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(authors))]</span>
<span id="cb32-81"><a href="#cb32-81" aria-hidden="true" tabindex="-1"></a>fig.legend(handles<span class="op">=</span>legend_handles, title<span class="op">=</span><span class="st">'Author'</span>, loc<span class="op">=</span><span class="st">'upper right'</span>, frameon<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-82"><a href="#cb32-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-83"><a href="#cb32-83" aria-hidden="true" tabindex="-1"></a>plt.tight_layout(rect<span class="op">=</span>[<span class="dv">0</span>, <span class="dv">0</span>, <span class="fl">0.95</span>, <span class="fl">0.96</span>])</span>
<span id="cb32-84"><a href="#cb32-84" aria-hidden="true" tabindex="-1"></a>fig.suptitle(<span class="st">'Topic Similarity by Author</span><span class="ch">\n</span><span class="st">(Mean Labeled by Diamond)'</span>, fontsize<span class="op">=</span><span class="dv">14</span>, y<span class="op">=</span><span class="fl">0.99</span>)</span>
<span id="cb32-85"><a href="#cb32-85" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="llm-and-zero-shot-classification" class="level3">
<h3 class="anchored" data-anchor-id="llm-and-zero-shot-classification">LLM and Zero-Shot Classification</h3>
<p>Another powerful technique for text analysis is <strong>zero-shot classification</strong>, which allows us to classify text into predefined categories without requiring labeled training data, with the help of large language models (LLMs). This approach is particularly useful when we have a limited amount of labeled data or when the categories are not well-defined.</p>
<p>In addition to classifying text into specific categories, zero-shot classification can also be used to evaluate the stance of a text towards a particular issue or topic by <strong>calculating the probability of the text belonging to each category</strong>, the calculated probabilities will sum to 1 and they can be interpreted as the model’s confidence in each category. In this workshop, we mainly focus on the second aspect, which allows us to assess the stance of each text.</p>
<p>We will use the Hugging Face Transformers library to implement zero-shot classification using a pre-trained model <strong>facebook/bart-large-mnli</strong>. The model is trained on the Multi-Genre Natural Language Inference (MultiNLI) dataset, which contains pairs of sentences labeled with their relationship (pros, cons, or neutral). This allows the model to learn how to classify text based on its semantic meaning and context, which is particularly useful for our analysis of historical texts.</p>
<p>The key steps in our zero-shot classification process are as follows:</p>
<ol type="1">
<li><strong>Define the zero-shot pipeline</strong>: We create a zero-shot classification pipeline using the pre-trained model and tokenizer from Hugging Face Transformers, and specify a hypothesis template “In this snippet of a historical legal text, the author {}” that will be used to generate hypotheses for classification.</li>
<li><strong>Define the candidate labels</strong>: We define a set of candidate labels that represent the stance categories we want to classify the text into, which correspond to the basic stance categories we are interested in, such as “pro”, “neutral” or “cons” the equal rights of Chinese immigrants.</li>
<li><strong>Classify the text</strong>: We apply the zero-shot classification pipeline to each text in our corpus, generating a probability distribution over the candidate labels for each text.</li>
<li><strong>Investigate the results</strong>: We analyze the classification results to identify the stance of each text, focusing on the highest probability label for each text. We also calculate the average probability for each candidate label across all texts to compare the overall stance of different authors and documents.</li>
</ol>
<div class="cell" data-execution_count="33">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the full snippets dictionary</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>act_1884_full <span class="op">=</span> <span class="st">" "</span>.join(act_1884)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>crease_cases_full <span class="op">=</span> <span class="st">" "</span>.join(crease_cases)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>begbie_cases_full <span class="op">=</span> <span class="st">" "</span>.join(begbie_cases)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>full_cases <span class="op">=</span> {<span class="st">"Crease"</span>: crease_cases_full, <span class="st">"Begbie"</span>: begbie_cases_full, <span class="st">"Act 1884"</span>: act_1884_full}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="34">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We create a dictionary to hold the full snippets for each author</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>full_snippets <span class="op">=</span> {}</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> author, text <span class="kw">in</span> full_cases.items():</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize using Spacy</span></span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    sentence <span class="op">=</span> [sent.text <span class="cf">for</span> sent <span class="kw">in</span> nlp(text).sents]</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    snippets <span class="op">=</span> []</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sent <span class="kw">in</span> sentence:</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(sent) <span class="op">&gt;</span> <span class="dv">30</span>:  <span class="co"># Filter out short and meaningless sentences created by tokenization</span></span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>            snippets.append(sent)</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    full_snippets[author] <span class="op">=</span> snippets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="35">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame to display snippet size by author</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>snippet_sizes <span class="op">=</span> [{<span class="st">'Author'</span>: auth, <span class="st">'Snippet Count'</span>: <span class="bu">len</span>(snippets)} <span class="cf">for</span> auth, snippets <span class="kw">in</span> full_snippets.items()]</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>snippet_sizes_df <span class="op">=</span> pd.DataFrame(snippet_sizes)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the DataFrame</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(snippet_sizes_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>To ensure that the zero-shot classification results are meaningful, we carefully treat the candidate labels as prompts that guide the model’s understanding of the stance categories. This allows us to leverage the model’s ability to generalize and adapt to new tasks without requiring extensive retraining or fine-tuning. Here, we will use the following candidate labels: - Pro: “… advocates for equal legal treatment of Chinese immigrants compared to white or European settlers, opposing racial discrimination” - Neutral: “… describes or retells the status or treatment of Chinese immigrants without expressing support or opposition to racial inequality, is unrelated to Chinese immigrants, or cannot be classified as either” - Cons: “… justifies or reinforces unequal legal treatment of Chinese immigrants relative to white or European settlers, supporting racially discriminatory policies”</p>
<p>However, we must note that this is also a major limitation of the zero-shot classification approach, as it relies on the quality and relevance of the candidate labels to the text being classified. If the labels are not well-defined or do not accurately reflect the stance categories, the classification results may be misleading or inaccurate. This is particularly important when working with historical texts, where the language and context may differ significantly from modern usage. Therefore, it is essential to carefully select and define the candidate labels to ensure that they accurately reflect the stance categories we are interested in.</p>
<div class="cell" data-execution_count="36">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create pipeline for zero-shot classification with error handling and efficiency improvements</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>zero_shot <span class="op">=</span> pipeline(</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"zero-shot-classification"</span>,</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli"</span>,</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span><span class="st">"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli"</span>,</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    hypothesis_template<span class="op">=</span><span class="st">"In this snippet of a historical legal text, the author </span><span class="sc">{}</span><span class="st">."</span>,</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    device<span class="op">=</span><span class="dv">0</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span>  <span class="co"># Use GPU if available</span></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplified and clearer labels for better classification</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [</span>
<span id="cb36-15"><a href="#cb36-15" aria-hidden="true" tabindex="-1"></a>    <span class="st">"advocates for equal legal treatment of Chinese immigrants compared to white or European settlers, opposing racial discrimination"</span>,</span>
<span id="cb36-16"><a href="#cb36-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"describes or retells the status or treatment of Chinese immigrants without expressing support or opposition to racial inequality, is unrelated to Chinese immigrants, or cannot be classified as either"</span>,</span>
<span id="cb36-17"><a href="#cb36-17" aria-hidden="true" tabindex="-1"></a>    <span class="st">"justifies or reinforces unequal legal treatment of Chinese immigrants relative to white or European settlers, supporting racially discriminatory policies"</span></span>
<span id="cb36-18"><a href="#cb36-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb36-19"><a href="#cb36-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-20"><a href="#cb36-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_scores(snippet, max_length<span class="op">=</span><span class="dv">512</span>):</span>
<span id="cb36-21"><a href="#cb36-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure snippet is not empty and is reasonable length</span></span>
<span id="cb36-22"><a href="#cb36-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> snippet <span class="kw">or</span> <span class="bu">len</span>(snippet.strip()) <span class="op">&lt;</span> <span class="dv">10</span>:</span>
<span id="cb36-23"><a href="#cb36-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {label: <span class="fl">0.33</span> <span class="cf">for</span> label <span class="kw">in</span> labels}  <span class="co"># Return neutral scores for empty/short text</span></span>
<span id="cb36-24"><a href="#cb36-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-25"><a href="#cb36-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Truncate if too long to avoid token limit issues</span></span>
<span id="cb36-26"><a href="#cb36-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(snippet) <span class="op">&gt;</span> max_length <span class="op">*</span> <span class="dv">4</span>:  </span>
<span id="cb36-27"><a href="#cb36-27" aria-hidden="true" tabindex="-1"></a>        snippet <span class="op">=</span> snippet[:max_length <span class="op">*</span> <span class="dv">4</span>]</span>
<span id="cb36-28"><a href="#cb36-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-29"><a href="#cb36-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run classification</span></span>
<span id="cb36-30"><a href="#cb36-30" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> zero_shot(snippet, candidate_labels<span class="op">=</span>labels, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span>max_length)</span>
<span id="cb36-31"><a href="#cb36-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-32"><a href="#cb36-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create score dictionary</span></span>
<span id="cb36-33"><a href="#cb36-33" aria-hidden="true" tabindex="-1"></a>    score_dict <span class="op">=</span> <span class="bu">dict</span>(<span class="bu">zip</span>(out[<span class="st">"labels"</span>], out[<span class="st">"scores"</span>]))</span>
<span id="cb36-34"><a href="#cb36-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-35"><a href="#cb36-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Ensure all labels are present</span></span>
<span id="cb36-36"><a href="#cb36-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> label <span class="kw">in</span> labels:</span>
<span id="cb36-37"><a href="#cb36-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> label <span class="kw">not</span> <span class="kw">in</span> score_dict:</span>
<span id="cb36-38"><a href="#cb36-38" aria-hidden="true" tabindex="-1"></a>            score_dict[label] <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb36-39"><a href="#cb36-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-40"><a href="#cb36-40" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> score_dict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can test the zero-shot classification pipeline on a sample text to see how it works. For example, we can use a paragraph from Sir Chapleau’s report to the Royal Commission on Chinese immigration:</p>
<blockquote class="blockquote">
<p>That assuming Chinese immigrants of the laboring class will persist in retaining their present characteristics of Asiatic life, where these are strikingly peculiar and distinct from western, and that the influx will continue to increase, this immigration should be dealt with by Parliament ; but no legislation should be such as would give a shock to great interests and enterprises established before any probability that Parliament would interfere with that immigration arose. Questions of vested rights might come up, and these ought to be carefully considered before action is taken.</p>
</blockquote>
<div class="cell" data-execution_count="37">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the snippet</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>chapleau_snippet <span class="op">=</span> <span class="st">"That assuming Chinese immigrants of the laboring class will persist in retaining their present characteristics of Asiatic life, where these are strikingly peculiar and distinct from western, and that the influx will continue to increase, this immigration should be dealt with by Parliament; but no legislation should be such as would give a shock to great interests and enterprises established before any probability that Parliament would interfere with that immigration arose. Questions of vested rights might come up, and these ought to be carefully considered before action is taken."</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the scores for the snippet</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>chapleau_scores <span class="op">=</span> get_scores(chapleau_snippet)</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the scores</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Classification Scores for Chapleau's Snippet:"</span>)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> label, score <span class="kw">in</span> chapleau_scores.items():</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">: </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="sentence-approach" class="level4">
<h4 class="anchored" data-anchor-id="sentence-approach">Sentence Approach</h4>
<p>Another limitation of the zero-shot classification is that the number of tokens in the text is limited, which means that we cannot classify the entire text at once if it exceeds the token limit. To avoid exceeding the token limit, we can split the text into smaller chunks, such as sentences or windows of text, and classify each chunk separately.</p>
<p>The <strong>sentence approach</strong> is to classify each sentence in the text separately, which allows us to capture the stance of each sentence and its relationship to the overall text. This approach is particularly useful when the text is long or complex, as it allows us to analyze the stance of each sentence in isolation.</p>
<p>However, it has limitations in understanding the overall stance of the text, as it does not consider the context in which the sentences are used and therefore may capture too much variation in the stance of the text.</p>
<div class="cell" data-execution_count="38">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Run zero-shot classification on the snippets from the Chinese Regulation Act 1884</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a><span class="co"># act_scores = {}</span></span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a><span class="co"># for auth, snippets in full_snippets.items():</span></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     scores = []</span></span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a><span class="co">#     for snip in snippets:</span></span>
<span id="cb38-7"><a href="#cb38-7" aria-hidden="true" tabindex="-1"></a><span class="co">#         score = get_scores(snip)</span></span>
<span id="cb38-8"><a href="#cb38-8" aria-hidden="true" tabindex="-1"></a><span class="co">#         scores.append(score)</span></span>
<span id="cb38-9"><a href="#cb38-9" aria-hidden="true" tabindex="-1"></a><span class="co">#     act_scores[auth] = scores</span></span>
<span id="cb38-10"><a href="#cb38-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-11"><a href="#cb38-11" aria-hidden="true" tabindex="-1"></a><span class="co"># rows = []</span></span>
<span id="cb38-12"><a href="#cb38-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-13"><a href="#cb38-13" aria-hidden="true" tabindex="-1"></a><span class="co"># for auth, snippets in full_snippets.items():</span></span>
<span id="cb38-14"><a href="#cb38-14" aria-hidden="true" tabindex="-1"></a><span class="co">#     for snip, score_dict in zip(snippets, act_scores[auth]):</span></span>
<span id="cb38-15"><a href="#cb38-15" aria-hidden="true" tabindex="-1"></a><span class="co">#         row = {</span></span>
<span id="cb38-16"><a href="#cb38-16" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Author": auth,</span></span>
<span id="cb38-17"><a href="#cb38-17" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Text": snip,</span></span>
<span id="cb38-18"><a href="#cb38-18" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Pro": score_dict[labels[0]],</span></span>
<span id="cb38-19"><a href="#cb38-19" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Neutral": score_dict[labels[1]],</span></span>
<span id="cb38-20"><a href="#cb38-20" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Cons": score_dict[labels[2]]</span></span>
<span id="cb38-21"><a href="#cb38-21" aria-hidden="true" tabindex="-1"></a><span class="co">#         }</span></span>
<span id="cb38-22"><a href="#cb38-22" aria-hidden="true" tabindex="-1"></a><span class="co">#         rows.append(row)</span></span>
<span id="cb38-23"><a href="#cb38-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-24"><a href="#cb38-24" aria-hidden="true" tabindex="-1"></a><span class="co"># # Create DataFrame to store the scores</span></span>
<span id="cb38-25"><a href="#cb38-25" aria-hidden="true" tabindex="-1"></a><span class="co"># df_scores = pd.DataFrame(rows)</span></span>
<span id="cb38-26"><a href="#cb38-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-27"><a href="#cb38-27" aria-hidden="true" tabindex="-1"></a><span class="co"># # Save the DataFrame to a CSV file</span></span>
<span id="cb38-28"><a href="#cb38-28" aria-hidden="true" tabindex="-1"></a><span class="co"># df_scores.to_csv("data/zero_shot_sentence_scores.csv", index=False)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="39">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the saved DataFrame</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>df_scores <span class="op">=</span> pd.read_csv(<span class="st">"data/zero_shot_sentence_scores.csv"</span>)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the top 10 sentences with the highest "Pro" scores</span></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>top_pro_sentences <span class="op">=</span> df_scores.nlargest(<span class="dv">10</span>, <span class="st">'Pro'</span>)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 10 sentences with the highest 'Pro' scores:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> top_pro_sentences.iterrows():</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Sentence: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Pro Score: </span><span class="sc">{</span>row[<span class="st">'Pro'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="40">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the top 10 sentences with the highest "Cons" scores</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>top_cons_sentences <span class="op">=</span> df_scores.nlargest(<span class="dv">10</span>, <span class="st">'Cons'</span>)</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 10 sentences with the highest 'Cons' scores:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> top_cons_sentences.iterrows():</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Sentence: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Cons Score: </span><span class="sc">{</span>row[<span class="st">'Cons'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="41">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Group by author and calculate mean scores</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a>mean_scores <span class="op">=</span> df_scores.groupby(<span class="st">"Author"</span>)[[<span class="st">"Pro"</span>, <span class="st">"Neutral"</span>, <span class="st">"Cons"</span>]].mean()</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>median_scores <span class="op">=</span> df_scores.groupby(<span class="st">"Author"</span>)[[<span class="st">"Pro"</span>, <span class="st">"Neutral"</span>, <span class="st">"Cons"</span>]].median()</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean scores by author:"</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_scores)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Median scores by author:"</span>)</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(median_scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="42">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>df_scores[<span class="st">'Wrapped Text'</span>] <span class="op">=</span> df_scores[<span class="st">'Text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> t: wrap_text(t, width <span class="op">=</span> <span class="dv">50</span>))</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    df_scores,</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"Pro"</span>,</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">"Cons"</span>,</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"Author"</span>,</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    hover_data<span class="op">=</span>[<span class="st">"Wrapped Text"</span>],</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Pros vs Cons Scores by Author"</span>,</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span><span class="dv">800</span>,</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">600</span></span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>fig.update_traces(marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">5</span>))</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="window-approach" class="level4">
<h4 class="anchored" data-anchor-id="window-approach">Window Approach</h4>
<p>In comparison to the sentence approach, the <strong>window approach</strong> is to classify larger chunks of text that contain multiple sentences with an overlapping context between windows. This allows us to capture the stance of the text in a more holistic way, while still being able to classify each window separately.</p>
<p>The limitation of this approach is that it may not capture the nuances of each sentence, as it treats the window containing multiple sentences as a single unit. However, it allows us to capture the overall stance of the text while still being able to classify each window separately.</p>
<p>Here, we define the function to split the text into overlapping windows of a specified size (maximum number of tokens) with a certain overlap (stride that overlap between consecutive windows). We then apply the zero-shot classification pipeline to each window and average the results to obtain the final classification for the entire text.</p>
<div class="cell" data-execution_count="43">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to chunk text into overlapping windows</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chunk_into_windows(text, max_tokens<span class="op">=</span><span class="dv">512</span>, stride<span class="op">=</span><span class="dv">128</span>):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Break into sentences first for cleaner boundaries</span></span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    sents <span class="op">=</span> sent_tokenize(text)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>    windows <span class="op">=</span> []</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    current <span class="op">=</span> <span class="st">""</span></span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> sent <span class="kw">in</span> sents:</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Tentative window if we add this sentence</span></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>        cand <span class="op">=</span> current <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> sent <span class="cf">if</span> current <span class="cf">else</span> sent</span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Count tokens</span></span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        n_tokens <span class="op">=</span> <span class="bu">len</span>(tokenizer.encode(cand, add_special_tokens<span class="op">=</span><span class="va">False</span>))</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n_tokens <span class="op">&lt;=</span> max_tokens:</span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> cand</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># finalize current window, then start new from overlapping tail</span></span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>            windows.append(current)</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># keep the stride tokens from the end of the current window</span></span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>            tail_ids <span class="op">=</span> tokenizer.encode(current, add_special_tokens<span class="op">=</span><span class="va">False</span>)[<span class="op">-</span>stride:]</span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>            tail_text <span class="op">=</span> tokenizer.decode(tail_ids)</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>            current <span class="op">=</span> tail_text <span class="op">+</span> <span class="st">" "</span> <span class="op">+</span> sent</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> current:</span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a>        windows.append(current)</span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> windows</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="44">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># # Run classification per author</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="co"># rows = []</span></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="co"># for author, text in full_cases.items():</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="co">#     windows = chunk_into_windows(text, max_tokens=256, stride=64)</span></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a><span class="co">#     # classify each window</span></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a><span class="co">#     for win in windows:</span></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="co">#         out = zero_shot(win, candidate_labels=labels, truncation=True, max_length=256)</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a><span class="co">#         # Extract scores and labels</span></span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a><span class="co">#         score_dict = dict(zip(out["labels"], out["scores"]))</span></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="co">#         rows.append({</span></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Author": author,</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Text": win,</span></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Pro": score_dict[labels[0]],</span></span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Neutral": score_dict[labels[1]],</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co">#             "Cons": score_dict[labels[2]]</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="co">#         })</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="co"># all_scores = pd.DataFrame(rows)</span></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="co"># # Save the DataFrame to a CSV file</span></span>
<span id="cb44-23"><a href="#cb44-23" aria-hidden="true" tabindex="-1"></a><span class="co"># all_scores.to_csv("data/zero_shot_windowed_scores.csv", index=False)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="45">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Read the saved DataFrame</span></span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>all_scores <span class="op">=</span> pd.read_csv(<span class="st">"data/zero_shot_windowed_scores.csv"</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the top 5 windows with the highest "Pro" scores</span></span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>top_pro_windows <span class="op">=</span> all_scores.nlargest(<span class="dv">5</span>, <span class="st">'Pro'</span>)</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 5 windows with the highest 'Pro' scores:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> top_pro_windows.iterrows():</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Window: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Pro Score: </span><span class="sc">{</span>row[<span class="st">'Pro'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="46">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the top 5 windows with the highest "Cons" scores</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>top_cons_windows <span class="op">=</span> all_scores.nlargest(<span class="dv">5</span>, <span class="st">'Cons'</span>)</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top 5 windows with the highest 'Cons' scores:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> top_cons_windows.iterrows():</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Window: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Cons Score: </span><span class="sc">{</span>row[<span class="st">'Cons'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="47">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate the mean scores and median scores for each author</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>mean_scores <span class="op">=</span> all_scores.groupby(<span class="st">"Author"</span>)[[<span class="st">"Pro"</span>, <span class="st">"Neutral"</span>, <span class="st">"Cons"</span>]].mean()</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>median_scores <span class="op">=</span> all_scores.groupby(<span class="st">"Author"</span>)[[<span class="st">"Pro"</span>, <span class="st">"Neutral"</span>, <span class="st">"Cons"</span>]].median()</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean scores by author:"</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_scores)</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Median scores by author:"</span>)</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(median_scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="48">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>all_scores[<span class="st">'Text'</span>] <span class="op">=</span> all_scores[<span class="st">'Text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> t: wrap_text(t, width <span class="op">=</span> <span class="dv">50</span>))</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    all_scores,</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"Pro"</span>,</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">"Cons"</span>,</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"Author"</span>,</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>    hover_data<span class="op">=</span>[<span class="st">"Text"</span>],</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Pros vs Cons Scores by Author"</span>,</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span><span class="dv">800</span>,</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">600</span></span>
<span id="cb48-12"><a href="#cb48-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb48-13"><a href="#cb48-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-14"><a href="#cb48-14" aria-hidden="true" tabindex="-1"></a>fig.update_traces(marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">5</span>))</span>
<span id="cb48-15"><a href="#cb48-15" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>While both approach successfully identified that the Chinese Regulation Act of 1884 was more discriminatory towards Chinese people, and the relative positions of the three authors matched our expectations, we note that the zero-shot classification results are not deterministic, as they still depend on the specific language used in defining the candidate labels and the context in which the text is used.</p>
<p>For instance, the window</p>
<blockquote class="blockquote">
<p>…act the legal presumption of innocence until conviction is reversed ; in every case the onus probandi, though in a statute highly penal, is shifted from the informant on to the shoulders of the accused, and he a foreigner not knowing one word of the law, or even the language of the accuser. In other words, every Chinese is guilty until proved innocent—a provision which fills one conversant with subjects with alarm; for if such a law can be tolerated as against Chinese, the precedent is set, and in time of any popular outcry can easily be acted on for putting any other foreigners or even special classes among ourselves, as coloured people, or French, Italians, Americans, or Germans, under equally the same law. That certainly is interfering with aliens. The proposition that it is a Provincial tax for revenue purposes, supposing it to be so intended under the provisions of the Act, is so manifestly calculated to defeat that object, by diminishing the numbers of the members of the persons to be affected by it, that it is difficult to regard it in that light, or in any other light than an indirect mode of getting rid of persons whom it affects out of the country.</p>
</blockquote>
<p>is classified as “cons” by the zero-shot classification, which is not accurate, as it essentially reflects Crease’s criticism of the discriminatory nature of the law, and there is no indication that he supports the unequal treatment of Chinese immigrants in this passage.</p>
<p>This highlights the limitations of zero-shot classification when dealing with historical legal texts: because the semantics and context of these texts are not obvious, and the rhetorical devices used in them are difficult for models to accurately capture.</p>
<p>Such example emphasizes the importance of human interpretation and analysis in understanding the positions in historical texts, and also highlights the need for caution when selecting and defining candidate labels to ensure that these labels accurately reflect the position categories we are interested in. We should still only use machine learning techniques as an auxiliary tool in research.</p>
</section>
</section>
<section id="putting-it-all-together" class="level3">
<h3 class="anchored" data-anchor-id="putting-it-all-together">Putting It All Together</h3>
<p>Now that we’ve examined topic alignment and quantified stances toward Chinese immigrants using zero‑shot classification, we synthesize the results to evaluate how thematic emphasis correlates with expressed stance. By combining TF‑IDF keywords, embedding‑based topic anchors, and zero‑shot probabilities, we assess whether focus on themes such as “labor”, “taxation”, or “legislation” corresponds with pro‑, neutral, or anti‑discriminatory language across authors and documents.</p>
<p>The steps we will take are as follows: 1. <strong>Aggregate Topic Alignment Scores</strong>: For each sentence, we will aggregate the topic alignment scores to each topic that we previously identified using TF-IDF. 2. <strong>Combine with Zero-Shot Scores</strong>: We will combine the aggregated topic alignment scores with the zero-shot classification scores to create a paired dataset of topic alignment and stance scores for each sentence. 3. <strong>Analyze Correlation</strong>: We will analyze the correlation between the topic alignment scores and the zero-shot classification scores to identify patterns and relationships between the topics and the stances expressed in the texts. We will also calculate the correlation coefficients for each author to see how their topic alignments relate to their stances.</p>
<div class="cell" data-execution_count="49">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># define a function to clean text for matching</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(text):</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.lower()</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'[^\w\s]'</span>, <span class="st">''</span>, text)  </span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text.strip()</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert the similarity DataFrame to wide format</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>similarity_wide <span class="op">=</span> similarity_df.pivot(index<span class="op">=</span>[<span class="st">'Author'</span>,<span class="st">'Text'</span>], columns<span class="op">=</span><span class="st">'Topic'</span>, values<span class="op">=</span><span class="st">'Similarity Score'</span>).reset_index()</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean the text for matching</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>similarity_wide[<span class="st">'Clean Text'</span>] <span class="op">=</span> similarity_wide[<span class="st">'Text'</span>].<span class="bu">apply</span>(clean_text)</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The total size of our corpus is </span><span class="sc">{</span><span class="bu">len</span>(similarity_wide)<span class="sc">}</span><span class="ss"> sentences."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="50">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Clean the text in df_scores for matching</span></span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>df_scores[<span class="st">'Clean Text'</span>] <span class="op">=</span> df_scores[<span class="st">'Text'</span>].<span class="bu">apply</span>(clean_text)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Match the cleaned text in similarity_wide with df_scores</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>matched_rows <span class="op">=</span> []</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> similarity_wide.iterrows():</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>    match <span class="op">=</span> df_scores[df_scores[<span class="st">'Clean Text'</span>] <span class="op">==</span> row[<span class="st">'Clean Text'</span>]]</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> match.empty:</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        matched_row <span class="op">=</span> {</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Author'</span>: row[<span class="st">'Author'</span>],</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Text'</span>: row[<span class="st">'Text'</span>],</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Pro'</span>: match[<span class="st">'Pro'</span>].values[<span class="dv">0</span>] <span class="cf">if</span> <span class="st">'Pro'</span> <span class="kw">in</span> match <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb50-13"><a href="#cb50-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Neutral'</span>: match[<span class="st">'Neutral'</span>].values[<span class="dv">0</span>] <span class="cf">if</span> <span class="st">'Neutral'</span> <span class="kw">in</span> match <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb50-14"><a href="#cb50-14" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Cons'</span>: match[<span class="st">'Cons'</span>].values[<span class="dv">0</span>] <span class="cf">if</span> <span class="st">'Cons'</span> <span class="kw">in</span> match <span class="cf">else</span> <span class="va">None</span>,</span>
<span id="cb50-15"><a href="#cb50-15" aria-hidden="true" tabindex="-1"></a>            <span class="op">**</span>row.drop([<span class="st">'Author'</span>, <span class="st">'Text'</span>, <span class="st">'Clean Text'</span>]).to_dict()</span>
<span id="cb50-16"><a href="#cb50-16" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb50-17"><a href="#cb50-17" aria-hidden="true" tabindex="-1"></a>        matched_rows.append(matched_row)</span>
<span id="cb50-18"><a href="#cb50-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb50-19"><a href="#cb50-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame from the matched rows</span></span>
<span id="cb50-20"><a href="#cb50-20" aria-hidden="true" tabindex="-1"></a>merged_df <span class="op">=</span> pd.DataFrame(matched_rows)</span>
<span id="cb50-21"><a href="#cb50-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-22"><a href="#cb50-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The size of our merged corpus is </span><span class="sc">{</span><span class="bu">len</span>(merged_df)<span class="sc">}</span><span class="ss"> sentences."</span>)</span>
<span id="cb50-23"><a href="#cb50-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-24"><a href="#cb50-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-25"><a href="#cb50-25" aria-hidden="true" tabindex="-1"></a>merged_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="51">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Ensure wrapped text for hover</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>merged_df[<span class="st">'wrapped_text'</span>] <span class="op">=</span> merged_df[<span class="st">'Text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> t: wrap_text(t, width<span class="op">=</span><span class="dv">50</span>))</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>author_order <span class="op">=</span> [<span class="st">'Crease'</span>, <span class="st">'Begbie'</span>, <span class="st">'Act 1884'</span>]</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>color_map <span class="op">=</span> {<span class="st">'Crease'</span>: <span class="st">'#1f77b4'</span>, <span class="st">'Begbie'</span>: <span class="st">'#d62728'</span>, <span class="st">'Act 1884'</span>: <span class="st">'#2ca02c'</span>}</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_topic_vs_stance(df, topic<span class="op">=</span><span class="st">'labor'</span>, authors<span class="op">=</span><span class="va">None</span>, color_map<span class="op">=</span><span class="va">None</span>, show_regression<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>                         width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">800</span>):</span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> df.copy()</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prepare text wrapping if needed</span></span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">'wrapped_text'</span> <span class="kw">not</span> <span class="kw">in</span> df.columns:</span>
<span id="cb51-13"><a href="#cb51-13" aria-hidden="true" tabindex="-1"></a>        df[<span class="st">'wrapped_text'</span>] <span class="op">=</span> df[<span class="st">'Text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> t: wrap_text(t, width<span class="op">=</span><span class="dv">50</span>))</span>
<span id="cb51-14"><a href="#cb51-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-15"><a href="#cb51-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Default authors if not provided</span></span>
<span id="cb51-16"><a href="#cb51-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> authors <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb51-17"><a href="#cb51-17" aria-hidden="true" tabindex="-1"></a>        authors <span class="op">=</span> df[<span class="st">'Author'</span>].unique()</span>
<span id="cb51-18"><a href="#cb51-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-19"><a href="#cb51-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Function to create a slightly darker variant of a color for regression lines</span></span>
<span id="cb51-20"><a href="#cb51-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> darken_color(color, factor<span class="op">=</span><span class="fl">0.7</span>):</span>
<span id="cb51-21"><a href="#cb51-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb51-22"><a href="#cb51-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(color, <span class="bu">str</span>) <span class="kw">and</span> color.startswith(<span class="st">'#'</span>):</span>
<span id="cb51-23"><a href="#cb51-23" aria-hidden="true" tabindex="-1"></a>            hex_color <span class="op">=</span> color.lstrip(<span class="st">'#'</span>)</span>
<span id="cb51-24"><a href="#cb51-24" aria-hidden="true" tabindex="-1"></a>            rgb <span class="op">=</span> <span class="bu">tuple</span>(<span class="bu">int</span>(hex_color[i:i<span class="op">+</span><span class="dv">2</span>], <span class="dv">16</span>) <span class="cf">for</span> i <span class="kw">in</span> (<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">4</span>))</span>
<span id="cb51-25"><a href="#cb51-25" aria-hidden="true" tabindex="-1"></a>            darkened_rgb <span class="op">=</span> <span class="bu">tuple</span>(<span class="bu">int</span>(c <span class="op">*</span> factor) <span class="cf">for</span> c <span class="kw">in</span> rgb)</span>
<span id="cb51-26"><a href="#cb51-26" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> <span class="ss">f"rgb(</span><span class="sc">{</span>darkened_rgb[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>darkened_rgb[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">, </span><span class="sc">{</span>darkened_rgb[<span class="dv">2</span>]<span class="sc">}</span><span class="ss">)"</span></span>
<span id="cb51-27"><a href="#cb51-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> color  <span class="co"># Return as-is if not hex format</span></span>
<span id="cb51-28"><a href="#cb51-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-29"><a href="#cb51-29" aria-hidden="true" tabindex="-1"></a>    default_colors <span class="op">=</span> [</span>
<span id="cb51-30"><a href="#cb51-30" aria-hidden="true" tabindex="-1"></a>        <span class="st">'#2E86C1'</span>, <span class="st">'#E74C3C'</span>, <span class="st">'#28B463'</span>, <span class="st">'#F39C12'</span>, <span class="st">'#8E44AD'</span>,</span>
<span id="cb51-31"><a href="#cb51-31" aria-hidden="true" tabindex="-1"></a>        <span class="st">'#17A2B8'</span>, <span class="st">'#FD7E14'</span>, <span class="st">'#20C997'</span>, <span class="st">"#87020F"</span>, <span class="st">'#6F42C1'</span></span>
<span id="cb51-32"><a href="#cb51-32" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb51-33"><a href="#cb51-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-34"><a href="#cb51-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> color_map <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb51-35"><a href="#cb51-35" aria-hidden="true" tabindex="-1"></a>        color_map <span class="op">=</span> {author: default_colors[i <span class="op">%</span> <span class="bu">len</span>(default_colors)]</span>
<span id="cb51-36"><a href="#cb51-36" aria-hidden="true" tabindex="-1"></a>                     <span class="cf">for</span> i, author <span class="kw">in</span> <span class="bu">enumerate</span>(authors)}</span>
<span id="cb51-37"><a href="#cb51-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-38"><a href="#cb51-38" aria-hidden="true" tabindex="-1"></a>    <span class="co"># More distinct symbols</span></span>
<span id="cb51-39"><a href="#cb51-39" aria-hidden="true" tabindex="-1"></a>    symbols <span class="op">=</span> [<span class="st">'circle'</span>, <span class="st">'square'</span>, <span class="st">'diamond'</span>, <span class="st">'triangle-up'</span>, <span class="st">'star'</span>, <span class="st">'hexagon'</span>]</span>
<span id="cb51-40"><a href="#cb51-40" aria-hidden="true" tabindex="-1"></a>    author_symbols <span class="op">=</span> {author: symbols[i <span class="op">%</span> <span class="bu">len</span>(symbols)] <span class="cf">for</span> i, author <span class="kw">in</span> <span class="bu">enumerate</span>(authors)}</span>
<span id="cb51-41"><a href="#cb51-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-42"><a href="#cb51-42" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Create subplots with reduced vertical spacing for a tighter layout</span></span>
<span id="cb51-43"><a href="#cb51-43" aria-hidden="true" tabindex="-1"></a>    fig <span class="op">=</span> make_subplots(</span>
<span id="cb51-44"><a href="#cb51-44" aria-hidden="true" tabindex="-1"></a>        rows<span class="op">=</span><span class="dv">2</span>, cols<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-45"><a href="#cb51-45" aria-hidden="true" tabindex="-1"></a>        shared_xaxes<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-46"><a href="#cb51-46" aria-hidden="true" tabindex="-1"></a>        subplot_titles<span class="op">=</span>(<span class="ss">f"Pro Score vs </span><span class="sc">{</span>topic<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Similarity"</span>,</span>
<span id="cb51-47"><a href="#cb51-47" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f"Cons Score vs </span><span class="sc">{</span>topic<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Similarity"</span>),</span>
<span id="cb51-48"><a href="#cb51-48" aria-hidden="true" tabindex="-1"></a>        horizontal_spacing<span class="op">=</span><span class="fl">0.06</span>,</span>
<span id="cb51-49"><a href="#cb51-49" aria-hidden="true" tabindex="-1"></a>        vertical_spacing<span class="op">=</span><span class="fl">0.06</span></span>
<span id="cb51-50"><a href="#cb51-50" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-51"><a href="#cb51-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-52"><a href="#cb51-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Track which authors have data for legend management</span></span>
<span id="cb51-53"><a href="#cb51-53" aria-hidden="true" tabindex="-1"></a>    authors_with_data <span class="op">=</span> <span class="bu">set</span>()</span>
<span id="cb51-54"><a href="#cb51-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-55"><a href="#cb51-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> author <span class="kw">in</span> authors:</span>
<span id="cb51-56"><a href="#cb51-56" aria-hidden="true" tabindex="-1"></a>        df_author <span class="op">=</span> df[df[<span class="st">'Author'</span>] <span class="op">==</span> author]</span>
<span id="cb51-57"><a href="#cb51-57" aria-hidden="true" tabindex="-1"></a>        author_color <span class="op">=</span> color_map.get(author, <span class="st">'#7F8C8D'</span>)</span>
<span id="cb51-58"><a href="#cb51-58" aria-hidden="true" tabindex="-1"></a>        author_symbol <span class="op">=</span> author_symbols.get(author, <span class="st">'circle'</span>)</span>
<span id="cb51-59"><a href="#cb51-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-60"><a href="#cb51-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Pro scores (top subplot)</span></span>
<span id="cb51-61"><a href="#cb51-61" aria-hidden="true" tabindex="-1"></a>        df_pro <span class="op">=</span> df_author.dropna(subset<span class="op">=</span>[<span class="st">'Pro'</span>, topic])</span>
<span id="cb51-62"><a href="#cb51-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(df_pro) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb51-63"><a href="#cb51-63" aria-hidden="true" tabindex="-1"></a>            authors_with_data.add(author)</span>
<span id="cb51-64"><a href="#cb51-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-65"><a href="#cb51-65" aria-hidden="true" tabindex="-1"></a>            fig.add_trace(</span>
<span id="cb51-66"><a href="#cb51-66" aria-hidden="true" tabindex="-1"></a>                go.Scatter(</span>
<span id="cb51-67"><a href="#cb51-67" aria-hidden="true" tabindex="-1"></a>                    x<span class="op">=</span>df_pro[topic],</span>
<span id="cb51-68"><a href="#cb51-68" aria-hidden="true" tabindex="-1"></a>                    y<span class="op">=</span>df_pro[<span class="st">'Pro'</span>],</span>
<span id="cb51-69"><a href="#cb51-69" aria-hidden="true" tabindex="-1"></a>                    mode<span class="op">=</span><span class="st">'markers'</span>,</span>
<span id="cb51-70"><a href="#cb51-70" aria-hidden="true" tabindex="-1"></a>                    marker<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb51-71"><a href="#cb51-71" aria-hidden="true" tabindex="-1"></a>                        color<span class="op">=</span>author_color,</span>
<span id="cb51-72"><a href="#cb51-72" aria-hidden="true" tabindex="-1"></a>                        size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb51-73"><a href="#cb51-73" aria-hidden="true" tabindex="-1"></a>                        opacity<span class="op">=</span><span class="fl">0.85</span>,</span>
<span id="cb51-74"><a href="#cb51-74" aria-hidden="true" tabindex="-1"></a>                        symbol<span class="op">=</span>author_symbol,</span>
<span id="cb51-75"><a href="#cb51-75" aria-hidden="true" tabindex="-1"></a>                        line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'white'</span>, width<span class="op">=</span><span class="fl">1.25</span>)</span>
<span id="cb51-76"><a href="#cb51-76" aria-hidden="true" tabindex="-1"></a>                    ),</span>
<span id="cb51-77"><a href="#cb51-77" aria-hidden="true" tabindex="-1"></a>                    name<span class="op">=</span>author,</span>
<span id="cb51-78"><a href="#cb51-78" aria-hidden="true" tabindex="-1"></a>                    legendgroup<span class="op">=</span>author,</span>
<span id="cb51-79"><a href="#cb51-79" aria-hidden="true" tabindex="-1"></a>                    hovertext<span class="op">=</span>df_pro[<span class="st">'wrapped_text'</span>],</span>
<span id="cb51-80"><a href="#cb51-80" aria-hidden="true" tabindex="-1"></a>                    hovertemplate<span class="op">=</span>(</span>
<span id="cb51-81"><a href="#cb51-81" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f"&lt;b&gt;</span><span class="sc">{</span>author<span class="sc">}</span><span class="ss">&lt;/b&gt;&lt;br&gt;"</span></span>
<span id="cb51-82"><a href="#cb51-82" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f"</span><span class="sc">{</span>topic<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Similarity: %</span><span class="ch">{{</span><span class="ss">x:.3f</span><span class="ch">}}</span><span class="ss">&lt;br&gt;"</span></span>
<span id="cb51-83"><a href="#cb51-83" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"Pro Score: %</span><span class="sc">{y:.3f}</span><span class="st">&lt;br&gt;"</span></span>
<span id="cb51-84"><a href="#cb51-84" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"%</span><span class="sc">{hovertext}</span><span class="st">&lt;br&gt;"</span></span>
<span id="cb51-85"><a href="#cb51-85" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"&lt;extra&gt;&lt;/extra&gt;"</span></span>
<span id="cb51-86"><a href="#cb51-86" aria-hidden="true" tabindex="-1"></a>                    ),</span>
<span id="cb51-87"><a href="#cb51-87" aria-hidden="true" tabindex="-1"></a>                ),</span>
<span id="cb51-88"><a href="#cb51-88" aria-hidden="true" tabindex="-1"></a>                row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">1</span></span>
<span id="cb51-89"><a href="#cb51-89" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb51-90"><a href="#cb51-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-91"><a href="#cb51-91" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Regression</span></span>
<span id="cb51-92"><a href="#cb51-92" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> show_regression <span class="kw">and</span> <span class="bu">len</span>(df_pro) <span class="op">&gt;=</span> <span class="dv">3</span>:</span>
<span id="cb51-93"><a href="#cb51-93" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> df_pro[topic].values</span>
<span id="cb51-94"><a href="#cb51-94" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> df_pro[<span class="st">'Pro'</span>].values</span>
<span id="cb51-95"><a href="#cb51-95" aria-hidden="true" tabindex="-1"></a>                coeffs <span class="op">=</span> np.polyfit(x_vals, y_vals, <span class="dv">1</span>)</span>
<span id="cb51-96"><a href="#cb51-96" aria-hidden="true" tabindex="-1"></a>                x_range <span class="op">=</span> np.linspace(x_vals.<span class="bu">min</span>(), x_vals.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb51-97"><a href="#cb51-97" aria-hidden="true" tabindex="-1"></a>                y_pred <span class="op">=</span> np.polyval(coeffs, x_range)</span>
<span id="cb51-98"><a href="#cb51-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-99"><a href="#cb51-99" aria-hidden="true" tabindex="-1"></a>                fig.add_trace(</span>
<span id="cb51-100"><a href="#cb51-100" aria-hidden="true" tabindex="-1"></a>                    go.Scatter(</span>
<span id="cb51-101"><a href="#cb51-101" aria-hidden="true" tabindex="-1"></a>                        x<span class="op">=</span>x_range,</span>
<span id="cb51-102"><a href="#cb51-102" aria-hidden="true" tabindex="-1"></a>                        y<span class="op">=</span>y_pred,</span>
<span id="cb51-103"><a href="#cb51-103" aria-hidden="true" tabindex="-1"></a>                        mode<span class="op">=</span><span class="st">'lines'</span>,</span>
<span id="cb51-104"><a href="#cb51-104" aria-hidden="true" tabindex="-1"></a>                        line<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb51-105"><a href="#cb51-105" aria-hidden="true" tabindex="-1"></a>                            color<span class="op">=</span>darken_color(author_color),</span>
<span id="cb51-106"><a href="#cb51-106" aria-hidden="true" tabindex="-1"></a>                            width<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb51-107"><a href="#cb51-107" aria-hidden="true" tabindex="-1"></a>                            dash<span class="op">=</span><span class="st">'dot'</span></span>
<span id="cb51-108"><a href="#cb51-108" aria-hidden="true" tabindex="-1"></a>                        ),</span>
<span id="cb51-109"><a href="#cb51-109" aria-hidden="true" tabindex="-1"></a>                        name<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>author<span class="sc">}</span><span class="ss"> trend"</span>,</span>
<span id="cb51-110"><a href="#cb51-110" aria-hidden="true" tabindex="-1"></a>                        legendgroup<span class="op">=</span>author,</span>
<span id="cb51-111"><a href="#cb51-111" aria-hidden="true" tabindex="-1"></a>                        showlegend<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb51-112"><a href="#cb51-112" aria-hidden="true" tabindex="-1"></a>                        hoverinfo<span class="op">=</span><span class="st">'skip'</span></span>
<span id="cb51-113"><a href="#cb51-113" aria-hidden="true" tabindex="-1"></a>                    ),</span>
<span id="cb51-114"><a href="#cb51-114" aria-hidden="true" tabindex="-1"></a>                    row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">1</span></span>
<span id="cb51-115"><a href="#cb51-115" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb51-116"><a href="#cb51-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-117"><a href="#cb51-117" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Cons scores (bottom subplot)</span></span>
<span id="cb51-118"><a href="#cb51-118" aria-hidden="true" tabindex="-1"></a>        df_cons <span class="op">=</span> df_author.dropna(subset<span class="op">=</span>[<span class="st">'Cons'</span>, topic])</span>
<span id="cb51-119"><a href="#cb51-119" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">len</span>(df_cons) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb51-120"><a href="#cb51-120" aria-hidden="true" tabindex="-1"></a>            fig.add_trace(</span>
<span id="cb51-121"><a href="#cb51-121" aria-hidden="true" tabindex="-1"></a>                go.Scatter(</span>
<span id="cb51-122"><a href="#cb51-122" aria-hidden="true" tabindex="-1"></a>                    x<span class="op">=</span>df_cons[topic],</span>
<span id="cb51-123"><a href="#cb51-123" aria-hidden="true" tabindex="-1"></a>                    y<span class="op">=</span>df_cons[<span class="st">'Cons'</span>],</span>
<span id="cb51-124"><a href="#cb51-124" aria-hidden="true" tabindex="-1"></a>                    mode<span class="op">=</span><span class="st">'markers'</span>,</span>
<span id="cb51-125"><a href="#cb51-125" aria-hidden="true" tabindex="-1"></a>                    marker<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb51-126"><a href="#cb51-126" aria-hidden="true" tabindex="-1"></a>                        color<span class="op">=</span>author_color,</span>
<span id="cb51-127"><a href="#cb51-127" aria-hidden="true" tabindex="-1"></a>                        size<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb51-128"><a href="#cb51-128" aria-hidden="true" tabindex="-1"></a>                        opacity<span class="op">=</span><span class="fl">0.85</span>,</span>
<span id="cb51-129"><a href="#cb51-129" aria-hidden="true" tabindex="-1"></a>                        symbol<span class="op">=</span>author_symbol,</span>
<span id="cb51-130"><a href="#cb51-130" aria-hidden="true" tabindex="-1"></a>                        line<span class="op">=</span><span class="bu">dict</span>(color<span class="op">=</span><span class="st">'white'</span>, width<span class="op">=</span><span class="fl">1.25</span>)</span>
<span id="cb51-131"><a href="#cb51-131" aria-hidden="true" tabindex="-1"></a>                    ),</span>
<span id="cb51-132"><a href="#cb51-132" aria-hidden="true" tabindex="-1"></a>                    name<span class="op">=</span>author,</span>
<span id="cb51-133"><a href="#cb51-133" aria-hidden="true" tabindex="-1"></a>                    legendgroup<span class="op">=</span>author,</span>
<span id="cb51-134"><a href="#cb51-134" aria-hidden="true" tabindex="-1"></a>                    showlegend<span class="op">=</span><span class="va">False</span>, </span>
<span id="cb51-135"><a href="#cb51-135" aria-hidden="true" tabindex="-1"></a>                    hovertext<span class="op">=</span>df_cons[<span class="st">'wrapped_text'</span>],</span>
<span id="cb51-136"><a href="#cb51-136" aria-hidden="true" tabindex="-1"></a>                    hovertemplate<span class="op">=</span>(</span>
<span id="cb51-137"><a href="#cb51-137" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f"&lt;b&gt;</span><span class="sc">{</span>author<span class="sc">}</span><span class="ss">&lt;/b&gt;&lt;br&gt;"</span></span>
<span id="cb51-138"><a href="#cb51-138" aria-hidden="true" tabindex="-1"></a>                        <span class="ss">f"</span><span class="sc">{</span>topic<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Similarity: %</span><span class="ch">{{</span><span class="ss">x:.3f</span><span class="ch">}}</span><span class="ss">&lt;br&gt;"</span></span>
<span id="cb51-139"><a href="#cb51-139" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"Cons Score: %</span><span class="sc">{y:.3f}</span><span class="st">&lt;br&gt;"</span></span>
<span id="cb51-140"><a href="#cb51-140" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"%</span><span class="sc">{hovertext}</span><span class="st">&lt;br&gt;"</span></span>
<span id="cb51-141"><a href="#cb51-141" aria-hidden="true" tabindex="-1"></a>                        <span class="st">"&lt;extra&gt;&lt;/extra&gt;"</span></span>
<span id="cb51-142"><a href="#cb51-142" aria-hidden="true" tabindex="-1"></a>                    ),</span>
<span id="cb51-143"><a href="#cb51-143" aria-hidden="true" tabindex="-1"></a>                ),</span>
<span id="cb51-144"><a href="#cb51-144" aria-hidden="true" tabindex="-1"></a>                row<span class="op">=</span><span class="dv">2</span>, col<span class="op">=</span><span class="dv">1</span></span>
<span id="cb51-145"><a href="#cb51-145" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb51-146"><a href="#cb51-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-147"><a href="#cb51-147" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> show_regression <span class="kw">and</span> <span class="bu">len</span>(df_cons) <span class="op">&gt;=</span> <span class="dv">3</span>:</span>
<span id="cb51-148"><a href="#cb51-148" aria-hidden="true" tabindex="-1"></a>                x_vals <span class="op">=</span> df_cons[topic].values</span>
<span id="cb51-149"><a href="#cb51-149" aria-hidden="true" tabindex="-1"></a>                y_vals <span class="op">=</span> df_cons[<span class="st">'Cons'</span>].values</span>
<span id="cb51-150"><a href="#cb51-150" aria-hidden="true" tabindex="-1"></a>                coeffs <span class="op">=</span> np.polyfit(x_vals, y_vals, <span class="dv">1</span>)</span>
<span id="cb51-151"><a href="#cb51-151" aria-hidden="true" tabindex="-1"></a>                x_range <span class="op">=</span> np.linspace(x_vals.<span class="bu">min</span>(), x_vals.<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb51-152"><a href="#cb51-152" aria-hidden="true" tabindex="-1"></a>                y_pred <span class="op">=</span> np.polyval(coeffs, x_range)</span>
<span id="cb51-153"><a href="#cb51-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-154"><a href="#cb51-154" aria-hidden="true" tabindex="-1"></a>                fig.add_trace(</span>
<span id="cb51-155"><a href="#cb51-155" aria-hidden="true" tabindex="-1"></a>                    go.Scatter(</span>
<span id="cb51-156"><a href="#cb51-156" aria-hidden="true" tabindex="-1"></a>                        x<span class="op">=</span>x_range,</span>
<span id="cb51-157"><a href="#cb51-157" aria-hidden="true" tabindex="-1"></a>                        y<span class="op">=</span>y_pred,</span>
<span id="cb51-158"><a href="#cb51-158" aria-hidden="true" tabindex="-1"></a>                        mode<span class="op">=</span><span class="st">'lines'</span>,</span>
<span id="cb51-159"><a href="#cb51-159" aria-hidden="true" tabindex="-1"></a>                        line<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb51-160"><a href="#cb51-160" aria-hidden="true" tabindex="-1"></a>                            color<span class="op">=</span>darken_color(author_color),</span>
<span id="cb51-161"><a href="#cb51-161" aria-hidden="true" tabindex="-1"></a>                            width<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb51-162"><a href="#cb51-162" aria-hidden="true" tabindex="-1"></a>                            dash<span class="op">=</span><span class="st">'dot'</span></span>
<span id="cb51-163"><a href="#cb51-163" aria-hidden="true" tabindex="-1"></a>                        ),</span>
<span id="cb51-164"><a href="#cb51-164" aria-hidden="true" tabindex="-1"></a>                        showlegend<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb51-165"><a href="#cb51-165" aria-hidden="true" tabindex="-1"></a>                        hoverinfo<span class="op">=</span><span class="st">'skip'</span></span>
<span id="cb51-166"><a href="#cb51-166" aria-hidden="true" tabindex="-1"></a>                    ),</span>
<span id="cb51-167"><a href="#cb51-167" aria-hidden="true" tabindex="-1"></a>                    row<span class="op">=</span><span class="dv">2</span>, col<span class="op">=</span><span class="dv">1</span></span>
<span id="cb51-168"><a href="#cb51-168" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb51-169"><a href="#cb51-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-170"><a href="#cb51-170" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update axes styling</span></span>
<span id="cb51-171"><a href="#cb51-171" aria-hidden="true" tabindex="-1"></a>    fig.update_xaxes(</span>
<span id="cb51-172"><a href="#cb51-172" aria-hidden="true" tabindex="-1"></a>        title_text<span class="op">=</span><span class="st">""</span>,</span>
<span id="cb51-173"><a href="#cb51-173" aria-hidden="true" tabindex="-1"></a>        showgrid<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-174"><a href="#cb51-174" aria-hidden="true" tabindex="-1"></a>        gridwidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-175"><a href="#cb51-175" aria-hidden="true" tabindex="-1"></a>        gridcolor<span class="op">=</span><span class="st">'rgba(128,128,128,0.18)'</span>,</span>
<span id="cb51-176"><a href="#cb51-176" aria-hidden="true" tabindex="-1"></a>        zeroline<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-177"><a href="#cb51-177" aria-hidden="true" tabindex="-1"></a>        zerolinewidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-178"><a href="#cb51-178" aria-hidden="true" tabindex="-1"></a>        zerolinecolor<span class="op">=</span><span class="st">'rgba(128,128,128,0.25)'</span>,</span>
<span id="cb51-179"><a href="#cb51-179" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">1</span></span>
<span id="cb51-180"><a href="#cb51-180" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-181"><a href="#cb51-181" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb51-182"><a href="#cb51-182" aria-hidden="true" tabindex="-1"></a>    fig.update_xaxes(</span>
<span id="cb51-183"><a href="#cb51-183" aria-hidden="true" tabindex="-1"></a>        title_text<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>topic<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Topic Similarity"</span>,</span>
<span id="cb51-184"><a href="#cb51-184" aria-hidden="true" tabindex="-1"></a>        showgrid<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-185"><a href="#cb51-185" aria-hidden="true" tabindex="-1"></a>        gridwidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-186"><a href="#cb51-186" aria-hidden="true" tabindex="-1"></a>        gridcolor<span class="op">=</span><span class="st">'rgba(128,128,128,0.18)'</span>,</span>
<span id="cb51-187"><a href="#cb51-187" aria-hidden="true" tabindex="-1"></a>        zeroline<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-188"><a href="#cb51-188" aria-hidden="true" tabindex="-1"></a>        zerolinewidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-189"><a href="#cb51-189" aria-hidden="true" tabindex="-1"></a>        zerolinecolor<span class="op">=</span><span class="st">'rgba(128,128,128,0.25)'</span>,</span>
<span id="cb51-190"><a href="#cb51-190" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">2</span>, col<span class="op">=</span><span class="dv">1</span></span>
<span id="cb51-191"><a href="#cb51-191" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-192"><a href="#cb51-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-193"><a href="#cb51-193" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(</span>
<span id="cb51-194"><a href="#cb51-194" aria-hidden="true" tabindex="-1"></a>        title_text<span class="op">=</span><span class="st">"Pro Score"</span>,</span>
<span id="cb51-195"><a href="#cb51-195" aria-hidden="true" tabindex="-1"></a>        showgrid<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-196"><a href="#cb51-196" aria-hidden="true" tabindex="-1"></a>        gridwidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-197"><a href="#cb51-197" aria-hidden="true" tabindex="-1"></a>        gridcolor<span class="op">=</span><span class="st">'rgba(128,128,128,0.18)'</span>,</span>
<span id="cb51-198"><a href="#cb51-198" aria-hidden="true" tabindex="-1"></a>        zeroline<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-199"><a href="#cb51-199" aria-hidden="true" tabindex="-1"></a>        zerolinewidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-200"><a href="#cb51-200" aria-hidden="true" tabindex="-1"></a>        zerolinecolor<span class="op">=</span><span class="st">'rgba(128,128,128,0.25)'</span>,</span>
<span id="cb51-201"><a href="#cb51-201" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">1</span>, col<span class="op">=</span><span class="dv">1</span></span>
<span id="cb51-202"><a href="#cb51-202" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-203"><a href="#cb51-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-204"><a href="#cb51-204" aria-hidden="true" tabindex="-1"></a>    fig.update_yaxes(</span>
<span id="cb51-205"><a href="#cb51-205" aria-hidden="true" tabindex="-1"></a>        title_text<span class="op">=</span><span class="st">"Cons Score"</span>,</span>
<span id="cb51-206"><a href="#cb51-206" aria-hidden="true" tabindex="-1"></a>        showgrid<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-207"><a href="#cb51-207" aria-hidden="true" tabindex="-1"></a>        gridwidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-208"><a href="#cb51-208" aria-hidden="true" tabindex="-1"></a>        gridcolor<span class="op">=</span><span class="st">'rgba(128,128,128,0.18)'</span>,</span>
<span id="cb51-209"><a href="#cb51-209" aria-hidden="true" tabindex="-1"></a>        zeroline<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb51-210"><a href="#cb51-210" aria-hidden="true" tabindex="-1"></a>        zerolinewidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-211"><a href="#cb51-211" aria-hidden="true" tabindex="-1"></a>        zerolinecolor<span class="op">=</span><span class="st">'rgba(128,128,128,0.25)'</span>,</span>
<span id="cb51-212"><a href="#cb51-212" aria-hidden="true" tabindex="-1"></a>        row<span class="op">=</span><span class="dv">2</span>, col<span class="op">=</span><span class="dv">1</span></span>
<span id="cb51-213"><a href="#cb51-213" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-214"><a href="#cb51-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-215"><a href="#cb51-215" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Layout: title moved to top left, legend moved to top right</span></span>
<span id="cb51-216"><a href="#cb51-216" aria-hidden="true" tabindex="-1"></a>    fig.update_layout(</span>
<span id="cb51-217"><a href="#cb51-217" aria-hidden="true" tabindex="-1"></a>        width<span class="op">=</span>width,</span>
<span id="cb51-218"><a href="#cb51-218" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span>height,</span>
<span id="cb51-219"><a href="#cb51-219" aria-hidden="true" tabindex="-1"></a>        title<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb51-220"><a href="#cb51-220" aria-hidden="true" tabindex="-1"></a>            text<span class="op">=</span><span class="ss">f"Stance Towards Chinese Immigrants vs </span><span class="sc">{</span>topic<span class="sc">.</span>title()<span class="sc">}</span><span class="ss"> Topic Similarity"</span>,</span>
<span id="cb51-221"><a href="#cb51-221" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span><span class="fl">0.02</span>,  <span class="co"># Move title to top left</span></span>
<span id="cb51-222"><a href="#cb51-222" aria-hidden="true" tabindex="-1"></a>            xanchor<span class="op">=</span><span class="st">'left'</span>,</span>
<span id="cb51-223"><a href="#cb51-223" aria-hidden="true" tabindex="-1"></a>            font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">16</span>, color<span class="op">=</span><span class="st">'#2C3E50'</span>),</span>
<span id="cb51-224"><a href="#cb51-224" aria-hidden="true" tabindex="-1"></a>            pad<span class="op">=</span><span class="bu">dict</span>(t<span class="op">=</span><span class="dv">12</span>, b<span class="op">=</span><span class="dv">20</span>)</span>
<span id="cb51-225"><a href="#cb51-225" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb51-226"><a href="#cb51-226" aria-hidden="true" tabindex="-1"></a>        legend<span class="op">=</span><span class="bu">dict</span>(</span>
<span id="cb51-227"><a href="#cb51-227" aria-hidden="true" tabindex="-1"></a>            title<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb51-228"><a href="#cb51-228" aria-hidden="true" tabindex="-1"></a>            orientation<span class="op">=</span><span class="st">'v'</span>,  <span class="co"># Vertical orientation for top right</span></span>
<span id="cb51-229"><a href="#cb51-229" aria-hidden="true" tabindex="-1"></a>            y<span class="op">=</span><span class="fl">0.98</span>,  <span class="co"># Position at top</span></span>
<span id="cb51-230"><a href="#cb51-230" aria-hidden="true" tabindex="-1"></a>            yanchor<span class="op">=</span><span class="st">'top'</span>,</span>
<span id="cb51-231"><a href="#cb51-231" aria-hidden="true" tabindex="-1"></a>            x<span class="op">=</span><span class="fl">0.98</span>,  <span class="co"># Position at right</span></span>
<span id="cb51-232"><a href="#cb51-232" aria-hidden="true" tabindex="-1"></a>            xanchor<span class="op">=</span><span class="st">'right'</span>,</span>
<span id="cb51-233"><a href="#cb51-233" aria-hidden="true" tabindex="-1"></a>            bgcolor<span class="op">=</span><span class="st">'rgba(255,255,255,0.92)'</span>,</span>
<span id="cb51-234"><a href="#cb51-234" aria-hidden="true" tabindex="-1"></a>            bordercolor<span class="op">=</span><span class="st">'rgba(52,73,94,0.12)'</span>,</span>
<span id="cb51-235"><a href="#cb51-235" aria-hidden="true" tabindex="-1"></a>            borderwidth<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb51-236"><a href="#cb51-236" aria-hidden="true" tabindex="-1"></a>            font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">11</span>),</span>
<span id="cb51-237"><a href="#cb51-237" aria-hidden="true" tabindex="-1"></a>            traceorder<span class="op">=</span><span class="st">'normal'</span></span>
<span id="cb51-238"><a href="#cb51-238" aria-hidden="true" tabindex="-1"></a>        ),</span>
<span id="cb51-239"><a href="#cb51-239" aria-hidden="true" tabindex="-1"></a>        template<span class="op">=</span><span class="st">"plotly_white"</span>,</span>
<span id="cb51-240"><a href="#cb51-240" aria-hidden="true" tabindex="-1"></a>        plot_bgcolor<span class="op">=</span><span class="st">'rgba(248,249,250,0.95)'</span>,</span>
<span id="cb51-241"><a href="#cb51-241" aria-hidden="true" tabindex="-1"></a>        margin<span class="op">=</span><span class="bu">dict</span>(t<span class="op">=</span><span class="dv">90</span>, b<span class="op">=</span><span class="dv">80</span>, l<span class="op">=</span><span class="dv">60</span>, r<span class="op">=</span><span class="dv">60</span>),</span>
<span id="cb51-242"><a href="#cb51-242" aria-hidden="true" tabindex="-1"></a>        font<span class="op">=</span><span class="bu">dict</span>(family<span class="op">=</span><span class="st">"Arial, sans-serif"</span>, color<span class="op">=</span><span class="st">'#2C3E50'</span>)</span>
<span id="cb51-243"><a href="#cb51-243" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-244"><a href="#cb51-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-245"><a href="#cb51-245" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Subplot title styling (keep compact)</span></span>
<span id="cb51-246"><a href="#cb51-246" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(fig.layout.annotations) <span class="op">&gt;=</span> <span class="dv">2</span>:</span>
<span id="cb51-247"><a href="#cb51-247" aria-hidden="true" tabindex="-1"></a>        fig.layout.annotations[<span class="dv">0</span>].update(font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">13</span>, color<span class="op">=</span><span class="st">'#34495E'</span>))</span>
<span id="cb51-248"><a href="#cb51-248" aria-hidden="true" tabindex="-1"></a>        fig.layout.annotations[<span class="dv">1</span>].update(font<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">13</span>, color<span class="op">=</span><span class="st">'#34495E'</span>))</span>
<span id="cb51-249"><a href="#cb51-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-250"><a href="#cb51-250" aria-hidden="true" tabindex="-1"></a>    fig.show()</span>
<span id="cb51-251"><a href="#cb51-251" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="52">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example usage for all four topics:</span></span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>plot_topic_vs_stance(merged_df, topic<span class="op">=</span><span class="st">"labor"</span>, color_map<span class="op">=</span>color_map, show_regression<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="53">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization for legislation</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>plot_topic_vs_stance(merged_df, topic<span class="op">=</span><span class="st">"legislation"</span>, color_map<span class="op">=</span>color_map, show_regression<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="54">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization for license</span></span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>plot_topic_vs_stance(merged_df, topic<span class="op">=</span><span class="st">"license"</span>, color_map<span class="op">=</span>color_map, show_regression<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="55">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create visualization for taxation</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>plot_topic_vs_stance(merged_df, topic<span class="op">=</span><span class="st">"taxation"</span>, color_map<span class="op">=</span>color_map, show_regression<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Correlation coefficients will be calculated to quantify the strength and direction of the relationship between topic alignment and each stance scores. A positive correlation indicates that as the topic alignment score increases, the stance score also tends to increase (e.g., more pro‑discriminatory language), while a negative correlation indicates the opposite (e.g., more anti‑discriminatory language). The closer the correlation coefficient is to 1 or -1, the stronger the relationship between the topic alignment and stance scores.</p>
<p>The math behind correlation coefficients is as follows: <span class="math display">\[
\text{corr}(X, Y) = \frac{\text{cov}(X, Y)}{\sigma_X \cdot \sigma_Y}
\]</span> Where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the topic alignment scores and stance scores, respectively, <span class="math inline">\(\text{cov}(X, Y)\)</span> is the covariance between the two variables, and <span class="math inline">\(\sigma_X\)</span> and <span class="math inline">\(\sigma_Y\)</span> are their standard deviations.</p>
<div class="cell" data-execution_count="56">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize the correlation between topic similarity and stance scores</span></span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>correlation_results <span class="op">=</span> []</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Overall correlations (across all authors)</span></span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic <span class="kw">in</span> [<span class="st">'labor'</span>, <span class="st">'legislation'</span>, <span class="st">'license'</span>, <span class="st">'taxation'</span>]:</span>
<span id="cb56-6"><a href="#cb56-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> stance <span class="kw">in</span> [<span class="st">'Pro'</span>, <span class="st">'Cons'</span>]:</span>
<span id="cb56-7"><a href="#cb56-7" aria-hidden="true" tabindex="-1"></a>        corr <span class="op">=</span> merged_df[[topic, stance]].corr().iloc[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb56-8"><a href="#cb56-8" aria-hidden="true" tabindex="-1"></a>        correlation_results.append({</span>
<span id="cb56-9"><a href="#cb56-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Author'</span>: <span class="st">'All Authors'</span>,</span>
<span id="cb56-10"><a href="#cb56-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Topic'</span>: topic,</span>
<span id="cb56-11"><a href="#cb56-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Stance'</span>: stance,</span>
<span id="cb56-12"><a href="#cb56-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">'Correlation'</span>: corr</span>
<span id="cb56-13"><a href="#cb56-13" aria-hidden="true" tabindex="-1"></a>        })</span>
<span id="cb56-14"><a href="#cb56-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-15"><a href="#cb56-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Correlations by individual author</span></span>
<span id="cb56-16"><a href="#cb56-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> author <span class="kw">in</span> merged_df[<span class="st">'Author'</span>].unique():</span>
<span id="cb56-17"><a href="#cb56-17" aria-hidden="true" tabindex="-1"></a>    author_data <span class="op">=</span> merged_df[merged_df[<span class="st">'Author'</span>] <span class="op">==</span> author]</span>
<span id="cb56-18"><a href="#cb56-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> topic <span class="kw">in</span> [<span class="st">'labor'</span>, <span class="st">'legislation'</span>, <span class="st">'license'</span>, <span class="st">'taxation'</span>]:</span>
<span id="cb56-19"><a href="#cb56-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> stance <span class="kw">in</span> [<span class="st">'Pro'</span>, <span class="st">'Cons'</span>]:</span>
<span id="cb56-20"><a href="#cb56-20" aria-hidden="true" tabindex="-1"></a>            corr <span class="op">=</span> author_data[[topic, stance]].corr().iloc[<span class="dv">0</span>, <span class="dv">1</span>]</span>
<span id="cb56-21"><a href="#cb56-21" aria-hidden="true" tabindex="-1"></a>            correlation_results.append({</span>
<span id="cb56-22"><a href="#cb56-22" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Author'</span>: author,</span>
<span id="cb56-23"><a href="#cb56-23" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Topic'</span>: topic,</span>
<span id="cb56-24"><a href="#cb56-24" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Stance'</span>: stance,</span>
<span id="cb56-25"><a href="#cb56-25" aria-hidden="true" tabindex="-1"></a>                <span class="st">'Correlation'</span>: corr</span>
<span id="cb56-26"><a href="#cb56-26" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb56-27"><a href="#cb56-27" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb56-28"><a href="#cb56-28" aria-hidden="true" tabindex="-1"></a>correlation_df <span class="op">=</span> pd.DataFrame(correlation_results)</span>
<span id="cb56-29"><a href="#cb56-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-30"><a href="#cb56-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Pivot the DataFrame to show correlations in a more readable format</span></span>
<span id="cb56-31"><a href="#cb56-31" aria-hidden="true" tabindex="-1"></a>correlation_pivot <span class="op">=</span> correlation_df.pivot_table(</span>
<span id="cb56-32"><a href="#cb56-32" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>[<span class="st">'Author'</span>, <span class="st">'Topic'</span>], </span>
<span id="cb56-33"><a href="#cb56-33" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span><span class="st">'Stance'</span>, </span>
<span id="cb56-34"><a href="#cb56-34" aria-hidden="true" tabindex="-1"></a>    values<span class="op">=</span><span class="st">'Correlation'</span></span>
<span id="cb56-35"><a href="#cb56-35" aria-hidden="true" tabindex="-1"></a>).reset_index()</span>
<span id="cb56-36"><a href="#cb56-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-37"><a href="#cb56-37" aria-hidden="true" tabindex="-1"></a>correlation_pivot.columns.name <span class="op">=</span> <span class="va">None</span>  <span class="co"># Remove the name of the columns index</span></span>
<span id="cb56-38"><a href="#cb56-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-39"><a href="#cb56-39" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to wide format for better display</span></span>
<span id="cb56-40"><a href="#cb56-40" aria-hidden="true" tabindex="-1"></a>correlation_wide <span class="op">=</span> correlation_pivot.pivot_table(</span>
<span id="cb56-41"><a href="#cb56-41" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span><span class="st">'Author'</span>,</span>
<span id="cb56-42"><a href="#cb56-42" aria-hidden="true" tabindex="-1"></a>    columns<span class="op">=</span><span class="st">'Topic'</span>,</span>
<span id="cb56-43"><a href="#cb56-43" aria-hidden="true" tabindex="-1"></a>    values<span class="op">=</span>[<span class="st">'Pro'</span>, <span class="st">'Cons'</span>]</span>
<span id="cb56-44"><a href="#cb56-44" aria-hidden="true" tabindex="-1"></a>).<span class="bu">round</span>(<span class="dv">4</span>)</span>
<span id="cb56-45"><a href="#cb56-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb56-46"><a href="#cb56-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Correlation Coefficients of Topic Alignments and Pro/Cons Scores by Each Author:"</span>)</span>
<span id="cb56-47"><a href="#cb56-47" aria-hidden="true" tabindex="-1"></a>correlation_wide</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>These enable us to see how different topics relate to the stances expressed in the texts, and whether certain topics are more associated with pro‑, neutral, or anti‑discriminatory language.</p>
</section>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">Conclusion</h3>
<p>In this workshop, we explored how to effectively digitalize aged text document, as well as how to use various computational techniques to analyze historical legal texts, focusing on the stance of different authors towards Chinese immigrants in British Columbia. We applied a range of methods, including TF-IDF analysis, word embeddings, stance embeddings, zero-shot classification, and topic modeling, to gain insights into the language used in these texts and the positions expressed by different authors.</p>
<p>The results of our analysis matched the expectations based on previous historical research, confirming that Crease and Begbie indeed held similar positions on the issue, while the Chinese Regulation Act reflected a more discriminatory stance (with higher “Cons” scores overall). In addition, we found interesting clustering patterns in the UMAP projection of the stance embeddings, which poses further questions for investigation, such as what each UMAP cluster represents and what each axis in the UMAP projection means. We also identified several key topics that were prominent in the texts, and explored how these topics relate to the stances expressed in the texts using zero-shot classification scores and topic alignment scores.</p>
<p>However, we must note that these techniques are not perfect and have limitations, as the pre-trained models may contain biases and may not accurately capture the nuances of historical legal language. We also noted that the topic alignment scores and zero-shot classification scores are not deterministic, as they depend on the specific language used in defining the candidate labels and the context in which the text is used. Therefore, it is essential to interpret the results with caution and consider the limitations of the models used. Machine learning techniques should be used as auxiliary tools in research, and human interpretation and analysis are still crucial for understanding the positions expressed in historical texts.</p>
<p>In conclusion, this workshop displayed a new perspective on how we can quantitatively analyze classical historical materials, but it also highlighted the limitations of these techniques and the importance of human interpretation in understanding the positions expressed in historical texts. We hope that this workshop has provided you with valuable insights into how computational techniques can be used to analyze historical legal texts, and how they can complement traditional research methods in the humanities and social sciences.</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ol type="1">
<li><em>Regina v. Wing Chong</em>, 1 B.C.R. Pt. II 150 (1885).</li>
<li><em>Wong Hoy Woon v. Duncan</em>, 3 B.C.R. 318 (1894).</li>
<li><em>Regina v. Mee Wah</em>, 3 B.C.R. 403 (1886).</li>
<li><em>Regina v. Corporation of Victoria</em>, 1 B.C.R. Pt. II 331 (1888).</li>
<li><em>Quong Wing v. The King</em>, 49 S.C.R. 440 (1914).</li>
<li>Law Society of British Columbia. (1896). <em>The British Columbia Reports: Being reports of cases determined in the Supreme and County Courts and in Admiralty and on appeal in the Full Court and Divisional Court</em> (Vol. 3). Victoria, BC: The Province Publishing Company.</li>
<li>Canada. Royal Commission on Chinese Immigration. (1885). <em>Report of the Royal Commission on Chinese Immigration: report and evidence</em>. Ottawa: Printed by order of the Commission.</li>
<li>Thomas, P. (2012, June 12–14). Courts of last resort: The judicialization of Asian Canadian politics 1878 to 1913. Paper presented at the Annual Conference of the Canadian Political Science Association, University of Alberta, Edmonton, Canada. Retrieved from <a href="https://cpsa-acsp.ca/papers-2012/Thomas-Paul.pdf" class="uri">https://cpsa-acsp.ca/papers-2012/Thomas-Paul.pdf</a></li>
<li>McLaren, J. P. S. (1991). The Early British Columbia Supreme Court and the “Chinese Question”: Echoes of the rule of law. Manitoba Law Journal, 20(1), 107–147. Retrieved from <a href="https://www.canlii.org/w/canlii/1991CanLIIDocs168.pdf" class="uri">https://www.canlii.org/w/canlii/1991CanLIIDocs168.pdf</a></li>
<li>Chalkidis, I., Fergadiotis, M., Malakasiotis, P., Aletras, N., &amp; Androutsopoulos, I. (2020). LEGAL-BERT: The Muppets straight out of Law School (arXiv:2010.02559). arXiv. <a href="https://doi.org/10.48550/arXiv.2010.02559" class="uri">https://doi.org/10.48550/arXiv.2010.02559</a></li>
<li>Loo, T. (1994). Crease, Sir Henry Pering Pellew. In Dictionary of Canadian Biography (Vol. 13). University of Toronto/Université Laval. Retrieved August 8, 2025, from <a href="https://www.biographi.ca/en/bio/crease_henry_pering_pellew_13E.html" class="uri">https://www.biographi.ca/en/bio/crease_henry_pering_pellew_13E.html</a></li>
<li>Williams, D. R. (1990). Begbie, Sir Matthew Baillie. In Dictionary of Canadian Biography (Vol. 12). University of Toronto/Université Laval. Retrieved August 8, 2025, from <a href="https://www.biographi.ca/en/bio/begbie_matthew_baillie_12E.html" class="uri">https://www.biographi.ca/en/bio/begbie_matthew_baillie_12E.html</a></li>
<li>Ariai, F., Mackenzie, J., &amp; De Martini, G. (2025). <em>Natural Language Processing for the legal domain: A survey of tasks, datasets, models and challenges</em>. arXiv preprint arXiv:2410.21306.</li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png"></a>.  <a rel="license" href="https://comet.arts.ubc.ca/pages/copyright.html">See details.</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 The prAxIs Project and UBC are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples.
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>