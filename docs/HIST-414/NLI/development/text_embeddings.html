<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-GB" xml:lang="en-GB"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Praxis - Text Embedding Analysis Through Legal-BERT</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../../">
<link href="../../../../media/praxis-badge.png" rel="icon" type="image/png">
<script src="../../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<meta name="keywords" content="economics, econometrics, R, data, machine learning, UBC, COMET, geog 374, econ 325, econ 326, learning, teaching, learn r, r help, help, tutorial, r tutorial for beginners,learning statistics with r, learn r programming, learn statistics, linear regression, r machine learning, learn machine learning, university of british columbia, british columbia, r programming for beginners, r language tutorial, r tutorial for beginners, economic data, econometrics tutoring, economics help for students, economics homework help, oer resources for teachers, open educational resources for teachers, educational resource, oer project, oer materials, oer resources, learn economics online, learn econometrics, teach yourself economics, teach yourself econometrics, econometrics basics for beginners">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


<link rel="stylesheet" href="../../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../../media/praxis-badge-white.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../../index.html">
    <span class="navbar-title">Praxis</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-get-startedplaceholder_1" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Get Started/Placeholder_1</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-get-startedplaceholder_1">    
        <li>
    <a class="dropdown-item" href="../../../../pages/quickstart.html" rel="" target="">
 <span class="dropdown-text">Quickstart Guide</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-courses" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Courses</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-courses">    
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_HIST-414.html" rel="" target="">
 <span class="dropdown-text">HIST-414</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_AMNE-376.html" rel="" target="">
 <span class="dropdown-text">AMNE-376</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_AMNE-170.html" rel="" target="">
 <span class="dropdown-text">AMNE-170</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_SOCI415.html" rel="" target="">
 <span class="dropdown-text">SOCI-415</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_SOCI217.html" rel="" target="">
 <span class="dropdown-text">SOCI-217</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_SOCI280.html" rel="" target="">
 <span class="dropdown-text">SOCI-280</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_ECON227.html" rel="" target="">
 <span class="dropdown-text">ECON-227</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="../../../../pages/index/all.html" rel="" target="">
 <span class="dropdown-text">Browse All</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-topics" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Topics</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-topics">    
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_CNN.html" rel="" target="">
 <span class="dropdown-text">CNN</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_convolution.html" rel="" target="">
 <span class="dropdown-text">Convolution</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/index/index_word_embeddings.html" rel="" target="">
 <span class="dropdown-text">Word Embeddings</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-teach-with-praxis" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Teach With prAxIs</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-teach-with-praxis">    
        <li>
    <a class="dropdown-item" href="../../../../pages/teaching_with_comet.html" rel="" target="">
 <span class="dropdown-text">Learn how to teach with prAxIs</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/using_comet.html" rel="" target="">
 <span class="dropdown-text">Using prAxIs in the Classroom</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-launch-praxis" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
      <i class="bi bi-play" role="img">
</i> 
 <span class="menu-text">Launch prAxIs</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-launch-praxis">    
        <li>
    <a class="dropdown-item" href="https://open.jupyter.ubc.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-notebooks&amp;urlpath=lab%2Ftree%2Fcomet-notebooks%2F&amp;branch=main" rel="" target=""><i class="bi bi-cloud-check" role="img">
</i> 
 <span class="dropdown-text">Launch on JupyterOpen (with Data)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://open.jupyter.ubc.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-project&amp;urlpath=lab%2Ftree%2Fcomet-project%2F&amp;branch=main" rel="" target=""><i class="bi bi-cloud-check" role="img">
</i> 
 <span class="dropdown-text">Launch on JupyterOpen (lite)</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://ubc.syzygy.ca/jupyter/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2Fubcecon%2Fcomet-project&amp;urlpath=lab%2Ftree%2Fcomet-project%2F&amp;branch=main" rel="" target=""><i class="bi bi-gear" role="img">
</i> 
 <span class="dropdown-text">Launch on Syzygy</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://colab.research.google.com/github/ubcecon/comet-notebooks/blob/main/" rel="" target=""><i class="bi bi-google" role="img">
</i> 
 <span class="dropdown-text">Launch on Colab</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/comet-notebooks/archive/refs/heads/main.zip" rel="" target=""><i class="bi bi-cloud-download" role="img">
</i> 
 <span class="dropdown-text">Launch Locally</span></a>
  </li>  
        <li><hr class="dropdown-divider"></li>
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/comet-open/archive/refs/heads/datasets.zip" rel="" target=""><i class="bi bi-clipboard-data" role="img">
</i> 
 <span class="dropdown-text">Project Datasets</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="https://github.com/ubcecon/comet-open" rel="" target="">
 <span class="dropdown-text">Github Repository</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="../../../../#" rel="" target="">
 <span class="menu-text">|</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-about" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">About</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-about">    
        <li>
    <a class="dropdown-item" href="../../../../pages/team.html" rel="" target="">
 <span class="dropdown-text">prAxIs Team</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="../../../../pages/copyright.html" rel="" target="">
 <span class="dropdown-text">Copyright Information</span></a>
  </li>  
    </ul>
  </li>
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-full page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#dataset-reading" id="toc-dataset-reading" class="nav-link active" data-scroll-target="#dataset-reading">Dataset Reading</a></li>
  <li><a href="#naive-word-embedding-analysis" id="toc-naive-word-embedding-analysis" class="nav-link" data-scroll-target="#naive-word-embedding-analysis">Naive Word Embedding Analysis</a></li>
  <li><a href="#text-embedding-and-analysis-of-crease-and-begbie-corpus" id="toc-text-embedding-and-analysis-of-crease-and-begbie-corpus" class="nav-link" data-scroll-target="#text-embedding-and-analysis-of-crease-and-begbie-corpus">Text Embedding and Analysis of Crease and Begbie Corpus</a></li>
  <li><a href="#the-comparison-of-creases-and-begbies-rulings-and-the-chinese-regulation-act-of-1884" id="toc-the-comparison-of-creases-and-begbies-rulings-and-the-chinese-regulation-act-of-1884" class="nav-link" data-scroll-target="#the-comparison-of-creases-and-begbies-rulings-and-the-chinese-regulation-act-of-1884">The Comparison of Crease’s and Begbie’s Rulings and the Chinese Regulation Act of 1884</a></li>
  <li><a href="#zero-shot-stance-classification-with-legal-bert" id="toc-zero-shot-stance-classification-with-legal-bert" class="nav-link" data-scroll-target="#zero-shot-stance-classification-with-legal-bert">Zero-shot Stance Classification with legal-BERT</a></li>
  <li><a href="#domainadaptive-pretraining" id="toc-domainadaptive-pretraining" class="nav-link" data-scroll-target="#domainadaptive-pretraining">Domain‐Adaptive Pretraining</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/ubcecon/praxis-ubc/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content column-page-left" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Text Embedding Analysis Through Legal-BERT</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="dataset-reading" class="level3">
<h3 class="anchored" data-anchor-id="dataset-reading">Dataset Reading</h3>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the dataset</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"../data/metadata.csv"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">filename</th>
<th data-quarto-table-cell-role="th">author</th>
<th data-quarto-table-cell-role="th">type</th>
<th data-quarto-table-cell-role="th">text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>regina_v_wing_chong.txt</td>
<td>Crease</td>
<td>case</td>
<td>CREASE, J. 1885. REGINA v. WING CHONG. \r\n\r\...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>wong_hoy_woon_v_duncan.txt</td>
<td>Crease</td>
<td>case</td>
<td>CREASE, J.\r\n\r\nWONG HOY WOON v. DUNCAN.\r\n...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>regina_v_mee_wah.txt</td>
<td>Begbie</td>
<td>case</td>
<td>BRITISH COLUMBIA REPORTS.\r\n\r\nREGINA v. MEE...</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>regina_v_victoria.txt</td>
<td>Begbie</td>
<td>case</td>
<td>OF BRITISH COLUMBIA.\r\n\r\nREGINA r, CORPORAT...</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>quong_wing_v_the_king.txt</td>
<td>Fitzpatrick</td>
<td>case</td>
<td>QUONG WING v. THE KING. CAN. \r\n\r\nSupreme ...</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="naive-word-embedding-analysis" class="level3">
<h3 class="anchored" data-anchor-id="naive-word-embedding-analysis">Naive Word Embedding Analysis</h3>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to clean the text</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> clean_text(text):</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> text.lower()</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="vs">r'[^\w\s]'</span>, <span class="st">''</span>, text)  <span class="co"># Remove punctuation</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> text.strip()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk.tokenize <span class="im">import</span> word_tokenize</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the large corpus by joining all text from all authors</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>all_text <span class="op">=</span> <span class="st">" "</span>.join(df[<span class="st">"text"</span>].tolist())</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>clean_text <span class="op">=</span> clean_text(all_text)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the tokenizer and model from Hugging Face</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> AutoTokenizer, AutoModel</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We will use the Legal-BERT model for this task</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> AutoTokenizer.from_pretrained(<span class="st">'nlpaueb/legal-bert-base-uncased'</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AutoModel.from_pretrained(<span class="st">'nlpaueb/legal-bert-base-uncased'</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co"># set the model to evaluation mode</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>BertModel(
  (embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
  )
  (encoder): BertEncoder(
    (layer): ModuleList(
      (0-11): 12 x BertLayer(
        (attention): BertAttention(
          (self): BertSdpaSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELUActivation()
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
  )
)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the word embeddings</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Tokenize the cleaned text into words</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> word_tokenize(clean_text)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Get unique words to avoid redundant computation</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>unique_tokens <span class="op">=</span> <span class="bu">list</span>(<span class="bu">set</span>(tokens))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the shape of unique tokens</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'There are </span><span class="sc">{</span><span class="bu">len</span>(unique_tokens)<span class="sc">}</span><span class="ss"> unique tokens in this corpus.'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>There are 4905 unique tokens in this corpus.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare a dictionary to store word embeddings</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>word_embeddings <span class="op">=</span> {}</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Batch processing for efficiency</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>batches <span class="op">=</span> [unique_tokens[i:i <span class="op">+</span> batch_size] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(unique_tokens), batch_size)]</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> batch <span class="kw">in</span> batches:</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Tokenize the batch</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    batch_inputs <span class="op">=</span> tokenizer(batch, return_tensors<span class="op">=</span><span class="st">'pt'</span>, padding<span class="op">=</span><span class="va">True</span>, truncation<span class="op">=</span><span class="va">True</span>, max_length<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        batch_outputs <span class="op">=</span> model(<span class="op">**</span>batch_inputs)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use the [CLS] token embedding as the word embedding</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, word <span class="kw">in</span> <span class="bu">enumerate</span>(batch):</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>            word_embedding <span class="op">=</span> batch_outputs.last_hidden_state[i, <span class="dv">0</span>, :].numpy()</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>            word_embeddings[word] <span class="op">=</span> word_embedding</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
  return forward_call(*args, **kwargs)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print embedding for the word of interest 'chinese'</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Embedding for 'chinese':</span><span class="ch">\n</span><span class="sc">{</span>word_embeddings<span class="sc">.</span>get(<span class="st">'chinese'</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Embedding for 'chinese':
[-6.20929956e-01 -1.41669914e-01  6.38973951e-01  5.66694997e-02
  2.49504417e-01  3.55757445e-01 -9.64457169e-02  3.54798347e-01
 -2.72701204e-01 -6.37608767e-01  1.72130004e-01  5.87600231e-01
  5.80052547e-02 -1.98573306e-01 -6.22220993e-01  6.23441458e-01
 -2.84138501e-01 -2.01131850e-01 -1.16010487e-01  3.39486867e-01
 -1.49681792e-01  4.16030079e-01  4.64206189e-01 -4.62918758e-01
  3.87408853e-01  6.31607652e-01  6.86673462e-01  2.19447494e-01
 -3.76842201e-01  1.29364863e-01 -2.28453383e-01 -2.85087019e-01
  3.50299895e-01  4.33139503e-01 -4.69817609e-01  2.95414388e-01
  5.21553159e-02 -2.85932124e-02  4.41666007e-01  2.89368480e-01
  3.54161978e-01 -7.48490632e-01  7.74222836e-02 -1.15736835e-01
 -1.74301475e-01  1.22696489e-01 -2.15352607e+00 -3.29317093e-01
  1.01303458e-02 -3.54923382e-02 -1.23484582e-01  6.59714282e-01
 -8.31694156e-03  6.29764497e-01  6.69251561e-01 -4.71155584e-01
  7.91467354e-02 -6.24099374e-01 -4.18075711e-01 -6.55549914e-02
 -2.87118733e-01 -7.29588091e-01 -1.68598548e-01  1.50030822e-01
  5.73546231e-01 -3.64863038e-01 -1.75638422e-01 -1.08312464e+00
 -8.72777045e-01  5.20283341e-01 -4.19047356e-01 -1.03994772e-01
 -4.50686663e-01  3.96263808e-01  4.59026873e-01  1.49249226e-01
 -9.73936677e-01  2.40058377e-01  5.98394834e-02  2.12372527e-01
 -5.36928058e-01 -2.63254791e-01 -1.00200519e-01 -2.81153798e-01
 -8.00029695e-01 -2.07510263e-01  1.99834585e-01  4.22091097e-01
 -4.92548853e-01  4.73332137e-01  2.41993725e-01  2.99992532e-01
 -6.62101805e-02  4.97656286e-01  6.22859597e-01 -1.12074003e-01
  2.90469170e-01 -7.17187077e-02  1.00461102e+00 -4.98112887e-01
 -1.79656237e-01  1.90938503e-01 -2.20559061e-01 -1.83572412e-01
  3.41635972e-01  3.36762846e-01  3.71979505e-01  2.52298228e-02
 -2.27320552e-01 -9.94918868e-02 -3.79481077e-01  1.15087226e-01
 -5.67131303e-02 -1.43991798e-01  2.93666482e-01 -2.44778529e-01
 -1.55897692e-01  3.21055874e-02  1.62579507e-01 -2.96471626e-01
  3.47169191e-01  2.43933558e-01  3.24788690e-01  5.33173680e-01
  7.26383626e-01 -2.29867995e-01 -3.03071707e-01 -2.68376768e-01
 -6.95385337e-01 -4.25722748e-01  4.37337130e-01  6.58007935e-02
  3.43149225e-03  3.50311279e-01 -6.52863622e-01  8.04584146e-01
 -7.14763224e-01 -3.13350976e-01 -4.31784183e-01 -1.15973637e-01
  5.64839691e-02  3.03973496e-01  9.12815481e-02  4.21348840e-01
  1.21128201e-01  1.19176537e-01 -1.22158878e-01  8.35701168e-01
 -4.48836505e-01  2.71129847e-01  3.85445319e-02  9.48291063e-01
  2.80741602e-01  5.03640592e-01  2.05556616e-01  2.82688320e-01
  3.95921826e-01  2.34585017e-01 -4.72099602e-01  4.53259498e-02
  8.21071938e-02  3.81634474e-01  2.54405200e-01  2.08593875e-01
  2.25974157e-01 -1.33065522e-01 -4.37187612e-01  2.77443707e-01
  3.75223011e-01  8.46717730e-02  6.87633157e-02 -6.03798091e-01
  4.42020833e-01  1.27284899e-01 -1.15679063e-01  2.35863045e-01
 -1.95469931e-01  2.60744274e-01  2.57575095e-01 -9.72114503e-04
 -7.11989403e-03  2.02457681e-02 -3.01856250e-01 -3.68264824e-01
 -5.51048756e-01 -3.21638107e-01  6.82744801e-01  3.11158597e-01
  6.41475201e-01  4.92584072e-02  3.97575051e-01  1.49251878e-01
 -6.21115148e-01  1.83911607e-01  9.18093994e-02 -3.95073116e-01
  4.48636059e-03 -4.84976768e-01 -5.95896304e-01  4.45791148e-02
 -1.56988084e-01 -3.84183414e-02  5.43418348e-01 -8.11479092e-01
 -6.62249386e-01  3.90995383e-01  6.33010343e-02  5.95393836e-01
  5.37799112e-02 -1.34044200e-01  7.78971165e-02  5.71762860e-01
  7.11518079e-02 -5.18555701e-01 -6.77722543e-02 -1.98609218e-01
  3.19237523e-02 -1.81521446e-01  7.71615386e-01  4.34639424e-01
 -3.50789547e-01 -6.15721047e-01  7.47690797e-01  1.04829222e-01
 -3.23299944e-01  4.18362141e-01 -4.87017900e-01  3.74183536e-01
  1.56449273e-01 -5.77521548e-02  5.83280206e-01 -5.13220251e-01
 -2.18854100e-03 -2.09312946e-01  3.91384870e-01  3.79638702e-01
  9.49216425e-01 -1.29741237e-01  5.69063306e-01 -4.44785506e-01
 -3.64393830e-01 -2.67691910e-01  1.04195893e-01  5.82311749e-01
 -1.04580969e-01 -3.69361848e-01 -2.09110811e-01 -4.69661951e-01
  3.93494815e-02  1.20271072e-01 -4.09655809e-01  3.17582756e-01
 -4.20122355e-01 -1.83060527e-01 -1.00395709e-01  2.06323966e-01
 -1.38006851e-01 -4.76040810e-01  4.09804583e-01  3.76812190e-01
  3.12000930e-01 -8.93614113e-01 -4.87620503e-01 -3.83996904e-01
  1.66279912e-01 -4.89073694e-01  1.85237497e-01 -2.76643574e-01
  2.62034088e-01  1.78466216e-01  3.37472558e-01  2.73906887e-01
 -2.62220383e-01 -3.90416145e-01 -4.20720190e-01  2.96501964e-02
 -6.88901782e-01  9.03950632e-02  4.92249221e-01 -1.56695545e-01
 -2.83631593e-01  3.26625034e-02 -6.74628764e-02  8.64049271e-02
  9.33768451e-02  2.47449368e-01  1.16173588e-01 -4.40933466e-01
 -3.54221940e-01  4.36044246e-01 -4.43125516e-01 -1.34401396e-01
 -2.39204153e-01 -3.71705174e-01  4.45067547e-02 -1.50166914e-01
  2.20735818e-01  8.04604143e-02 -4.51403528e-01 -3.34843814e-01
 -1.17283873e-01  9.54926550e-01  1.49068922e-01 -1.64130867e-01
  2.71167696e-01  5.29190339e-02 -2.54569560e-01  5.92227936e-01
 -8.31328583e+00 -1.58776045e-01  2.74165452e-01  4.40410018e-01
  3.51175398e-01 -1.75801247e-01 -4.99369167e-02 -3.76174986e-01
  3.19188148e-01 -7.06144333e-01  8.99769068e-01  2.41788909e-01
  4.14058030e-01  2.70290822e-01 -8.15664157e-02 -3.30294192e-01
 -1.47519931e-01 -6.20588362e-01 -4.30307984e-01 -9.92837697e-02
  1.35646656e-01 -3.30278993e-01  8.26435164e-04 -3.72957140e-02
 -8.20025563e-01 -1.79837734e-01  2.61670411e-01  5.46909034e-01
  2.47776508e-01 -9.14679348e-01  4.57372546e-01 -7.14825928e-01
  2.61447102e-01  3.62202264e-02 -2.40433916e-01 -1.51181951e-01
  5.14292456e-02  2.96004355e-01 -3.33741009e-02  7.91547775e-01
 -2.90189207e-01 -2.67834395e-01  1.55358255e-01 -3.04543316e-01
 -6.61973655e-03  5.82447290e-01 -8.84473324e-02 -2.95755386e-01
 -2.36821383e-01  2.25739777e-01  1.06621131e-01  4.18176949e-02
  2.94192970e-01 -4.99745190e-01  6.70128882e-01 -4.35431786e-02
  7.06657469e-01  7.02272058e-01 -1.19627364e-01 -7.91100383e-01
 -6.75487995e-01 -1.07149065e-01 -7.84962595e-01  4.35174704e-01
 -3.13441977e-02  2.44785011e-01 -8.73120725e-01  3.50022353e-02
  2.09037378e-01 -1.19712971e-01 -1.83357909e-01  6.73410237e-01
  6.20655060e-01 -4.46414083e-01 -4.69991937e-02 -2.27657735e-01
 -1.87436327e-01 -5.16896725e-01 -6.93054259e-01 -6.81385517e-01
 -7.97633290e-01 -5.03173769e-01  3.59278142e-01 -9.57255736e-02
 -4.03269604e-02 -4.18358952e-01  1.79500312e-01 -2.03033864e-01
  4.86599207e-01  4.59470987e-01  8.80892158e-01 -8.17529619e-01
 -1.26835912e-01 -4.91106033e-01 -2.95128614e-01 -4.75888133e-01
 -5.21908216e-02  2.00935096e-01  3.80766988e-01  1.80223122e-01
  4.48103130e-01 -3.33189778e-02 -4.20342296e-01 -7.12261915e-01
 -3.03021073e-01  1.31230146e-01 -6.22782826e-01  3.06484938e-01
 -6.40364215e-02 -3.02132308e-01  8.56923699e-01  1.15723848e-01
  3.39410037e-01  5.69929004e-01  1.52576774e-01 -5.94419658e-01
 -1.86862931e-01 -5.08795083e-01  2.31838319e-02 -3.78885031e-01
 -1.16499260e-01  5.03729165e-01 -6.11288905e-01  4.04664040e-01
 -1.57213137e-01  2.46463999e-01 -8.03821445e-01  2.13734694e-02
  1.84446663e-01  1.37956098e-01  4.44984883e-01  3.24014008e-01
  5.92168689e-01  4.99686182e-01 -2.96122074e-01 -1.36218712e-01
  3.44956547e-01  3.80547382e-02  2.13075086e-01 -1.07691221e-01
 -2.26502255e-01  4.00369644e-01  4.69331175e-01  4.01728719e-01
  1.76137567e-01 -5.67294478e-01  9.60933790e-02  6.66244209e-01
  5.86347163e-01 -1.36371404e-01  3.98506761e-01 -6.31231308e-01
  1.67884022e-01  3.80158991e-01 -1.42207444e-02 -6.25287741e-03
  8.95560384e-02 -3.62140477e-01  1.87743872e-01  4.58195776e-01
 -7.12240458e-01 -1.88387841e-01 -1.19760895e+00 -3.01312715e-01
  1.09179556e-01  7.96130240e-01 -5.37170544e-02  1.74531769e-02
  3.53715181e-01  1.95554882e-01  7.30575204e-01 -5.38034201e-01
  4.53047007e-01  1.16199367e-01  1.04140759e-01  1.75456032e-02
 -1.89646021e-01 -4.50457111e-02 -2.96386540e-01  8.41702640e-01
  1.34698525e-01  2.22343564e-01  3.41144174e-01  3.16154957e-01
 -1.96809396e-01 -1.29942954e-01  1.70184225e-01 -2.71234393e-01
  3.57916534e-01  6.67839423e-02 -4.23583910e-02 -4.82594728e-01
  1.66769296e-01  4.94927704e-01  4.79743063e-01  3.16114455e-01
  2.30504364e-01  3.55769023e-02 -5.56551456e-01  3.27263087e-01
  7.53074363e-02  4.10910100e-02  4.74388115e-02 -7.82817900e-02
 -3.72774750e-01  1.69891298e-01  7.65117764e-01 -2.97236681e-01
  9.89364535e-02 -1.80658445e-01 -3.42060685e-01  2.08858326e-02
 -4.04214978e-01  2.49380052e-01  3.97761703e-01  6.12111926e-01
  2.35218033e-01  5.57211339e-01 -2.22379059e-01  8.74022245e-02
 -1.54909298e-01  2.01315686e-01  5.51343322e-01  4.11243141e-01
  1.35008991e-01 -7.33748376e-01  1.75054118e-01  9.85418335e-02
 -2.71403968e-01 -4.27066609e-02 -7.39626288e-02 -8.95793512e-02
 -6.73125267e-01  3.34185362e-03  4.13883299e-01 -8.63712072e-01
 -7.17986166e-01  4.02059108e-01 -5.30343503e-03 -4.63847704e-02
 -9.58980799e-01 -5.41762769e-01 -2.94652343e-01 -1.40150785e-01
 -5.82643487e-02 -3.00248891e-01  4.91785020e-01 -7.32115865e-01
  7.90896490e-02  6.98811293e-01 -1.66678444e-01  2.70790845e-01
 -3.74785125e-01  5.75993583e-02  2.90283650e-01  2.36416310e-01
 -7.49478340e-01 -5.69795251e-01 -1.31442502e-01  6.77229762e-02
  5.24142459e-02 -2.42648467e-01 -3.51804316e-01 -2.79660672e-01
  2.92793125e-01  2.59541478e-02  1.31968647e-01 -1.84148461e-01
 -3.91101480e-01  3.43214124e-02  3.82385030e-02  3.72994915e-02
 -2.93730259e-01  5.79028249e-01  5.89166403e-01  2.75530010e-01
  5.50004840e-03  2.74638981e-01 -1.79950923e-01  2.87223980e-02
 -1.35058343e-01 -4.60491091e-01 -8.06818187e-01  3.41826946e-01
 -4.05554414e-01  2.48564854e-01  2.85961777e-01 -8.20883363e-02
 -1.83619842e-01 -4.03567672e-01  4.51681554e-01  6.78313673e-01
 -2.61587054e-01 -6.08418941e-01 -7.37745225e-01  5.62529683e-01
  2.67416030e-01  3.26011866e-01 -3.88305783e-01  1.19363032e-01
  1.32869795e-01 -2.45447546e-01  2.76360720e-01  4.10816789e-01
 -1.66601807e-01 -4.75261152e-01  4.95802276e-02 -4.01173294e-01
  4.47842479e-02  2.52735853e-01  4.99048799e-01 -2.71846294e-01
 -2.30236679e-01 -4.19545352e-01 -6.44784272e-01 -2.02411860e-02
  1.57229856e-01  5.70324719e-01 -4.36489373e-01 -5.75618327e-01
 -3.09315890e-01  7.87956774e-01  1.20356046e-01  1.53235763e-01
 -3.91700119e-02 -6.28403127e-01 -5.95002592e-01  2.11278677e-01
 -1.00958347e-01 -6.25989020e-01 -1.93989336e-01  2.68235356e-01
  6.01909697e-01 -1.00548573e-01 -7.74189308e-02 -6.57544732e-02
  1.52519777e-01  2.57917996e-02  2.68099010e-02 -1.81484312e-01
  9.75855887e-02 -3.44188064e-01 -4.13170643e-02  2.30506361e-01
  4.41174924e-01 -4.94555026e-01 -5.63982010e-01 -3.73879462e-01
 -1.57981247e-01 -1.60825193e-01  6.00513339e-01  1.28841642e-02
 -1.66598916e-01  2.62471616e-01 -4.88434464e-01  1.59599692e-01
  8.19031179e-01  3.17419261e-01  2.73416519e-01 -2.63669342e-01
  4.64785457e-01 -6.24555834e-02  4.31151867e-01 -3.60418260e-02
 -2.06647277e-01  4.98503923e-01 -2.39020959e-01  2.77610064e-01
 -2.38970906e-01  2.57991642e-01 -4.60961968e-01 -6.56904638e-01
 -5.68882048e-01  3.67466897e-01  1.33221045e-01  6.14784420e-01
 -6.97932363e-01  1.25866085e-01 -7.44488120e-01  7.49795198e-01
  1.52957171e-01 -6.04382813e-01 -5.13626337e-02 -1.60399359e-02
 -2.52286434e-01  2.35126168e-01  2.17649221e-01 -1.34040356e-01
 -3.57570648e-01  2.62209028e-01  8.41782987e-02  3.69732797e-01
 -1.52155936e-01 -1.49425477e-01  2.26673886e-01 -1.80154204e-01
  3.90131682e-01 -1.35756254e-01  3.67597282e-01 -1.23757668e-01
 -5.05584955e-01 -1.76664218e-01  4.05613303e-01  2.53209800e-01
 -6.05255246e-01 -4.27263796e-01  1.87023267e-01 -7.18621790e-01
 -3.03840339e-01 -9.55155343e-02  5.01942158e-01  8.24584484e-01
  2.96675533e-01 -5.18148124e-01  3.41293961e-01 -4.17495072e-01
 -5.55484295e-01  4.51619178e-01  2.32937664e-01  2.70074666e-01
  2.09832400e-01 -5.74077368e-01  1.88415691e-01  5.79990566e-01
 -3.45205307e-01  6.84873685e-02 -1.38211906e-01  4.09526706e-01
  6.40893579e-01  5.76397896e-01 -4.55712646e-01 -4.62623239e-01
 -2.42349565e-01  6.17270052e-01  2.84024298e-01  2.09292769e-03
 -3.66119117e-01  4.20001596e-01 -3.87326509e-01 -3.73405725e-01
  2.78264396e-02  2.48537898e-01 -7.01753974e-01 -1.90863177e-01
 -4.03226405e-01  4.49352920e-01  1.93640038e-01  1.57222137e-01
  5.84116727e-02 -2.18468346e-02 -7.76933804e-02  5.18231802e-02
  2.96478182e-01 -5.23954153e-01  1.57790691e-01 -1.08946666e-01
  5.36393166e-01 -1.97972674e-02  3.74067098e-01 -3.51414710e-01]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute cosine similarity between all words with Chinese in the model</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.spatial.distance <span class="im">import</span> cosine</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>similarity_scores <span class="op">=</span> {}</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> other_word <span class="kw">in</span> word_embeddings.keys():</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> other_word <span class="op">!=</span> <span class="st">"chinese"</span>:</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> cosine(word_embeddings[<span class="st">"chinese"</span>], word_embeddings[other_word])</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>        similarity_scores[other_word] <span class="op">=</span> similarity</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by cosine similarity</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>sorted_similarity <span class="op">=</span> <span class="bu">sorted</span>(similarity_scores.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the top 10 most similar words</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 10 most similar words to 'chinese':"</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> sorted_similarity[:<span class="dv">10</span>]:</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 10 most similar words to 'chinese':
japanese: 0.8790
chong: 0.8652
alien: 0.8581
fourteen: 0.8564
jaw: 0.8557
king: 0.8519
hong: 0.8516
contradiction: 0.8485
cousin: 0.8480
inferior: 0.8472</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute cosine similarity between all words with Chinaman in the model</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>similarity_scores <span class="op">=</span> {}</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> other_word <span class="kw">in</span> word_embeddings.keys():</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> other_word <span class="op">!=</span> <span class="st">"chinaman"</span>:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> cosine(word_embeddings[<span class="st">"chinaman"</span>], word_embeddings[other_word])</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        similarity_scores[other_word] <span class="op">=</span> similarity</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by cosine similarity</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>sorted_similarity <span class="op">=</span> <span class="bu">sorted</span>(similarity_scores.items(), key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the top 10 most similar words</span></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 10 most similar words to 'chinaman'"</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word, score <span class="kw">in</span> sorted_similarity[:<span class="dv">10</span>]:</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>word<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>score<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 10 most similar words to 'chinaman'
provei: 0.9922
clamor: 0.9916
totally: 0.9916
today: 0.9914
chinamen: 0.9911
interred: 0.9910
unequalled: 0.9909
surely: 0.9906
semitropical: 0.9904
quickly: 0.9904</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate a t-SNE plot for visualization</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.manifold <span class="im">import</span> TSNE</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>tsne <span class="op">=</span> TSNE(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> np.array(<span class="bu">list</span>(word_embeddings.values()))</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>tsne_results <span class="op">=</span> tsne.fit_transform(embeddings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a DataFrame for visualization</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>df_tsne <span class="op">=</span> pd.DataFrame(tsne_results, columns<span class="op">=</span>[<span class="st">'x'</span>, <span class="st">'y'</span>])</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>df_tsne[<span class="st">'word'</span>] <span class="op">=</span> <span class="bu">list</span>(word_embeddings.keys())</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Highlight the word 'chinese' in the plot</span></span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>keywords <span class="op">=</span> [<span class="st">"chinese"</span>, <span class="st">"china"</span>, <span class="st">"chinaman"</span>, <span class="st">"chinamen"</span>]</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>df_tsne[<span class="st">'highlight'</span>] <span class="op">=</span> df_tsne[<span class="st">'word'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> x: x <span class="cf">if</span> x <span class="kw">in</span> keywords <span class="cf">else</span> <span class="st">''</span>)</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    df_tsne,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">'x'</span>,</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">'y'</span>,</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">'t-SNE Visualization of legal-BERT Word Embeddings'</span>,</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">'highlight'</span>,                        </span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    hover_data<span class="op">=</span>[<span class="st">'word'</span>], </span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    text<span class="op">=</span> <span class="st">'highlight'</span>,</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">600</span>,</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span><span class="dv">800</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\plotly\express\_core.py:1983: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.
  sf: grouped.get_group(s if len(s) &gt; 1 else s[0])</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
</section>
<section id="text-embedding-and-analysis-of-crease-and-begbie-corpus" class="level3">
<h3 class="anchored" data-anchor-id="text-embedding-and-analysis-of-crease-and-begbie-corpus">Text Embedding and Analysis of Crease and Begbie Corpus</h3>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the Crease texts into a single text list</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>crease_texts <span class="op">=</span> df[df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Crease'</span>][<span class="st">'text'</span>].tolist()</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Compile the Begbie texts into a single text list</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>begbie_texts <span class="op">=</span> df[df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Begbie'</span>][<span class="st">'text'</span>].tolist()</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine both lists in a dictionary</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>judge_dict <span class="op">=</span> {</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Crease'</span>: crease_texts,</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Begbie'</span>: begbie_texts</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to embed text using the model</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Union, List</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> embed_text(</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>    text: <span class="bu">str</span>,</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    focus_token: Union[<span class="bu">str</span>, List[<span class="bu">str</span>]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    window: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span>tokenizer,</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span>model)<span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co">    text: the raw string</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="co">    focus_token: either a single word, or a list of words to look for</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co">    window: how many tokens on each side to include</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="co">    tokenizer: HuggingFace tokenizer</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a><span class="co">    model: BERT model</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Run the model once</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    inputs <span class="op">=</span> tokenizer(text, return_tensors<span class="op">=</span><span class="st">"pt"</span>, truncation<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> model(<span class="op">**</span>inputs)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    hidden <span class="op">=</span> outputs.last_hidden_state.squeeze(<span class="dv">0</span>) </span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> focus_token <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> hidden[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Normalize to list</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    keywords <span class="op">=</span> (</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>        [focus_token] <span class="cf">if</span> <span class="bu">isinstance</span>(focus_token, <span class="bu">str</span>)</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span> focus_token</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Pre-tokenize each keyword to its subtoken ids</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    kw_token_ids <span class="op">=</span> {</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>        kw: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(kw))</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> kw <span class="kw">in</span> keywords</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>    input_ids <span class="op">=</span> inputs[<span class="st">"input_ids"</span>].squeeze(<span class="dv">0</span>).tolist()</span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>    spans <span class="op">=</span> []  <span class="co"># list of (start, end) index pairs</span></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># find every match of every keyword</span></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> kw, sub_ids <span class="kw">in</span> kw_token_ids.items():</span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>        L <span class="op">=</span> <span class="bu">len</span>(sub_ids)</span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(input_ids) <span class="op">-</span> L <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> input_ids[i:i<span class="op">+</span>L] <span class="op">==</span> sub_ids:</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>                spans.append((i, i<span class="op">+</span>L))</span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="kw">not</span> spans:</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># fallback on CLS vector</span></span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> hidden[<span class="dv">0</span>].cpu().numpy()</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For each span, grab the window around it</span></span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>    vecs <span class="op">=</span> []</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (start, end) <span class="kw">in</span> spans:</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>        lo <span class="op">=</span> <span class="bu">max</span>(<span class="dv">1</span>, start <span class="op">-</span> window)</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>        hi <span class="op">=</span> <span class="bu">min</span>(hidden.size(<span class="dv">0</span>), end <span class="op">+</span> window)</span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>        <span class="co"># mean‑pool over all tokens in this extended window</span></span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>        span_vec <span class="op">=</span> hidden[lo:hi].mean(dim<span class="op">=</span><span class="dv">0</span>).cpu().numpy()</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>        vecs.append(span_vec)</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Average across all spans</span></span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.mean(np.stack(vecs, axis<span class="op">=</span><span class="dv">0</span>), axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> nltk <span class="im">import</span> sent_tokenize </span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a dictionary to hold the mentionings of "Chinese" by author</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>judge_snippets <span class="op">=</span> {}</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>keywords <span class="op">=</span> [<span class="st">"Chinese"</span>, <span class="st">"China"</span>, <span class="st">"Chinaman"</span>, <span class="st">"Chinamen"</span>, <span class="st">"immigrant"</span>, <span class="st">"immigrants"</span>, <span class="st">"immigration"</span>]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, texts <span class="kw">in</span> judge_dict.items():</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    snippets <span class="op">=</span> []</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> txt <span class="kw">in</span> texts:</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        sentence <span class="op">=</span> sent_tokenize(txt)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sent <span class="kw">in</span> sentence:</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">any</span>(keyword <span class="kw">in</span> sent <span class="cf">for</span> keyword <span class="kw">in</span> keywords):</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>                snippets.append(sent)</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    judge_snippets[auth] <span class="op">=</span> snippets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Investigate the length of the snippets</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>n_snippet <span class="op">=</span> {auth: <span class="bu">len</span>(snippets) <span class="cf">for</span> auth, snippets <span class="kw">in</span> judge_snippets.items()}</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Snippet size by author:"</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, num <span class="kw">in</span> n_snippet.items():</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>auth<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>num<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Snippet size by author:
Crease: 146
Begbie: 108</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define an ethnicity anchor, not including "chinese"</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>ethnicities <span class="op">=</span> [</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Japanese"</span>,   <span class="st">"Korean"</span>,    <span class="st">"Vietnamese"</span>,</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Filipino"</span>,    <span class="st">"Thai"</span>,       <span class="st">"Malay"</span>,     <span class="st">"Indian"</span>,</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Pakistani"</span>,   <span class="st">"Bangladeshi"</span>,<span class="st">"Nepalese"</span>,  <span class="st">"Tibetan"</span>,</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Arab"</span>,        <span class="st">"Persian"</span>,    <span class="st">"Turkish"</span>,   <span class="st">"Slavic"</span>,</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Germanic"</span>,    <span class="st">"Celtic"</span>,     <span class="st">"Slavic"</span>,    <span class="st">"Romani"</span>,</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Jewish"</span>,      <span class="st">"Zulu"</span>,       <span class="st">"Xhosa"</span>,     <span class="st">"Maori"</span>,</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Sami"</span>,        <span class="st">"Berber"</span>,     <span class="st">"Tamil"</span>,     <span class="st">"Punjabi"</span>,</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Bengali"</span>,     <span class="st">"Kazakh"</span>,     <span class="st">"Uyghur"</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Create embeddings</span></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>eth_vecs <span class="op">=</span> []</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> e <span class="kw">in</span> ethnicities:</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    eth_vecs.append(embed_text(e))</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>eth_anchor <span class="op">=</span> np.mean(eth_vecs, axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py:1762: FutureWarning:

`encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create embeddings</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>embeddings_dict <span class="op">=</span> {<span class="st">'Crease'</span>: [], <span class="st">'Begbie'</span>: []}</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> judge_snippets.items():</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snip <span class="kw">in</span> snippets:</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> embed_text(snip, focus_token<span class="op">=</span>keywords, window<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>        embeddings_dict[auth].append(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create embeddings that subtract the ethnicity anchor</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>subtracted_embeddings_dict <span class="op">=</span> {<span class="st">'Crease'</span>: [], <span class="st">'Begbie'</span>: []}</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, embeddings <span class="kw">in</span> embeddings_dict.items():</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> emb <span class="kw">in</span> embeddings:</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> emb <span class="op">-</span> eth_anchor</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        subtracted_embeddings_dict[auth].append(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute per author mean and cosine similarity</span></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>mean_crease <span class="op">=</span> np.mean(embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)  </span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>mean_begbie <span class="op">=</span> np.mean(embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pairwise cosine similarity</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie <span class="op">=</span> cosine_similarity(mean_crease, mean_begbie)[<span class="dv">0</span>, <span class="dv">0</span>] </span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Crease and Begbie: </span><span class="sc">{</span>sim_crease_begbie<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine similarity between Crease and Begbie: 0.9948</code></pre>
</div>
</div>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute per author mean and cosine similarity</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>subtracted_mean_crease <span class="op">=</span> np.mean(subtracted_embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)  </span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>subtracted_mean_begbie <span class="op">=</span> np.mean(subtracted_embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pairwise cosine similarity</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie_sub <span class="op">=</span> cosine_similarity(subtracted_mean_crease, subtracted_mean_begbie)[<span class="dv">0</span>, <span class="dv">0</span>] </span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Crease and Begbie with ethical axis removed: </span><span class="sc">{</span>sim_crease_begbie_sub<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine similarity between Crease and Begbie with ethical axis removed: 0.9971</code></pre>
</div>
</div>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We check if the cosine similarity is lower when one subtracted the ethnicity anchor but not the other</span></span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>sim_crease_sub_begbie <span class="op">=</span> cosine_similarity(mean_crease, subtracted_mean_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>sim_sub_crease_begbie <span class="op">=</span> cosine_similarity(subtracted_mean_crease, mean_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Crease and Begbie with Begbie removed ethnical axis: </span><span class="sc">{</span>sim_crease_sub_begbie<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Crease and Begbie with Crease removed ethnical axis: </span><span class="sc">{</span>sim_sub_crease_begbie<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine similarity between Crease and Begbie with Begbie removed ethnical axis: 0.6105
Cosine similarity between Crease and Begbie with Crease removed ethnical axis: 0.6063</code></pre>
</div>
</div>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We check the cosine similarity of max and min embeddings</span></span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>max_crease <span class="op">=</span> np.<span class="bu">max</span>(embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>min_crease <span class="op">=</span> np.<span class="bu">min</span>(embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>max_begbie <span class="op">=</span> np.<span class="bu">max</span>(embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>min_begbie <span class="op">=</span> np.<span class="bu">min</span>(embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie_max <span class="op">=</span> cosine_similarity(max_crease, max_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie_min <span class="op">=</span> cosine_similarity(min_crease, min_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between max Crease and max Begbie: </span><span class="sc">{</span>sim_crease_begbie_max<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between min Crease and min Begbie: </span><span class="sc">{</span>sim_crease_begbie_min<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine similarity between max Crease and max Begbie: 0.9785
Cosine similarity between min Crease and min Begbie: 0.9843</code></pre>
</div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We can bootstrap the similarity score to get a confidence interval</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>crease_embeddings <span class="op">=</span> np.array(embeddings_dict[<span class="st">"Crease"</span>])</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>begbie_embeddings <span class="op">=</span> np.array(embeddings_dict[<span class="st">"Begbie"</span>])</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to arrays</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>crease_embeddings <span class="op">=</span> np.vstack(crease_embeddings)</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>begbie_embeddings <span class="op">=</span> np.vstack(begbie_embeddings)</span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>n_boot <span class="op">=</span> <span class="dv">1000</span> </span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>cosine_scores <span class="op">=</span> []</span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-13"><a href="#cb37-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_boot):</span>
<span id="cb37-14"><a href="#cb37-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample with replacement</span></span>
<span id="cb37-15"><a href="#cb37-15" aria-hidden="true" tabindex="-1"></a>    crease_sample <span class="op">=</span> crease_embeddings[np.random.choice(<span class="bu">len</span>(crease_embeddings), size<span class="op">=</span><span class="bu">len</span>(crease_embeddings), replace<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb37-16"><a href="#cb37-16" aria-hidden="true" tabindex="-1"></a>    begbie_sample <span class="op">=</span> begbie_embeddings[np.random.choice(<span class="bu">len</span>(begbie_embeddings), size<span class="op">=</span><span class="bu">len</span>(begbie_embeddings), replace<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb37-17"><a href="#cb37-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-18"><a href="#cb37-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute mean embeddings</span></span>
<span id="cb37-19"><a href="#cb37-19" aria-hidden="true" tabindex="-1"></a>    mean_crease_boot <span class="op">=</span> np.mean(crease_sample, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-20"><a href="#cb37-20" aria-hidden="true" tabindex="-1"></a>    mean_begbie_boot <span class="op">=</span> np.mean(begbie_sample, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb37-21"><a href="#cb37-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-22"><a href="#cb37-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute cosine similarity</span></span>
<span id="cb37-23"><a href="#cb37-23" aria-hidden="true" tabindex="-1"></a>    cos_sim <span class="op">=</span> cosine_similarity(mean_crease_boot, mean_begbie_boot)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb37-24"><a href="#cb37-24" aria-hidden="true" tabindex="-1"></a>    cosine_scores.append(cos_sim)</span>
<span id="cb37-25"><a href="#cb37-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-26"><a href="#cb37-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to numpy array</span></span>
<span id="cb37-27"><a href="#cb37-27" aria-hidden="true" tabindex="-1"></a>cosine_scores <span class="op">=</span> np.array(cosine_scores)</span>
<span id="cb37-28"><a href="#cb37-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-29"><a href="#cb37-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute 95% confidence interval</span></span>
<span id="cb37-30"><a href="#cb37-30" aria-hidden="true" tabindex="-1"></a>lower <span class="op">=</span> np.percentile(cosine_scores, <span class="fl">2.5</span>)</span>
<span id="cb37-31"><a href="#cb37-31" aria-hidden="true" tabindex="-1"></a>upper <span class="op">=</span> np.percentile(cosine_scores, <span class="fl">97.5</span>)</span>
<span id="cb37-32"><a href="#cb37-32" aria-hidden="true" tabindex="-1"></a>mean_sim <span class="op">=</span> np.mean(cosine_scores)</span>
<span id="cb37-33"><a href="#cb37-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-34"><a href="#cb37-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the bootstrap distribution</span></span>
<span id="cb37-35"><a href="#cb37-35" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb37-36"><a href="#cb37-36" aria-hidden="true" tabindex="-1"></a>plt.hist(cosine_scores, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">'skyblue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb37-37"><a href="#cb37-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-38"><a href="#cb37-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Add lines for mean and confidence interval</span></span>
<span id="cb37-39"><a href="#cb37-39" aria-hidden="true" tabindex="-1"></a>plt.axvline(mean_sim, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Mean: </span><span class="sc">{</span>mean_sim<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb37-40"><a href="#cb37-40" aria-hidden="true" tabindex="-1"></a>plt.axvline(lower, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'2.5%: </span><span class="sc">{</span>lower<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb37-41"><a href="#cb37-41" aria-hidden="true" tabindex="-1"></a>plt.axvline(upper, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'97.5%: </span><span class="sc">{</span>upper<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb37-42"><a href="#cb37-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-43"><a href="#cb37-43" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bootstrap Distribution of Cosine Similarity Between Crease and Begbie'</span>)</span>
<span id="cb37-44"><a href="#cb37-44" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Cosine Similarity'</span>)</span>
<span id="cb37-45"><a href="#cb37-45" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb37-46"><a href="#cb37-46" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb37-47"><a href="#cb37-47" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb37-48"><a href="#cb37-48" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb37-49"><a href="#cb37-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-50"><a href="#cb37-50" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb37-51"><a href="#cb37-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bootstrap mean cosine similarity between Crease and Begbie: </span><span class="sc">{</span>mean_sim<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb37-52"><a href="#cb37-52" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"95% Confidence Interval: [</span><span class="sc">{</span>lower<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>upper<span class="sc">:.4f}</span><span class="ss">]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="text_embeddings_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Bootstrap mean cosine similarity between Crease and Begbie: 0.9936
95% Confidence Interval: [0.9919, 0.9951]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Similarly, bootstrap the similarity score to get a confidence interval</span></span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>crease_embeddings <span class="op">=</span> np.array(subtracted_embeddings_dict[<span class="st">"Crease"</span>])</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>begbie_embeddings <span class="op">=</span> np.array(subtracted_embeddings_dict[<span class="st">"Begbie"</span>])</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to arrays</span></span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>crease_embeddings <span class="op">=</span> np.vstack(crease_embeddings)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>begbie_embeddings <span class="op">=</span> np.vstack(begbie_embeddings)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>n_boot <span class="op">=</span> <span class="dv">1000</span> </span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>subtracted_cosine_scores <span class="op">=</span> []</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_boot):</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Sample with replacement</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    crease_sample <span class="op">=</span> crease_embeddings[np.random.choice(<span class="bu">len</span>(crease_embeddings), size<span class="op">=</span><span class="bu">len</span>(crease_embeddings), replace<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>    begbie_sample <span class="op">=</span> begbie_embeddings[np.random.choice(<span class="bu">len</span>(begbie_embeddings), size<span class="op">=</span><span class="bu">len</span>(begbie_embeddings), replace<span class="op">=</span><span class="va">True</span>)]</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute mean embeddings</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    mean_crease_boot <span class="op">=</span> np.mean(crease_sample, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    mean_begbie_boot <span class="op">=</span> np.mean(begbie_sample, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute cosine similarity</span></span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>    cos_sim <span class="op">=</span> cosine_similarity(mean_crease_boot, mean_begbie_boot)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>    subtracted_cosine_scores.append(cos_sim)</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert to numpy array</span></span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a>subtracted_cosine_scores <span class="op">=</span> np.array(subtracted_cosine_scores)</span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute 95% confidence interval</span></span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>lower <span class="op">=</span> np.percentile(subtracted_cosine_scores, <span class="fl">2.5</span>)</span>
<span id="cb39-30"><a href="#cb39-30" aria-hidden="true" tabindex="-1"></a>upper <span class="op">=</span> np.percentile(subtracted_cosine_scores, <span class="fl">97.5</span>)</span>
<span id="cb39-31"><a href="#cb39-31" aria-hidden="true" tabindex="-1"></a>mean_sim <span class="op">=</span> np.mean(subtracted_cosine_scores)</span>
<span id="cb39-32"><a href="#cb39-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-33"><a href="#cb39-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the bootstrap distribution</span></span>
<span id="cb39-34"><a href="#cb39-34" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">4</span>))</span>
<span id="cb39-35"><a href="#cb39-35" aria-hidden="true" tabindex="-1"></a>plt.hist(subtracted_cosine_scores, bins<span class="op">=</span><span class="dv">30</span>, color<span class="op">=</span><span class="st">'skyblue'</span>, edgecolor<span class="op">=</span><span class="st">'black'</span>, alpha<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb39-36"><a href="#cb39-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-37"><a href="#cb39-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Add lines for mean and confidence interval</span></span>
<span id="cb39-38"><a href="#cb39-38" aria-hidden="true" tabindex="-1"></a>plt.axvline(mean_sim, color<span class="op">=</span><span class="st">'black'</span>, linestyle<span class="op">=</span><span class="st">'-'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'Mean: </span><span class="sc">{</span>mean_sim<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb39-39"><a href="#cb39-39" aria-hidden="true" tabindex="-1"></a>plt.axvline(lower, color<span class="op">=</span><span class="st">'red'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'2.5%: </span><span class="sc">{</span>lower<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb39-40"><a href="#cb39-40" aria-hidden="true" tabindex="-1"></a>plt.axvline(upper, color<span class="op">=</span><span class="st">'green'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>, linewidth<span class="op">=</span><span class="dv">2</span>, label<span class="op">=</span><span class="ss">f'97.5%: </span><span class="sc">{</span>upper<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb39-41"><a href="#cb39-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-42"><a href="#cb39-42" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Bootstrap Distribution of Cosine Similarity Between Crease and Begbie (Ethnical Axis Removed)'</span>)</span>
<span id="cb39-43"><a href="#cb39-43" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Cosine Similarity'</span>)</span>
<span id="cb39-44"><a href="#cb39-44" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Frequency'</span>)</span>
<span id="cb39-45"><a href="#cb39-45" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb39-46"><a href="#cb39-46" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb39-47"><a href="#cb39-47" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb39-48"><a href="#cb39-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-49"><a href="#cb39-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb39-50"><a href="#cb39-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Bootstrap mean cosine similarity between Crease and Begbie with ethnical axis removed: </span><span class="sc">{</span>mean_sim<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb39-51"><a href="#cb39-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"95% Confidence Interval: [</span><span class="sc">{</span>lower<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>upper<span class="sc">:.4f}</span><span class="ss">]"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="text_embeddings_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Bootstrap mean cosine similarity between Crease and Begbie with ethnical axis removed: 0.9964
95% Confidence Interval: [0.9953, 0.9972]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Testing if the ethnical axis removal has a significant effect on the similarity score</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="co"># We can use a bootstrapped t-test to compare the means of the two distributions</span></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> ttest_ind</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform bootstrapped t-test</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>t_stat, p_value <span class="op">=</span> ttest_ind(subtracted_cosine_scores, cosine_scores, equal_var<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"t-statistic: </span><span class="sc">{</span>t_stat<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"p-value: </span><span class="sc">{</span>p_value<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> p_value <span class="op">&lt;</span> <span class="fl">0.05</span>:</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The difference in mean cosine similarity is statistically significant."</span>)</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"The difference in mean cosine similarity is not statistically significant."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>t-statistic: 88.7248
p-value: 0.0000
The difference in mean cosine similarity is statistically significant.</code></pre>
</div>
</div>
<p>With the above test on embeddings with or without the ethnical axis, we confirmed that the difference between cosine similarities of Crease and Begbie are statistically significantly biased by the ethnical factors. However, with the small difference in real cosine similarity, we can conclude that the two characters are still very similar in terms of their text embeddings, thus the bias is not very significant in the real world.</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create UMAP projection for visualization</span></span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> umap </span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>all_vecs <span class="op">=</span> np.vstack(embeddings_dict[<span class="st">"Crease"</span>] <span class="op">+</span> embeddings_dict[<span class="st">"Begbie"</span>])</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>labels  <span class="op">=</span> ([<span class="st">"Crease"</span>] <span class="op">*</span> <span class="bu">len</span>(embeddings_dict[<span class="st">"Crease"</span>])) <span class="op">+</span> ([<span class="st">"Begbie"</span>] <span class="op">*</span> <span class="bu">len</span>(embeddings_dict[<span class="st">"Begbie"</span>]))</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> umap.UMAP(n_neighbors<span class="op">=</span><span class="dv">15</span>, min_dist<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>proj <span class="op">=</span> reducer.fit_transform(all_vecs) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot using plotly to further explore</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> plotly.express <span class="im">as</span> px</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> textwrap</span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wrap_text(text, width<span class="op">=</span><span class="dv">60</span>):</span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">'&lt;br&gt;'</span>.join(textwrap.wrap(text, width<span class="op">=</span>width))</span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a>umap_df <span class="op">=</span> pd.DataFrame(proj, columns<span class="op">=</span>[<span class="st">'UMAP 1'</span>, <span class="st">'UMAP 2'</span>])</span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Author'</span>] <span class="op">=</span> labels</span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Text'</span>] <span class="op">=</span> [snip <span class="cf">for</span> auth <span class="kw">in</span> judge_snippets <span class="cf">for</span> snip <span class="kw">in</span> judge_snippets[auth]]</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Text'</span>] <span class="op">=</span> umap_df[<span class="st">'Text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> t: wrap_text(t, width<span class="op">=</span><span class="dv">50</span>))</span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(umap_df, x<span class="op">=</span><span class="st">'UMAP 1'</span>, y<span class="op">=</span><span class="st">'UMAP 2'</span>, </span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'Author'</span>, hover_data<span class="op">=</span>[<span class="st">'Text'</span>], </span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>                 width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">500</span> )</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a>fig.update_traces(marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">5</span>))</span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a>fig.update_layout(title<span class="op">=</span><span class="st">'UMAP Projection of Word Embeddings by Author'</span>)</span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\plotly\express\_core.py:1983: FutureWarning:

When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the 10 most similar embedding sentences to Crease's mean embedding</span></span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>crease_similarity_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'Author'</span>, <span class="st">'Text'</span>, <span class="st">'Similarity Score'</span>])</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through the embeddings and their corresponding sentences</span></span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> judge_snippets.items():</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snippet, emb <span class="kw">in</span> <span class="bu">zip</span>(snippets, embeddings_dict[auth]):</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> cosine_similarity(emb.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), mean_crease)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>        crease_similarity_df.loc[<span class="bu">len</span>(crease_similarity_df)] <span class="op">=</span> [</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>            auth, snippet, similarity</span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by similarity score</span></span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>crease_sorted_similarity <span class="op">=</span> crease_similarity_df.sort_values(by<span class="op">=</span><span class="st">'Similarity Score'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 10 most similar sentences to Crease's mean embedding:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> crease_sorted_similarity.head(<span class="dv">10</span>).iterrows():</span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Sentence: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity Score: </span><span class="sc">{</span>row[<span class="st">'Similarity Score'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 10 most similar sentences to Crease's mean embedding:

Author: Crease
Sentence: The Act is found associated with another Act now disallowed, the express object of which is to
prevent the Chinese altogether from coming to this country, and the principle "noscitur a sociis" is
kept up by the preamble of the present Act, which describes the Chinese in terms which, I venture to
think, have never before in any other country found a place in an Act of Parliament.
Similarity Score: 0.9635

Author: Crease
Sentence: In coming to British Columbia, and while here, the Chinese have no idea of interfering with the
property of the white population in any way beyond the ordinary competition which they offer in the
labor market.
Similarity Score: 0.9630

Author: Crease
Sentence: The strike of the Chinese in Victoria when resisting an intentionally discriminating and illegal tax
of $30 a head on all Chinese-although it occurred a few years ago-is too fresh in the recollection
to be forgotten.
Similarity Score: 0.9628

Author: Crease
Sentence: In the case of the Chinese treaties, they were forced at the point of the bayonet on China, to
obtain a right for us to enter China, and in return for a similar permission to us, full permission
was given for the Chinese to trade and reside in British dominions everywhere.
Similarity Score: 0.9621

Author: Begbie
Sentence: Statutes were by their title and preamble REGINA v. MEE WAH expressly aimed at Chinamen by name;
that this distinction also renders inapplicable all the United States' cases cited; that this
enactment is quite general extending to all laundries without exception and we must not look beyond
the words of the enactment to enquire what its object was; that there is in fact one laundry in
Victoria not conducted by Chinamen on which the tax will fall with equal force so that it is
impossible to say that Chinamen are hereby exclusively selected for taxation; the circumstance that
they are chiefly affected being a mere coincidence; that the bylaw only imposes $100.00 per annum,
keeping far within the limit of $150.00 permitted by the Statute; that the tax clearly is calculated
to procuring additional Municipal revenue and that no other object is hinted at.
Similarity Score: 0.9606

Author: Crease
Sentence: Though possessed of all the qualities I have described, Chinamen do not make good settlers in the
sense of raising up citizens of a free.
Similarity Score: 0.9605

Author: Crease
Sentence: It has generally been supposed from the secrecy with which some murders were committed (they are
confined to two or three Chinese murders altogether in many years), which have occurred in British
Columbia undiscovered, that the victims were executed by the decrees of some secret Chinese
tribunal, like the Vehm-Gericht, having its centre in San Francisco, but I have not been able to
discover a single fact which tended to corroborate that suspicion.
Similarity Score: 0.9596

Author: Begbie
Sentence: When we find (1st) no other description of labour taxed at all ; (2nd) this description of labour
practically quite abandoned to Chinamen alone ; (3rd) this description of labour taxed at fifteen
times the rate permitted to be levied on any retail shop ; (4th) that a preliminary Provincial Act
has declared Chinamen incapable of the franchise which they formerly exercised.
Similarity Score: 0.9590

Author: Crease
Sentence: The grounds alleged by the Health Officer for this arbitrary treatment is “that China is an infected
locality,” and all persons coming from China and especially in this instance Hongkong, and
especially also the natives of China, come from an infected locality, and that he has that authority
under the Health By-Law, 1893, which was passed at the time of the small pox panic in Victoria, and,
in all he did, was simply doing his duty.
Similarity Score: 0.9588

Author: Crease
Sentence: I do not say that all these evils whether white or Chinese should not be determinedly suppressed,
but there is such a manifest spirit of exaggeration in the complaints that are made, for the purpose
I have described, as very materially to lessen in impartial eyes the accusations levelled against
the Chinese.
Similarity Score: 0.9586
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Print out the 10 most similar embedding sentences to Begbie's mean embedding</span></span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>begbie_similarity_df <span class="op">=</span> pd.DataFrame(columns<span class="op">=</span>[<span class="st">'Author'</span>, <span class="st">'Text'</span>, <span class="st">'Similarity Score'</span>])</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Iterate through the embeddings and their corresponding sentences</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> judge_snippets.items():</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snippet, emb <span class="kw">in</span> <span class="bu">zip</span>(snippets, embeddings_dict[auth]):</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>        similarity <span class="op">=</span> cosine_similarity(emb.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), mean_begbie)[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>        begbie_similarity_df.loc[<span class="bu">len</span>(begbie_similarity_df)] <span class="op">=</span> [</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>            auth, snippet, similarity</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Sort by similarity score</span></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>begbie_sorted_similarity <span class="op">=</span> begbie_similarity_df.sort_values(by<span class="op">=</span><span class="st">'Similarity Score'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Top 10 most similar sentences to Begbie's mean embedding:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _, row <span class="kw">in</span> begbie_sorted_similarity.head(<span class="dv">10</span>).iterrows():</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    wrapped_para <span class="op">=</span> textwrap.fill(row[<span class="st">'Text'</span>], width<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Author: </span><span class="sc">{</span>row[<span class="st">'Author'</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">Sentence: </span><span class="sc">{</span>wrapped_para<span class="sc">}</span><span class="ch">\n</span><span class="ss">Similarity Score: </span><span class="sc">{</span>row[<span class="st">'Similarity Score'</span>]<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Top 10 most similar sentences to Begbie's mean embedding:

Author: Crease
Sentence: Though possessed of all the qualities I have described, Chinamen do not make good settlers in the
sense of raising up citizens of a free.
Similarity Score: 0.9644

Author: Begbie
Sentence: Whites who have evil communications with Chinese must themselves be lamentably depraved beforehand;
and so, I should be disposed to say, immoral Chinese are not only not more injurious, but they are
quite innocuous to the morals of the whites, in comparison with white people of similar or allied
habits.
Similarity Score: 0.9620

Author: Crease
Sentence: The strike of the Chinese in Victoria when resisting an intentionally discriminating and illegal tax
of $30 a head on all Chinese-although it occurred a few years ago-is too fresh in the recollection
to be forgotten.
Similarity Score: 0.9610

Author: Crease
Sentence: I do not say that all these evils whether white or Chinese should not be determinedly suppressed,
but there is such a manifest spirit of exaggeration in the complaints that are made, for the purpose
I have described, as very materially to lessen in impartial eyes the accusations levelled against
the Chinese.
Similarity Score: 0.9595

Author: Crease
Sentence: I know of retired officers and persons of settled incomes who would not have thought of coming here
if they had not known that Chinese servants could he had here, though very indifferent compared with
those one can obtain in China itself.
Similarity Score: 0.9588

Author: Begbie
Sentence: All the evils arising from opium in British Columbia in a year do not, probably, equal the damage,
trouble and expense occasioned to individuals and to the state by whiskey in a single month, or
perhaps in some single night, As already observed, I do not remember ever to have seen a drunken
Chinaman; and the argument against Chinamen founded on opium appears to be analogous to the
comparison of the mote and the beam.
Similarity Score: 0.9585

Author: Crease
Sentence: Good white labor is so far superior to Chinese, that it will of itself, when it can be contented
with reasonable prices as in the East, infallibly work Chinese manual labor out of the field.
Similarity Score: 0.9577

Author: Begbie
Sentence: When we find (1st) no other description of labour taxed at all ; (2nd) this description of labour
practically quite abandoned to Chinamen alone ; (3rd) this description of labour taxed at fifteen
times the rate permitted to be levied on any retail shop ; (4th) that a preliminary Provincial Act
has declared Chinamen incapable of the franchise which they formerly exercised.
Similarity Score: 0.9572

Author: Begbie
Sentence: The enormous consumption which this implies does not appear to prevent Chinamen from being the most
prolific race, the most indefatigable laborers, and the keenest traders in the world.
Similarity Score: 0.9569

Author: Crease
Sentence: In coming to British Columbia, and while here, the Chinese have no idea of interfering with the
property of the white population in any way beyond the ordinary competition which they offer in the
labor market.
Similarity Score: 0.9562
</code></pre>
</div>
</div>
<p>The mixture occurrence of the two judges in the most similar sentences to the mean embeddings of the two also shows that the stance are very similar in terms of their text embeddings.</p>
</section>
<section id="the-comparison-of-creases-and-begbies-rulings-and-the-chinese-regulation-act-of-1884" class="level3">
<h3 class="anchored" data-anchor-id="the-comparison-of-creases-and-begbies-rulings-and-the-chinese-regulation-act-of-1884">The Comparison of Crease’s and Begbie’s Rulings and the Chinese Regulation Act of 1884</h3>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>crease_cases <span class="op">=</span> df[(df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Crease'</span>) <span class="op">&amp;</span> (df[<span class="st">'type'</span>] <span class="op">==</span> <span class="st">'case'</span>)][<span class="st">'text'</span>].tolist()</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>begbie_cases <span class="op">=</span> df[(df[<span class="st">'author'</span>] <span class="op">==</span> <span class="st">'Begbie'</span>) <span class="op">&amp;</span> (df[<span class="st">'type'</span>] <span class="op">==</span> <span class="st">'case'</span>)][<span class="st">'text'</span>].tolist()</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>act_1884 <span class="op">=</span> df[df[<span class="st">'type'</span>] <span class="op">==</span> <span class="st">'act'</span>][<span class="st">'text'</span>].tolist()</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>act_dict <span class="op">=</span> {</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Crease'</span>: crease_cases,</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Begbie'</span>: begbie_cases,</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Act 1884'</span>: act_1884}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>act_snippets <span class="op">=</span> {}</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>keywords <span class="op">=</span> [<span class="st">"Chinese"</span>, <span class="st">"China"</span>, <span class="st">"Chinaman"</span>, <span class="st">"Chinamen"</span>]</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, texts <span class="kw">in</span> act_dict.items():</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    snippets <span class="op">=</span> []</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> txt <span class="kw">in</span> texts:</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>        sentence <span class="op">=</span> sent_tokenize(txt)</span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> sent <span class="kw">in</span> sentence:</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">any</span>(keyword <span class="kw">in</span> sent <span class="cf">for</span> keyword <span class="kw">in</span> keywords):</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a>                snippets.append(sent)</span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>    act_snippets[auth] <span class="op">=</span> snippets</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Investigate the length of the snippets</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>n_snippet <span class="op">=</span> {auth: <span class="bu">len</span>(snippets) <span class="cf">for</span> auth, snippets <span class="kw">in</span> act_snippets.items()}</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Snippet size by author:"</span>)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, num <span class="kw">in</span> n_snippet.items():</span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>auth<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>num<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Snippet size by author:
Crease: 86
Begbie: 18
Act 1884: 24</code></pre>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create embeddings</span></span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>embeddings_dict <span class="op">=</span> {<span class="st">'Crease'</span>: [], <span class="st">'Begbie'</span>: [], <span class="st">'Act 1884'</span>: []}</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> act_snippets.items():</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snip <span class="kw">in</span> snippets:</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> embed_text(snip, focus_token<span class="op">=</span>keywords, window<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>        embeddings_dict[auth].append(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\torch\nn\modules\module.py:1762: FutureWarning:

`encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create embeddings that subtract the ethnicity anchor</span></span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>subtracted_embeddings_dict <span class="op">=</span> {<span class="st">'Crease'</span>: [], <span class="st">'Begbie'</span>: [], <span class="st">'Act 1884'</span>: []}</span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, embeddings <span class="kw">in</span> embeddings_dict.items():</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> emb <span class="kw">in</span> embeddings:</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> emb <span class="op">-</span> eth_anchor</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        subtracted_embeddings_dict[auth].append(v)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pairwise cosine similarity</span></span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>mean_crease <span class="op">=</span> np.mean(embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>mean_begbie <span class="op">=</span> np.mean(embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>mean_act_1884 <span class="op">=</span> np.mean(embeddings_dict[<span class="st">"Act 1884"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie <span class="op">=</span> cosine_similarity(mean_crease, mean_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>sim_crease_act_1884 <span class="op">=</span> cosine_similarity(mean_crease, mean_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb58-8"><a href="#cb58-8" aria-hidden="true" tabindex="-1"></a>sim_begbie_act_1884 <span class="op">=</span> cosine_similarity(mean_begbie, mean_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb58-9"><a href="#cb58-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-10"><a href="#cb58-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Crease and Begbie: </span><span class="sc">{</span>sim_crease_begbie<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb58-11"><a href="#cb58-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Crease and Act 1884: </span><span class="sc">{</span>sim_crease_act_1884<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb58-12"><a href="#cb58-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Begbie and Act 1884: </span><span class="sc">{</span>sim_begbie_act_1884<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine similarity between Crease and Begbie: 0.9893
Cosine similarity between Crease and Act 1884: 0.9757
Cosine similarity between Begbie and Act 1884: 0.9600</code></pre>
</div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pairwise cosine similarity by max and min poolings</span></span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>max_crease <span class="op">=</span> np.<span class="bu">max</span>(embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>min_crease <span class="op">=</span> np.<span class="bu">min</span>(embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>max_begbie <span class="op">=</span> np.<span class="bu">max</span>(embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>min_begbie <span class="op">=</span> np.<span class="bu">min</span>(embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>max_act_1884 <span class="op">=</span> np.<span class="bu">max</span>(embeddings_dict[<span class="st">"Act 1884"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>min_act_1884 <span class="op">=</span> np.<span class="bu">min</span>(embeddings_dict[<span class="st">"Act 1884"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-9"><a href="#cb60-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pairwise cosine similarity</span></span>
<span id="cb60-10"><a href="#cb60-10" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie_max <span class="op">=</span> cosine_similarity(max_crease, max_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb60-11"><a href="#cb60-11" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie_min <span class="op">=</span> cosine_similarity(min_crease, min_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb60-12"><a href="#cb60-12" aria-hidden="true" tabindex="-1"></a>sim_crease_act_1884_max <span class="op">=</span> cosine_similarity(max_crease, max_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb60-13"><a href="#cb60-13" aria-hidden="true" tabindex="-1"></a>sim_crease_act_1884_min <span class="op">=</span> cosine_similarity(min_crease, min_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb60-14"><a href="#cb60-14" aria-hidden="true" tabindex="-1"></a>sim_begbie_act_1884_max <span class="op">=</span> cosine_similarity(max_begbie, max_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb60-15"><a href="#cb60-15" aria-hidden="true" tabindex="-1"></a>sim_begbie_act_1884_min <span class="op">=</span> cosine_similarity(min_begbie, min_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb60-16"><a href="#cb60-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb60-17"><a href="#cb60-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb60-18"><a href="#cb60-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between max Crease and max Begbie: </span><span class="sc">{</span>sim_crease_begbie_max<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb60-19"><a href="#cb60-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between min Crease and min Begbie: </span><span class="sc">{</span>sim_crease_begbie_min<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb60-20"><a href="#cb60-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between max Crease and max Act 1884: </span><span class="sc">{</span>sim_crease_act_1884_max<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb60-21"><a href="#cb60-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between min Crease and min Act 1884: </span><span class="sc">{</span>sim_crease_act_1884_min<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb60-22"><a href="#cb60-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between max Begbie and max Act 1884: </span><span class="sc">{</span>sim_begbie_act_1884_max<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb60-23"><a href="#cb60-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between min Begbie and min Act 1884: </span><span class="sc">{</span>sim_begbie_act_1884_min<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine similarity between max Crease and max Begbie: 0.9540
Cosine similarity between min Crease and min Begbie: 0.9648
Cosine similarity between max Crease and max Act 1884: 0.9588
Cosine similarity between min Crease and min Act 1884: 0.9687
Cosine similarity between max Begbie and max Act 1884: 0.9401
Cosine similarity between min Begbie and min Act 1884: 0.9587</code></pre>
</div>
</div>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the pairwise cosine similarity with subtracted embeddings</span></span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>subtracted_mean_crease <span class="op">=</span> np.mean(subtracted_embeddings_dict[<span class="st">"Crease"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>subtracted_mean_begbie <span class="op">=</span> np.mean(subtracted_embeddings_dict[<span class="st">"Begbie"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>subtracted_mean_act_1884 <span class="op">=</span> np.mean(subtracted_embeddings_dict[<span class="st">"Act 1884"</span>], axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-6"><a href="#cb62-6" aria-hidden="true" tabindex="-1"></a>sim_crease_begbie_sub <span class="op">=</span> cosine_similarity(subtracted_mean_crease, subtracted_mean_begbie)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb62-7"><a href="#cb62-7" aria-hidden="true" tabindex="-1"></a>sim_crease_act_1884_sub <span class="op">=</span> cosine_similarity(subtracted_mean_crease, subtracted_mean_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb62-8"><a href="#cb62-8" aria-hidden="true" tabindex="-1"></a>sim_begbie_act_1884_sub <span class="op">=</span> cosine_similarity(subtracted_mean_begbie, subtracted_mean_act_1884)[<span class="dv">0</span>, <span class="dv">0</span>]</span>
<span id="cb62-9"><a href="#cb62-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb62-10"><a href="#cb62-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Crease and Begbie with ethnical axis removed: </span><span class="sc">{</span>sim_crease_begbie_sub<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb62-11"><a href="#cb62-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Crease and Act 1884 with ethnical axis removed: </span><span class="sc">{</span>sim_crease_act_1884_sub<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb62-12"><a href="#cb62-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Cosine similarity between Begbie and Act 1884 with ethnical axis removed: </span><span class="sc">{</span>sim_begbie_act_1884_sub<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine similarity between Crease and Begbie with ethnical axis removed: 0.9940
Cosine similarity between Crease and Act 1884 with ethnical axis removed: 0.9865
Cosine similarity between Begbie and Act 1884 with ethnical axis removed: 0.9776</code></pre>
</div>
</div>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create UMAP projection for visualization</span></span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> umap </span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>all_vecs <span class="op">=</span> np.vstack(embeddings_dict[<span class="st">"Crease"</span>] <span class="op">+</span> embeddings_dict[<span class="st">"Begbie"</span>] <span class="op">+</span> embeddings_dict[<span class="st">"Act 1884"</span>])</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>labels  <span class="op">=</span> ([<span class="st">"Crease"</span>] <span class="op">*</span> <span class="bu">len</span>(embeddings_dict[<span class="st">"Crease"</span>])) <span class="op">+</span> ([<span class="st">"Begbie"</span>] <span class="op">*</span> <span class="bu">len</span>(embeddings_dict[<span class="st">"Begbie"</span>])) <span class="op">+</span> ([<span class="st">'Act 1884'</span>] <span class="op">*</span> <span class="bu">len</span>(embeddings_dict[<span class="st">"Act 1884"</span>]))</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> umap.UMAP(n_neighbors<span class="op">=</span><span class="dv">15</span>, min_dist<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>proj <span class="op">=</span> reducer.fit_transform(all_vecs) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>umap_df <span class="op">=</span> pd.DataFrame(proj, columns<span class="op">=</span>[<span class="st">'UMAP 1'</span>, <span class="st">'UMAP 2'</span>])</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Author'</span>] <span class="op">=</span> labels</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Text'</span>] <span class="op">=</span> [snip <span class="cf">for</span> auth <span class="kw">in</span> act_snippets <span class="cf">for</span> snip <span class="kw">in</span> act_snippets[auth]]</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>umap_df[<span class="st">'Text'</span>] <span class="op">=</span> umap_df[<span class="st">'Text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> t: wrap_text(t, width<span class="op">=</span><span class="dv">50</span>))</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(umap_df, x<span class="op">=</span><span class="st">'UMAP 1'</span>, y<span class="op">=</span><span class="st">'UMAP 2'</span>, </span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>                 color<span class="op">=</span><span class="st">'Author'</span>, hover_data<span class="op">=</span>[<span class="st">'Text'</span>], </span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>                 width<span class="op">=</span><span class="dv">800</span>, height<span class="op">=</span><span class="dv">500</span> )</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>fig.update_traces(marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">5</span>))</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>fig.update_layout(title<span class="op">=</span><span class="st">'UMAP Projection of Word Embeddings by Author'</span>)</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\plotly\express\_core.py:1983: FutureWarning:

When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
</section>
<section id="zero-shot-stance-classification-with-legal-bert" class="level3">
<h3 class="anchored" data-anchor-id="zero-shot-stance-classification-with-legal-bert">Zero-shot Stance Classification with legal-BERT</h3>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the full snippets dictionary</span></span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>act_1884_full <span class="op">=</span> <span class="st">" "</span>.join(act_1884)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a>crease_cases_full <span class="op">=</span> <span class="st">" "</span>.join(crease_cases)</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>begbie_cases_full <span class="op">=</span> <span class="st">" "</span>.join(begbie_cases)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>full_cases <span class="op">=</span> {<span class="st">"act_1884"</span>: act_1884_full, <span class="st">"crease"</span>: crease_cases_full, <span class="st">"begbie"</span>: begbie_cases_full}</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>full_snippets <span class="op">=</span> {}</span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> author, text <span class="kw">in</span> full_cases.items():</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>    sentence <span class="op">=</span> sent_tokenize(text)</span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>    full_snippets[author] <span class="op">=</span> sentence</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="bu">len</span>(full_snippets[<span class="st">"act_1884"</span>]), <span class="bu">len</span>(full_snippets[<span class="st">"crease"</span>]), <span class="bu">len</span>(full_snippets[<span class="st">"begbie"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>(71, 356, 260)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create pipeline for zero-shot classification</span></span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a>zero_shot <span class="op">=</span> pipeline(</span>
<span id="cb72-5"><a href="#cb72-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"zero-shot-classification"</span>,</span>
<span id="cb72-6"><a href="#cb72-6" aria-hidden="true" tabindex="-1"></a>    model<span class="op">=</span><span class="st">"facebook/bart-large-mnli"</span>,</span>
<span id="cb72-7"><a href="#cb72-7" aria-hidden="true" tabindex="-1"></a>    tokenizer<span class="op">=</span><span class="st">"facebook/bart-large-mnli"</span>,</span>
<span id="cb72-8"><a href="#cb72-8" aria-hidden="true" tabindex="-1"></a>    hypothesis_template<span class="op">=</span><span class="st">"This legal text </span><span class="sc">{}</span><span class="st">."</span></span>
<span id="cb72-9"><a href="#cb72-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb72-10"><a href="#cb72-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-11"><a href="#cb72-11" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> [</span>
<span id="cb72-12"><a href="#cb72-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"advocates for equal legal treatment of Chinese immigrants compared to white or European settlers, opposing racial discrimination"</span>,</span>
<span id="cb72-13"><a href="#cb72-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"describes the status or treatment of Chinese immigrants without expressing support or opposition to racial inequality"</span>,</span>
<span id="cb72-14"><a href="#cb72-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">"justifies or reinforces unequal legal treatment of Chinese immigrants relative to white or European settlers, supporting racially discriminatory policies"</span></span>
<span id="cb72-15"><a href="#cb72-15" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb72-16"><a href="#cb72-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb72-17"><a href="#cb72-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_scores(snippet):</span>
<span id="cb72-18"><a href="#cb72-18" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> zero_shot(snippet, candidate_labels<span class="op">=</span>labels)</span>
<span id="cb72-19"><a href="#cb72-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">dict</span>(<span class="bu">zip</span>(out[<span class="st">"labels"</span>], out[<span class="st">"scores"</span>]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Device set to use cpu</code></pre>
</div>
</div>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Run zero-shot classification on the snippets from the Chinese Regulation Act 1884</span></span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>act_scores <span class="op">=</span> {}</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> full_snippets.items():</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>    scores <span class="op">=</span> []</span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snip <span class="kw">in</span> snippets:</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>        score <span class="op">=</span> get_scores(snip)</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>        scores.append(score)</span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    act_scores[auth] <span class="op">=</span> scores</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>rows <span class="op">=</span> []</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> auth, snippets <span class="kw">in</span> full_snippets.items():</span>
<span id="cb74-14"><a href="#cb74-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> snip, score_dict <span class="kw">in</span> <span class="bu">zip</span>(snippets, act_scores[auth]):</span>
<span id="cb74-15"><a href="#cb74-15" aria-hidden="true" tabindex="-1"></a>        row <span class="op">=</span> {</span>
<span id="cb74-16"><a href="#cb74-16" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Author"</span>: auth,</span>
<span id="cb74-17"><a href="#cb74-17" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Text"</span>: snip,</span>
<span id="cb74-18"><a href="#cb74-18" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Pro"</span>: score_dict[labels[<span class="dv">0</span>]],</span>
<span id="cb74-19"><a href="#cb74-19" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Neutral"</span>: score_dict[labels[<span class="dv">1</span>]],</span>
<span id="cb74-20"><a href="#cb74-20" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Cons"</span>: score_dict[labels[<span class="dv">2</span>]]</span>
<span id="cb74-21"><a href="#cb74-21" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb74-22"><a href="#cb74-22" aria-hidden="true" tabindex="-1"></a>        rows.append(row)</span>
<span id="cb74-23"><a href="#cb74-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-24"><a href="#cb74-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Create long-form DataFrame</span></span>
<span id="cb74-25"><a href="#cb74-25" aria-hidden="true" tabindex="-1"></a>df_scores <span class="op">=</span> pd.DataFrame(rows)</span>
<span id="cb74-26"><a href="#cb74-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-27"><a href="#cb74-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Group by author and calculate mean scores</span></span>
<span id="cb74-28"><a href="#cb74-28" aria-hidden="true" tabindex="-1"></a>mean_scores <span class="op">=</span> df_scores.groupby(<span class="st">"Author"</span>)[[<span class="st">"Pro"</span>, <span class="st">"Neutral"</span>, <span class="st">"Cons"</span>]].mean()</span>
<span id="cb74-29"><a href="#cb74-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-30"><a href="#cb74-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean scores by author:"</span>)</span>
<span id="cb74-31"><a href="#cb74-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mean_scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean scores by author:
               Pro   Neutral      Cons
Author                                
act_1884  0.331260  0.215666  0.453073
begbie    0.311259  0.295174  0.393567
crease    0.275872  0.272703  0.451425</code></pre>
</div>
</div>
<div class="cell" data-execution_count="76">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a>df_scores[<span class="st">'Text'</span>] <span class="op">=</span> df_scores[<span class="st">'Text'</span>].<span class="bu">apply</span>(<span class="kw">lambda</span> t: wrap_text(t, width <span class="op">=</span> <span class="dv">50</span>))</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> px.scatter(</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>    df_scores,</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"Pro"</span>,</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    y<span class="op">=</span><span class="st">"Cons"</span>,</span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"Author"</span>,</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>    hover_data<span class="op">=</span>[<span class="st">"Text"</span>],</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Pros vs Cons Scores by Author"</span>,</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span><span class="dv">800</span>,</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">600</span></span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>fig.update_traces(marker<span class="op">=</span><span class="bu">dict</span>(size<span class="op">=</span><span class="dv">5</span>))</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>fig.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\plotly\express\_core.py:1983: FutureWarning:

When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histogram for Pro scores</span></span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>fig_pro <span class="op">=</span> px.histogram(</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>    df_scores,</span>
<span id="cb79-4"><a href="#cb79-4" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"Pro"</span>,</span>
<span id="cb79-5"><a href="#cb79-5" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"Author"</span>,</span>
<span id="cb79-6"><a href="#cb79-6" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Distribution of Pro Scores by Author"</span>,</span>
<span id="cb79-7"><a href="#cb79-7" aria-hidden="true" tabindex="-1"></a>    nbins<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb79-8"><a href="#cb79-8" aria-hidden="true" tabindex="-1"></a>    opacity<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb79-9"><a href="#cb79-9" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>{<span class="st">"Pro"</span>: <span class="st">"Pro Score"</span>}</span>
<span id="cb79-10"><a href="#cb79-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-11"><a href="#cb79-11" aria-hidden="true" tabindex="-1"></a>fig_pro.update_layout(</span>
<span id="cb79-12"><a href="#cb79-12" aria-hidden="true" tabindex="-1"></a>    xaxis_title<span class="op">=</span><span class="st">"Pro Score"</span>,</span>
<span id="cb79-13"><a href="#cb79-13" aria-hidden="true" tabindex="-1"></a>    yaxis_title<span class="op">=</span><span class="st">"Frequency"</span>,</span>
<span id="cb79-14"><a href="#cb79-14" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span><span class="dv">800</span>,</span>
<span id="cb79-15"><a href="#cb79-15" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">500</span></span>
<span id="cb79-16"><a href="#cb79-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-17"><a href="#cb79-17" aria-hidden="true" tabindex="-1"></a>fig_pro.show()</span>
<span id="cb79-18"><a href="#cb79-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb79-19"><a href="#cb79-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot histogram for Cons scores</span></span>
<span id="cb79-20"><a href="#cb79-20" aria-hidden="true" tabindex="-1"></a>fig_cons <span class="op">=</span> px.histogram(</span>
<span id="cb79-21"><a href="#cb79-21" aria-hidden="true" tabindex="-1"></a>    df_scores,</span>
<span id="cb79-22"><a href="#cb79-22" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"Cons"</span>,</span>
<span id="cb79-23"><a href="#cb79-23" aria-hidden="true" tabindex="-1"></a>    color<span class="op">=</span><span class="st">"Author"</span>,</span>
<span id="cb79-24"><a href="#cb79-24" aria-hidden="true" tabindex="-1"></a>    title<span class="op">=</span><span class="st">"Distribution of Cons Scores by Author"</span>,</span>
<span id="cb79-25"><a href="#cb79-25" aria-hidden="true" tabindex="-1"></a>    nbins<span class="op">=</span><span class="dv">30</span>,</span>
<span id="cb79-26"><a href="#cb79-26" aria-hidden="true" tabindex="-1"></a>    opacity<span class="op">=</span><span class="fl">0.7</span>,</span>
<span id="cb79-27"><a href="#cb79-27" aria-hidden="true" tabindex="-1"></a>    labels<span class="op">=</span>{<span class="st">"Cons"</span>: <span class="st">"Cons Score"</span>}</span>
<span id="cb79-28"><a href="#cb79-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-29"><a href="#cb79-29" aria-hidden="true" tabindex="-1"></a>fig_cons.update_layout(</span>
<span id="cb79-30"><a href="#cb79-30" aria-hidden="true" tabindex="-1"></a>    xaxis_title<span class="op">=</span><span class="st">"Cons Score"</span>,</span>
<span id="cb79-31"><a href="#cb79-31" aria-hidden="true" tabindex="-1"></a>    yaxis_title<span class="op">=</span><span class="st">"Frequency"</span>,</span>
<span id="cb79-32"><a href="#cb79-32" aria-hidden="true" tabindex="-1"></a>    width<span class="op">=</span><span class="dv">800</span>,</span>
<span id="cb79-33"><a href="#cb79-33" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">500</span></span>
<span id="cb79-34"><a href="#cb79-34" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb79-35"><a href="#cb79-35" aria-hidden="true" tabindex="-1"></a>fig_cons.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\plotly\express\_core.py:1983: FutureWarning:

When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.

c:\Users\Kaiyan Zhang\AppData\Local\Programs\Python\Python312\Lib\site-packages\plotly\express\_core.py:1983: FutureWarning:

When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.
</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
<div class="cell-output cell-output-display">
<pre><code>Unable to display output for mime type(s): application/vnd.plotly.v1+json</code></pre>
</div>
</div>
</section>
<section id="domainadaptive-pretraining" class="level3">
<h3 class="anchored" data-anchor-id="domainadaptive-pretraining">Domain‐Adaptive Pretraining</h3>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/80x15.png"></a>.  <a rel="license" href="https://comet.arts.ubc.ca/pages/copyright.html">See details.</a>
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 The prAxIs Project and UBC are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples.
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>