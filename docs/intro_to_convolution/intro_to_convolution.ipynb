{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b6191346-dcb7-4db1-bc3f-3d4de7e3aa50",
      "metadata": {},
      "source": [
        "# Introduction to Convolutions\n",
        "\n",
        "prAxIs UBC Team <br>*Kaiyan Zhang*  \n",
        "2025-07-22\n",
        "\n",
        "<figure>\n",
        "<img src=\"images/convolution-operation-diagram.png\"\n",
        "alt=\"Figure 1. An example of convolution operation (Created by the author)\" />\n",
        "<figcaption aria-hidden=\"true\"><em>Figure 1.</em> An example of\n",
        "convolution operation (Created by the author)</figcaption>\n",
        "</figure>\n",
        "\n",
        "Before you start, make sure you have the required libraries installed,\n",
        "if not, simply **uncomment the lines below** and run the cell to install\n",
        "them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d86f9a47",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install opencv-python\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n",
        "# !pip install pandas\n",
        "# !pip install scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f42fcea-07b3-4b77-a424-dfc6828c3577",
      "metadata": {},
      "source": [
        "### Why Convolutions?\n",
        "\n",
        "Mathematically speaking, **Convolution** is an operation that combines\n",
        "two functions to produce a third function, which has a variety of\n",
        "applications in signal processing, image analysis and more. While this\n",
        "may sound complex, we can minimize the math behind it and explain it in\n",
        "a less harmful and vivid way.\n",
        "\n",
        "Let’s begin by imagining a simple scenario: you took a picture of a cute\n",
        "dog, and you want to apply a filter to it so that it looks more vibrant\n",
        "and colorful. Now that you input the image into a computer, how does a\n",
        "computer “see” it? The computer would see the image as a grid of\n",
        "numbers, where the combination of 3 numbers (R, G, B) in a grid\n",
        "represents the color of a pixel, and with all the colored pixels\n",
        "combined, it forms the image. Given the numeric nature of a computer\n",
        "image, we say that the image is digitalized.\n",
        "\n",
        "Here, let’s read in the image into Python using `OpenCV` and display it\n",
        "using `matplotlib`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e41a72a4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Read the image in BGR format\n",
        "# Note this is the default input format of Python OpenCV\n",
        "doge_bgr = cv2.imread('images/Original_Doge_meme.jpg')\n",
        "\n",
        "# Therefore we need to convert the color mapping to RGB\n",
        "doge_rgb = cv2.cvtColor(doge_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Display the image using matplotlib\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(doge_rgb)\n",
        "plt.axis('off')  \n",
        "plt.title('Input Image \\n Sato (2010).\"Kabosu the Shiba Inu\"')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27096ec9-e0b3-49db-a1f9-54c7c30e7fd4",
      "metadata": {},
      "source": [
        "The image doesn’t look like numbers, does it? In fact, if we zoom in\n",
        "close enough, we can clearly see that the image is made up of pixels.\n",
        "But we still don’t see the numbers, this is because the numerical\n",
        "information is decoded by the computer and displayed as a colored pixel.\n",
        "However, we can easily convert the image into a numerical\n",
        "representation.\n",
        "\n",
        "For demonstration purpose, the image is rescaled to a 10 $\\times$ 10\n",
        "grid, where within each cell, the numbers represents the R, G, B values\n",
        "(0-255) of each pixel.\n",
        "\n",
        "Note that compressing images to a smaller size is always easier\n",
        "comparing to enhancing images to a larger size, as compression can be\n",
        "done by going through the image pixel by pixel and averaging the color\n",
        "values in each cell, while enhancement usually requires more complex\n",
        "operations. This gives us a hint on why convolution is important."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d039ab8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "img_rgb = cv2.resize(doge_rgb, (10, 10), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "# Plot the RGB matrix\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "\n",
        "\n",
        "h, w = img_rgb.shape[:2]\n",
        "ax.imshow(img_rgb, extent=[0, w, h, 0]) \n",
        "\n",
        "# Set ticks to show grid at pixel boundaries\n",
        "ax.set_xticks(np.arange(0, w + 1, 1))\n",
        "ax.set_yticks(np.arange(0, h + 1, 1))\n",
        "ax.set_xticklabels([])\n",
        "ax.set_yticklabels([])\n",
        "ax.grid(color='black', linewidth=1)\n",
        "\n",
        "for i in range(h):\n",
        "    for j in range(w):\n",
        "        r, g, b = img_rgb[i, j]\n",
        "        brightness = int(r) + int(g) + int(b)\n",
        "        color = 'white' if brightness < 380 else 'black'\n",
        "        ax.text(j + 0.5, i + 0.5, f'({r},{g},{b})',\n",
        "                ha=\"center\", va=\"center\", fontsize=6, color=color)\n",
        "        \n",
        "        \n",
        "# Display the Grid\n",
        "ax.set_title(\"RGB Value Grid of Doge Image Resized to 10x10\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08166d5d-8480-470e-9e30-8fe6bbe19226",
      "metadata": {},
      "source": [
        "The image has become a little abstract. Can you still identify the\n",
        "original image out of it?\n",
        "\n",
        "While the resized image looked significantly different, it still\n",
        "contains the necessary information, and same ideas also applies to\n",
        "larger images. That is, all images can be represented as a grid of\n",
        "numbers, where the 3 numbers in each cell corresponds to the color of a\n",
        "pixel. Computers can’t see colors like we do, the way they see colors is\n",
        "as if they were mixing colors using a palette that only has red, green\n",
        "and blue (**RGB**), where each color has an “amount” of intensity\n",
        "between 0 and 255. With the 3 values for red, green and blue, computers\n",
        "can create any color we see in the world.\n",
        "\n",
        "Back to the dog picture, it is easy to see that resizing the image to a\n",
        "smaller grid loses a lot of details, especially the rich color that\n",
        "makes the image vibrant. If we want to keep the complete color\n",
        "information, alternatively, we can plot out the distribution of the RGB\n",
        "values and frequencies in the image using a histogram. While this gives\n",
        "us a good idea of the color distribution, it does not tell us much about\n",
        "the spatial relationships between the pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d5cc6d94",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute and plot the color histogram\n",
        "colors = ('r', 'g', 'b')\n",
        "plt.figure(figsize=(6, 4))\n",
        "\n",
        "for i, col in enumerate(colors):\n",
        "    hist = cv2.calcHist(\n",
        "        images=[doge_bgr],       # source image (still in BGR)\n",
        "        channels=[i],           # which channel: 0=B, 1=G, 2=R\n",
        "        mask=None,              \n",
        "        histSize=[256],         \n",
        "        ranges=[0, 256]         \n",
        "    )\n",
        "    plt.plot(hist, color=col)\n",
        "    plt.xlim([0, 256])\n",
        "    \n",
        "# Display the histogram\n",
        "plt.title('RGB Histogram')\n",
        "plt.xlabel('Pixel Value')\n",
        "plt.ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8c94179-f818-43d6-a0bf-aa23681be360",
      "metadata": {},
      "source": [
        "Can you still identify the original image out of it?\n",
        "\n",
        "The example above shows us what images are like in the eyes of a\n",
        "computer. Computers do not understand images in the same way that humans\n",
        "do, they can only see them as a collection of numbers. It thus make\n",
        "sense that we need to apply some math to these numbers to either\n",
        "**change the image** or **extract some useful information** from it, and\n",
        "that’s where convolution comes in.\n",
        "\n",
        "### How Does Convolution Work?\n",
        "\n",
        "Before we dive into the application of it, let’s first understand how\n",
        "convolution operate on an image in a more intuitive way:\n",
        "\n",
        "Think of copying a painting by first sketching its outline at the same\n",
        "size. To add your own flair, you use a patterned brush: wherever the\n",
        "brush touches, it brightens the paint. You move this brush methodically\n",
        "across your sketch, from left to right, top to bottom so every spot is\n",
        "stamped with the pattern, tweaking the original image into something\n",
        "familiar yet distinctly styled.\n",
        "\n",
        "Here, the brush you used is called a **kernel** in the context of\n",
        "convolution, and the process of applying the brush is what we call\n",
        "**convolution operation**. We would define the kernel as a small matrix\n",
        "of numbers that represents the pattern of the brush, and the convolution\n",
        "operation as the process of transforming the original image by applying\n",
        "the kernel to it.\n",
        "\n",
        "Here is a gif illustrating how our filter (the kernel) will work on the\n",
        "image mathematically. You can see it as the small brush that slides over\n",
        "the image, operating on a small region of the image at a time, and\n",
        "eventually producing a new image that was completely transformed by the\n",
        "filter.\n",
        "\n",
        "<figure>\n",
        "<img src=\"images/sharpener_kernel.gif\"\n",
        "alt=\"Figure 2. Visual explanation of sharpening filter (Michael Plotke, Wikipedia, 2013)\" />\n",
        "<figcaption aria-hidden=\"true\"><em>Figure 2.</em> Visual explanation of\n",
        "sharpening filter (Michael Plotke, Wikipedia, 2013)</figcaption>\n",
        "</figure>\n",
        "\n",
        "Now, let’s return to the example of the cute dog picture. What we are\n",
        "going to apply is a brush called **sharpening filter**, it is a 3\n",
        "$\\times$ 3 matrix that looks like this:\n",
        "\n",
        "$$\n",
        "\\text{Sharpening Filter} = \\begin{bmatrix}\n",
        " 0 &-1 & 0 \\\\\n",
        "-1 & 5 & -1 \\\\\n",
        " 0 & -1 & 0 \n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Don’t panic as we are not going to do any math here, we will just let\n",
        "computer do the math for us. The only thing you need to know is that\n",
        "this kernel will enhance the edges of the image, making it look sharper\n",
        "and more defined. For better understanding, let’s visualize it to see\n",
        "what this kernel look like in grey-scale:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "37db6be7",
      "metadata": {},
      "outputs": [],
      "source": [
        "sharp_kernel = np.array([\n",
        "    [ 0, -1,  0],\n",
        "    [-1,  5, -1],\n",
        "    [ 0, -1,  0]\n",
        "], dtype=np.float32)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(4, 4))\n",
        "\n",
        "# Display with true gray-scale between kernel's min and max\n",
        "im = ax.imshow(sharp_kernel, cmap='gray_r',\n",
        "               vmin=sharp_kernel.min(), vmax=sharp_kernel.max())\n",
        "ax.set_title('Sharpening Kernel')\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371e4252-e3cd-439d-9233-dbeef89884d6",
      "metadata": {},
      "source": [
        "If you take a closer look at the kernel, you will see that it has a\n",
        "positive value in the center and negative values around it, and it has 0\n",
        "values on the corners. This exactly looks like a brush that enhances the\n",
        "center of a region while reducing the intensity of the surrounding\n",
        "pixels, which is exactly what we want to achieve with the filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d2a37df1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the sharpening filter\n",
        "kernel = np.array([\n",
        "    [ 0, -1,  0],\n",
        "    [-1,  5, -1],\n",
        "    [ 0, -1,  0]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Apply the kernel to the color image using filter2D.\n",
        "filtered = cv2.filter2D(doge_rgb, ddepth=-1, kernel=kernel)\n",
        "\n",
        "filtered = np.clip(filtered, 0, 255).astype(np.uint8)\n",
        "\n",
        "# Display the original and filtered images side by side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "# Input\n",
        "ax1.imshow(doge_rgb)\n",
        "ax1.set_title('Input Image \\n Sato (2010).\"Kabosu the Shiba Inu\"')\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "# Output\n",
        "ax2.imshow(filtered)\n",
        "ax2.set_title(\"Filtered Image (Sharper & More Vibrant)\")\n",
        "ax2.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea845e59-2f90-4761-8984-20aa4ae81dc7",
      "metadata": {},
      "source": [
        "The difference is quite obvious now. You can clearly see that the\n",
        "sharpened image (R) has more contrast and the edges are more defined,\n",
        "making it look more vibrant and colorful. This is the power of\n",
        "convolution, it allows us to apply filters to images and transform them\n",
        "in a way that is not possible with simple pixel manipulation. Similarly,\n",
        "we can blur an image quite easily, for which the “brush” we are going to\n",
        "use looks like this\n",
        "\n",
        "$$\\text{Box Blur Filter} = \\begin{bmatrix}\n",
        "\\frac{1}{9} &\\frac{1}{9} & \\frac{1}{9} \\\\\n",
        "\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9} \\\\\n",
        "\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9} \n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Visually, this kernel is a filter that looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c5abd157",
      "metadata": {},
      "outputs": [],
      "source": [
        "id_kernel = np.array([\n",
        "  [ 1, 1, 1],\n",
        "  [ 1, 1, 1],\n",
        "  [ 1, 1, 1]\n",
        "])\n",
        "\n",
        "blur_kernel = np.array([\n",
        "    [1/9, 1/9, 1/9],\n",
        "    [1/9, 1/9, 1/9],\n",
        "    [1/9, 1/9, 1/9]\n",
        "], dtype=np.float32)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "for ax, kernel, title in zip(axes, [id_kernel, blur_kernel],\n",
        "                             ['Input Image', 'Box Blur Kernel']):\n",
        "    # Show actual values in gray-scale\n",
        "    im = ax.imshow(kernel, cmap='gray_r', vmin=0, vmax=1)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Draw horizontal and vertical boundary lines at half‐integer positions\n",
        "    for i in range(1, kernel.shape[0]):\n",
        "        ax.axhline(i - .5, color='white', linewidth=1.5, zorder=1)\n",
        "        ax.axvline(i - .5, color='white', linewidth=1.5, zorder=1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba4fc6b-2d9d-40e8-87c7-92c9e195feff",
      "metadata": {},
      "source": [
        "Let’s see what it will make on our input image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a18efeaa",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the box blur filter\n",
        "kernel = np.array([\n",
        "    [1/9, 1/9, 1/9],\n",
        "    [1/9, 1/9, 1/9],\n",
        "    [1/9, 1/9, 1/9]\n",
        "], dtype=np.float32)\n",
        "\n",
        "# Apply the kernel to the color image using filter2D.\n",
        "filtered = cv2.filter2D(doge_rgb, ddepth=-1, kernel=kernel)\n",
        "\n",
        "filtered = np.clip(filtered, 0, 255).astype(np.uint8)\n",
        "\n",
        "# Display the original and filtered images side by side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "# Input\n",
        "ax1.imshow(doge_rgb)\n",
        "ax1.set_title('Input Image\\n Sato (2010).\"Kabosu the Shiba Inu\"')\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "# Output\n",
        "ax2.imshow(filtered)\n",
        "ax2.set_title(\"Filtered Image (Blurred)\")\n",
        "ax2.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c93cb8-ba35-4a9f-a85f-373a78898983",
      "metadata": {},
      "source": [
        "It worked as we expected, now the dog image becomes more blurry.\n",
        "\n",
        "However, convolution on image is not limited to filtering, it can also\n",
        "be used to **extract features** from the image. For example, we can use\n",
        "a kernel to detect edges, lines and texts in images, we can even use\n",
        "specific kernels to detect specific shapes or patterns in images, such\n",
        "as a kernel that detects anything that looks like a dog. As you can\n",
        "imagine, this is a very powerful tool in many applications, in fact, you\n",
        "most likely have already used it in your daily life. For example, when\n",
        "you use a photo editing app to apply a filter to your picture, the app\n",
        "is using convolution. When you use a search engine to search do image\n",
        "searches, the search engine is using convolution to extract features\n",
        "from the images and match them with your search query. Let’s say, the\n",
        "thing you are trying to find is eyes, the gif below shows how a\n",
        "convolution kernel detects “eyes” in an image:\n",
        "\n",
        "<figure>\n",
        "<img src=\"images/eye_detection_kernel.gif\" alt=\"Figure 3. Eye detection kernel (Neuromatch Academy, 2025)\" style=\"width:50%; height:auto;\" />\n",
        "<figcaption aria-hidden=\"true\"><em>Figure 3.</em> Eye detection kernel (Neuromatch Academy, 2025)</figcaption>\n",
        "</figure>\n",
        "\n",
        "This is a very simple example, and it is implemented exactly the same\n",
        "way as we did with the sharpening filter. The only difference is that to\n",
        "extract a specific features, we need to use a “brush” designed to detect\n",
        "that feature, which usually requires some knowledge of the feature we\n",
        "want to extract. For example, if we want to detect eyes in the painting,\n",
        "we would need our “brush” to understand what eyes look like and what\n",
        "typical colors they have. This could be way too complicated for a single\n",
        "“brush”, so we often use multiple brushes to detect different features\n",
        "when it comes to the task of feature extraction.\n",
        "\n",
        "To demonstrate how convolution extracts a specific feature from an\n",
        "image, let’s take a look at a different “art tool”. Let’s say this time\n",
        "you don’t want to color the painting differently, but rather you want to\n",
        "sketch a line art based on the original painting. You would use a\n",
        "fineliner pen that detects the edges of a painting and draw a line along\n",
        "them. In the eyes of a computer, these are the tools it is going to use:\n",
        "\n",
        "$$\\text{Horizontal Sobel} = \\begin{bmatrix}\n",
        " 1 & 0 & -1 \\\\\n",
        " 2 & 0 & -2 \\\\\n",
        " 1 & 0 & -1\n",
        "\\end{bmatrix} \n",
        "\\text{, }\n",
        "\\text{Vertical Sobel} = \\begin{bmatrix}\n",
        " 1 & 2 & 1 \\\\\n",
        " 0 & 0 & 0 \\\\\n",
        " -1 & -2 & -1\n",
        "\\end{bmatrix} \n",
        "$$\n",
        "\n",
        "Still, don’t panic, we won’t do any math in this notebook. All you need\n",
        "to know is these two kernels together are called **Sobel filter**, and\n",
        "what they do is highlighting the edges in the image, making them more\n",
        "visible. We can also visualize them as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3149f67e",
      "metadata": {},
      "outputs": [],
      "source": [
        "h_kernel = np.array([\n",
        "    [ 1, 0, -1],\n",
        "    [ 2, 0, -2],\n",
        "    [ 1, 0, -1]\n",
        "])\n",
        "\n",
        "v_kernel = np.array([\n",
        "    [ 1, 2, 1],\n",
        "    [ 0, 0, 0],\n",
        "    [ -1, -2, -1]\n",
        "])\n",
        "\n",
        "# Plot the kernels\n",
        "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
        "\n",
        "for ax, kernel, title in zip(axes, [np.abs(h_kernel), np.abs(v_kernel)],\n",
        "                             ['Horizontal Kernel', 'Vertical Kernel']):\n",
        "    # Show actual values in gray-scale\n",
        "    im = ax.imshow(kernel, cmap='gray_r', vmin=0, vmax=2)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a84841f5-cec7-4271-9bf6-b15107d2d707",
      "metadata": {},
      "source": [
        "If you take a closer look at the kernels, you will see that the first\n",
        "one has positive values on the left and negative values on the right,\n",
        "while the second one has positive values on the top and negative values\n",
        "on the bottom. This pattern intuitively tells us that the first “pen”\n",
        "will scan through the image horizontally and extract the horizontal\n",
        "edges, while the second “pen” will scan through the image vertically and\n",
        "extract the vertical edges.\n",
        "\n",
        "Let’s now look at a different example and see what happens when we apply\n",
        "the Sobel filter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "403384a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a greyscale version of the image\n",
        "hill_bgr = cv2.imread('images/xiangbishan.jpg')\n",
        "\n",
        "hill_rgb = cv2.cvtColor(hill_bgr, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "hill_gray = cv2.cvtColor(hill_bgr, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Define the Sobel filter kernel\n",
        "sb_kernel_h = np.array([\n",
        "    [ 1, 0, -1],\n",
        "    [ 2, 0, -2],\n",
        "    [ 1, 0, -1]\n",
        "], dtype=np.float32)\n",
        "\n",
        "sb_kernel_v = np.array([\n",
        "    [ 1, 2, 1],\n",
        "    [ 0, 0, 0],\n",
        "    [-1, -2, -1]], dtype=np.float32)\n",
        "\n",
        "# Apply the kernel to the color image using filter2D.\n",
        "horiz = cv2.filter2D(hill_gray, -1, sb_kernel_h)\n",
        "\n",
        "vert = cv2.filter2D(hill_gray, -1, sb_kernel_v)\n",
        "\n",
        "combined = cv2.convertScaleAbs(np.sqrt(vert.astype(np.float32)**2 + horiz.astype(np.float32)**2))\n",
        "\n",
        "# Display the original and filtered images side by side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n",
        "\n",
        "# Input\n",
        "ax1.imshow(hill_rgb)\n",
        "ax1.set_title('Input Image \\n xiquinhosilva (2018). \"Elephant Trunk Hill\"')\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "# Output\n",
        "ax2.imshow(combined, cmap = 'gray')\n",
        "ax2.set_title(\"Filtered Image (Edges Highlighted)\")\n",
        "ax2.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a1f46d2-5024-42ba-891d-ab7529e3bdda",
      "metadata": {},
      "source": [
        "As we can see in the example above, the Sobel filter detects the edges\n",
        "in the image and highlights them like a fineliner pen. This is a very\n",
        "useful technique in image processing, as it allows us to extract\n",
        "features from the image that can be used for further analysis or\n",
        "classification. Let’s do more explorations with the example above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fdf2821d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the magnitude and orientation of the gradient.\n",
        "# To put it simpler, they represents the length and direction of the edges at each pixel.\n",
        "# For those who are familiar with pythagorean theorem, the magnitude is exactly the length of the\"long edge\" calculated with pythagorean theorem.\n",
        "magnitude = np.sqrt(vert.astype(np.float32)**2 + horiz.astype(np.float32)**2)\n",
        "orientation = np.arctan2(vert.astype(np.float32), horiz.astype(np.float32))\n",
        "\n",
        "# Generate an edge binary map\n",
        "mag_norm = cv2.normalize(magnitude, None, 0,255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "_, edge_binary = cv2.threshold(mag_norm, 50, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "# Create the Contour plot\n",
        "contours, _ = cv2.findContours(edge_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "contour_img = hill_rgb.copy()\n",
        "cv2.drawContours(contour_img, contours, -1, (255,0,0), 1)\n",
        "\n",
        "# Create an edge density heatmap\n",
        "block = 32\n",
        "h, w = edge_binary.shape\n",
        "int_img = cv2.integral(edge_binary)\n",
        "\n",
        "heatmap = np.zeros_like(edge_binary, dtype=np.float32)\n",
        "\n",
        "for y in range(0, h - block + 1):\n",
        "    for x in range(0, w - block + 1):\n",
        "        y1, x1 = y,       x\n",
        "        y2, x2 = y + block, x + block\n",
        "        total = (int_img[y2, x2]\n",
        "               - int_img[y1, x2]\n",
        "               - int_img[y2, x1]\n",
        "               + int_img[y1, x1])\n",
        "        # center the block’s density back into the heatmap\n",
        "        heatmap[y + block//2, x + block//2] = total\n",
        "        \n",
        "heatmap = cv2.normalize(heatmap, None, 0,255, cv2.NORM_MINMAX).astype(np.uint8)\n",
        "\n",
        "# Plot out the visualizations\n",
        "fig, axs = plt.subplots(2, 2, figsize=(8, 5))\n",
        "ax1, ax2, ax3, ax4 = axs.flatten()\n",
        "\n",
        "# Original Image\n",
        "ax1.imshow(hill_rgb)\n",
        "ax1.set_title(\"Input Image\")\n",
        "ax1.axis(\"off\")\n",
        "\n",
        "# Contours on Original Image\n",
        "ax2.imshow(contour_img)\n",
        "ax2.set_title(\"Contours on Original Image\")\n",
        "ax2.axis(\"off\")\n",
        "\n",
        "# Edge Density Heatmap\n",
        "ax3.imshow(heatmap, cmap=\"hot\")\n",
        "ax3.set_title(\"Edge Density Heatmap\")\n",
        "ax3.axis(\"off\")\n",
        "\n",
        "# Edge Orientation Histogram\n",
        "angles_deg = np.degrees(orientation[magnitude > 50].flatten())\n",
        "\n",
        "ax4.hist(angles_deg, bins=36, range=(0, 90), color='purple')\n",
        "ax4.set_title(\"Edge Orientation Histogram\")\n",
        "ax4.set_xlabel(\"Angle (degrees)\")\n",
        "ax4.set_ylabel(\"Frequency\")\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71ac136c-5f59-4fa2-bbd9-c98f74cc787c",
      "metadata": {},
      "source": [
        "Here, we have generated three other visualizations based on the\n",
        "extracted edge data.\n",
        "\n",
        "-   The first visualization is a **Contour Map** on the original image.\n",
        "    It circles contours from the image that are distinguishable from\n",
        "    others. In many cases, this implies the circled regions have\n",
        "    different visual patterns or belong to different objects, which is\n",
        "    very useful in tasks such as image recognition.\n",
        "-   The second visualization is the **Edge Density Heatmap** that\n",
        "    assigns corresponding heat values to regions on the image based on\n",
        "    the density distribution of the edges. The lighter the color, the\n",
        "    higher the edge density of the region. This helps us to understand\n",
        "    which area of the image carries the most information that may be of\n",
        "    interest to us.\n",
        "-   The third visualization is a histogram showing the distribution of\n",
        "    **edge orientations**. edge orientation is the direction of the edge\n",
        "    in the graph, expressed as the angle between the edge and the x-axis\n",
        "    (horizontal line). Understanding the distribution of edge\n",
        "    orientation can help us better recognize the graphical features of\n",
        "    objects in an image for tasks such as classification. For example,\n",
        "    based on this image featuring a hill and a water surface, we now\n",
        "    know that the edges of these two objects typically have either a\n",
        "    horizontal or vertical orientation.\n",
        "\n",
        "The examples above demonstrated the power of convolution in both image\n",
        "processing and image analysis, and more importantly, convolution is very\n",
        "efficient, as it is easy for computers to understand and process, and\n",
        "can be applied to images of any size without losing any information.\n",
        "This is why convolution has become a fundamental operation in computer\n",
        "vision and image processing.\n",
        "\n",
        "But as you have probably aware, implementing convolution from scratch\n",
        "can be quite tedious, especially when we need to perform more specific\n",
        "tasks such as detecting texts or some specific shapes. To protect the\n",
        "brains of computer scientists, a more advanced, adaptable and efficient\n",
        "way of applying convolution has been developed, which is called\n",
        "**Convolutional Neural Network (CNN)**, and we will discuss it in the\n",
        "next notebook, hopefully in a way that also protects your brain.\n",
        "\n",
        "### Key takeaways:\n",
        "\n",
        "1.  Computers see images as grids of numbers, where each grid cell\n",
        "    contains the RGB values of a pixel. With the spatial relationships\n",
        "    and the color information, computers can understand images without\n",
        "    losing information.\n",
        "2.  **Convolution** is an operation that applies a filter (kernel) to an\n",
        "    image, transforming it in a way that enhances certain features or\n",
        "    extracts useful information. It is like using a brush to color a\n",
        "    painting or a pen to sketch a line art.\n",
        "3.  The kernel that sharpens an image is called a **sharpening filter**,\n",
        "    which enhances the edges of the image and makes it look more\n",
        "    vibrant. The kernel that detects edges is called **Sobel filter**,\n",
        "    which highlights the edges in the image and makes them more visible.\n",
        "4.  Convolution is a powerful tool that can be used in many\n",
        "    applications, such as photo editing, image search and feature\n",
        "    extraction. It is efficient and can be applied to images of any size\n",
        "    without losing information.\n",
        "\n",
        "### Glossary\n",
        "\n",
        "-   **Computer Vision**: Computer vision is a field of artificial\n",
        "    intelligence that enables computers to “see” and interpret images\n",
        "    and videos in a way that is similar to human vision.\n",
        "-   **Convolution**: Convolution is an operation that applies a filter\n",
        "    (kernel) to an image, transforming it in a way that enhances certain\n",
        "    features or extracts useful information. It is like using a brush to\n",
        "    color a painting or a pen to sketch a line art.\n",
        "-   **Kernel/Filter/Mask**: A convolution kernel, also known as a filter\n",
        "    or mask, is a small matrix (array of numbers) used in image\n",
        "    processing and computer vision to perform operations like blurring,\n",
        "    sharpening, edge detection, and more.\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "-   **3Blue1Brown.** (2018, August 15). *But what is a convolution?*\n",
        "    \\[Video\\]. YouTube. <https://www.youtube.com/watch?v=KuXjwB4LzSA>\n",
        "    This video graphically explains the mathematical operation of\n",
        "    convolution.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "### References\n",
        "\n",
        "-   Wikipedia contributors. (2025, June 18). *Convolution*. In\n",
        "    *Wikipedia, The Free Encyclopedia*. Retrieved June 18, 2025, from\n",
        "    <https://en.wikipedia.org/wiki/Convolution>\n",
        "\n",
        "-   Sato, A. (2010, September 13). Kabosu the Shiba Inu (“Doge”)\n",
        "    \\[Photograph\\]. Flickr. This photo is cited and used under  \n",
        "    <https://creativecommons.org/licenses/by-sa/2.0/>\n",
        "\n",
        "-   xiquinhosilva. (2018, June 14), Elephant Trunk Hill \\[Photograph\\].\n",
        "    Flickr. This photo is cited and used under  \n",
        "    <https://creativecommons.org/licenses/by-sa/2.0/>\n",
        "\n",
        "-   GeeksforGeeks. (n.d.). *Types of Convolution Kernels*. Retrieved\n",
        "    July 16, 2025, from\n",
        "    <https://www.geeksforgeeks.org/deep-learning/types-of-convolution-kernels>\n",
        "\n",
        "-   OpenCV.org. (n.d.). *Image Processing in OpenCV*. Retrieved\n",
        "    July 16, 2025, from\n",
        "    <https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "C:Zhang"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
