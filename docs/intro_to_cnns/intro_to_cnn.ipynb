{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mostly Harmless Convolutional Neural Networks (CNNs)\n",
        "\n",
        "*Kaiyan Zhang, prAxIs UBC Team*  \n",
        "2025-07-24\n",
        "\n",
        "**Before you begin: Install the dependencies that you don’t have by\n",
        "running the code cell below.**"
      ],
      "id": "49d60ad3-12b5-4578-bcb2-8e545d861af9"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install opencv-python\n",
        "# !pip install numpy\n",
        "# !pip install matplotlib\n",
        "# !pip install pandas\n",
        "# !pip install scikit-learn\n",
        "# !pip install seaborn\n",
        "# !pip install datasets\n",
        "# !pip install torch\n",
        "# !pip install tqdm"
      ],
      "id": "f36a36fe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"media/CNN_example.jpg\" alt=\"A visual explanation of CNN\" width=\"800\"/>\n",
        "\n",
        "### What are Neural Networks?\n",
        "\n",
        "What is the first thing that comes to your mind when you hear the word\n",
        "“**neural network**”? If you are thinking about the human brain and\n",
        "neurons, you are not wrong. In fact, the term “neural network” is\n",
        "inspired by the way how human brain and nervous systems work, where\n",
        "neurons are connected to each other and communicate with each other to\n",
        "process information.\n",
        "\n",
        "In the context of **machine learning**, a neural network, or more\n",
        "precisely, an **artificial neural network (ANN)** is defined as “a\n",
        "program, or model, that makes decisions in a manner similar to the human\n",
        "brain, by using processes that mimic the way biological neurons work\n",
        "together to identify phenomena, weigh options and arrive at\n",
        "conclusions”.\n",
        "\n",
        "The definition seems way too formal and scientific, but we can easily\n",
        "translate it into daily language. Think of taking a closed-book multiple\n",
        "choice exam (Oops, gross). Your brain calls on a team of “experts”, one\n",
        "for course facts, another for what you remember about the professor’s\n",
        "hints in class, another for gut instincts, etc. When you read a\n",
        "question, each expert gives you a confidence score. You weight each\n",
        "score by how much you trust that expert, then add them up. The answer\n",
        "with the highest total “trust $\\times$ confidence” wins. After the exam,\n",
        "you see which answers were wrong and adjust those trust weights (trust\n",
        "the right experts more, the wrong ones less), and prepare for the next\n",
        "exam based on this experience. This exactly how a neural network makes\n",
        "decisions and learns via its **feedback loop**. Neural networks are\n",
        "following a similar thought and learning process as you and me, and this\n",
        "is why they are flexible and powerful, being able to handle complex,\n",
        "abstract tasks and evolve on their own, **like an intelligent\n",
        "creature**.\n",
        "\n",
        "While the core idea is not complex, you may want to master some bluffing\n",
        "terms to translate the professional discussions. In the example above,\n",
        "the “experts” you consulted in your mind are called **neurons**; the key\n",
        "clues you noticed when reading question are called **features**; your\n",
        "understanding of exam question is called the **input layer**; your\n",
        "thought process rounds are called **hidden layers**; your chosen answer\n",
        "is reflected as the **output layer**; the mind map that connects all the\n",
        "“experts” and input features is **architecture**; and each exam attempt\n",
        "with the review of feedback is called a **training epoch**. See, they\n",
        "are really not that deep! You now can also talk about it as an expert.\n",
        "\n",
        "### An Intuitive Understanding of Convolutional Neural Networks (CNNs)\n",
        "\n",
        "Now that we understood what is an artificial neural network, let’s dive\n",
        "into the real topic here: What’s unique about **convolutional neural\n",
        "networks (CNNs)** and why they are revolutionary to computer vision and\n",
        "image processing?\n",
        "\n",
        "Let’s start by discussing the unique point of CNNs. Imagine you are\n",
        "reading a bird guide and trying to learn the characteristics of a night\n",
        "heron and a grey heron so that you can easily distinguish between the\n",
        "two in the field, what would you do? I believe you would naturally try\n",
        "to observe the birds piece by piece: first comparing the features of the\n",
        "juveniles and adults, then noting how they look both in flight and on\n",
        "land. Gradually, your brain forms a complete comparison: the night heron\n",
        "has a shorter beak, a shorter neck, striking red eyes, and dark blue\n",
        "plumage as an adult; while the grey heron has a longer beak, a longer\n",
        "neck, yellow eyes, and wears grey color plumage."
      ],
      "id": "be473275-c916-4742-b255-f9b0191bb4c1"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<p align=\"center\">"
      ],
      "id": "19761a61-92fd-411a-85fe-b151693fb5fc"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure style=\"display:inline-block; margin:10px;\">"
      ],
      "id": "8cd2a4a3-d80a-4ce5-ad6f-fabcc3e8bdae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"media/night_heron.jpg\" alt=\"A night heron\" width=\"300\"/>"
      ],
      "id": "8292278d-f155-4577-a9d4-88fa9389e96b"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figcaption style=\"text-align:center;\">"
      ],
      "id": "82906880-31f2-43af-8a9d-2ee337182454"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Night heron in a bird guide"
      ],
      "id": "901518ab-5306-4871-b48f-fae93c6d6ca4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figcaption>"
      ],
      "id": "bf24ddf6-d8b5-4ba4-adcd-341855a5c57e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "299bbfe7-0595-4af2-82c7-d67cf0eaccf3"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figure style=\"display:inline-block; margin:10px;\">"
      ],
      "id": "c6810ad3-2089-471f-b77a-c81a3307f5b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<img src=\"media/grey_heron.jpg\" alt=\"A grey herom\" width=\"370\"/>"
      ],
      "id": "ca97df19-93bb-470e-b1ea-199db35ed6af"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<figcaption style=\"text-align:center;\">"
      ],
      "id": "058843f2-8483-4702-9b99-4dda77e12aed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Grey heron in a bird guide"
      ],
      "id": "2b18379c-af37-46bf-b35d-ed73293f4a78"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figcaption>"
      ],
      "id": "41eebefe-117f-4f08-b8f1-83422278672e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</figure>"
      ],
      "id": "5998e73d-6ba3-4273-ba4e-06610a9e2962"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</p>"
      ],
      "id": "e2def5cd-8567-4081-91c6-2c61ba823270"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A convolutional neural network would read things in the same way, as it\n",
        "doesn’t look at things in a big picture directly (which is usually\n",
        "costly and slow), but would see an image as multiple small **patches**\n",
        "to study the unique features and construct a detailed field guide of its\n",
        "own. The way how a CNN sees things this way is through **convolution**:\n",
        "it has a **convolutional layer** on top of the input layer to learn\n",
        "features piece by piece in its architecture, such that it can process\n",
        "information from an image in a cleverer way. Moreover, CNNs work quite\n",
        "well even when training images are not as tidy and organized as those in\n",
        "a field guide, which makes it efficient in solving real-life problems.\n",
        "\n",
        "Let’s recall some basic concepts of convolution and see how they are\n",
        "applied in the CNNs, typically within the convolutional layer. Here, the\n",
        "**inputs** are images, and they are interpreted by a computer as grids\n",
        "of numbers. The **kernels** (also called filters) are still the\n",
        "“brushes” you apply on the input image to extract certain features, but\n",
        "in a CNN, there are usually multiple distinct kernels applied at the\n",
        "same time to extract and map different features. After different\n",
        "features are extracted, they will be pooled together with another kernel\n",
        "and produce a summarized output to be passed into the **fully-connected\n",
        "layer** for classification or other tasks.\n",
        "\n",
        "While the principles behind the architectures are complicated, many\n",
        "python libraries now offer easy ways to implement these architectures.\n",
        "In a word, with a labeled image dataset, you can also train a CNN\n",
        "classifier yourself. Let’s try out an example together.\n",
        "\n",
        "### (Optional) Build Our Own CNN Classifier: An Example Using CIFAR-10 Dataset\n",
        "\n",
        "Classifying is central in the application of CNNs, so let’s try building\n",
        "a classifier using CNN and see how it works with an example. Let’s say,\n",
        "we want to train a model (the “expert”) that identify and distinguish\n",
        "between some daily objects, such as cars, planes, cat, dogs, etc. We\n",
        "first need to find a dataset that contains images of these objects with\n",
        "labels. **This is usually hard as we wouldn’t always have clean,\n",
        "labelled datasets of a specific topic**. But luckily, we have many\n",
        "datasets for daily objects.\n",
        "\n",
        "The dataset we are using here is\n",
        "[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), it is a widely\n",
        "used practice dataset for beginners to image processing that consists of\n",
        "60000 32 $\\times$ 32 colour images in 10 classes, with 6000 images per\n",
        "class. There are 50000 training images and 10000 test images. Let’s\n",
        "first load the dataset and see what it’s like.\n",
        "\n",
        "#### Step 1: Data Preprocessing"
      ],
      "id": "edb5a1b3-eff4-4a0a-8d62-2cdcba81136c"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import CIFAR-10 dataset from HuggingFace\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset(\n",
        "    'cifar10',\n",
        "    split='train'# training dataset\n",
        ")\n",
        "\n",
        "dataset_train"
      ],
      "id": "2eb2bb4a"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check how many labels/number of classes\n",
        "num_classes = len(set(dataset_train['label']))\n",
        "num_classes"
      ],
      "id": "345e4b9d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also display one of the images to see what it’s like."
      ],
      "id": "02854772-d21b-4a79-9b3f-19e7a23f796c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# let's view the image (it's very small)\n",
        "import matplotlib.pyplot as plt\n",
        "sample = dataset_train[0]['img']\n",
        "\n",
        "plt.imshow(sample)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "id": "a730ccc8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Can you see what the image is about? Can you imagine how computers\n",
        "understands it?\n",
        "\n",
        "As most CNNs can only accept images of a fixed size, we will reshape all\n",
        "images to 32 $\\times$ 32 pixels using `torchvision.transforms`; a\n",
        "**pipeline** built for image preprocessing. You can think of a pipeline\n",
        "as a series of small programs that together handles a specific task in a\n",
        "sequential order, which in here is resizing the images in the training\n",
        "set."
      ],
      "id": "2e3e5194-6ac3-4eec-a7ae-81614bc4fcd7"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchvision.transforms as transforms\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# image size\n",
        "img_size = 32\n",
        "\n",
        "# preprocess variable, to be used ahead\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((img_size,img_size)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "inputs_train = []\n",
        "\n",
        "for record in tqdm(dataset_train):\n",
        "    image = record['img']\n",
        "    label = record['label']\n",
        "\n",
        "    # convert from grayscale to RGB\n",
        "    if image.mode == 'L':\n",
        "        image = image.convert(\"RGB\")\n",
        "        \n",
        "    # prepocessing\n",
        "    input_tensor = preprocess(image)\n",
        "    \n",
        "    # append to batch list\n",
        "    inputs_train.append([input_tensor, label]) "
      ],
      "id": "33c03d89"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other than normalizing the general size of the images, we should also\n",
        "normalize the pixel values in the dataset."
      ],
      "id": "38f55e86-f059-47bb-8e5b-353d063610fd"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean = [0.4670, 0.4735, 0.4662]\n",
        "std = [0.2496, 0.2489, 0.2521]\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "for i in tqdm(range(len(inputs_train))):\n",
        "    # prepocessing\n",
        "    input_tensor = preprocess(inputs_train[i][0])\n",
        "    # replace with normalized tensor\n",
        "    inputs_train[i][0] = input_tensor"
      ],
      "id": "b9ee85ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we load and process the training set that we are using to validate\n",
        "the model quality."
      ],
      "id": "d76bfb8c-1f15-4232-888c-fb48fd9f80d1"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the dataset\n",
        "dataset_val = load_dataset(\n",
        "    'cifar10',\n",
        "    split='test'  # test set (used as validation set)\n",
        ")\n",
        "\n",
        "# Integrate the preprocessing steps\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((img_size,img_size)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "inputs_val = []\n",
        "i = 0\n",
        "for record in tqdm(dataset_val):\n",
        "    image = record['img']\n",
        "    label = record['label']\n",
        "\n",
        "    # convert from grayscale to RBG\n",
        "    if image.mode == 'L':\n",
        "        image = image.convert(\"RGB\")\n",
        "        \n",
        "    # prepocessing\n",
        "    input_tensor = preprocess(image)\n",
        "    inputs_val.append((input_tensor, label)) # append to batch list"
      ],
      "id": "8532a0a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We noticed that the testing and training data are gigantic in size,\n",
        "which would lag our trainings. To avoid the long training time and huge\n",
        "training cost, we often need to split our data into multiple small\n",
        "**batches**.\n",
        "\n",
        "In CNN training, choosing a batch size of, say, 32 or 64 gives you the\n",
        "best of both worlds: you “study” small, manageable mini-quizzes, get\n",
        "regular feedback to adjust your filter-weights, and keep your compute\n",
        "requirements reasonable, all while learning robustly across the entire\n",
        "image dataset."
      ],
      "id": "1de6f411-86d3-4081-bd90-3dc23bd2b29d"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Given the amount of data, we set the batch size as 64 to improve the efficiency when running our model\n",
        "batch_size = 64\n",
        "\n",
        "# We use DataLoader to split both the training and validation dataset into shuffled batches. \n",
        "# Shuffle helps prevent model overfitting by ensuring that batches are more representative of the entire dataset.\n",
        "dloader_train = torch.utils.data.DataLoader(\n",
        "    inputs_train, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "dloader_val = torch.utils.data.DataLoader(\n",
        "    inputs_val, batch_size=batch_size, shuffle=False\n",
        ")"
      ],
      "id": "dd17d891"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 2: Training the CNN Classifier\n",
        "\n",
        "After carefully processing both the training and the test data, we\n",
        "finally came to a stage where we can train our own CNN classifier. The\n",
        "first thing we need to do is to decide which **architecture** we want to\n",
        "use for the model.\n",
        "\n",
        "Architecture determines the way how a CNN integrate and learn from the\n",
        "features it extracted, and thus largely determines the performance of a\n",
        "model. Throughout the years, there have been several hugely successful\n",
        "CNN architectures, which we won’t be able to discuss in detail. Here, I\n",
        "will only demonstrate the architecture of **LeNet-5**: It reads images\n",
        "in a sequence that starts with a partial and combines the partials into\n",
        "a comprehensive one. Intuitively, the learning process of this\n",
        "architecture can be thought as learning to write a new character: You\n",
        "learn to write each stroke first, and then follow the structure of the\n",
        "character to put those strokes together into a complete character.\n",
        "\n",
        "<img src=\"media/lenet5.png\" alt=\"A visual comparison of two neural networks\" width=\"700\"/>"
      ],
      "id": "35c1de79-d0e2-435e-976d-5e57bc9ebb9b"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# creating a CNN class\n",
        "class ConvNeuralNet(nn.Module):\n",
        "    #  determine what layers and their order in CNN object \n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNeuralNet, self).__init__()\n",
        "        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "\n",
        "        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        \n",
        "        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.relu4 = nn.ReLU()\n",
        "\n",
        "        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.relu5 = nn.ReLU()\n",
        "        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        \n",
        "        self.dropout6 = nn.Dropout(p=0.5)\n",
        "        self.fc6 = nn.Linear(1024, 512)\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.dropout7 = nn.Dropout(p=0.5)\n",
        "        self.fc7 = nn.Linear(512, 256)\n",
        "        self.relu7 = nn.ReLU()\n",
        "        self.fc8 = nn.Linear(256, num_classes)\n",
        "    \n",
        "    # progresses data across layers    \n",
        "    def forward(self, x):\n",
        "        out = self.conv_layer1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.max_pool1(out)\n",
        "        \n",
        "        out = self.conv_layer2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.max_pool2(out)\n",
        "\n",
        "        out = self.conv_layer3(out)\n",
        "        out = self.relu3(out)\n",
        "\n",
        "        out = self.conv_layer4(out)\n",
        "        out = self.relu4(out)\n",
        "\n",
        "        out = self.conv_layer5(out)\n",
        "        out = self.relu5(out)\n",
        "        out = self.max_pool5(out)\n",
        "        \n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        \n",
        "        out = self.dropout6(out)\n",
        "        out = self.fc6(out)\n",
        "        out = self.relu6(out)\n",
        "\n",
        "        out = self.dropout7(out)\n",
        "        out = self.fc7(out)\n",
        "        out = self.relu7(out)\n",
        "\n",
        "        out = self.fc8(out)  # final logits\n",
        "        return out"
      ],
      "id": "2537b9dc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After designing the network architecture, we initialize it. And if we\n",
        "have access to hardware acceleration (through CUDA or MPS), we move the\n",
        "model to that device to speed up the training."
      ],
      "id": "1bd6ae04-05cc-417e-9765-6cdef2d4dd0b"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# set the model to device\n",
        "model = ConvNeuralNet(num_classes).to(device)"
      ],
      "id": "bde349d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we will set the loss and optimizer function used during the\n",
        "training. These are the factors that determine how much your network\n",
        "learn from the mistakes and adjust the distribution of weights how it\n",
        "trust the “experts”.\n",
        "\n",
        "The **loss function** is a metric that measures the classification\n",
        "performance. Here the Cross-Entropy Loss function is one of them that is\n",
        "commonly used in neural networks. The **optimizer function drives** the\n",
        "model to reflect and adjust its weights after each validation, and its\n",
        "parameter **learning rate** decides how much the model absorb from the\n",
        "lessons. While it seems that a higher learning rate is beneficial, it is\n",
        "actually not as a high learning rate could lead to severe overshooting.\n",
        "That’s why we set the learning rate `lr = 0.01` here to prevent overly\n",
        "progressive learning."
      ],
      "id": "aa81e4fb-264a-45a7-b025-94202214f820"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "# set learning rate \n",
        "lr = 0.01\n",
        "# set optimizer as SGD\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(), lr=lr\n",
        ") "
      ],
      "id": "8d5a896d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will train the model for 25 epochs. To ensure we’re not overfitting\n",
        "to the training set, we pass the validation set through the model for\n",
        "inference only at the end of each epoch. If we see validation set\n",
        "performance suddenly degrade while train set performance improves, we\n",
        "are likely overfitting.\n",
        "\n",
        "You can run the training and fitting loop as follows, but **be cautious:\n",
        "This cell will take a long time to run**. Alternatively, you can skip 3\n",
        "cells and load the model we pre-trained directly."
      ],
      "id": "159958ed-6e82-4fda-95fb-752c0c10a59f"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "num_epochs = 25\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    for i, (images, labels) in enumerate(dloader_train):  \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        outputs = model(images)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    avg_train_loss = running_loss / len(dloader_train)\n",
        "    train_losses.append(avg_train_loss)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_val_loss = []\n",
        "        for images, labels in dloader_val:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            total += labels.size(0)\n",
        "            predicted = torch.argmax(outputs, dim=1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            all_val_loss.append(loss_func(outputs, labels).item())\n",
        "            \n",
        "        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n",
        "        mean_val_acc = 100 * (correct / total)\n",
        "        \n",
        "        val_losses.append(mean_val_loss)\n",
        "        val_accuracies.append(mean_val_acc)\n",
        "        \n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {mean_val_loss:.4f}, Val Acc: {mean_val_acc:.2f}%')"
      ],
      "id": "6e94eb5b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize how training loss, validation loss and validation\n",
        "accuracy evolve over time."
      ],
      "id": "af6f486c-c3e5-42f2-8b4a-a347e3422979"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy', color='green')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Validation Accuracy Curve')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "08d9d3d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss curve and validation accuracy curve show that the training loss\n",
        "and validation loss goes down while the validation accuracy of the model\n",
        "goes up as the training epochs increase. If we add more epochs\n",
        "(costly!), the validation accuracy of the model will be higher, but the\n",
        "model will also be more at risk of overfitting. To prevent which, we\n",
        "often need to regularize the model.\n",
        "\n",
        "After training for 25 epochs, we see our validation accuracy has passed\n",
        "70%, we can save the model to file and load it again with the following\n",
        "codes:"
      ],
      "id": "3878ec06-3b04-4c6e-be16-f00a276a811b"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save to file\n",
        "torch.save(model, 'data/cnn.pt')"
      ],
      "id": "573bae94"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load from file and switch to inference mode\n",
        "model = torch.load('data/cnn.pt', weights_only=False)\n",
        "model.eval()"
      ],
      "id": "5a4db2e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 3: Inference the Classifier\n",
        "\n",
        "Now, we can use the trained classifier to predict the labels of the new\n",
        "input. But here, we are just using the test set for validation (which is\n",
        "not recommended)."
      ],
      "id": "660a4eaf-be50-4311-b8be-1e731811b042"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "input_tensors = []\n",
        "\n",
        "for image in dataset_val['img'][:10]:\n",
        "    tensor = preprocess(image)\n",
        "    input_tensors.append(tensor.to(device))\n",
        "\n",
        "# stack into a single tensor\n",
        "input_tensors = torch.stack(input_tensors)\n",
        "input_tensors.shape"
      ],
      "id": "903a8eca"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# process through model to get output logits\n",
        "outputs = model(input_tensors)\n",
        "# calculate predictions\n",
        "predicted = torch.argmax(outputs, dim=1)\n",
        "predicted\n",
        "\n",
        "# here are the class names\n",
        "dataset_val.features['label'].names"
      ],
      "id": "f0530463"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print out the output label and the true label\n",
        "for i in range(10):\n",
        "    example = dataset_val[i]           # get the i-th example as a dict\n",
        "    image   = example['img']\n",
        "    true_id = example['label']\n",
        "    pred_id = predicted[i]\n",
        "    \n",
        "    true_label = dataset_val.features['label'].names[true_id]\n",
        "    pred_label = dataset_val.features['label'].names[pred_id]\n",
        "    \n",
        "    plt.figure(figsize=(4,4))\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"True: {true_label}   |   Pred: {pred_label}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "id": "f5ae9996"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can visualize the feature maps at different layers to see how the CNN\n",
        "see images."
      ],
      "id": "4736d2e9-9080-4d04-865a-a382264e9762"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization of features at different layers\n",
        "import torchvision.transforms as T\n",
        "\n",
        "to_tensor = T.ToTensor()\n",
        "\n",
        "pil_img, _ = dataset_val[0]['img'], dataset_val[0]['label']\n",
        "\n",
        "input_tensor = to_tensor(pil_img).unsqueeze(0).to(device)\n",
        "\n",
        "activations = {}\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activations[name] = output.detach().cpu()\n",
        "    return hook\n",
        "\n",
        "for layer_name in ['conv_layer1','conv_layer2','conv_layer3','conv_layer4','conv_layer5']:\n",
        "    getattr(model, layer_name).register_forward_hook(get_activation(layer_name))\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    _ = model(input_tensor)\n",
        "\n",
        "# Plot the feature map\n",
        "for name, fmap in activations.items():\n",
        "    num_filters = fmap.shape[1]\n",
        "    cols = 6\n",
        "    rows = min((num_filters + cols - 1) // cols, 4)\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n",
        "    fig.suptitle(f'Feature maps from {name}', fontsize=16)\n",
        "    for i in range(rows * cols):\n",
        "        r, c = divmod(i, cols)\n",
        "        ax = axes[r, c] if rows > 1 else axes[c]\n",
        "        if i < num_filters:\n",
        "            ax.imshow(fmap[0, i], cmap='viridis')\n",
        "            ax.set_title(f'#{i}')\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "f6da56d5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the model made mostly correct predictions, despite the\n",
        "image quality was low that even human may have difficulty to correctly\n",
        "classify. This somewhat shows the advantage of CNNs over humans when\n",
        "confronted with complex, blurry images, but CNNs have more applications\n",
        "than that. They power a host of real-world applications, from enabling\n",
        "your smartphone’s camera to automatically recognize faces and apply\n",
        "portrait effects, to guiding autonomous vehicles by detecting\n",
        "pedestrians, road signs, and lane markings in real time. In healthcare,\n",
        "CNNs help radiologists spot tumors in MRI and CT scans, and\n",
        "dermatologists classify skin lesions from photos. They underpin optical\n",
        "character recognition for digitizing handwritten forms, fuel\n",
        "quality-control systems that spot manufacturing defects on assembly\n",
        "lines, and even drive wildlife monitoring by identifying animals in\n",
        "camera-trap images.\n",
        "\n",
        "This technology is also reshaping some humanities and social science\n",
        "research. For example, in archaeology, CNNs are being used to\n",
        "categorize, complete, and translate broken clay tablets and cuneiform\n",
        "texts; in art history, CNNs are being used to study the pigments and\n",
        "materials used in paintings, as well as the expressions and gestures of\n",
        "the figures in them; and in anthropology, CNNs are being used to\n",
        "distinguish between human races and complex kinships. It is for this\n",
        "reason that we are here to introduce it to you! I hope you enjoyed the\n",
        "class and got something different out of it!\n",
        "\n",
        "### Key takeaways from this part:\n",
        "\n",
        "1.  **Artificial Neural Networks (ANNs)** are programs or models that\n",
        "    make decisions in a similar manner to the thought process of a human\n",
        "    brain.\n",
        "2.  **Convolutional Neural Networks (CNNs)** differ from other neural\n",
        "    networks in the **convolutional layer** that allows them to\n",
        "    understand features from image input in a more efficient way.\n",
        "3.  **Architectures** are central in neural networks as they determine\n",
        "    the ways how a model learn from the input features and thereby\n",
        "    determine the model performance.\n",
        "4.  Machine Learning and CNNs are fun and practical in the field of\n",
        "    humanities and social sciences!\n",
        "\n",
        "### Additional Resources\n",
        "\n",
        "-   [MLU-EXPLAIN: Neural\n",
        "    Networks](https://mlu-explain.github.io/neural-networks/): A website\n",
        "    with straightforward explanation and interactive visualizations of\n",
        "    neural networks (with some math and technical terms), including more\n",
        "    professional terminologies and advanced concepts that we won’t cover\n",
        "    in this notebook. But if you find this notebook to be too light and\n",
        "    really hope to learn more, this is a good place to go!\n",
        "-   [CNN Explainer](https://poloclub.github.io/cnn-explainer/): An\n",
        "    interesting interactive tutorial that explains how CNN work in a\n",
        "    more visual way (but you may also find the explanation a little too\n",
        "    technical). Try it out! You can also upload your own images of\n",
        "    interest to see how the neural network processes them and classify\n",
        "    them. Do you get the same results as you expected? What can you say\n",
        "    about it?\n",
        "\n",
        "### References\n",
        "\n",
        "-   Pinecone. Embedding Methods for Image Search.\n",
        "    <https://www.pinecone.io/learn/series/image-search>\n",
        "-   IBM. What is a neural network?\n",
        "    <https://www.ibm.com/think/topics/neural-networks>\n",
        "-   IBM. What are convolutional neural networks?\n",
        "    <https://www.ibm.com/think/topics/convolutional-neural-networks>\n",
        "-   Convolutional Neural Network From Scratch.\n",
        "    <https://medium.com/latinxinai/convolutional-neural-network-from-scratch-6b1c856e1c07>"
      ],
      "id": "bb9b5c63-2c66-47fe-9501-36c6c9904d32"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  }
}