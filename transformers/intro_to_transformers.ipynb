{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers : ...\n",
    "\n",
    "Author: *Krishaant Pathmanathan, PRAXIS UBC Team*\n",
    "\n",
    "Date: 2025-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Game: Where Will the Ball Go?\n",
    "\n",
    "Transformers are all about **making predictions from context**. But what does that really mean? Before diving into tokens and attention, let’s play a game.\n",
    "\n",
    "**Imagine this:**\n",
    "You see a bouncing ball mid-air. Where will it go next? How do you know? Just like us, models try to guess what comes next based on what they've already seen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given an image of a ball can you predict where it will go next ?\n",
    "<div style=\"display: flex; gap: 20px;\">\n",
    "  <img src=\"data/static_ball2.png\" alt=\"Static ball\" width=\"300\"/>\n",
    "  <img src=\"data/moving_ball3.png\" alt=\"Moving ball\" width=\"300\"/>\n",
    "</div>\n",
    "\n",
    "Given a sequence, can you now tell ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out there are many sequences in the world - words in a sentence, frames in a video, notes in a melody, steps in a recipe. The challenge isn't just seeing them, it's predicting what comes next. To understand or predict them, we need a model that doesn’t just look at things in isolation…  It has to **remember what came before**.\n",
    "\n",
    "That’s where **Recurrent Neural Networks (RNNs)** come in.\n",
    "\n",
    "<!-- maybe insert some pictures of that here  -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Prediction with RNNs\n",
    "\n",
    "Imagine reading one word at a time, keeping track of what came before... that's what an RNN does. RNN stands for **Recurrent Neural Network**. It’s a type of model that learns from **sequences** — like sentences, music, or even time. It's an important precursor to a **transformer**\n",
    "\n",
    "### How It Works (Step-by-Step)\n",
    "\n",
    "1. **Give it a sentence** — for example:  \n",
    "   `\"I love recurrent neural ____\"`\n",
    "\n",
    "2. **Turn words into numbers** (this is called *tokenizing*)  \n",
    "   > Computers can’t understand words — they only understand numbers!\n",
    "\n",
    "3. **Send the numbers into the RNN** — one by one\n",
    "\n",
    "4. **At each step**, the RNN tries to **remember what came before**  \n",
    "   > It passes a little memory called a **hidden state** from word to word\n",
    "\n",
    "5. **At the end**, it **guesses what comes next!**  \n",
    "   > Like filling in the blank at the end of the sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next word prediction: networks!\n"
     ]
    }
   ],
   "source": [
    "# Simulated RNN function (takes a look at past to make prediction)\n",
    "def my_rnn(word, hidden_state):\n",
    "    # Imagine the RNN does something with the word and memory\n",
    "    new_hidden_state = [h + 1 for h in hidden_state]  # update memory\n",
    "    prediction = \"networks!\" if word == \"neural\" else None\n",
    "    return prediction, new_hidden_state\n",
    "\n",
    "# Initial hidden state\n",
    "hidden_state = [0, 0, 0, 0]\n",
    "\n",
    "# Input sentence (tokenized)\n",
    "sentence = [\"I\", \"love\", \"recurrent\", \"neural\"]\n",
    "\n",
    "# RNN step-by-step loop\n",
    "for word in sentence:\n",
    "    prediction, hidden_state = my_rnn(word, hidden_state)\n",
    "\n",
    "# Final predicted word\n",
    "next_word_prediction = prediction\n",
    "print(\"Next word prediction:\", next_word_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Why RNNs Can Be Tricky ?\n",
    "\n",
    "- They **read one word at a time**, so it’s slow\n",
    "- They **forget things** after a while (just like people!)\n",
    "- They **can’t look at everything at once**\n",
    "\n",
    "\n",
    "RNNs are like someone reading a story **out loud, one word at a time**. Transformers (like GPT) are like someone **looking at the whole page at once**.\n",
    "\n",
    "**RNNs walked, so Transformers could fly.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a GPT model ?\n",
    "\n",
    "A GPT is a Generative Pre-Trained Transformer. The first two words are self-explanatory: generative means the model generates new text; pre-trained means the model was trained on large amounts of data. \n",
    "\n",
    "What we will focus on is the **transformer** aspect of the language model, the main proponent of the recent boom in AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's a transformer ?\n",
    "\n",
    "A transformer is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence.\n",
    "It is the main component that underlies tools like ChatGPT. It can trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, then produce a prediction of what comes next, in the form of a probability distribution over all chunks of text that might follow.\n",
    "\n",
    "*Note there are many other types of transformers (voice-to-text, text-to-image, etc.). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/transformerpredict.png\" alt=\"Transformer Prediction\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Applications \n",
    "\n",
    "To further understand how transformers work we will walk through examples of these transformers being used. For now you can think of transformers as a black box, but there are componenets within it that allow it to understand context, exceeding the capabilities of an RNN. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Just like text is broken into tokens, images are broken into patches, and audio is split into time steps or spectrogram slices before being passed to transformer models like ViT or Wav2Vec2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 3608, 2003, 2461, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize an example text\n",
    "text = 'The ball is round.'\n",
    "tokens = tokenizer(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Prediction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488969626acc4fc9ad147f1c96dbed12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37830f1e242a4813b9a3691eb37bd373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5794cc0ddc474f2bad1bde78e7e90b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7b3f00e65e4d309df61e66889b8e79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d265cb97c3d14b9c9906622549d624d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8599fbc8f8544bc3977d1bfceea62ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0ac8afa29b478bbb9ec59ae6547852",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history of the world is inextricably linked with its history in Europe,\" said Gutt\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation')\n",
    "prompt = 'The history of the world is'\n",
    "output = generator(prompt, max_length=20)\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention Block\n",
    "\n",
    "The Attention Block where they communicate with each other to update their values based on context. For example, the meaning of the word model in the phrase a machine learning model is different from its meaning in the phrase a fashion model. The Attention Block is responsible for figuring out which words in the context are relevant to updating the meanings of other words and how exactly those meanings should be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12.68941421]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Toy example of query-key-value\n",
    "Q = np.array([[1, 0]])\n",
    "K = np.array([[1, 0], [0, 1]])\n",
    "V = np.array([[10], [20]])\n",
    "\n",
    "attention_scores = Q @ K.T\n",
    "attention_weights = softmax(attention_scores, axis=1)\n",
    "output = attention_weights @ V\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
