[
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html",
    "href": "docs/SOCI-415/cbdb_dataset.html",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "",
    "text": "The China Biographical Database Abstract: The China Biographical Database is a freely accessible relational database with biographical information about approximately 641,568 individuals as of August 2024, currently mainly from the 7th through 19th centuries. With both online and offline versions, the data is meant to be useful for statistical, social network, and spatial analysis as well as serving as a kind of biographical reference. The image below shows the spatial distribution of a cross dynastic subset of 190,000 people in CBDB by basic affiliations\nDisplay values within the dataset\nimport sqlite3\nimport pandas as pd\n\ndb_path = r'/arc/project/st-lknelson-1/alexr951/git/data/latest.db'\nconn = sqlite3.connect(db_path)\ncursor = conn.cursor()\n\n# List all tables\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\ntables = cursor.fetchall()\n\nprint(\"Tables in database:\", tables)\n\nconn.close()\n\nTables in database: [('ADDR_BELONGS_DATA',), ('ADDR_CODES',), ('ADDR_PLACE_DATA',), ('ADDR_XY',), ('ADDRESSES',), ('ALTNAME_CODES',), ('ALTNAME_DATA',), ('APPOINTMENT_CODE_TYPE_REL',), ('APPOINTMENT_CODES',), ('APPOINTMENT_TYPES',), ('ASSOC_CODE_TYPE_REL',), ('ASSOC_CODES',), ('ASSOC_DATA',), ('ASSOC_TYPES',), ('ASSUME_OFFICE_CODES',), ('BIOG_ADDR_CODES',), ('BIOG_ADDR_DATA',), ('BIOG_INST_CODES',), ('BIOG_INST_DATA',), ('BIOG_MAIN',), ('BIOG_SOURCE_DATA',), ('BIOG_TEXT_DATA',), ('CHORONYM_CODES',), ('Copy Of CopyTables',), ('CopyMissingTables',), ('CopyTables',), ('CopyTablesDefault',), ('COUNTRY_CODES',), ('DATABASE_LINK_CODES',), ('DATABASE_LINK_DATA',), ('DYNASTIES',), ('ENTRY_CODE_TYPE_REL',), ('ENTRY_CODES',), ('ENTRY_DATA',), ('ENTRY_TYPES',), ('ETHNICITY_TRIBE_CODES',), ('EVENT_CODES',), ('EVENTS_ADDR',), ('EVENTS_DATA',), ('EXTANT_CODES',), ('ForeignKeys',), ('FormLabels',), ('GANZHI_CODES',), ('HOUSEHOLD_STATUS_CODES',), ('INDEXYEAR_TYPE_CODES',), ('KIN_DATA',), ('KIN_Mourning',), ('KIN_MOURNING_STEPS',), ('KINSHIP_CODES',), ('LITERARYGENRE_CODES',), ('MEASURE_CODES',), ('NIAN_HAO',), ('OCCASION_CODES',), ('OFFICE_CATEGORIES',), ('OFFICE_CODE_TYPE_REL',), ('OFFICE_CODES',), ('OFFICE_CODES_CONVERSION',), ('OFFICE_TYPE_TREE',), ('OFFICE_TYPE_TREE_backup',), ('PARENTAL_STATUS_CODES',), ('PLACE_CODES',), ('POSSESSION_ACT_CODES',), ('POSSESSION_ADDR',), ('POSSESSION_DATA',), ('POSTED_TO_ADDR_DATA',), ('POSTED_TO_OFFICE_DATA',), ('POSTING_DATA',), ('SCHOLARLYTOPIC_CODES',), ('SOCIAL_INSTITUTION_ADDR',), ('SOCIAL_INSTITUTION_ADDR_TYPES',), ('SOCIAL_INSTITUTION_ALTNAME_CODES',), ('SOCIAL_INSTITUTION_ALTNAME_DATA',), ('SOCIAL_INSTITUTION_CODES',), ('SOCIAL_INSTITUTION_CODES_CONVERSION',), ('SOCIAL_INSTITUTION_NAME_CODES',), ('SOCIAL_INSTITUTION_TYPES',), ('STATUS_CODE_TYPE_REL',), ('STATUS_CODES',), ('STATUS_DATA',), ('STATUS_TYPES',), ('TablesFields',), ('TablesFieldsChanges',), ('TEXT_BIBLCAT_CODE_TYPE_REL',), ('TEXT_BIBLCAT_CODES',), ('TEXT_BIBLCAT_TYPES',), ('TEXT_CODES',), ('TEXT_INSTANCE_DATA',), ('TEXT_ROLE_CODES',), ('TEXT_TYPE',), ('TMP_INDEX_YEAR',), ('YEAR_RANGE_CODES',)]\nLoad the one we want\ndb_path = r'/arc/project/st-lknelson-1/alexr951/git/data/latest.db'\n\n# Connect to the database\nconn = sqlite3.connect(db_path)\n\n# Replace 'person' with the actual table name you want to load\ndf = pd.read_sql_query(\"SELECT * FROM KIN_DATA\", conn)\n\n# Show the first few rows\nprint(df.head())\n\nconn.close()\n\n   c_personid  c_kin_id  c_kin_code  c_source        c_pages  \\\n0           0    689344         337   64847.0  088  周真夫人黃氏墓誌   \n1           1         2         180       0.0           None   \n2           1     13310         210    7596.0           3047   \n3           1     13311         180       0.0           None   \n4           1    119997         182    2229.0           None   \n\n                  c_notes c_autogen_notes c_created_by c_created_date  \\\n0                    None            None   Qi Xinghai       20250329   \n1  [待考。《宋史·安惇傳》云，惇二子郊、邦。]            None          TTS       20070312   \n2                    None            None          TTS       20070312   \n3                    None            None          TTS       20070312   \n4      據宋史列傳CBDB宋史分傳#2159            None         load       20121116   \n\n  c_modified_by c_modified_date  \n0    Qi Xinghai        20250329  \n1          None            None  \n2          None            None  \n3          None            None  \n4          None            None\nBuild the NetworkX Graph\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\nG = nx.Graph()\n\n# Add edges with kinship type as edge attribute\nfor _, row in df.iterrows():\n    person = row['c_personid']\n    kin = row['c_kin_id']\n    kin_type = row['c_kin_code']\n    G.add_edge(person, kin, kinship=kin_type)\n\nprint(f\"Number of nodes: {G.number_of_nodes()}\")\nprint(f\"Number of edges: {G.number_of_edges()}\")\n\nNumber of nodes: 278262\nNumber of edges: 272717\nVisualize the network\nimport sqlite3\nimport pandas as pd\n\n# -------- Update this only once for all cells ------- #\nDB_PATH = r'/arc/project/st-lknelson-1/alexr951/git/data/latest.db'\n# --------------------------------------------------- #\n\ndef load_kin_data(db_path=DB_PATH):\n    \"\"\"Return a DataFrame with all rows from KIN_DATA.\"\"\"\n    with sqlite3.connect(db_path) as conn:\n        df = pd.read_sql_query(\"SELECT * FROM KIN_DATA\", conn)\n    # Drop obvious self-loops\n    df = df[df[\"c_personid\"] != df[\"c_kin_id\"]]\n    return df\n\nkin_df = load_kin_data()\nprint(\"Rows:\", len(kin_df))\nkin_df.head()\n\n\ndef basic_overview(df):\n    print(\"=== BASIC COLUMN SUMMARY ===\")\n    print(df.dtypes)                 # data types\n    print(\"\\nNull counts:\")\n    print(df.isna().sum())\n    print(\"\\nUnique counts:\")\n    for col in df.columns:\n        print(f\"  {col}: {df[col].nunique():,}\")\n\nbasic_overview(kin_df)\n\nRows: 543809\n=== BASIC COLUMN SUMMARY ===\nc_personid           int64\nc_kin_id             int64\nc_kin_code           int64\nc_source           float64\nc_pages             object\nc_notes             object\nc_autogen_notes     object\nc_created_by        object\nc_created_date      object\nc_modified_by       object\nc_modified_date     object\ndtype: object\n\nNull counts:\nc_personid              0\nc_kin_id                0\nc_kin_code              0\nc_source               14\nc_pages            139587\nc_notes            417131\nc_autogen_notes    510431\nc_created_by            1\nc_created_date          2\nc_modified_by      517420\nc_modified_date    517420\ndtype: int64\n\nUnique counts:\n  c_personid: 277,411\n  c_kin_id: 277,976\n  c_kin_code: 426\n  c_source: 454\n  c_pages: 24,017\n  c_notes: 37,534\n  c_autogen_notes: 13,960\n  c_created_by: 168\n  c_created_date: 1,990\n  c_modified_by: 150\n  c_modified_date: 1,154\nfrom collections import Counter\n\ndef relationship_and_people_stats(df):\n    total_rels = len(df)\n    unique_people = set(df[\"c_personid\"]) | set(df[\"c_kin_id\"])\n    kin_type_counts = Counter(df[\"c_kin_code\"])\n\n    print(\"=== RELATIONSHIP & PEOPLE STATS ===\")\n    print(f\"Total relationships  : {total_rels:,}\")\n    print(f\"Unique individuals   : {len(unique_people):,}\")\n    print(\"\\nTop 10 kinship codes:\")\n    for kin, cnt in kin_type_counts.most_common(10):\n        share = cnt / total_rels * 100\n        print(f\"  {kin:&lt;10} {cnt:&gt;8,}  ({share:5.1f}%)\")\n\nrelationship_and_people_stats(kin_df)\n\n=== RELATIONSHIP & PEOPLE STATS ===\nTotal relationships  : 543,809\nUnique individuals   : 278,257\n\nTop 10 kinship codes:\n  180          88,917  ( 16.4%)\n  75           88,636  ( 16.3%)\n  126          56,512  ( 10.4%)\n  125          56,487  ( 10.4%)\n  134          28,301  (  5.2%)\n  62           28,101  (  5.2%)\n  243          27,533  (  5.1%)\n  135          24,464  (  4.5%)\n  48           19,910  (  3.7%)\n  255          19,468  (  3.6%)\nimport networkx as nx\nimport numpy as np\n\ndef quick_network_snapshot(df):\n    G = nx.from_pandas_edgelist(\n        df,\n        source=\"c_personid\",\n        target=\"c_kin_id\",\n        edge_attr=\"c_kin_code\",\n        create_using=nx.Graph(),\n    )\n\n    degrees = [d for _, d in G.degree()]\n    comps   = [len(c) for c in nx.connected_components(G)]\n\n    print(\"=== NETWORK SNAPSHOT ===\")\n    print(f\"Nodes                 : {G.number_of_nodes():,}\")\n    print(f\"Edges                 : {G.number_of_edges():,}\")\n    print(f\"Density               : {nx.density(G):.8f}\")\n    print(f\"Connected components  : {len(comps):,}\")\n    print(f\"Largest component     : {max(comps):,} nodes\")\n    print(\"\\nDegree distribution:\")\n    print(f\"  mean   {np.mean(degrees):.2f}\")\n    print(f\"  median {np.median(degrees):.2f}\")\n    print(f\"  max    {np.max(degrees)}\")\n    print(f\"  std    {np.std(degrees):.2f}\")\n\nquick_network_snapshot(kin_df)\n\n=== NETWORK SNAPSHOT ===\nNodes                 : 278,257\nEdges                 : 272,691\nDensity               : 0.00000704\nConnected components  : 30,357\nLargest component     : 52,992 nodes\n\nDegree distribution:\n  mean   1.96\n  median 1.00\n  max    147\n  std    2.57\nimport sqlite3\nimport pandas as pd\n\n#DB_PATH = r'/arc/project/st-lknelson-1/alexr951/git/data/latest.db'\n\ndef decode_kinship_relationships():\n    \"\"\"Get the actual meaning of your kinship codes\"\"\"\n    with sqlite3.connect(DB_PATH) as conn:\n        # Load kinship codes lookup table\n        kinship_lookup = pd.read_sql_query(\"SELECT * FROM KINSHIP_CODES\", conn)\n        \n        # Load your KIN_DATA\n        kin_data = pd.read_sql_query(\"SELECT * FROM KIN_DATA\", conn)\n        kin_data = kin_data[kin_data['c_personid'] != kin_data['c_kin_id']]\n        \n    print(f\"=== RELATIONSHIP TYPES IN YOUR DATA ===\")\n    print(f\"Total kinship records: {len(kin_data):,}\")\n    \n    # Get the actual relationship meanings\n    relationships = kin_data['c_kin_code'].value_counts().head(15)\n    print(f\"\\nTop 15 relationship types (by frequency):\")\n    for code, count in relationships.items():\n        pct = count/len(kin_data)*100\n        print(f\"  Code {code}: {count:,} ({pct:.1f}%)\")\n    \n    print(f\"\\nKinship lookup table structure:\")\n    print(kinship_lookup.columns.tolist())\n    print(f\"Lookup table has {len(kinship_lookup)} relationship definitions\")\n    \n    return kin_data, kinship_lookup\n\nkin_df, lookup_df = decode_kinship_relationships()\n\n=== RELATIONSHIP TYPES IN YOUR DATA ===\nTotal kinship records: 543,809\n\nTop 15 relationship types (by frequency):\n  Code 180: 88,917 (16.4%)\n  Code 75: 88,636 (16.3%)\n  Code 126: 56,512 (10.4%)\n  Code 125: 56,487 (10.4%)\n  Code 134: 28,301 (5.2%)\n  Code 62: 28,101 (5.2%)\n  Code 243: 27,533 (5.1%)\n  Code 135: 24,464 (4.5%)\n  Code 48: 19,910 (3.7%)\n  Code 255: 19,468 (3.6%)\n  Code 111: 18,410 (3.4%)\n  Code 76: 7,826 (1.4%)\n  Code 181: 7,076 (1.3%)\n  Code 176: 4,986 (0.9%)\n  Code 182: 4,638 (0.9%)\n\nKinship lookup table structure:\n['c_kincode', 'c_kin_pair1', 'c_kin_pair2', 'c_kin_pair_notes', 'c_kinrel_chn', 'c_kinrel', 'c_kinrel_alt', 'c_pick_sorting', 'c_upstep', 'c_dwnstep', 'c_marstep', 'c_colstep', 'c_kinrel_simplified']\nLookup table has 479 relationship definitions\nimport networkx as nx\nimport numpy as np\n\ndef analyze_family_network_structure(df):\n    \"\"\"Understand the shape and clustering of your kinship network\"\"\"\n    \n    print(\"=== NETWORK STRUCTURE & FAMILY PATTERNS ===\")\n    \n    # Build the network\n    G = nx.from_pandas_edgelist(df, source='c_personid', target='c_kin_id', \n                               create_using=nx.Graph())\n    \n    print(f\"Network size: {G.number_of_nodes():,} people, {G.number_of_edges():,} relationships\")\n    print(f\"Network density: {nx.density(G):.8f}\")\n    \n    # Family cluster analysis (connected components = family groups)\n    components = list(nx.connected_components(G))\n    family_sizes = sorted([len(comp) for comp in components], reverse=True)\n    \n    print(f\"\\n=== FAMILY CLUSTER ANALYSIS ===\")\n    print(f\"Total family groups: {len(components):,}\")\n    print(f\"Largest family: {family_sizes[0]:,} people\")\n    print(f\"Top 10 family sizes: {family_sizes[:10]}\")\n    print(f\"Families with 100+ people: {sum(1 for size in family_sizes if size &gt;= 100)}\")\n    print(f\"Single-person 'families': {sum(1 for size in family_sizes if size == 1)}\")\n    \n    # Individual connectivity patterns  \n    degrees = dict(G.degree())\n    degree_values = list(degrees.values())\n    \n    print(f\"\\n=== INDIVIDUAL CONNECTIVITY ===\")\n    print(f\"Average relationships per person: {np.mean(degree_values):.2f}\")\n    print(f\"Most connected person has: {max(degree_values)} relationships\")\n    print(f\"People with only 1 relationship: {sum(1 for d in degree_values if d == 1):,}\")\n    print(f\"People with 10+ relationships: {sum(1 for d in degree_values if d &gt;= 10):,}\")\n    \n    return G, components, degrees\n\nG, families, connectivity = analyze_family_network_structure(kin_df)\n\n=== NETWORK STRUCTURE & FAMILY PATTERNS ===\nNetwork size: 278,257 people, 272,691 relationships\nNetwork density: 0.00000704\n\n=== FAMILY CLUSTER ANALYSIS ===\nTotal family groups: 30,357\nLargest family: 52,992 people\nTop 10 family sizes: [52992, 357, 331, 294, 267, 236, 188, 160, 152, 130]\nFamilies with 100+ people: 16\nSingle-person 'families': 0\n\n=== INDIVIDUAL CONNECTIVITY ===\nAverage relationships per person: 1.96\nMost connected person has: 147 relationships\nPeople with only 1 relationship: 205,427\nPeople with 10+ relationships: 7,755"
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html#posibilities-of-what-we-can-do",
    "href": "docs/SOCI-415/cbdb_dataset.html#posibilities-of-what-we-can-do",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "Posibilities of what we can do",
    "text": "Posibilities of what we can do\nLouvain (ML) - Can be applied to sub divide the 30,357 connected components (families) Family Tree Hierarchical Structure: - Look at how over time families evolved Degree centrality for important family members (Some have 147 edges from them) Betweenness Centrality for Bridges/Connectors between large families (Maybe a women who was married off etc)\n- Look at how networks change across generations and time periods Spatial Network Analysis - Look at how people and networks change over time Cross-Dynasty Comparisons - compare family structures across different Chinese dynasties (depending on how far back the data goes)"
  },
  {
    "objectID": "docs/AMNE-376/development/kouroi_embeddings.html",
    "href": "docs/AMNE-376/development/kouroi_embeddings.html",
    "title": "Praxis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('../data/sculpture_dataset_labeled.csv')\n\ndf.head()\n\n\n\n\n\n\n\n\nfilename\npage\ngroup\nera\n\n\n\n\n0\npage184_img01_photo2.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n1\npage184_img01_photo3.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n2\npage184_img01_photo4.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n3\npage184_img01_photo6.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n4\npage184_img01_photo7.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n\n\n\n\n\n\nimport torch\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport timm\nfrom tqdm.notebook import tqdm # Import tqdm for progress bar\n\n\nmodel_name = 'convnextv2_tiny' \nmodel = timm.create_model(model_name, pretrained=True)\nmodel.eval()  # set to eval mode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\n\nbatch_size = 32\nembeddings = []\n\nimage_directory = \"../data/richter_kouroi_filtered_photos\"\n\n# Collect filenames and eras for batching\nfilenames = df['filename'].tolist()\neras = df['era'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i:i + batch_size]\n\n    images = []\n    valid_indices = []\n    for j, filename in enumerate(batch_filenames):\n        image_path = os.path.join(image_directory, filename)\n        try:\n            image = Image.open(image_path).convert('RGB')  # convert to RGB to avoid issues\n            images.append(image)\n            valid_indices.append(i + j)  # Keep track of valid indices for later\n        except FileNotFoundError:\n            print(f\"Image not found at: {image_path}\")\n        except Exception as e:\n            print(f\"Error processing image {filename}: {e}\")\n\n    if len(images) == 0:\n        continue  # Skip empty batches\n\n    # Process the batch of images\n    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    # Mean pooling over patches per image in batch\n    batch_embeddings = last_hidden_states.mean(dim=1).cpu().numpy()\n\n    # Append embeddings maintaining order corresponding to valid images\n    embeddings.extend(batch_embeddings)\n\n\n\n\n\nnp.array(embeddings)\n\nnp.save('../data/embeddings/all_photos_embeddings.npy', embeddings)\nnp.save('../data/embeddings/all_photos_eras.npy', eras)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = np.load('../data/embeddings/all_photos_embeddings.npy')\ny = np.load('../data/embeddings/all_photos_eras.npy')\n\n# X = embeddings, y = your labels\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.5151515151515151\n\n\n\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')\n\nbatch_size = 32\nembeddings = []\n\nimage_directory = \"../data/richter_kouroi_complete_front_only\"\n\n# Collect filenames and eras for batching\nfilenames = df['filename'].tolist()\neras = df['era'].tolist()\nmaterial = df['material'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i:i + batch_size]\n\n    images = []\n    valid_indices = []\n    for j, filename in enumerate(batch_filenames):\n        image_path = os.path.join(image_directory, filename)\n        try:\n            image = Image.open(image_path).convert('RGB')  # convert to RGB to avoid issues\n            images.append(image)\n            valid_indices.append(i + j)  # Keep track of valid indices for later\n        except FileNotFoundError:\n            print(f\"Image not found at: {image_path}\")\n        except Exception as e:\n            print(f\"Error processing image {filename}: {e}\")\n\n    if len(images) == 0:\n        continue  # Skip empty batches\n\n    # Process the batch of images\n    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    # Mean pooling over patches per image in batch\n    batch_embeddings = last_hidden_states.mean(dim=1).cpu().numpy()\n\n    # Append embeddings maintaining order corresponding to valid images\n    embeddings.extend(batch_embeddings)\n\n\n\n\n\nnp.array(embeddings)\n\nnp.save('../data/embeddings/complete_sculptures_embeddings.npy', embeddings)\nnp.save('../data/embeddings/complete_sculptures_eras.npy', eras)\nnp.save('../data/embeddings/complete_sculptures_material.npy', material)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = embeddings\ny1 = eras\ny2 = material\n\n# X = embeddings, y = your labels\nX_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.3076923076923077\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.8461538461538461\n\n\n\ndf = pd.read_csv('../data/head_dataset_labeled.csv')\n\nbatch_size = 32\nembeddings = []\n\nimage_directory = \"../data/richter_kouroi_head_front_only\"\n\n# Collect filenames and eras for batching\nfilenames = df['filename'].tolist()\neras = df['era'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i:i + batch_size]\n\n    images = []\n    valid_indices = []\n    for j, filename in enumerate(batch_filenames):\n        image_path = os.path.join(image_directory, filename)\n        try:\n            image = Image.open(image_path).convert('RGB')  # convert to RGB to avoid issues\n            images.append(image)\n            valid_indices.append(i + j)  # Keep track of valid indices for later\n        except FileNotFoundError:\n            print(f\"Image not found at: {image_path}\")\n        except Exception as e:\n            print(f\"Error processing image {filename}: {e}\")\n\n    if len(images) == 0:\n        continue  # Skip empty batches\n\n    # Process the batch of images\n    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    # Mean pooling over patches per image in batch\n    batch_embeddings = last_hidden_states.mean(dim=1).cpu().numpy()\n\n    # Append embeddings maintaining order corresponding to valid images\n    embeddings.extend(batch_embeddings)\n\n\n\n\n\nnp.array(embeddings)\n\nnp.save('../data/embeddings/head_embeddings.npy', embeddings)\nnp.save('../data/embeddings/head_eras.npy', eras)\n\n\nX = embeddings\ny1 = eras\n\n# X = embeddings, y = your labels\nX_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.5555555555555556"
  },
  {
    "objectID": "docs/AMNE-376/development/AMNE_Statues.html",
    "href": "docs/AMNE-376/development/AMNE_Statues.html",
    "title": "Praxis",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv(\"../data/head_dataset_labeled.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nfilename\npage\ngroup\nera\n\n\n\n\n0\npage188_img01_photo13.jpg\n188\nSOUNION GROUP\n615 - 590 BC\n\n\n1\npage196_img01_photo5.jpg\n196\nSOUNION GROUP\n615 - 590 BC\n\n\n2\npage200_img01_photo7.jpg\n200\nSOUNION GROUP\n615 - 590 BC\n\n\n3\npage202_img01_photo3.jpg\n202\nSOUNION GROUP\n615 - 590 BC\n\n\n4\npage202_img01_photo4.jpg\n202\nSOUNION GROUP\n615 - 590 BC\n\n\n\n\n\n\n\n\nimport torch\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nfrom transformers import AutoImageProcessor, Dinov2Model\nfrom tqdm.notebook import tqdm # Import tqdm for progress bar\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Dinov2Model.from_pretrained(\"facebook/dinov2-base\").to(device)\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\n\nbatch_size = 32\nembeddings = []\n\nimage_directory = \"../data/richter_kouroi_head_front_only\"\n\n# Collect filenames and eras for batching\nfilenames = df['filename'].tolist()\neras = df['era'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i:i + batch_size]\n\n    images = []\n    valid_indices = []\n    for j, filename in enumerate(batch_filenames):\n        image_path = os.path.join(image_directory, filename)\n        try:\n            image = Image.open(image_path).convert('RGB')  # convert to RGB to avoid issues\n            images.append(image)\n            valid_indices.append(i + j)  # Keep track of valid indices for later\n        except FileNotFoundError:\n            print(f\"Image not found at: {image_path}\")\n        except Exception as e:\n            print(f\"Error processing image {filename}: {e}\")\n\n    if len(images) == 0:\n        continue  # Skip empty batches\n\n    # Process the batch of images\n    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    # Mean pooling over patches per image in batch\n    batch_embeddings = last_hidden_states.mean(dim=1).cpu().numpy()\n\n    # Append embeddings maintaining order corresponding to valid images\n    embeddings.extend(batch_embeddings)\n\nembeddings = np.array(embeddings)\nnp.save('dinov2_image_embeddings.npy', embeddings)\n\n\n\n\n\n\n\n# Proceed with your t-SNE and plotting code...\ntsne = TSNE(n_components=2, init='pca', method='exact')\ntsne_embeddings = tsne.fit_transform(embeddings)\n\nplt.figure(figsize=(12, 10))\nunique_eras = df['era'].unique()\ncolors = plt.cm.get_cmap('tab10', len(unique_eras))\n\nfor i, era in enumerate(unique_eras):\n    indices = df[df['era'] == era].index\n    plt.scatter(tsne_embeddings[indices, 0], tsne_embeddings[indices, 1], color=colors(i), label=era)\n\nplt.title('t-SNE of Image Embeddings Colored by Era')\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nC:\\Users\\Kaiyan Zhang\\AppData\\Local\\Temp\\ipykernel_16612\\3958630150.py:7: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  colors = plt.cm.get_cmap('tab10', len(unique_eras))\n\n\n\n\n\n\n!pip install plotly==5.14.0 --quiet\n\n\nimport plotly.express as px\n\n\n# Create a DataFrame for Plotly\nplotly_df = pd.DataFrame({\n    'TSNE_Dim1': tsne_embeddings[:, 0],\n    'TSNE_Dim2': tsne_embeddings[:, 1],\n    'Era': df['era'].tolist(), # Use the 'era' column from your original df\n    'Filename': df['filename'].tolist() # Include the filenames\n})\nfig = px.scatter(\n        plotly_df,\n        x='TSNE_Dim1',\n        y='TSNE_Dim2',\n        color='Era',\n        hover_data=['Filename'], # Show filename when hovering\n        title='Interactive t-SNE of Image Embeddings Colored by Era'\n    )\n\nfig.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  sf: grouped.get_group(s if len(s) &gt; 1 else s[0])\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfrom sklearn.decomposition import PCA\n\n# Perform PCA instead of t-SNE\npca = PCA(n_components=2)\npca_embeddings = pca.fit_transform(embeddings)\n\n# Plot the PCA result\nplt.figure(figsize=(12, 10))\nunique_eras = df['era'].unique()\ncolors = plt.cm.get_cmap('tab10', len(unique_eras))\n\nfor i, era in enumerate(unique_eras):\n    indices = df[df['era'] == era].index\n    plt.scatter(pca_embeddings[indices, 0], pca_embeddings[indices, 1], color=colors(i), label=era)\n\nplt.title('PCA of Image Embeddings Colored by Era')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nC:\\Users\\Kaiyan Zhang\\AppData\\Local\\Temp\\ipykernel_16612\\119444610.py:10: MatplotlibDeprecationWarning:\n\nThe get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n# e.g. compute 5 principal components\npca = PCA(n_components=6)\npca_embeddings = pca.fit_transform(embeddings)\n# now pca_embeddings.shape == (n_samples, 5)\n\npc_df = pd.DataFrame(\n    pca_embeddings[:, :6],\n    columns=['PC1','PC2','PC3','PC4', 'PC5', 'PC6']\n)\npc_df['era'] = df['era'].values\n\nimport seaborn as sns\nsns.pairplot(pc_df, hue='era', vars=['PC1','PC2','PC3','PC4'])\nplt.show()\n\n\n\n\n\n\n# Create a DataFrame for Plotly\nplotly_df = pd.DataFrame({\n    'TSNE_Dim1': pca_embeddings[:, 0],\n    'TSNE_Dim2': pca_embeddings[:, 1],\n    'Era': df['era'].tolist(), # Use the 'era' column from your original df\n    'Filename': df['filename'].tolist() # Include the filenames\n})\nfig = px.scatter(\n        plotly_df,\n        x='TSNE_Dim1',\n        y='TSNE_Dim2',\n        color='Era',\n        hover_data=['Filename'], # Show filename when hovering\n        title='Interactive t-SNE of Image Embeddings Colored by Era'\n    )\n\nfig.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nmodel = Dinov2Model.from_pretrained(\n    \"facebook/dinov2-base\",\n    output_attentions=True,   # &lt;-- get all the self-attention weights\n).eval().to(device)\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n\nimg = Image.open(\"../data/richter_kouroi_head_front_only/page286_img01_photo3.jpg\").convert(\"RGB\")\ninputs = processor(images=img, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# outputs.attentions: tuple of length L (num layers), each (batch=1, heads=H, seq_len=T, seq_len=T)\nattns = [attn[0].mean(dim=0).cpu()  # average over heads\n         for attn in outputs.attentions]\n\nT = attns[0].size(-1)  # total tokens = 1 (CLS) + num_patches\nrollout = torch.eye(T)\n\nfor layer_attn in attns:\n    # Add identity (residual) and renormalize rows\n    layer_aug = layer_attn + torch.eye(T)\n    layer_aug = layer_aug / layer_aug.sum(dim=-1, keepdim=True)\n    # Propagate\n    rollout = layer_aug @ rollout\n\n# Extract CLS → patch attentions (skip the CLS→CLS token at rollout[0,0])\npatch_attn = rollout[0, 1:]  # shape: (num_patches,)\n\n# Reshape to 2D grid\ngrid_size = int(np.sqrt(patch_attn.size(0)))\nheatmap = patch_attn.reshape(grid_size, grid_size).numpy()\n\nheatmap_tensor = torch.tensor(heatmap).unsqueeze(0).unsqueeze(0)  # (1,1,G,G)\nheatmap_up = torch.nn.functional.interpolate(\n    heatmap_tensor,\n    size=img.size[::-1],      # (height, width)\n    mode=\"bilinear\",\n    align_corners=False\n)[0,0].numpy()\n\nplt.figure(figsize=(6,6))\nplt.imshow(img, alpha=0.8)\nplt.imshow(heatmap_up, cmap=\"inferno\", alpha=0.5)\nplt.axis(\"off\")\nplt.title(\"DINO-V2 Attention Rollout Heatmap\")\nplt.show()\n\n\n\n\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nfrom transformers import AutoImageProcessor, Dinov2Model\nfrom tqdm.auto import tqdm\n\n\n\n# ——————————————————————————————————————\n# 1) Hyperparameters & paths\n# ——————————————————————————————————————\nIMAGE_DIR   = \"../data/richter_kouroi_complete_front_only\"\nCSV_PATH    = \"../data/complete_sculpture_dataset_labeled.csv\"   # with columns ['filename','era']\nBATCH_SIZE  = 32\nLR          = 1e-3\nEPOCHS      = 15\nDEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMODEL_NAME  = \"facebook/dinov2-base\"\n\n# Map your era strings to integer labels\nERA2IDX = {\n    '615 - 590 BC': 0,\n    '590 - 570 BC': 1,\n    '575 - 550 BC': 2,\n    '555 - 540 BC': 3,\n    '540 - 520 BC': 4,\n    '520 - 485 BC': 5,\n    '485 - 460 BC': 6,\n}\n\nMAT2IDX = {\n    'Marble': 0,\n    'Bronze': 1,\n    'Lead': 2,\n    'Alabaster': 3,\n    'Limestone': 4,\n    'Terracotta': 5,\n}\n\n\n\n# ——————————————————————————————————————\n# 2) Dataset & DataLoader\n# ——————————————————————————————————————\nclass KouroiEraDataset(Dataset):\n    def __init__(self, csv_path, img_dir, processor, mat2idx):\n        self.df = pd.read_csv(csv_path)\n        self.img_dir = img_dir\n        self.processor = processor\n        self.mat2idx = MAT2IDX\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(os.path.join(self.img_dir, row.filename)).convert(\"RGB\")\n        # turn to model inputs\n        inputs = self.processor(images=img, return_tensors=\"pt\")\n        # remove batch dim\n        for k,v in inputs.items():\n            inputs[k] = v.squeeze(0)\n        label = self.mat2idx[row.material]\n        return inputs, label\n\n# Initialize processor + dataset\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME)\ndataset = KouroiEraDataset(CSV_PATH, IMAGE_DIR, processor, MAT2IDX)\nloader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\n\n\n# ——————————————————————————————————————\n# 3) Build model: frozen DINO + MLP head\n# ——————————————————————————————————————\nclass EraClassifier(nn.Module):\n    def __init__(self, backbone_name, num_classes):\n        super().__init__()\n        # load DINO‐V2 without gradient updates\n        self.backbone = Dinov2Model.from_pretrained(\n            backbone_name, output_hidden_states=False, output_attentions=False\n        )\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n        embed_dim = self.backbone.config.hidden_size\n        # a simple 2‑layer MLP head\n        self.head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim//2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(embed_dim//2, num_classes)\n        )\n\n    def forward(self, pixel_values):\n        # pixel_values: (B,3,H,W)\n        outputs = self.backbone(pixel_values=pixel_values)\n        # mean‐pool the patch embeddings: (B, num_patches, D) → (B,D)\n        x = outputs.last_hidden_state.mean(dim=1)\n        logits = self.head(x)\n        return logits\n\nmodel = EraClassifier(MODEL_NAME, num_classes=len(MAT2IDX)).to(DEVICE)\n\n\n\n# ——————————————————————————————————————\n# 4) Training loop\n# ——————————————————————————————————————\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.head.parameters(), lr=LR)  \n# note: we only pass head.parameters() so backbone stays frozen\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n        inputs, labels = batch\n        # move to device\n        inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n        labels = labels.to(DEVICE)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        loss   = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * labels.size(0)\n\n    avg_loss = total_loss / len(dataset)\n    print(f\" Epoch {epoch} avg loss: {avg_loss:.4f}\")\n\n\n\n\n Epoch 1 avg loss: 1.5357\n Epoch 2 avg loss: 0.9983\n Epoch 3 avg loss: 0.8323\n Epoch 4 avg loss: 0.5830\n Epoch 5 avg loss: 0.4432\n Epoch 6 avg loss: 0.3940\n Epoch 7 avg loss: 0.3297\n Epoch 8 avg loss: 0.2696\n Epoch 9 avg loss: 0.1759\n Epoch 10 avg loss: 0.1656\n Epoch 11 avg loss: 0.1141\n Epoch 12 avg loss: 0.0824\n Epoch 13 avg loss: 0.0646\n Epoch 14 avg loss: 0.0565\n Epoch 15 avg loss: 0.0493\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# 1) Run one pass over your data in eval mode\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in loader:\n        if inputs is None: continue\n        inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n        logits = model(**inputs)\n        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# 2) Compute & plot\ncm = confusion_matrix(all_labels, all_preds, labels=list(MAT2IDX.values()))\ndisp = ConfusionMatrixDisplay(cm, display_labels=list(MAT2IDX.keys()))\nplt.figure(figsize=(8,8))\ndisp.plot(cmap=\"Blues\", xticks_rotation=45, values_format=\"d\")\nplt.title(\"Material Classification Confusion Matrix\")\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 800x800 with 0 Axes&gt;\n\n\n\n\n\n\nfeatures, preds, labels = [], [], []\nmodel.eval()\nwith torch.no_grad():\n    for inputs, labs in loader:\n        if inputs is None: continue\n        inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n        out = model.backbone(pixel_values=inputs['pixel_values'])\n        emb = out.last_hidden_state.mean(1).cpu().numpy()\n        features.append(emb)\n        logits = model.head(torch.from_numpy(emb).to(DEVICE))\n        preds.extend(logits.argmax(dim=1).cpu().numpy())\n        labels.extend(labs.numpy())\n\nfeatures = np.vstack(features)  # shape (N, D)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nfeat2d = pca.fit_transform(features)\n\nplt.figure(figsize=(10,8))\nfor idx, material in enumerate(MAT2IDX.keys()):\n    mask = np.array(labels) == idx\n    plt.scatter(feat2d[mask,0], feat2d[mask,1],\n                label=material, alpha=0.6)\nplt.legend(bbox_to_anchor=(1,1))\nplt.title(\"2D PCA of DINO-V2 Features (True Labels)\")\nplt.show()\n\n\n\n\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom torch.utils.data import DataLoader\n\n# 1) Prepare DataLoader (reuse your existing dataset & processor)\ndataset = KouroiEraDataset(CSV_PATH, IMAGE_DIR, processor, ERA2IDX)\nloader  = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n\n# 2) Extract embeddings\nmodel.backbone.eval()\nall_feats, all_eras = [], []\nwith torch.no_grad():\n    for batch in loader:\n        if batch is None: \n            continue\n        inputs, labels = batch\n        inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n        out = model.backbone(pixel_values=inputs['pixel_values'])\n        # (B, num_patches, D) → (B, D)\n        embs = out.last_hidden_state.mean(dim=1).cpu().numpy()\n        all_feats.append(embs)\n        all_eras.extend(labels.numpy())\n\nall_feats = np.vstack(all_feats)   # shape (N_images, D)\nall_eras  = np.array(all_eras)     # shape (N_images,)\n\n# 3) (Optional) PCA to 50 dims for faster, denoised clustering\npca50 = PCA(n_components=50)\nfeats50 = pca50.fit_transform(all_feats)\n\n# 4) Fit K‑Means (8 clusters)\nn_clusters = 8\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(feats50)\n\n# 5) Confusion between cluster IDs and true eras\ncm = confusion_matrix(all_eras, clusters, labels=list(ERA2IDX.values()))\ndisp = ConfusionMatrixDisplay(\n    cm, display_labels=[f\"Cl{c}\" for c in range(n_clusters)]\n)\nplt.figure(figsize=(6,6))\ndisp.plot(cmap=\"Oranges\", xticks_rotation=45)\nplt.title(\"Confusion: True Eras vs. K‑Means Clusters\")\nplt.tight_layout()\nplt.show()\n\n# 6) Visualize clusters in 2D PCA space\npca2 = PCA(n_components=2)\nfeats2 = pca2.fit_transform(all_feats)\n\nplt.figure(figsize=(8,6))\nfor cl in range(n_clusters):\n    mask = clusters == cl\n    plt.scatter(feats2[mask,0], feats2[mask,1],\n                label=f\"Cluster {cl}\", alpha=0.6)\nplt.legend(bbox_to_anchor=(1,1))\nplt.title(\"2D PCA of Embeddings Colored by K‑Means Cluster\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.show()\n\n&lt;Figure size 600x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# ——————————————————————————————————————\n# 5) (Optional) Save your head\n# ——————————————————————————————————————\ntorch.save(model.head.state_dict(), \"dino2_era_head.pt\")\nprint(\"⭐ Saved MLP head to dino2_era_head.pt\")\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Arc\nfrom matplotlib import rcParams\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# 1) Global white styling\nrcParams['figure.facecolor'] = 'white'\nrcParams['axes.facecolor']   = 'white'\nrcParams['font.family']      = 'serif'\nrcParams['font.serif']       = ['DejaVu Serif']\n\ncolor1, color2, arc_color = '#1f78b4', '#ff7f00', '#33a02c'\n\n# 2) Zero‐inclusive sliders\ndef make_slider(label, init):\n    return widgets.FloatSlider(\n        value=init, min=0.0, max=3.0, step=0.1,\n        description=label, continuous_update=True,\n        layout=widgets.Layout(width='280px')\n    )\n\ns_x1 = make_slider('Vec1 X', 1.2)\ns_y1 = make_slider('Vec1 Y', 2.4)\ns_x2 = make_slider('Vec2 X', 2.0)\ns_y2 = make_slider('Vec2 Y', 1.0)\n\n# 3) Compact controls in HBox/VBox, with white background\ncontrols = widgets.HBox([\n    widgets.VBox([s_x1, s_y1]),\n    widgets.VBox([s_x2, s_y2])\n], layout=widgets.Layout(background_color='white', padding='4px'))\n\n# 4) Output area fully white, no border\nout = widgets.Output(layout=widgets.Layout(background_color='white', border='0px'))\n\n# 5) Plotting function\ndef plot_vectors(x1, y1, x2, y2):\n    with out:\n        clear_output(wait=True)\n        # prepare vectors\n        emb1, emb2 = np.array([x1,y1]), np.array([x2,y2])\n        n1, n2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n\n        # cosine & angle\n        if n1&gt;0 and n2&gt;0:\n            cos_sim = float(np.clip(np.dot(emb1,emb2)/(n1*n2), -1,1))\n            angle_deg = np.degrees(np.arccos(cos_sim))\n        else:\n            cos_sim, angle_deg = 0.0, 0.0\n\n        # angles for arc\n        a1 = np.degrees(np.arctan2(y1,x1)) % 360\n        a2 = np.degrees(np.arctan2(y2,x2)) % 360\n\n        # figure\n        fig, ax = plt.subplots(figsize=(6,6), facecolor='white')\n        # white canvas behind the widget\n        fig.canvas.layout.background_color = 'white'\n        ax.set_facecolor('white')\n\n        # draw vectors\n        ax.quiver(0,0,x1,y1, color=color1, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  label=f'v₁ (|v₁|={n1:.2f})')\n        ax.quiver(0,0,x2,y2, color=color2, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  alpha=0.85, label=f'v₂ (|v₂|={n2:.2f})')\n\n        # draw arc if meaningful\n        if n1&gt;0 and n2&gt;0 and abs(cos_sim)&lt;0.9999:\n            r = 0.5\n            t0, t1 = sorted((a1,a2))\n            arc = Arc((0,0),2*r,2*r,theta1=t0,theta2=t1,color=arc_color,linewidth=2)\n            ax.add_patch(arc)\n            mid = np.radians((t0+t1)/2)\n            ax.text(r*np.cos(mid)+0.02, r*np.sin(mid)+0.02,\n                    f'θ={angle_deg:.1f}°', color=arc_color,\n                    fontsize=13, fontweight='bold')\n        else:\n            ax.text(0.6,0.6, f'θ={angle_deg:.1f}°',\n                    color=arc_color, fontsize=13, fontweight='bold')\n\n        # cosine annotation\n        ax.text(0.05,2.9, f'Cosine = {cos_sim:.2f}', fontsize=14,\n                bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n\n        # remove spines, add grid, equal aspect\n        ax.set_xlim(0,3); ax.set_ylim(0,3)\n        ax.set_aspect('equal','box')\n        ax.grid(True,linestyle='--',alpha=0.4)\n        for sp in ax.spines.values(): sp.set_visible(False)\n        ax.set_xlabel('Dim 1'); ax.set_ylabel('Dim 2')\n        ax.set_title('Visualizing Cosine Similarity', fontsize=14, fontweight='bold')\n        ax.legend(loc='upper right')\n\n        # zero‐margin\n        fig.tight_layout(pad=0)\n        plt.subplots_adjust(left=0,right=1,top=1,bottom=0)\n        plt.show()\n\n# 6) Wire sliders to the plot\nwidgets.interactive_output(\n    plot_vectors,\n    {'x1': s_x1, 'y1': s_y1, 'x2': s_x2, 'y2': s_y2}\n)\n\n# 7) Display everything\ndisplay(controls, out)\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Arc\nfrom matplotlib import rcParams\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# 2) Global styling: ensure white everywhere\nrcParams['figure.facecolor'] = 'white'\nrcParams['axes.facecolor']   = 'white'\nrcParams['font.family']      = 'serif'\nrcParams['font.serif']       = ['DejaVu Serif']\n\ncolor1, color2, arc_color = '#1f78b4', '#ff7f00', '#33a02c'\n\n# 3) Build sliders (allow zero)\ndef make_slider(label, init):\n    return widgets.FloatSlider(\n        value=init, min=0.0, max=3.0, step=0.1,\n        description=label, continuous_update=True,\n        layout=widgets.Layout(width='250px')\n    )\n\ns_x1 = make_slider('Vec1 X', 1.2)\ns_y1 = make_slider('Vec1 Y', 2.4)\ns_x2 = make_slider('Vec2 X', 2.0)\ns_y2 = make_slider('Vec2 Y', 1.0)\n\n# 4) Compact control layout (white background)\ncontrols = widgets.HBox([\n    widgets.VBox([s_x1, s_y1]),\n    widgets.VBox([s_x2, s_y2])\n], layout=widgets.Layout(background_color='white', padding='4px'))\n\n# 5) White Output widget\nout = widgets.Output(layout=widgets.Layout(background_color='white', border='0px'))\n\n# 6) Plotting function\ndef plot_vectors(x1, y1, x2, y2):\n    with out:\n        clear_output(wait=True)\n\n        emb1, emb2 = np.array([x1, y1]), np.array([x2, y2])\n        n1, n2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n\n        # cosine & angle\n        if n1&gt;0 and n2&gt;0:\n            cos_sim = float(np.clip(np.dot(emb1,emb2)/(n1*n2), -1,1))\n            angle_deg = np.degrees(np.arccos(cos_sim))\n        else:\n            cos_sim, angle_deg = 0.0, 0.0\n\n        # absolute angles for arc\n        a1 = np.degrees(np.arctan2(y1, x1)) % 360\n        a2 = np.degrees(np.arctan2(y2, x2)) % 360\n\n        # Create a truly white figure in the Output widget\n        fig, ax = plt.subplots(figsize=(6,6), facecolor='white')\n        fig.patch.set_facecolor('white')\n        ax.set_facecolor('white')\n\n        # Draw vectors\n        ax.quiver(0,0,x1,y1, color=color1, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  label=f'v₁ (|v₁|={n1:.2f})')\n        ax.quiver(0,0,x2,y2, color=color2, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  alpha=0.85, label=f'v₂ (|v₂|={n2:.2f})')\n\n        # Draw angle arc if valid\n        if n1&gt;0 and n2&gt;0 and abs(cos_sim)&lt;0.9999:\n            r = 0.5\n            t0, t1 = sorted((a1, a2))\n            arc = Arc((0,0), 2*r, 2*r, theta1=t0, theta2=t1, color=arc_color, linewidth=2)\n            ax.add_patch(arc)\n            mid = np.radians((t0 + t1)/2)\n            ax.text(r*np.cos(mid)+0.02, r*np.sin(mid)+0.02,\n                    f'θ={angle_deg:.1f}°', color=arc_color,\n                    fontsize=13, fontweight='bold')\n        else:\n            ax.text(0.6,0.6, f'θ={angle_deg:.1f}°',\n                    color=arc_color, fontsize=13, fontweight='bold')\n\n        # Cosine similarity label\n        ax.text(0.05,2.9, f'Cosine = {cos_sim:.2f}', fontsize=14,\n                bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n\n        # Grid, no spines, equal aspect\n        ax.set_xlim(0,3); ax.set_ylim(0,3)\n        ax.set_aspect('equal','box')\n        ax.grid(True, linestyle='--', alpha=0.4)\n        for sp in ax.spines.values(): sp.set_visible(False)\n        ax.set_xlabel('Dim 1'); ax.set_ylabel('Dim 2')\n        ax.set_title('Visualizing Cosine Similarity', fontsize=14, fontweight='bold')\n        ax.legend(loc='upper right')\n\n        # **Zero margins** around the figure\n        fig.tight_layout(pad=0)\n        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n        plt.show()\n\n# 7) Hook up interactive output\nwidgets.interactive_output(\n    plot_vectors,\n    {'x1': s_x1, 'y1': s_y1, 'x2': s_x2, 'y2': s_y2}\n)\n\n# 8) Display everything\ndisplay(controls, out)\n\n\n\n\n\n\n\n\n%matplotlib widget\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Arc\nfrom matplotlib import rcParams\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# 1) Force white everywhere\nrcParams['figure.facecolor']    = 'white'\nrcParams['axes.facecolor']      = 'white'\nrcParams['axes.edgecolor']      = 'white'\nrcParams['figure.edgecolor']    = 'white'\nrcParams['font.family']         = 'serif'\nrcParams['font.serif']          = ['DejaVu Serif']\ncolor1, color2, arc_color = '#1f78b4', '#ff7f00', '#33a02c'\n\n# 2) Sliders (allow zero)\ndef make_slider(label, init):\n    return widgets.FloatSlider(\n        value=init, min=0.0, max=3.0, step=0.1,\n        description=label, continuous_update=True,\n        layout=widgets.Layout(width='250px')\n    )\n\ns_x1 = make_slider('Vec1 X', 1.2)\ns_y1 = make_slider('Vec1 Y', 2.4)\ns_x2 = make_slider('Vec2 X', 2.0)\ns_y2 = make_slider('Vec2 Y', 1.0)\n\ncontrols = widgets.HBox([\n    widgets.VBox([s_x1, s_y1]), widgets.VBox([s_x2, s_y2])\n], layout=widgets.Layout(background_color='white', padding='4px'))\n\nout = widgets.Output(layout=widgets.Layout(background_color='white', border='0'))\n\n# 3) Plot fn\ndef plot_vectors(x1, y1, x2, y2):\n    with out:\n        clear_output(wait=True)\n\n        emb1, emb2 = np.array([x1,y1]), np.array([x2,y2])\n        n1, n2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n\n        if n1&gt;0 and n2&gt;0:\n            cos_sim = float(np.clip(np.dot(emb1,emb2)/(n1*n2), -1,1))\n            angle_deg = np.degrees(np.arccos(cos_sim))\n        else:\n            cos_sim, angle_deg = 0.0, 0.0\n\n        a1 = np.degrees(np.arctan2(y1,x1)) % 360\n        a2 = np.degrees(np.arctan2(y2,x2)) % 360\n\n        # create widget‐backed figure\n        fig, ax = plt.subplots(figsize=(5,5), facecolor='white')\n        # remove canvas borders\n        canvas = fig.canvas\n        if hasattr(canvas, 'layout'):\n            canvas.layout.border = '0'\n            canvas.layout.margin = '0'\n            canvas.layout.padding = '0'\n\n        ax.set_facecolor('white')\n        ax.spines['left'].set_color('white')\n        ax.spines['bottom'].set_color('white')\n        ax.spines['right'].set_color('white')\n        ax.spines['top'].set_color('white')\n\n        # vectors\n        ax.quiver(0,0,x1,y1, color=color1, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  label=f'v₁ (|v₁|={n1:.2f})')\n        ax.quiver(0,0,x2,y2, color=color2, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  alpha=0.85, label=f'v₂ (|v₂|={n2:.2f})')\n\n        # angle arc\n        if n1&gt;0 and n2&gt;0 and abs(cos_sim)&lt;0.9999:\n            r = 0.5\n            t0, t1 = sorted((a1,a2))\n            arc = Arc((0,0),2*r,2*r,theta1=t0,theta2=t1,color=arc_color,linewidth=2)\n            ax.add_patch(arc)\n            mid = np.radians((t0+t1)/2)\n            ax.text(r*np.cos(mid)+0.02, r*np.sin(mid)+0.02,\n                    f'θ={angle_deg:.1f}°', color=arc_color,\n                    fontsize=13, fontweight='bold')\n        else:\n            ax.text(0.6,0.6, f'θ={angle_deg:.1f}°',\n                    color=arc_color, fontsize=13, fontweight='bold')\n\n        # cosine label\n        ax.text(0.4,2.8, f'Cosine Similarity = {cos_sim:.2f}', fontsize=14,\n                bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n\n        # tidy up\n        ax.set_xlim(0,3); ax.set_ylim(0,3)\n        ax.set_aspect('equal','box')\n        ax.grid(True, linestyle='--', alpha=0.4)\n        ax.set_xlabel('Dim 1'); ax.set_ylabel('Dim 2')\n        ax.set_title('Visualizing Cosine Similarity', fontsize=14, fontweight='bold')\n        ax.legend(loc='upper right')\n\n        # zero margins\n        fig.tight_layout(pad=0)\n        plt.subplots_adjust(left=0,right=1,top=1,bottom=0)\n        plt.show()\n\n# 4) Link and display\nwidgets.interactive_output(\n    plot_vectors,\n    {'x1': s_x1, 'y1': s_y1, 'x2': s_x2, 'y2': s_y2}\n)\ndisplay(controls, out)\n\n\n\n\n\n\n\n\nimport plotly.graph_objects as go\nimport numpy as np\nimport math\nimport panel as pn\npn.extension()\n\ndef cosine_similarity_plot(x1=1.2, y1=2.4, x2=2.0, y2=1.0):\n    v1 = np.array([x1, y1])\n    v2 = np.array([x2, y2])\n    n1 = np.linalg.norm(v1)\n    n2 = np.linalg.norm(v2)\n\n    if n1 == 0 or n2 == 0:\n        cos_sim = 0\n        angle_deg = 0\n    else:\n        cos_sim = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n        angle_deg = np.degrees(np.arccos(cos_sim))\n\n    fig = go.Figure()\n\n    # Vector 1\n    fig.add_trace(go.Scatter(x=[0, x1], y=[0, y1],\n                             mode='lines+markers+text',\n                             line=dict(color='#1f78b4', width=4),\n                             name='Vector 1'))\n\n    # Vector 2\n    fig.add_trace(go.Scatter(x=[0, x2], y=[0, y2],\n                             mode='lines+markers+text',\n                             line=dict(color='#ff7f00', width=4),\n                             name='Vector 2'))\n\n    # Angle arc (circular sector)\n    if n1 &gt; 0 and n2 &gt; 0 and not np.isclose(abs(cos_sim), 1.0):\n        angle1 = np.arctan2(y1, x1)\n        angle2 = np.arctan2(y2, x2)\n        theta = np.linspace(angle1, angle2, 100)\n        if angle2 &lt; angle1:\n            theta = np.linspace(angle2, angle1, 100)\n        arc_radius = 0.5\n        arc_x = arc_radius * np.cos(theta)\n        arc_y = arc_radius * np.sin(theta)\n        fig.add_trace(go.Scatter(x=arc_x, y=arc_y,\n                                 mode='lines',\n                                 line=dict(color='#33a02c', dash='dot'),\n                                 name='Angle θ'))\n\n        # Label angle\n        mid = theta[len(theta)//2]\n        fig.add_trace(go.Scatter(x=[arc_radius * np.cos(mid)],\n                                 y=[arc_radius * np.sin(mid)],\n                                 text=[f'θ = {angle_deg:.1f}°'],\n                                 mode='text',\n                                 textposition='top center',\n                                 showlegend=False))\n\n    # Cosine similarity text\n    fig.add_annotation(x=0.05, y=max(y1, y2, 1.5),\n                       text=f\"&lt;b&gt;Cosine Similarity = {cos_sim:.2f}&lt;/b&gt;\",\n                       showarrow=False, font=dict(size=14))\n\n    # Layout tweaks\n    fig.update_layout(title='Cosine Similarity (Interactive)',\n                      xaxis=dict(range=[-0.1, 3.1], zeroline=True),\n                      yaxis=dict(range=[-0.1, 3.1], zeroline=True),\n                      width=600, height=600,\n                      plot_bgcolor='white',\n                      margin=dict(t=60, l=30, r=30, b=30),\n                      showlegend=True)\n    return fig\n\n# Interactive panel UI\npn.interact(cosine_similarity_plot,\n            x1=(0.0, 3.0, 0.1),\n            y1=(0.0, 3.0, 0.1),\n            x2=(0.0, 3.0, 0.1),\n            y2=(0.0, 3.0, 0.1))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimport numpy as np\nimport plotly.graph_objects as go\nimport ipywidgets as widgets\nfrom IPython.display import display\n\ndef plot_cosine_similarity(x1, y1, x2, y2):\n    v1 = np.array([x1, y1])\n    v2 = np.array([x2, y2])\n    n1 = np.linalg.norm(v1)\n    n2 = np.linalg.norm(v2)\n\n    if n1 == 0 or n2 == 0:\n        cos_sim = 0.0\n        angle_deg = 0.0\n    else:\n        cos_sim = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n        angle_deg = np.degrees(np.arccos(cos_sim))\n\n    fig = go.Figure()\n\n    # Draw vector lines (for visual clarity)\n    fig.add_trace(go.Scatter(\n        x=[0, x1], y=[0, y1],\n        mode='lines+markers',\n        name=f'Image 1 (|v₁|={n1:.2f})',\n        line=dict(color='#1f78b4', width=2)\n    ))\n    fig.add_trace(go.Scatter(\n        x=[0, x2], y=[0, y2],\n        mode='lines+markers',\n        name=f'Image 2 (|v₂|={n2:.2f})',\n        line=dict(color='#ff7f00', width=2)\n    ))\n\n    # Add true arrows using annotations\n    fig.update_layout(annotations=[\n        dict(\n            x=x1, y=y1, ax=0, ay=0,\n            xref=\"x\", yref=\"y\", axref=\"x\", ayref=\"y\",\n            showarrow=True, arrowhead=3, arrowwidth=2,\n            arrowcolor='#1f78b4', opacity=0.9, text=\"\"\n        ),\n        dict(\n            x=x2, y=y2, ax=0, ay=0,\n            xref=\"x\", yref=\"y\", axref=\"x\", ayref=\"y\",\n            showarrow=True, arrowhead=3, arrowwidth=2,\n            arrowcolor='#ff7f00', opacity=0.9, text=\"\"\n        ),\n        dict(\n            text=f\"&lt;b&gt;Cosine Similarity = {cos_sim:.2f}&lt;/b&gt;\",\n            xref=\"paper\", yref=\"paper\", x=0.35, y=1.05,\n            showarrow=False,\n            font=dict(size=14, color='black'),\n            bgcolor=\"white\", borderpad=4\n        ),\n        # Optional: vector magnitude text at tip\n        dict(\n            x=x1, y=y1, text=f\"|v₁|={n1:.2f}\",\n            showarrow=False, font=dict(color='#1f78b4', size=11),\n            xanchor='left', yanchor='bottom', xshift=5, yshift=5\n        ),\n        dict(\n            x=x2, y=y2, text=f\"|v₂|={n2:.2f}\",\n            showarrow=False, font=dict(color='#ff7f00', size=11),\n            xanchor='left', yanchor='bottom', xshift=5, yshift=5\n        )\n    ])\n\n    # Draw angle arc\n    if n1 &gt; 0 and n2 &gt; 0 and not np.isclose(abs(cos_sim), 1.0):\n        angle1 = np.arctan2(y1, x1)\n        angle2 = np.arctan2(y2, x2)\n        theta = np.linspace(min(angle1, angle2), max(angle1, angle2), 100)\n        arc_radius = 0.5\n        arc_x = arc_radius * np.cos(theta)\n        arc_y = arc_radius * np.sin(theta)\n        fig.add_trace(go.Scatter(\n            x=arc_x, y=arc_y,\n            mode='lines', name='θ',\n            line=dict(color='#33a02c', dash='dot')\n        ))\n\n        mid_angle = (angle1 + angle2) / 2\n        fig.add_annotation(\n            x=arc_radius * np.cos(mid_angle),\n            y=arc_radius * np.sin(mid_angle),\n            text=f\"θ = {angle_deg:.1f}°\",\n            showarrow=False,\n            font=dict(size=13, color='#33a02c')\n        )\n\n    # Layout styling\n    fig.update_layout(\n        width=520, height=520,\n        margin=dict(l=30, r=20, t=50, b=30),\n        xaxis=dict(range=[-0.2, 3.2], zeroline=True, showgrid=True, gridcolor='lightgray'),\n        yaxis=dict(range=[-0.2, 3.2], zeroline=True, showgrid=True, gridcolor='lightgray'),\n        plot_bgcolor='white',\n        paper_bgcolor='white',\n        title='Cosine Similarity Between Two Images',\n        showlegend=True\n    )\n\n    fig.show()\n\n# Sliders\nx1_slider = widgets.FloatSlider(value=1.2, min=0.0, max=3.0, step=0.1, description='Vec1 X')\ny1_slider = widgets.FloatSlider(value=2.4, min=0.0, max=3.0, step=0.1, description='Vec1 Y')\nx2_slider = widgets.FloatSlider(value=2.0, min=0.0, max=3.0, step=0.1, description='Vec2 X')\ny2_slider = widgets.FloatSlider(value=1.0, min=0.0, max=3.0, step=0.1, description='Vec2 Y')\n\nui = widgets.VBox([\n    widgets.HBox([x1_slider, y1_slider]),\n    widgets.HBox([x2_slider, y2_slider])\n])\n\nout = widgets.interactive_output(\n    plot_cosine_similarity,\n    {'x1': x1_slider, 'y1': y1_slider, 'x2': x2_slider, 'y2': y2_slider}\n)\n\ndisplay(ui, out)\n\n\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../data/complete_sculpture_dataset_labeled.csv\")\n\ndf['material']\n\n0     Marble\n1     Marble\n2     Marble\n3     Marble\n4       Lead\n       ...  \n57    Marble\n58    Marble\n59    Marble\n60    Bronze\n61    Bronze\nName: material, Length: 62, dtype: object\n\n\n\ndf['material'] = df['material'].where(df['material'].isin(['Marble', 'Bronze']), 'Other')\n\n\nprint(df)\n\n                     filename  page           group           era material\n0   page188_img01_photo13.jpg   188   SOUNION GROUP  615 - 590 BC   Marble\n1    page202_img01_photo3.jpg   202   SOUNION GROUP  615 - 590 BC   Marble\n2    page202_img01_photo4.jpg   202   SOUNION GROUP  615 - 590 BC   Marble\n3    page205_img01_photo4.jpg   205   SOUNION GROUP  615 - 590 BC   Marble\n4   page211_img01_photo12.jpg   211   SOUNION GROUP  615 - 590 BC    Other\n..                        ...   ...             ...           ...      ...\n57  page357_img01_photo12.jpg   357     MELOS GROUP  555 - 540 BC   Marble\n58  page358_img01_photo10.jpg   358     MELOS GROUP  555 - 540 BC   Marble\n59   page358_img01_photo4.jpg   358     MELOS GROUP  555 - 540 BC   Marble\n60   page363_img01_photo4.jpg   363  PTOON 20 GROUP  520 - 485 BC   Bronze\n61   page365_img01_photo3.jpg   365        EPILOGUE  485 - 460 BC   Bronze\n\n[62 rows x 5 columns]\n\n\n\ndf.to_csv('../data/complete_sculpture_dataset_labeled.csv', index = False)"
  },
  {
    "objectID": "pages/team.html",
    "href": "pages/team.html",
    "title": "prAxIs Team",
    "section": "",
    "text": "The prAxIs project is a true team effort; the learning materials that you see here have taken input from many writers, coders, editors, and reviewers from a variety of disciplines."
  },
  {
    "objectID": "pages/team.html#principal-investigators",
    "href": "pages/team.html#principal-investigators",
    "title": "prAxIs Team",
    "section": "Principal Investigators",
    "text": "Principal Investigators\n\nJonathan Graves\nLaura Nelson"
  },
  {
    "objectID": "pages/team.html#research-assistants",
    "href": "pages/team.html#research-assistants",
    "title": "prAxIs Team",
    "section": "Research Assistants",
    "text": "Research Assistants\n\n Summer 2025 \n\n\nIrene Berezin\n\n\nAlex Ronczewski\n\n\nNathan Zhang\n\n\nJalen Faddick\n\n\nYash Mali\n\n\nAnna Kovtunenko\n\n\nKrishaant Pathmanathan"
  },
  {
    "objectID": "pages/team.html#praxis-partners",
    "href": "pages/team.html#praxis-partners",
    "title": "prAxIs Team",
    "section": "prAxIs+ Partners",
    "text": "prAxIs+ Partners\n\nUBC’s Department of Economics: Vancouver School of Economics\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson"
  },
  {
    "objectID": "pages/team.html#special-thanks",
    "href": "pages/team.html#special-thanks",
    "title": "prAxIs Team",
    "section": "Special Thanks",
    "text": "Special Thanks\n\nThe UBC TLEF team, especially Jeff Miller and Jason Myers.\nThe team behind QuantEcon, especially Jesse Perla and Peifan Wu for their advice and support.\nThe UBC CTLT, Arts ISIT, and LT Hub Teams, for their support with Jupyter and GitLab, especially Stephen Michaud, Nausheen Shafiq, and Michael Ha.\nThe UBC DataScience Slack and the Jupyter team, especially Tiffany Timbers, Phil Austin, Firas Moosvi, and the presenters and attendees at JupyterDays 2020\nThe staff at the VSE for facilitating events, payments, and space use, especially Maria Smith and Caroline Gatchalian\nPlus many, many, more!"
  },
  {
    "objectID": "pages/soci400.html",
    "href": "pages/soci400.html",
    "title": "SOCI400 Placeholder",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In scelerisque lorem vel sollicitudin blandit. Donec et purus vitae metus posuere rutrum. Nunc id elit vitae libero blandit ultrices sed quis quam. Praesent suscipit nisl et tempor ornare. Phasellus laoreet mattis porttitor. Integer eu congue enim, vel auctor ligula. Curabitur gravida hendrerit lorem quis dictum.\nNunc pulvinar felis lectus, non finibus velit placerat id. Curabitur dignissim egestas lacinia. Nam ut urna non ante ultrices vulputate. Fusce malesuada venenatis porttitor. Mauris vulputate erat quis leo pharetra semper. Etiam semper arcu sit amet volutpat cursus. Mauris nec molestie tellus, sit amet consectetur risus. Nunc non nulla et velit elementum vulputate. Curabitur lectus tellus, consequat ut aliquam id, sodales sit amet velit. Praesent ac lacus urna.\nSuspendisse rutrum iaculis eleifend. Donec id ex accumsan, condimentum nisl blandit, lacinia metus. Proin fermentum facilisis placerat. Nullam auctor ex nisl, condimentum ornare metus cursus sed. Nullam ut nisl sed lorem dignissim consectetur sit amet id sem. Nam dolor nunc, lacinia iaculis nulla sit amet, scelerisque malesuada nibh. Praesent et nulla ac tellus tristique ultricies. Quisque condimentum libero felis, eget dapibus leo sodales vel.\nQuisque vel sapien nec nisi luctus dignissim. Proin id nisl sit amet enim semper sodales et ut est. Pellentesque pretium leo sapien, non viverra justo iaculis nec. Mauris congue tortor et eros gravida, quis imperdiet ante efficitur. Suspendisse sapien libero, dignissim suscipit nisi at, sodales porta elit. Sed et eros id turpis mattis rhoncus. Sed non arcu dui. Donec fermentum condimentum lectus. Donec sit amet felis vitae ex interdum dictum. Suspendisse vulputate neque quis semper mattis. Nulla imperdiet dolor odio, at porttitor risus volutpat quis. Nullam eu dictum risus. Aliquam erat volutpat. Suspendisse potenti. Integer volutpat tempus dui, in auctor massa dapibus a. Vestibulum facilisis velit leo, vitae accumsan tortor convallis id.\nSed vitae elit quis ligula luctus mattis eget eget nunc. Ut hendrerit rutrum condimentum. Nulla facilisi. Morbi pretium venenatis posuere. Integer tellus quam, porta vel massa et, fringilla tristique nisl. Sed dolor arcu, dignissim a tellus sit amet, interdum ornare quam. Etiam ullamcorper neque libero, vitae egestas velit maximus in. Proin ut blandit ante, ut faucibus nulla. Maecenas auctor malesuada augue. Duis auctor tincidunt lorem vitae viverra. Suspendisse ante neque, faucibus ut imperdiet ac, ornare eget urna. Nunc faucibus, dolor ut maximus consectetur, enim felis semper mi, sit amet luctus dolor erat sed urna. Morbi libero orci, aliquam et hendrerit id, pharetra ut ligula. Etiam ut elit est."
  },
  {
    "objectID": "pages/soci217.html",
    "href": "pages/soci217.html",
    "title": "SOCI217 Placeholder",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In scelerisque lorem vel sollicitudin blandit. Donec et purus vitae metus posuere rutrum. Nunc id elit vitae libero blandit ultrices sed quis quam. Praesent suscipit nisl et tempor ornare. Phasellus laoreet mattis porttitor. Integer eu congue enim, vel auctor ligula. Curabitur gravida hendrerit lorem quis dictum.\nNunc pulvinar felis lectus, non finibus velit placerat id. Curabitur dignissim egestas lacinia. Nam ut urna non ante ultrices vulputate. Fusce malesuada venenatis porttitor. Mauris vulputate erat quis leo pharetra semper. Etiam semper arcu sit amet volutpat cursus. Mauris nec molestie tellus, sit amet consectetur risus. Nunc non nulla et velit elementum vulputate. Curabitur lectus tellus, consequat ut aliquam id, sodales sit amet velit. Praesent ac lacus urna.\nSuspendisse rutrum iaculis eleifend. Donec id ex accumsan, condimentum nisl blandit, lacinia metus. Proin fermentum facilisis placerat. Nullam auctor ex nisl, condimentum ornare metus cursus sed. Nullam ut nisl sed lorem dignissim consectetur sit amet id sem. Nam dolor nunc, lacinia iaculis nulla sit amet, scelerisque malesuada nibh. Praesent et nulla ac tellus tristique ultricies. Quisque condimentum libero felis, eget dapibus leo sodales vel.\nQuisque vel sapien nec nisi luctus dignissim. Proin id nisl sit amet enim semper sodales et ut est. Pellentesque pretium leo sapien, non viverra justo iaculis nec. Mauris congue tortor et eros gravida, quis imperdiet ante efficitur. Suspendisse sapien libero, dignissim suscipit nisi at, sodales porta elit. Sed et eros id turpis mattis rhoncus. Sed non arcu dui. Donec fermentum condimentum lectus. Donec sit amet felis vitae ex interdum dictum. Suspendisse vulputate neque quis semper mattis. Nulla imperdiet dolor odio, at porttitor risus volutpat quis. Nullam eu dictum risus. Aliquam erat volutpat. Suspendisse potenti. Integer volutpat tempus dui, in auctor massa dapibus a. Vestibulum facilisis velit leo, vitae accumsan tortor convallis id.\nSed vitae elit quis ligula luctus mattis eget eget nunc. Ut hendrerit rutrum condimentum. Nulla facilisi. Morbi pretium venenatis posuere. Integer tellus quam, porta vel massa et, fringilla tristique nisl. Sed dolor arcu, dignissim a tellus sit amet, interdum ornare quam. Etiam ullamcorper neque libero, vitae egestas velit maximus in. Proin ut blandit ante, ut faucibus nulla. Maecenas auctor malesuada augue. Duis auctor tincidunt lorem vitae viverra. Suspendisse ante neque, faucibus ut imperdiet ac, ornare eget urna. Nunc faucibus, dolor ut maximus consectetur, enim felis semper mi, sit amet luctus dolor erat sed urna. Morbi libero orci, aliquam et hendrerit id, pharetra ut ligula. Etiam ut elit est."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#prerequisites",
    "href": "pages/installation/vscode_setup.html#prerequisites",
    "title": "Using VSCode",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHave installed VSCode on your device."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#why-use-an-ide",
    "href": "pages/installation/vscode_setup.html#why-use-an-ide",
    "title": "Using VSCode",
    "section": "Why use an IDE?",
    "text": "Why use an IDE?\nAn IDE, also known as a integrated development environment, is a software application that streamlines software development. Using an IDE such as VSCode is often better than locally hosting a jupyter notebook because it allows us to avoid using CLIs, offers code assistance just as syntax highlighting, and has a built-in highly customizable and extensible environment with a vast library of extensions."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#using-comet-with-vscode",
    "href": "pages/installation/vscode_setup.html#using-comet-with-vscode",
    "title": "Using VSCode",
    "section": "Using COMET with VSCode",
    "text": "Using COMET with VSCode"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-r",
    "href": "pages/installation/vscode_setup.html#installing-r",
    "title": "Using VSCode",
    "section": "1. Installing R",
    "text": "1. Installing R\nThe first thing we’ll need to do is install R onto our computer.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-a-r-package-compiler",
    "href": "pages/installation/vscode_setup.html#installing-a-r-package-compiler",
    "title": "Using VSCode",
    "section": "2. Installing a R package Compiler",
    "text": "2. Installing a R package Compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-miniconda",
    "href": "pages/installation/vscode_setup.html#installing-miniconda",
    "title": "Using VSCode",
    "section": "3. Installing MiniConda",
    "text": "3. Installing MiniConda\nAdditionally, we’ll need to do is install miniconda, a python distribution that allows us to simplify package installations. Head to anaconda.com and follow the instructions below, depending on your operating system.\n\nWindowsMacOS\n\n\n\nScroll to Latest Miniconda installer links and select Miniconda3 Windows 64-bit.\nOpen the installer, and select next &gt; I Agree &gt; Just Me (recommended).\nSelect your destination folder of choice and press next.\nSelect the following options:\n\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\nClear the package cache upon completion\n\n\nLastly, press install.\n\n\n\n\nScroll to Latest Miniconda installer links and select the version compatible with your device.\nOpen the installer and follow the instructions."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#setting-up-our-environment",
    "href": "pages/installation/vscode_setup.html#setting-up-our-environment",
    "title": "Using VSCode",
    "section": "4. Setting up our environment",
    "text": "4. Setting up our environment\nBefore we download the comet modules, we’ll need to set up our environment and install required packages.\n\nIn your computer file system, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet_env jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#configuring-the-irkernel-and-installing-reditorsupport",
    "href": "pages/installation/vscode_setup.html#configuring-the-irkernel-and-installing-reditorsupport",
    "title": "Using VSCode",
    "section": "5. Configuring the IRKernel and Installing REditorSupport",
    "text": "5. Configuring the IRKernel and Installing REditorSupport\nWe’ll now set up the kernel that will allow us to code in the R programming language in VSCode.\n\nOpen the copy of R 4.4.0 that we installed earlier.\nIn the terminal, paste the following lines of code one at a time:\n\n\ninstall.packages('IRkernel') This will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, that would be Canada (MB)).\n\n\nIRkernel::installspec()\n\n\nWe’ll now need to install the VSCode REditorSupport extension, which will allow us to interact with the R terminal from within VScode.\n\n\n\nOpen R 4.4.0 and paste install.packages(\"languageserver\"). Make sure to select the CRAN mirror closest to you.\nIn VSCode, open the extensions page. You can do so by pressing Ctrl+Shift+X on Windows, or Cmd+Shift+X on MacOS. Alternatively, you can find the extensions panel on the left-hand side of your screen.\n\n\n\n\n\n\n\n\n\n\n\nIn the extensions search, type r and select the first option.\nPress install. You should now be able to access the R terminal directly from the VSCode console. You may need to close and reopen VSCode.\n\n\n\n\nYou can access the R terminal directly though VSCode by right-clicking the arrow next to the + in the terminal and selecting R interactive."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#opening-the-comet-modules",
    "href": "pages/installation/vscode_setup.html#opening-the-comet-modules",
    "title": "Using VSCode",
    "section": "6. Opening the COMET modules",
    "text": "6. Opening the COMET modules\nLastly, we need to actually be able to open and work on the COMET modules in VSCode.\n\nOpen comet.arts.ubc.ca and, in the top right corner, select Launch Comet &gt; Launch Locally. This will download a .zip file to your computer.\nOpen the file in file explorer and extract the zip folder to your prefered destination.\nIn VSCode, select File &gt; Open Folder and select the COMET folder that you just unzipped. This will open the comet modules on your computer!"
  },
  {
    "objectID": "pages/installation/python_setup.html",
    "href": "pages/installation/python_setup.html",
    "title": "Using Python",
    "section": "",
    "text": "This page explains how to set up Python in order to access and use the COMET notebooks. Credit goes to Professor Daniel Chen and the Mac and Windows installation pages. Additional credit goes to https://www.digitalocean.com/community/tutorials/install-python-windows-10. We will go over how to install Python and how to use it locally through Jupyter, on JupyterHub, and through VSCode."
  },
  {
    "objectID": "pages/installation/python_setup.html#installing-python",
    "href": "pages/installation/python_setup.html#installing-python",
    "title": "Using Python",
    "section": "1. Installing Python",
    "text": "1. Installing Python\nThe first thing we’ll need to do is install Python onto our computer.\n\nWindowsMacOS\n\n\n\nOpen the Miniforge Installation Platform and select the appropriate link. For Windows users, this will be the Windows artifact.\nOnce you’ve downloaded the artifact, you can run the installer and use all of the default options. The install location should look like C:\\Users\\YOUR_USER_NAME\\miniforge3.\nNext, open the Start Menu and search for “Miniforge Prompt”. When this opens you will see a prompt similar to (base) C:\\Users\\your_name. We’ll also check that the Python installation works by running\n\npython --version. This should return Python 3.11.0 or greater. If it does not, confirm that you are in the (base) environment and update the base python with:\nconda install python=3.11.\n\n\n\nYou’ll want to first set your default Terminal shell to Bash as opposed to Zsh. Open the Terminal (see this video for help) and type chsh -s /bin/bash. The close the Terminal and reopen it (this restarts the Terminal). Now, you should see Bash at the top of your Terminal.\nNext, open the Miniforge Installation Platform and select the appropriate link. Check your device type by clicking “About This Mac” on the top left of your screen - if your chip is M1, M2,…, download the arm64 artifact. Otherwise, download the x86_64 artifact. Make sure that this file is in your Downloads folder.\nOpen the terminal and run the following code: bash ${HOME}/Downloads/Miniforge3.sh -b -p \"${HOME}/miniforge3\". You may need to rename the file Miniforge.sh.\nNow you can run source \"${HOME}/miniforge3/etc/profile.d/conda.sh\" conda activate conda init.\nYou should now have access to Python! If the installation was successful, you will see (base) in your terminal before your device name and username. To confirm that conda is working, run\n\nconda --version. You should see something like this: conda 23.5.2. We will also confirm that Python is working. To do so, run python --version. It should return Python 3.11.0. If you do not see Python &gt;3.11, close your terminal and open a new one. Confirm that you are in the (base) environment. Then update the base python with:\nconda install python=3.11."
  },
  {
    "objectID": "pages/installation/python_setup.html#installing-the-python-kernel",
    "href": "pages/installation/python_setup.html#installing-the-python-kernel",
    "title": "Using Python",
    "section": "2. Installing the Python Kernel",
    "text": "2. Installing the Python Kernel\nWe’ll need to install the Python kernel in order to use the Python programming language in Jupyter. To do so, in your terminal, run python. You should see a line with your version of Python and the date that you loaded it.\nThen, run the following lines of code in the terminal:\nconda install ipykernel\nSelect yes if prompted. This will install the Python kernel which will allow us to run Python code!"
  },
  {
    "objectID": "pages/installation/python_setup.html#using-python-locally-through-jupyter",
    "href": "pages/installation/python_setup.html#using-python-locally-through-jupyter",
    "title": "Using Python",
    "section": "3. Using Python Locally through Jupyter",
    "text": "3. Using Python Locally through Jupyter\nThe first option for running Python is locally through Jupyter. Some students may prefer to use the local version of Jupyter that is accessible via browser, rather than through Jupyter desktop. The latter allows for more customizability at the expense of a more intuitive installation and activation process.\n\n3.1. Creating a New Environment\nHere, we will create a new Python environment called “comet”. An environment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nTo do this, in the terminal, enter the following line code:\nconda create -n python_comet_env jupyterlab\nWhat this is doing is creating an environment called python_comet_env using the conda package manager, and we’re asking it to include Jupyter in the environment.\nThen, to enable the environment, run conda activate python_comet_env.\n\n\n\n\n\n\nWarning\n\n\n\nEvery time you want to run a COMET notebook through Jupyter, you will have to run conda activate python_comet_env. Otherwise, your computer will not know how to access Jupyter."
  },
  {
    "objectID": "pages/installation/python_setup.html#opening-jupyter",
    "href": "pages/installation/python_setup.html#opening-jupyter",
    "title": "Using Python",
    "section": "3.2. Opening Jupyter",
    "text": "3.2. Opening Jupyter\nFinally, to open Jupyter, run the following command in your terminal: jupyter lab. This will open up Jupyter as a local copy on your search engine. This must be done after you run conda activate [name of environment]\", otherwise your computer will not know where to find Jupyter.\n\n\n\n\n\n\nWarning\n\n\n\nThis terminal acts as your local Jupyter server. Closing it will shut down your server!"
  },
  {
    "objectID": "pages/installation/python_setup.html#opening-the-comet-modules-locally",
    "href": "pages/installation/python_setup.html#opening-the-comet-modules-locally",
    "title": "Using Python",
    "section": "3.3. Opening the COMET modules Locally",
    "text": "3.3. Opening the COMET modules Locally\nLastly, you’ll want to download the COMET files. In the COMET website, press launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer. Extract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. When you launch Jupyter, you will see all of the files on your computer. Locate the folder where you saved the COMET and start working from there!"
  },
  {
    "objectID": "pages/installation/python_setup.html#using-python-remotely-through-jupyterhub",
    "href": "pages/installation/python_setup.html#using-python-remotely-through-jupyterhub",
    "title": "Using Python",
    "section": "4. Using Python Remotely through JupyterHub",
    "text": "4. Using Python Remotely through JupyterHub\nJupyterHub is a cloud-based server hosts all of the computational resources needed to run the notebooks. Since it runs in the cloud, you don’t need to install anything on your computer, and the performance is not affected by your system resources.\nTo launch in a cloud environment, select launch COMET from the top navigation bar, and then choose your hub. For UBC students, we recommend JupyterOpen, which is a UBC-supported hub which we maintain, and has all of the necessary packages pre-installed and robust space and processor support. Note that you must have a Campus-Wide Login (CWL) in order to access these resources.\nIf JupyterOpen is not working, you can also use PIMS Syzygy (Siz-za-gee). There is a UBC-specific version of this server, and several others that use a Google account or other authentication method.\nFinally, if you are part of the Google ecosystem, you can launch them on Google Collab which has a slightly different appearance but it otherwise the same.\n\n\n\n\n\n\nWarning\n\n\n\nSyzygy and Google Collab are not officially supported UBC software, and may not be privacy-compliant. You should take responsibility for your own use of these resources if you choose to use them. If Google already has all your data, too bad!\n\n\nOnce you are on the server, use the file navigation on the left to find the notebook you are interested in!"
  },
  {
    "objectID": "pages/installation/jupyterhub_setup.html",
    "href": "pages/installation/jupyterhub_setup.html",
    "title": "Accessing COMET using a JupyterHub",
    "section": "",
    "text": "The easiest way to load and use these notebooks is via a JupyterHub. This cloud-based server hosts all of the computational resources needed to run the notebooks. Better still, since it runs in the cloud you don’t need to install anything on your computer, and the performance is not affected by your system resources.\n\nTo launch in a cloud environment, select launch COMET from the top navigation bar, and then choose your hub. For UBC students, we recommend JupyterOpen, which is a UBC-supported hub which we maintain, and has all of the necessary packages pre-installed and robust space and processor support. Note that you must have a Campus-Wide Login (CWL) in order to access these resources.\nIf JupyterOpen is not working, you can also use PIMS Syzygy (Siz-za-gee). There is a UBC-specific version of this server, and several others that use a Google account or other authentication method.\nFinally, if you are part of the Google ecosystem, you can launch them on Google Collab which has a slightly different appearance but it otherwise the same.\n\n\n\n\n\n\n\nWarning\n\n\n\nSyzygy and Google Collab are not officially supported UBC software, and may not be privacy-compliant. You should take responsibility for your own use of these resources if you choose to use them. If Google already has all your data, too bad!\n\n\nOnce you are on the server, use the file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .ipynb file to start the notebook."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html",
    "href": "pages/installation/installing_locally_old.html",
    "title": "Install and Use COMET",
    "section": "",
    "text": "A Word of Warning\n\n\n\nWe are still fine-tuning this part. It might not work properly. Let us know if you have any issues here (comet.project at ubc dot ca)\nWe have designed COMET to be easy to use and install for a wide range of students. If there’s one thing we know about teaching data and econometrics, it’s that everyone finds their own routine and tools that work best for them. We encourage you to explore and try different things - see what works best for you!\nIf you’re not sure, start with the cloud-based option, then get more sophisticated in the future."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#installing-packages",
    "href": "pages/installation/installing_locally_old.html#installing-packages",
    "title": "Install and Use COMET",
    "section": "Installing Packages",
    "text": "Installing Packages\nYou may at some points need to install some extra packages if you are not working on JupyterOpen. You can do this by opening the server, then clicking on the R Console in the launcher tab.\n\nOnce the console opens, you should see a command line with R version 4.2.1 (2022-06-23 ucrt) or something similar.\nIn the bottom cell window, you should enter:\n\ninstall.packages(c(\"tidyverse\", \"car\", \"stargazer\", \"estimatr\", \"sandwich\"))\nthen hit ctrl-enter to run the command. It should start installing, and may prompt you to select a CRAN mirror (choose any one near you). Be patient: this might take a while!\nYou should only have to do this once for each server you work with."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#create-a-project",
    "href": "pages/installation/installing_locally_old.html#create-a-project",
    "title": "Install and Use COMET",
    "section": "Create a Project",
    "text": "Create a Project\nNext, you’ll want to download the files. Select launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer.\n\nExtract the .zip file to a folder on your computer that you can find easily and give it a name (e.g. comet-project).\nIn RStudio, go to “File &gt; New Project” then select “Existing Directory”. Browse and select the directory from the previous step.\n\nYou’re now ready to go! Use RStudio’s file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .Rmd or .qmd file to open the notebook."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#sec-step1",
    "href": "pages/installation/installing_locally_old.html#sec-step1",
    "title": "Install and Use COMET",
    "section": "Step 1: Install the Environment Manager",
    "text": "Step 1: Install the Environment Manager\nDownload the most recent version of miniconda for your computer operating system from:\nhttps://docs.conda.io/en/latest/miniconda.html\nThe version selection is a little bit different for different operating systems, so click on the appropriate tab below.\n\nWindows 10/11Macintosh\n\n\nIt is important to pay attention to the version you download: different processors will require different versions.\n\nTo check your processor, open the Start menu and search “Processor” then click “View Processor Info”\nUnder “Device Specifications” look at System Type.\n\nif this says 64-bit operating system or something like x64-based processor choose the 64-bit version\nif this says 32-bit operating system choose the 32-bit version.\n\n\n\n\nIt is important to pay attention to the version you download: different processors will require different versions.\n\nTo check your processor, click on the Apple logo in the top-left of your screen, and select “About This Mac”\nLook for the processor information.\n\nif this says something like 3.2 GHz Intel Core i5 or something like x64-based processor choose the macOS Intel x86 64-bit version\nif this says something like Apple M1 choose the macOS Apple M1 64-bit version\n\nWe recommend choosing the .pkg version of the installer.\n\n\n\n\nOnce you have downloaded the installer, run the installer.\n\nMake sure you choose a sensible installation location; you can ignore any warnings about spaces in names.\nCheck the following options, if available:\n\nCreate shortcuts\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.10\nClear the package cache upon completion.\n\nRun the installer, which can take a while."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-2-install-the-environment",
    "href": "pages/installation/installing_locally_old.html#step-2-install-the-environment",
    "title": "Install and Use COMET",
    "section": "Step 2: Install the Environment",
    "text": "Step 2: Install the Environment\nNow that we have our environment manager installed, we need to add in the necessary packages.\n\n\n\n\n\n\nTip\n\n\n\nThis will take a while and requires a stable internet connection; make sure you’re plugged in and not on a bus or something!\n\n\nTo make this easier, we have create an environment file, which contains all of the necessary packages and installation files for miniconda. Download this file and place it in a directory that you can easily find.\n\nYou can find this file here. Right-click, save-as, to download.\n\nRight-click on comet-environment.yml and write down the file path. You will need this in a moment. Next, launch your system’s command prompt:\n\nWindows 10/11Macintosh\n\n\n\nOpen the Start Menu and type in cmd\nRight-click on “Command Prompt” and/or select “Run as Administrator”\nAgree to the warning that pops up, if it does.\n\n\n\n\nClick the Launchpad icon in the Dock, type Terminal in the search field, then click “Terminal”.\n\nIf this doesn’t work, open the Finder, then open the /Applications/Utilities folder, and finally double-click Terminal.\n\n\n\nOnce your command prompt is running, enter the following command:\nconda env create -f \"MYPATH/comet-environment.yml\"\nreplacing \"MYPATH/ with the file path you noted earlier. Hit enter to run it.\nminiconda will run, and install all of the files. This may take some time, so grab a sandwich, and don’t turn-off your computer."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-3-configure-the-irkernel",
    "href": "pages/installation/installing_locally_old.html#step-3-configure-the-irkernel",
    "title": "Install and Use COMET",
    "section": "Step 3: Configure the IRkernel",
    "text": "Step 3: Configure the IRkernel\nThe last major step is to set up the kernel properly. Enter the following into the command prompt and hit enter:\nconda activate comet\nThen, type R. Once R loads, enter the following two commands, hitting enter to run each one:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThey should complete, and you’re now ready to go. Close the command prompt."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-4-download-the-notebooks",
    "href": "pages/installation/installing_locally_old.html#step-4-download-the-notebooks",
    "title": "Install and Use COMET",
    "section": "Step 4: Download the Notebooks",
    "text": "Step 4: Download the Notebooks\nNext, you’ll want to download the files. Select launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer.\n\nExtract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. Find the file path of the this directory, and copy it down."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-5-using-jupyter-and-creating-a-short-cut",
    "href": "pages/installation/installing_locally_old.html#step-5-using-jupyter-and-creating-a-short-cut",
    "title": "Install and Use COMET",
    "section": "Step 5: Using Jupyter and Creating a Short-Cut",
    "text": "Step 5: Using Jupyter and Creating a Short-Cut\nTest your Jupyter installation by opening a new command prompt, then entering the following two commands:\ncd FILEPATH\nconda activate comet\njupyter lab\nwhere FILEPATH is the directory from Step 4, above.\nYour web-brower should launch, and Jupyter should load. You can now load or create a notebook. Use the file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .ipynb file to start the notebook.\n\n\n\n\n\n\nImportant\n\n\n\nThe command window will stay open, and report the server status. Don’t close this window until you’ve saved your work or your JupyterHub will die and you’ll have to re-do everything.\n\n\nWhenever you want to launch JupyterLab, repeat the two steps above. This can be a little tedious: an alterative is to create a shortcut, which you can do below.\n\nCreating a Shortcut\nThis is different for other operating systems, so choose the version.\n\nWindowsMacintosh\n\n\nOpen notepad from the Start Menu, and then enter:\n@call conda run -n comet --no-capture-output jupyter lab \n@CMD /K\nSave this file as run_comet.bat and place it in your comet-project folder. When you double-click on it, it should immediately launch Jupyter Lab for you in the associated folder.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI haven’t tested this yet! Let me know if it’s busted.\n\n\nClick the Launchpad icon in the Dock, type TextEdit in the search field, then click “TextEdit”.\nLaunch the terminal, and type in nano to launch the (very old-school) nano text editor. In the editor, enter:\n#!/bin/zsh\nconda run -n comet --no-capture-output jupyter lab \nread\nThen save the file by hitting CTRL and X on your keyboard together, and type in run_comet.sh, then hit y to save. The file should now be saved to your computer as run_comet.sh. Make sure you place it in the comet-project folder. There’s one last step: making it executable.\nIn your Terminal then enter:\ncd FILEPATH\nwhere FILEPATH is the location of your run_comet.sh script. Then, enter:\nchmod 755 run_comet.sh\nFinally, to make it run on your computer:\n\nfind the file in your Finder, and right-click, and select “Open with…” and select “Other…”.\ntoggle the dropdown to “All Applications” from “Recommended Applications”\nunder “Utilities” select Terminal\ncheck the “always use this application” box, and hit OK.\n\nYou should now be able to double-click the shortcut to launch Jupyter on your computer!\nAll done! No more command line stuff, hopefully!"
  },
  {
    "objectID": "pages/installation/installing_for_development.html",
    "href": "pages/installation/installing_for_development.html",
    "title": "Installing for Development",
    "section": "",
    "text": "So, you want to develop and submit new material to prAxis? Awesome! The official workflow for hacking on COMET uses VSCode. Other IDEs such as RStudio or Jupyter will also work, but we do not provide official instructions.\n\nFirst, you will need to install R using your preferred method for your platform. If you’re not sure what this means, follow the instructions on the R website.\nInstall VSCode using your preferred method for your platform (the linked instructions will be fine for most people). Open VSCode and install the REditorSupport extension, then close VSCode.\nInstall git using your preferred method for your platform (the linked instructions will be fine for most people). If you are developing on the Github version, you may also wish to install GitHub Desktop.\nFinally, install Quarto, and follow the steps to integrate VSCode.\nOpen an R terminal and install renv by runnning install.packages(\"renv\").\nUse git to clone the COMET repository and open it in VSCode. A popup should ask you to install languageserver, click yes.\nWhen Step 6 is complete, restart VSCode. Clone the COMET repository folder and open it. Select the terminal option in the bottom panel and choose “R Terminal” in the dropdown menu next to the plus sign.\nIn the VSCode terminal, run renv::restore(), and type Y to accept installing the packages.\n\nCongratulations, you’re ready to run and hack on all of the COMET notebooks!"
  },
  {
    "objectID": "pages/index/index_SOCI415.html",
    "href": "pages/index/index_SOCI415.html",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "The modules in this unit are for SOCI 415 - Theories of Family and Kinship. Major theoretical approaches to the study of the family. Each approach is assessed for its strengths and weaknesses on the basis of empirical data. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_SOCI415.html#modules",
    "href": "pages/index/index_SOCI415.html#modules",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n22 Jun 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - CBDB Dataset\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n12 Jun 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - KINMATRIX Dataset\n\n\nUsing the KINMATRIX Dataset to explore network analysis for SOCI 415\n\n\n\n27 Jun 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_SOCI217.html",
    "href": "pages/index/index_SOCI217.html",
    "title": "SOCI217",
    "section": "",
    "text": "The modules in this unit are for the classes under SOCI 217. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_SOCI217.html#modules",
    "href": "pages/index/index_SOCI217.html#modules",
    "title": "SOCI217",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nSOCI 217 Notebook 1\n\n\nPlaceholder for Hist414 Notebook 1 for Website Render.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI217 Notebook 2\n\n\nPlaceholder for Hist414 Notebook 1 for Website Render.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI217 Notebook 3\n\n\nPlaceholder for Hist414 Notebook 1 for Website Render.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_ECON227.html",
    "href": "pages/index/index_ECON227.html",
    "title": "ECON227 - Data in Economics",
    "section": "",
    "text": "This section contains material to support UBC’s Data in Economics: Application-driven introduction to the analysis of economic data. Descriptive analysis, causality, experimental and observational data, hypothesis testing. Restricted to BIE students."
  },
  {
    "objectID": "pages/index/index_ECON227.html#modules",
    "href": "pages/index/index_ECON227.html#modules",
    "title": "ECON227 - Data in Economics",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nECON 227 Notebook 1\n\n\nPlaceholder for ECON227 for Website Render.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_AMNE-376.html",
    "href": "pages/index/index_AMNE-376.html",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "",
    "text": "This section contains material to support UBC’s Greek Art and Architecture (AMNE 376). The visual culture of the ancient Greek world in the second and first millennia BCE, especially from c. 1000 to 30 BCE. Credit will be granted for only one of CLST_V 331, AMNE_V 376 or ARTH_V 331. Equivalency: ARTH_V 331 or CLST_V 331.\nCHANGE BELOW * These modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision, or in a flipped classroom setting (i.e. as a lecture) * They include a number of exercises which are intended to be formative (i.e. not graded) in nature, and can be addressed during the lab * They can also be used for self-study, with some additional effort\n\nModules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nAMNE 376: A Study of Richter’s Kouroi Through Image Embedding\n\n\nUsing examples from Richter’s classic book on Kouroi, this notebook demonstrates the basics of computer vision and image embedding, introducing students to how computers…\n\n\n\n1 Jul 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/all.html",
    "href": "pages/index/all.html",
    "title": "All Modules",
    "section": "",
    "text": "This section contains all materials and case studies to support a variety of classes. Filter by topic using the categories on the right. Filter by class by choosing the appropriate class category."
  },
  {
    "objectID": "pages/index/all.html#modules",
    "href": "pages/index/all.html#modules",
    "title": "All Modules",
    "section": "Modules",
    "text": "Modules\n\n\n\n\n\n\n\n\n\n\nECON 227 Notebook 1\n\n\nPlaceholder for ECON227 for Website Render.\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Convolutional Neural Networks (CNNs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Convolutions\n\n\nThis notebook provides an intuitive explanation of convolution and its application in multiple fields with some simple examples.\n\n\n\n22 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 217 Notebook 1\n\n\nPlaceholder for Hist414 Notebook 1 for Website Render.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n22 Jun 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - CBDB Dataset\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n12 Jun 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - KINMATRIX Dataset\n\n\nUsing the KINMATRIX Dataset to explore network analysis for SOCI 415\n\n\n\n27 Jun 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI217 Notebook 2\n\n\nPlaceholder for Hist414 Notebook 1 for Website Render.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI217 Notebook 3\n\n\nPlaceholder for Hist414 Notebook 1 for Website Render.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/econ227.html",
    "href": "pages/econ227.html",
    "title": "ECON227 Placeholder",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In scelerisque lorem vel sollicitudin blandit. Donec et purus vitae metus posuere rutrum. Nunc id elit vitae libero blandit ultrices sed quis quam. Praesent suscipit nisl et tempor ornare. Phasellus laoreet mattis porttitor. Integer eu congue enim, vel auctor ligula. Curabitur gravida hendrerit lorem quis dictum.\nNunc pulvinar felis lectus, non finibus velit placerat id. Curabitur dignissim egestas lacinia. Nam ut urna non ante ultrices vulputate. Fusce malesuada venenatis porttitor. Mauris vulputate erat quis leo pharetra semper. Etiam semper arcu sit amet volutpat cursus. Mauris nec molestie tellus, sit amet consectetur risus. Nunc non nulla et velit elementum vulputate. Curabitur lectus tellus, consequat ut aliquam id, sodales sit amet velit. Praesent ac lacus urna.\nSuspendisse rutrum iaculis eleifend. Donec id ex accumsan, condimentum nisl blandit, lacinia metus. Proin fermentum facilisis placerat. Nullam auctor ex nisl, condimentum ornare metus cursus sed. Nullam ut nisl sed lorem dignissim consectetur sit amet id sem. Nam dolor nunc, lacinia iaculis nulla sit amet, scelerisque malesuada nibh. Praesent et nulla ac tellus tristique ultricies. Quisque condimentum libero felis, eget dapibus leo sodales vel.\nQuisque vel sapien nec nisi luctus dignissim. Proin id nisl sit amet enim semper sodales et ut est. Pellentesque pretium leo sapien, non viverra justo iaculis nec. Mauris congue tortor et eros gravida, quis imperdiet ante efficitur. Suspendisse sapien libero, dignissim suscipit nisi at, sodales porta elit. Sed et eros id turpis mattis rhoncus. Sed non arcu dui. Donec fermentum condimentum lectus. Donec sit amet felis vitae ex interdum dictum. Suspendisse vulputate neque quis semper mattis. Nulla imperdiet dolor odio, at porttitor risus volutpat quis. Nullam eu dictum risus. Aliquam erat volutpat. Suspendisse potenti. Integer volutpat tempus dui, in auctor massa dapibus a. Vestibulum facilisis velit leo, vitae accumsan tortor convallis id.\nSed vitae elit quis ligula luctus mattis eget eget nunc. Ut hendrerit rutrum condimentum. Nulla facilisi. Morbi pretium venenatis posuere. Integer tellus quam, porta vel massa et, fringilla tristique nisl. Sed dolor arcu, dignissim a tellus sit amet, interdum ornare quam. Etiam ullamcorper neque libero, vitae egestas velit maximus in. Proin ut blandit ante, ut faucibus nulla. Maecenas auctor malesuada augue. Duis auctor tincidunt lorem vitae viverra. Suspendisse ante neque, faucibus ut imperdiet ac, ornare eget urna. Nunc faucibus, dolor ut maximus consectetur, enim felis semper mi, sit amet luctus dolor erat sed urna. Morbi libero orci, aliquam et hendrerit id, pharetra ut ligula. Etiam ut elit est."
  },
  {
    "objectID": "pages/amne376.html",
    "href": "pages/amne376.html",
    "title": "AMNE376 Placeholder",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In scelerisque lorem vel sollicitudin blandit. Donec et purus vitae metus posuere rutrum. Nunc id elit vitae libero blandit ultrices sed quis quam. Praesent suscipit nisl et tempor ornare. Phasellus laoreet mattis porttitor. Integer eu congue enim, vel auctor ligula. Curabitur gravida hendrerit lorem quis dictum.\nNunc pulvinar felis lectus, non finibus velit placerat id. Curabitur dignissim egestas lacinia. Nam ut urna non ante ultrices vulputate. Fusce malesuada venenatis porttitor. Mauris vulputate erat quis leo pharetra semper. Etiam semper arcu sit amet volutpat cursus. Mauris nec molestie tellus, sit amet consectetur risus. Nunc non nulla et velit elementum vulputate. Curabitur lectus tellus, consequat ut aliquam id, sodales sit amet velit. Praesent ac lacus urna.\nSuspendisse rutrum iaculis eleifend. Donec id ex accumsan, condimentum nisl blandit, lacinia metus. Proin fermentum facilisis placerat. Nullam auctor ex nisl, condimentum ornare metus cursus sed. Nullam ut nisl sed lorem dignissim consectetur sit amet id sem. Nam dolor nunc, lacinia iaculis nulla sit amet, scelerisque malesuada nibh. Praesent et nulla ac tellus tristique ultricies. Quisque condimentum libero felis, eget dapibus leo sodales vel.\nQuisque vel sapien nec nisi luctus dignissim. Proin id nisl sit amet enim semper sodales et ut est. Pellentesque pretium leo sapien, non viverra justo iaculis nec. Mauris congue tortor et eros gravida, quis imperdiet ante efficitur. Suspendisse sapien libero, dignissim suscipit nisi at, sodales porta elit. Sed et eros id turpis mattis rhoncus. Sed non arcu dui. Donec fermentum condimentum lectus. Donec sit amet felis vitae ex interdum dictum. Suspendisse vulputate neque quis semper mattis. Nulla imperdiet dolor odio, at porttitor risus volutpat quis. Nullam eu dictum risus. Aliquam erat volutpat. Suspendisse potenti. Integer volutpat tempus dui, in auctor massa dapibus a. Vestibulum facilisis velit leo, vitae accumsan tortor convallis id.\nSed vitae elit quis ligula luctus mattis eget eget nunc. Ut hendrerit rutrum condimentum. Nulla facilisi. Morbi pretium venenatis posuere. Integer tellus quam, porta vel massa et, fringilla tristique nisl. Sed dolor arcu, dignissim a tellus sit amet, interdum ornare quam. Etiam ullamcorper neque libero, vitae egestas velit maximus in. Proin ut blandit ante, ut faucibus nulla. Maecenas auctor malesuada augue. Duis auctor tincidunt lorem vitae viverra. Suspendisse ante neque, faucibus ut imperdiet ac, ornare eget urna. Nunc faucibus, dolor ut maximus consectetur, enim felis semper mi, sit amet luctus dolor erat sed urna. Morbi libero orci, aliquam et hendrerit id, pharetra ut ligula. Etiam ut elit est."
  },
  {
    "objectID": "index.html#about-praxis",
    "href": "index.html#about-praxis",
    "title": "Praxis",
    "section": "About prAxIs",
    "text": "About prAxIs\n\nprAxIs is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2025 that seeks to bring Machine Learning and AI into arts classes at UBC.\nBased at UBC’s Vancouver School of Economics, our team consists of faculty and students (at all levels) working in partnership to develop hands-on learning modules that explore the real-world applications of economic theory using statistics.\n\n\n\nUBC Arts"
  },
  {
    "objectID": "index.html#getting-started-with-praxis",
    "href": "index.html#getting-started-with-praxis",
    "title": "Praxis",
    "section": "Getting Started with prAxIs",
    "text": "Getting Started with prAxIs\n\n\n\n\n For Learners \nThese modules cover topics from beginner to advanced applications of statistical methods in economics. Topics include:\n\nHIST 414\nAMNE 170\nAMNE 376\nSOCI 415\nSOCI 217\nSOCI 280\nECON 227\n\nADD HERE\n\nModules can be accessed on the left-hand side of this page. They are written as JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself, complete the exercises and get automatic feedback throughout your learning. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n For Educators \nThese modules, focused on economic questions, models, and data, use the interactive notebook software, JupyterNotebooks to synthesize theoretical learning, practice, and analysis into a single learning experience that is appropriate for either laboratory or flipped classroom instruction.\nOur work adapts proven pedagogical strategies to integrate practical learning throughout economics curriculum in order to provide students with the opportunity to improve their technical data skills.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating prAxIs resources into your instruction, check out our Using prAxIs for Teaching page."
  },
  {
    "objectID": "index.html#citing-praxis",
    "href": "index.html#citing-praxis",
    "title": "Praxis",
    "section": "Citing prAxIs",
    "text": "Citing prAxIs\n\nThis project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teachings’. https://comet.arts.ubc.ca/.\n\n\n\n\n\nHowever, some notebooks have an additional suggested attribution (especially those in GEOG 374). Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page."
  },
  {
    "objectID": "index.html#get-involved",
    "href": "index.html#get-involved",
    "title": "Praxis",
    "section": "Get Involved",
    "text": "Get Involved\n\nprAxIs is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with prAxIs!\n\nSome partners from other fields have made substantial contributions to this project. We call these people part of  prAxIs+. These materials are particularly relevant for interdisciplinary students in economics or other fields who need to combine insights from econometrics with other statistical approaches.\n\nprAxIs+ Partners\n\n\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe prAxIs Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  },
  {
    "objectID": "docs/intro_to_cnn/intro_to_cnn.html",
    "href": "docs/intro_to_cnn/intro_to_cnn.html",
    "title": "Introduction to Convolutional Neural Networks (CNNs)",
    "section": "",
    "text": "Author: Kaiyan Zhang, PRAXIS UBC Team\nDate: 2025-06\nBefore you begin: Install the dependencies that you don’t have by running the code cell below.\n\n# !pip install opencv-python\n# !pip install numpy\n# !pip install matplotlib\n# !pip install pandas\n# !pip install scikit-learn\n# !pip install seaborn\n# !pip install datasets\n# !pip install torch\n# !pip install tqdm\n\n\n\n1. What are Neural Networks?\nWhat is the first thing that comes to your mind when you hear the word “neural network”? If you are thinking about the human brain and neurons, you are not wrong. In fact, the term “neural network” is inspired by the way how human brain and nervous systems work, where neurons are connected to each other and communicate with each other to process information.\nIn the context of machine learning, a neural network, or more precisely, an artificial neural network (ANN) is defined as “a program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions”.\nThe definition seems way too formal and scientific, but we can easily translate it into daily language. Think of taking a closed-book multiple choice exam (Oops, gross). Your brain calls on a team of “experts”, one for course facts, another for what you remember about the professor’s hints in class, another for gut instincts, etc. When you read a question, each expert gives you a confidence score. You weight each score by how much you trust that expert, then add them up. The answer with the highest total “trust \\(\\times\\) confidence” wins. After the exam, you see which answers were wrong and adjust those trust weights (trust the right experts more, the wrong ones less), and prepare for the next exam based on this experience. This exactly how a neural network makes decisions and learns via its feedback loop. Neural networks are following a similar thought and learning process as you and me, and this is why they are flexible and powerful, being able to handle complex, abstract tasks and evolve on their own, like an intelligent creature.\nWhile the core idea is not complex, you may want to master some bluffing terms to translate the professional discussions. In the example above, the “experts” you consulted in your mind are called neurons; the key clues you noticed when reading question are called features; your understanding of exam question is called the input layer; your thought process rounds are called hidden layers; your chosen answer is reflected as the output layer; the mind map that connects all the “experts” and input features is architecture; and each exam attempt with the review of feedback is called a training epoch. See, they are really not that deep! You now can also talk about it as an expert.\n\n\n2. An Intuitive Understanding of Convolutional Neural Networks (CNNs)\nNow that we understood what is an artificial neural network, let’s dive into the real topic here: What’s unique about convolutional neural networks (CNNs) and why they are revolutionary to computer vision and image processing?\nLet’s start by discussing the unique point of CNNs. Imagine you are reading a bird guide and trying to learn the characteristics of a night heron and a grey heron so that you can easily distinguish between the two in the field, what would you do? I believe you would naturally try to observe the birds piece by piece: first comparing the features of the juveniles and adults, then noting how they look both in flight and on land. Gradually, your brain forms a complete comparison: the night heron has a shorter beak, a shorter neck, striking red eyes, and dark blue plumage as an adult; while the grey heron has a longer beak, a longer neck, yellow eyes, and wears grey color plumage.\n\n\n\n\nNight heron in a bird guide\n\n\n\n\n\nGrey heron in a bird guide\n\n\n\nA convolutional neural network would read things in the same way, as it doesn’t look at things in a big picture directly (which is usually costly and slow), but would see an image as multiple small patches to study the unique features and construct a detailed field guide of its own. The way how a CNN sees things this way is through convolution: it has a convolutional layer on top of the input layer to learn features piece by piece in its architecture, such that it can process information from an image in a cleverer way. Moreover, CNNs work quite well even when training images are not as tidy and organized as those in a field guide, which makes it efficient in solving real-life problems.\nLet’s recall some basic concepts of convolution and see how they are applied in the CNNs, typically within the convolutional layer. Here, the inputs are images, and they are interpreted by a computer as grids of numbers. The kernels (also called filters) are still the “brushes” you apply on the input image to extract certain features, but in a CNN, there are usually multiple distinct kernels applied at the same time to extract and map different features. After different features are extracted, they will be pooled together with another kernel and produce a summarized output to be passed into the fully-connected layer for classification or other tasks.\nWhile the principles behind the architectures are complicated, many python libraries now offer easy ways to implement these architectures. In a word, with a labeled image dataset, you can also train a CNN classifier yourself. Let’s try out an example together.\n\n\n3. CNN Example: Classifying Handwritten Digits\n\n\n4. (Optional) Build Our Own CNN Classifier: An Example Using CIFAR-10 Dataset\nClassifying is central in the application of CNNs, so let’s try building a classifier using CNN and see how it works with an example. Let’s say, we want to train a model (the “expert”) that identify and distinguish between some daily objects, such as cars, planes, cat, dogs, etc. We first need to find a dataset that contains images of these objects with labels. This is usually hard as we wouldn’t always have clean, labelled datasets of a specific topic. But luckily, we have many datasets for daily objects.\nThe dataset we are using here is CIFAR-10, it is a widely used practice dataset for beginners to image processing that consists of 60000 32 \\(\\times\\) 32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. Let’s first load the dataset and see what it’s like.\n\nStep 1: Data Preprocessing\n\n# import CIFAR-10 dataset from HuggingFace\nfrom datasets import load_dataset\n\ndataset_train = load_dataset(\n    'cifar10',\n    split='train'# training dataset\n)\n\ndataset_train\n\n\n# check how many labels/number of classes\nnum_classes = len(set(dataset_train['label']))\nnum_classes\n\nWe can also display one of the images to see what it’s like.\n\n# let's view the image (it's very small)\nsample = dataset_train[0]['img']\n\nplt.imshow(sample)\nplt.axis('off')\nplt.show()\n\nCan you see what the image is about? Can you imagine how computers understands it?\nAs most CNNs can only accept images of a fixed size, we will reshape all images to 32 \\(\\times\\) 32 pixels using torchvision.transforms; a pipeline built for image preprocessing. You can think of a pipeline as a series of small programs that together handles a specific task in a sequential order, which in here is resizing the images in the training set.\n\nimport torchvision.transforms as transforms\nfrom tqdm.auto import tqdm\n\n# image size\nimg_size = 32\n\n# preprocess variable, to be used ahead\npreprocess = transforms.Compose([\n    transforms.Resize((img_size,img_size)),\n    transforms.ToTensor()\n])\n\ninputs_train = []\n\nfor record in tqdm(dataset_train):\n    image = record['img']\n    label = record['label']\n\n    # convert from grayscale to RGB\n    if image.mode == 'L':\n        image = image.convert(\"RGB\")\n        \n    # prepocessing\n    input_tensor = preprocess(image)\n    \n    # append to batch list\n    inputs_train.append([input_tensor, label]) \n\nOther than normalizing the general size of the images, we should also normalize the pixel values in the dataset.\n\nmean = [0.4670, 0.4735, 0.4662]\nstd = [0.2496, 0.2489, 0.2521]\n\npreprocess = transforms.Compose([\n    transforms.Normalize(mean=mean, std=std)\n])\n\nfor i in tqdm(range(len(inputs_train))):\n    # prepocessing\n    input_tensor = preprocess(inputs_train[i][0])\n    # replace with normalized tensor\n    inputs_train[i][0] = input_tensor\n\nHere, we load and process the training set that we are using to validate the model quality.\n\n# Loading the dataset\ndataset_val = load_dataset(\n    'cifar10',\n    split='test'  # test set (used as validation set)\n)\n\n# Integrate the preprocessing steps\npreprocess = transforms.Compose([\n    transforms.Resize((img_size,img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ninputs_val = []\ni = 0\nfor record in tqdm(dataset_val):\n    image = record['img']\n    label = record['label']\n\n    # convert from grayscale to RBG\n    if image.mode == 'L':\n        image = image.convert(\"RGB\")\n        \n    # prepocessing\n    input_tensor = preprocess(image)\n    inputs_val.append((input_tensor, label)) # append to batch list\n\nWe noticed that the testing and training data are gigantic in size, which would lag our trainings. To avoid the long training time and huge training cost, we often need to split our data into multiple small batches.\nIn CNN training, choosing a batch size of, say, 32 or 64 gives you the best of both worlds: you “study” small, manageable mini-quizzes, get regular feedback to adjust your filter-weights, and keep your compute requirements reasonable, all while learning robustly across the entire image dataset.\n\nimport torch\n\n# Given the amount of data, we set the batch size as 64 to improve the efficiency when running our model\nbatch_size = 64\n\n# We use DataLoader to split both the training and validation dataset into shuffled batches. \n# Shuffle helps prevent model overfitting by ensuring that batches are more representative of the entire dataset.\ndloader_train = torch.utils.data.DataLoader(\n    inputs_train, batch_size=batch_size, shuffle=True\n)\n\ndloader_val = torch.utils.data.DataLoader(\n    inputs_val, batch_size=batch_size, shuffle=False\n)\n\n\n\nStep 2: Training the CNN Classifier\nAfter carefully processing both the training and the test data, we finally came to a stage where we can train our own CNN classifier. The first thing we need to do is to decide which architecture we want to use for the model.\nArchitecture determines the way how a CNN integrate and learn from the features it extracted, and thus largely determines the performance of a model. Throughout the years, there have been several hugely successful CNN architectures, which we won’t be able to discuss in detail. Here, I will only demonstrate the architecture of LeNet-5: It reads images in a sequence that starts with a partial and combines the partials into a comprehensive one. Intuitively, the learning process of this architecture can be thought as learning to write a new character: You learn to write each stroke first, and then follow the structure of the character to put those strokes together into a complete character.\n\n\nimport torch.nn as nn\n\n# creating a CNN class\nclass ConvNeuralNet(nn.Module):\n    #  determine what layers and their order in CNN object \n    def __init__(self, num_classes):\n        super(ConvNeuralNet, self).__init__()\n        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n        self.relu1 = nn.ReLU()\n        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n        self.relu2 = nn.ReLU()\n        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1)\n        self.relu3 = nn.ReLU()\n        \n        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n        self.relu4 = nn.ReLU()\n\n        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n        self.relu5 = nn.ReLU()\n        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        self.dropout6 = nn.Dropout(p=0.5)\n        self.fc6 = nn.Linear(1024, 512)\n        self.relu6 = nn.ReLU()\n        self.dropout7 = nn.Dropout(p=0.5)\n        self.fc7 = nn.Linear(512, 256)\n        self.relu7 = nn.ReLU()\n        self.fc8 = nn.Linear(256, num_classes)\n    \n    # progresses data across layers    \n    def forward(self, x):\n        out = self.conv_layer1(x)\n        out = self.relu1(out)\n        out = self.max_pool1(out)\n        \n        out = self.conv_layer2(out)\n        out = self.relu2(out)\n        out = self.max_pool2(out)\n\n        out = self.conv_layer3(out)\n        out = self.relu3(out)\n\n        out = self.conv_layer4(out)\n        out = self.relu4(out)\n\n        out = self.conv_layer5(out)\n        out = self.relu5(out)\n        out = self.max_pool5(out)\n        \n        out = out.reshape(out.size(0), -1)\n        \n        out = self.dropout6(out)\n        out = self.fc6(out)\n        out = self.relu6(out)\n\n        out = self.dropout7(out)\n        out = self.fc7(out)\n        out = self.relu7(out)\n\n        out = self.fc8(out)  # final logits\n        return out\n\nAfter designing the network architecture, we initialize it. And if we have access to hardware acceleration (through CUDA or MPS), we move the model to that device to speed up the training.\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# set the model to device\nmodel = ConvNeuralNet(num_classes).to(device)\n\nNext, we will set the loss and optimizer function used during the training. These are the factors that determine how much your network learn from the mistakes and adjust the distribution of weights how it trust the “experts”.\nThe loss function is a metric that measures the classification performance. Here the Cross-Entropy Loss function is one of them that is commonly used in neural networks. The optimizer function drives the model to reflect and adjust its weights after each validation, and its parameter learning rate decides how much the model absorb from the lessons. While it seems that a higher learning rate is beneficial, it is actually not as a high learning rate could lead to severe overshooting. That’s why we set the learning rate lr = 0.01 here to prevent overly progressive learning.\n\n# set loss function\nloss_func = nn.CrossEntropyLoss()\n# set learning rate \nlr = 0.01\n# set optimizer as SGD\noptimizer = torch.optim.SGD(\n    model.parameters(), lr=lr\n) \n\nWe will train the model for 25 epochs. To ensure we’re not overfitting to the training set, we pass the validation set through the model for inference only at the end of each epoch. If we see validation set performance suddenly degrade while train set performance improves, we are likely overfitting.\nYou can run the training and fitting loop as follows, but be cautious: This cell will take a long time to run. Alternatively, you can skip 3 cells and load the model we pre-trained directly.\n\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\nnum_epochs = 25\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    for i, (images, labels) in enumerate(dloader_train):  \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = loss_func(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    avg_train_loss = running_loss / len(dloader_train)\n    train_losses.append(avg_train_loss)\n    \n    with torch.no_grad():\n        model.eval()\n        correct = 0\n        total = 0\n        all_val_loss = []\n        for images, labels in dloader_val:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            total += labels.size(0)\n            predicted = torch.argmax(outputs, dim=1)\n            correct += (predicted == labels).sum().item()\n            all_val_loss.append(loss_func(outputs, labels).item())\n            \n        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n        mean_val_acc = 100 * (correct / total)\n        \n        val_losses.append(mean_val_loss)\n        val_accuracies.append(mean_val_acc)\n        \n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {mean_val_loss:.4f}, Val Acc: {mean_val_acc:.2f}%')\n\nWe can visualize how training loss, validation loss and validation accuracy evolve over time.\n\nplt.figure(figsize=(8,4))\n\n# Plot Loss\nplt.subplot(1,2,1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\n\n# Plot Accuracy\nplt.subplot(1,2,2)\nplt.plot(val_accuracies, label='Validation Accuracy', color='green')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Validation Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nThe loss curve and validation accuracy curve show that the training loss and validation loss goes down while the validation accuracy of the model goes up as the training epochs increase. If we add more epochs (costly!), the validation accuracy of the model will be higher, but the model will also be more at risk of overfitting. To prevent which, we often need to regularize the model.\nAfter training for 25 epochs, we see our validation accuracy has passed 70%, we can save the model to file and load it again with the following codes:\n\n# save to file\ntorch.save(model, 'cnn.pt')\n\n\n# load from file and switch to inference mode\nmodel = torch.load('cnn.pt', weights_only=False)\nmodel.eval()\n\n\n\nStep 3: Inference the Classifier\nNow, we can use the trained classifier to predict the labels of the new input. But here, we are just using the test set for validation (which is not recommended).\n\ninput_tensors = []\n\nfor image in dataset_val['img'][:10]:\n    tensor = preprocess(image)\n    input_tensors.append(tensor.to(device))\n\n# stack into a single tensor\ninput_tensors = torch.stack(input_tensors)\ninput_tensors.shape\n\n\n# process through model to get output logits\noutputs = model(input_tensors)\n# calculate predictions\npredicted = torch.argmax(outputs, dim=1)\npredicted\n\n# here are the class names\ndataset_val.features['label'].names\n\n\n# Print out the output label and the true label\nfor i in range(10):\n    example = dataset_val[i]           # get the i-th example as a dict\n    image   = example['img']\n    true_id = example['label']\n    pred_id = predicted[i]\n    \n    true_label = dataset_val.features['label'].names[true_id]\n    pred_label = dataset_val.features['label'].names[pred_id]\n    \n    plt.figure(figsize=(4,4))\n    plt.imshow(image)\n    plt.title(f\"True: {true_label}   |   Pred: {pred_label}\")\n    plt.axis('off')\n    plt.show()\n\nWe can visualize the feature maps at different layers to see how the CNN see images.\n\n# Visualization of features at different layers\nimport torchvision.transforms as T\n\nto_tensor = T.ToTensor()\n\npil_img, _ = dataset_val[0]['img'], dataset_val[0]['label']\n\ninput_tensor = to_tensor(pil_img).unsqueeze(0).to(device)\n\nactivations = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activations[name] = output.detach().cpu()\n    return hook\n\nfor layer_name in ['conv_layer1','conv_layer2','conv_layer3','conv_layer4','conv_layer5']:\n    getattr(model, layer_name).register_forward_hook(get_activation(layer_name))\n\nmodel.eval()\nwith torch.no_grad():\n    _ = model(input_tensor)\n\n# Plot the feature map\nfor name, fmap in activations.items():\n    num_filters = fmap.shape[1]\n    cols = 6\n    rows = min((num_filters + cols - 1) // cols, 4)\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*1.2, rows*1.2))\n    fig.suptitle(f'Feature maps from {name}', fontsize=16)\n    for i in range(rows * cols):\n        r, c = divmod(i, cols)\n        ax = axes[r, c] if rows &gt; 1 else axes[c]\n        if i &lt; num_filters:\n            ax.imshow(fmap[0, i], cmap='viridis')\n            ax.set_title(f'#{i}')\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nWe can see that the model made mostly correct predictions, despite the image quality was low that even human may have difficulty to correctly classify. This somewhat shows the advantage of CNNs over humans when confronted with complex, blurry images, but CNNs have more applications than that. They power a host of real-world applications, from enabling your smartphone’s camera to automatically recognize faces and apply portrait effects, to guiding autonomous vehicles by detecting pedestrians, road signs, and lane markings in real time. In healthcare, CNNs help radiologists spot tumors in MRI and CT scans, and dermatologists classify skin lesions from photos. They underpin optical character recognition for digitizing handwritten forms, fuel quality-control systems that spot manufacturing defects on assembly lines, and even drive wildlife monitoring by identifying animals in camera-trap images.\nThis technology is also reshaping some humanities and social science research. For example, in archaeology, CNNs are being used to categorize, complete, and translate broken clay tablets and cuneiform texts; in art history, CNNs are being used to study the pigments and materials used in paintings, as well as the expressions and gestures of the figures in them; and in anthropology, CNNs are being used to distinguish between human races and complex kinships. It is for this reason that we are here to introduce it to you! I hope you enjoyed the class and got something different out of it!\n\n\n\nKey takeaways:\n\nArtificial Neural Networks (ANNs) are programs or models that make decisions in a similar manner to the thought process of a human brain.\nConvolutional Neural Networks (CNNs) differ from other neural networks in the convolutional layer that allows them to understand features from image input in a more efficient way.\nArchitectures are central in neural networks as they determine the ways how a model learn from the input features and thereby determine the model performance.\nMachine Learning and CNNs are fun and practical in the field of humanities and social sciences!\n\n\n\nGlossary\n\n\nAdditional Resources\n\nMLU-EXPLAIN: Neural Networks: A website with straightforward explanation and interactive visualizations of neural networks (with some math and technical terms), including more professional terminologies and advanced concepts that we won’t cover in this notebook. But if you find this notebook to be too light and really hope to learn more, this is a good place to go!\nCNN Explainer: An interesting interactive tutorial that explains how CNN work in a more visual way (but you may also find the explanation a little too technical). Try it out! You can also upload your own images of interest to see how the neural network processes them and classify them. Do you get the same results as you expected? What can you say about it?\n\n\n\nReferences\n\nPinecone. Embedding Methods for Image Search. https://www.pinecone.io/learn/series/image-search\nIBM. What is a neural network? https://www.ibm.com/think/topics/neural-networks\nIBM. What are convolutional neural networks? https://www.ibm.com/think/topics/convolutional-neural-networks\nConvolutional Neural Network From Scratch. https://medium.com/latinxinai/convolutional-neural-network-from-scratch-6b1c856e1c07"
  },
  {
    "objectID": "docs/SOCI-415/SOCI415-notebook-2.html",
    "href": "docs/SOCI-415/SOCI415-notebook-2.html",
    "title": "SOCI 415 Network Analysis - KINMATRIX Dataset",
    "section": "",
    "text": "SOCI 415 Network Anlysis Notebook (Basically the COMET one up to section 4.0)\n\n\nThe KINMATRIX dataset represents families as ego-centric networks of younger adults aged 25 to 35, collecting extensive data about both nuclear and extended kin across ten countries. The data include over 12,000 anchor respondents and more than 252,000 anchor-kin dyads, encompassing a wide range of relatives such as parents, siblings, grandparents, aunts, uncles, cousins, and complex kin (e.g., step- and half-relatives).\nAnchor respondents refer to the ones directly sampled from. These anchors filled out the survey about their families and kin meaning that they will always be at the center of these family networks.\nThe countries in the dataset are the Uk, Germany, Poland, Italy, Sweeden, Denmark, Finalnd, Norway, the Netherlands and the USA.\n\n\nWe will begin by looking at broader patterns across different countries. First we import all the necessary Python libraries for data manipulation, network analysis, and visualization. This will allow us to get results for each country.\nImport libraries and the data\n\n#Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.patches as mpatches \nimport networkx as nx \nimport pandas as pd \nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\nimport plotly.graph_objects as go\nfrom pyvis.network import Network\nimport re\nimport matplotlib.cm as cm\n\nfile_path = r'data\\ZA8825_v1-0-0.dta' #change here once we know how to present the notebook, or change here for a different path\ndf = pd.read_stata(file_path)\n\n#Print country codes\nprint(df['anc_cou'].unique())\n\n#Make a list of countries\ncountry_dfs = {}\nfor country in df['anc_cou'].unique():\n    country_dfs[country] = df[df['anc_cou'] == country]\n\n\n\nHere, we systematically construct a kinship network for each country in the dataset using the NetworkX python package. For each country, we filter the data, create a new graph, and add nodes representing anchor individuals and their kin. Then we draw edges between the nodes. Finally, we print out the number of nodes and edges for each country’s network, providing a quick overview of network size and complexity across the dataset.\n\n#Set up a dictionary of countries\ncountry_map = {\n    'UK': '1. UK',\n    'Germany': '2. Germany',\n    'Poland': '3. Poland',\n    'Italy': '4. Italy',\n    'Sweden': '5. Sweden',\n    'Denmark': '6. Denmark',\n    'Finland': '7. Finland',\n    'Norway': '8. Norway',\n    'Netherlands': '9. Netherlands',\n    'USA': '10. USA'\n}\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store the graph and anchor nodes\n    anchor_nodes = list(filtered_df['anc_id'].unique())\n    graphs[country] = {\n        'graph': G_filtered,\n        'anchor_nodes': anchor_nodes\n    }\n    \n    print(f\"{country}:\")\n    print(f\"  Number of nodes: {G_filtered.number_of_nodes()}\")\n    print(f\"  Number of edges: {G_filtered.number_of_edges()}\")\n    print()\n\n\n\n\nThis block computes and displays key network statistics for each country’s kinship network. We calculate the network density (how interconnected the network is), the mean degree (average number of connections per node), and the gender distribution among anchor nodes.\n\nfor country, data in graphs.items():\n    G = data['graph']\n    anchor_nodes = data['anchor_nodes']\n    \n    # Density\n    density = nx.density(G)\n    \n    # Mean degree\n    degrees = [deg for node, deg in G.degree()]\n    mean_degree = np.mean(degrees)\n    \n    # Gender distribution\n    female_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '2. Female')\n    male_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '1. Male')\n    other_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '3. Other gender or no gender')\n    \n    # Degree centrality (mean and top 5)\n    deg_centrality = nx.degree_centrality(G)\n    mean_deg_centrality = np.mean(list(deg_centrality.values()))\n    top5_deg_centrality = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n    \n    #Show our findings\n    print(f\"{country}:\")\n    print(f\"  Density: {density:.4f}\")\n    print(f\"  Mean degree: {mean_degree:.2f}\")\n    print(f\"  Female nodes: {female_count}\")\n    print(f\"  Male nodes: {male_count}\")\n    print(f\"  Other/No gender nodes: {other_count}\")\n    print(f\"  Mean degree centrality: {mean_deg_centrality:.4f}\")\n    print(f\"  Top 5 degree centrality nodes: {top5_deg_centrality}\")\n    print()\n\n\n\n\nHere we have enough information to do a visualization of our findings. We will use the Pyvis package. The code loads the NetworkX graph into Pyvis, cleans any missing attribute values, and provides interactive controls for exploring the network.\nNote (not for final version) Issues: I have also tried for Poland (35,000 nodes) - Very Laggy, but can be ran will need more guidance here as to how to go along with this in the class enviorment. - Tried using an external tool like Gephi, but did not really help. Just too laggy to display properly. - Seems that we need to use a small network of ~2000 nodes otherwise the display does not work properly\nMuch easier for a smaller one like Norway. My PC is also significantly better than most school laptops so we will need some kind of solution for displaying these visualizations.\nTest for Labels - Working\n\n# Retrieve the Norway Network\nG_Norway = graphs['Norway']['graph']\nanchor_nodes_norway = graphs['Norway']['anchor_nodes']\n\n# Clean None attributes\nfor n, attrs in G_Norway.nodes(data=True):\n    for k, v in attrs.items():\n        if v is None:\n            G_Norway.nodes[n][k] = \"NA\"\n\nfor u, v, attrs in G_Norway.edges(data=True):\n    for k, val in attrs.items():\n        if val is None:\n            G_Norway.edges[u, v][k] = \"NA\"\n\n# Create a new Pyvis Network\nnet = Network(height='800px', width='100%', notebook=True)\n\n# Create simple anchor mapping\nanchor_to_label = {}\nfor i, anchor in enumerate(anchor_nodes_norway, 1):\n    anchor_to_label[anchor] = f\"Anchor-{i}\"\n\n# Add nodes manually \nfor node in G_Norway.nodes():\n    if node in anchor_nodes_norway:\n        # Anchor nodes with labels\n        net.add_node(node, label=anchor_to_label[node], color='#4ECDC4', size=15)\n    else:\n        # Kin nodes\n        net.add_node(node, color='#4ECDC4', size=8)\n\n# Add edges manually\nfor u, v in G_Norway.edges():\n    net.add_edge(u, v)\n\nfor node in net.nodes:\n    if node['id'] not in anchor_nodes_norway:\n        node['label'] = '' \n\n# Minimal settings\nnet.show('norway_updated_network.html')\n\nprint(\"All blue nodes version saved as 'norway_all_blue_nodes.html'\")\nprint(f\"Total nodes: {G_Norway.number_of_nodes()}\")\nprint(f\"Total edges: {G_Norway.number_of_edges()}\")\nprint(f\"Anchor nodes labeled: {len(anchor_nodes_norway)}\")\n\n\n\n\n\nThis code block constructs kinship network graphs for each country in the KINMATRIX dataset and then analyzes how the density of family networks varies by age and gender within each country. This analysis helps reveal patterns and differences in family network structure across demographic groups and between countries, providing insight into how kinship connectivity varies by age and gender in different countries.\n\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store only the graph\n    graphs[country] = G_filtered\n\n\ndef analyze_family_density_by_demographics(G_filtered):\n    density_by_age = {}\n    density_by_gender = {}\n\n    for anchor_id in [n for n in G_filtered.nodes() if 'anc_' in str(n) or isinstance(n, (int, float))]:\n        ego_net = nx.ego_graph(G_filtered, anchor_id)\n        density = nx.density(ego_net)\n        age = G_filtered.nodes[anchor_id].get('age')\n        gender = G_filtered.nodes[anchor_id].get('gender')\n\n        if age:\n            density_by_age.setdefault(age // 10 * 10, []).append(density)\n        if gender:\n            density_by_gender.setdefault(gender, []).append(density)\n\n    print(\"Density by Age Group:\")\n    for age_group, densities in sorted(density_by_age.items()):\n        print(f\"Age {age_group}s: Mean Density = {np.mean(densities):.4f}, Count = {len(densities)}\")\n\n    print(\"\\nDensity by Gender:\")\n    for gender, densities in density_by_gender.items():\n        print(f\"Gender {gender}: Mean Density = {np.mean(densities):.4f}, Count = {len(densities)}\")\n\n# Loop through all countries\nfor country in country_map:\n    if country in graphs:\n        print(f\"\\n--- {country} ---\")\n        analyze_family_density_by_demographics(graphs[country])\n    else:\n        print(f\"No graph available for {country}\")\n\n\n\nInteractive part: Have a look at our findings from sections 1 and 2, based on frameworks taught in class explain a reason for them.\nSome example findings are listed below, but look for your own and try to link it to a concept from class.\n\nIn all countries, individuals in their 30s higher mean density than those in their 20s (Germany: 20s = 0.1355, 30s = 0.1468, USA: 20s = 0.1058, 30s = 0.1065)\nMen tend to have a higher mean density than women (Poland: Male = 0.1343, Female = 0.1130)\nThe USA has the lowest overall density but has the largest sample size? What are the reasons behind this, immigration and disconection from family, or maybe a smaller cultural focus on family?\n\nFrameworks (from syllabus 2019 can change here): - Social Exchange framework - Symbolic Interaction framework - Life Course Developmental framework - Family Stress Theory - Systems framework - Conflict and Critical Theories framework - Feminist framework - Ecological framework - Future of family theory"
  },
  {
    "objectID": "docs/SOCI-415/SOCI415-notebook-2.html#kinmatrix-data-description",
    "href": "docs/SOCI-415/SOCI415-notebook-2.html#kinmatrix-data-description",
    "title": "SOCI 415 Network Analysis - KINMATRIX Dataset",
    "section": "",
    "text": "The KINMATRIX dataset represents families as ego-centric networks of younger adults aged 25 to 35, collecting extensive data about both nuclear and extended kin across ten countries. The data include over 12,000 anchor respondents and more than 252,000 anchor-kin dyads, encompassing a wide range of relatives such as parents, siblings, grandparents, aunts, uncles, cousins, and complex kin (e.g., step- and half-relatives).\nAnchor respondents refer to the ones directly sampled from. These anchors filled out the survey about their families and kin meaning that they will always be at the center of these family networks.\nThe countries in the dataset are the Uk, Germany, Poland, Italy, Sweeden, Denmark, Finalnd, Norway, the Netherlands and the USA.\n\n\nWe will begin by looking at broader patterns across different countries. First we import all the necessary Python libraries for data manipulation, network analysis, and visualization. This will allow us to get results for each country.\nImport libraries and the data\n\n#Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.patches as mpatches \nimport networkx as nx \nimport pandas as pd \nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\nimport plotly.graph_objects as go\nfrom pyvis.network import Network\nimport re\nimport matplotlib.cm as cm\n\nfile_path = r'data\\ZA8825_v1-0-0.dta' #change here once we know how to present the notebook, or change here for a different path\ndf = pd.read_stata(file_path)\n\n#Print country codes\nprint(df['anc_cou'].unique())\n\n#Make a list of countries\ncountry_dfs = {}\nfor country in df['anc_cou'].unique():\n    country_dfs[country] = df[df['anc_cou'] == country]\n\n\n\nHere, we systematically construct a kinship network for each country in the dataset using the NetworkX python package. For each country, we filter the data, create a new graph, and add nodes representing anchor individuals and their kin. Then we draw edges between the nodes. Finally, we print out the number of nodes and edges for each country’s network, providing a quick overview of network size and complexity across the dataset.\n\n#Set up a dictionary of countries\ncountry_map = {\n    'UK': '1. UK',\n    'Germany': '2. Germany',\n    'Poland': '3. Poland',\n    'Italy': '4. Italy',\n    'Sweden': '5. Sweden',\n    'Denmark': '6. Denmark',\n    'Finland': '7. Finland',\n    'Norway': '8. Norway',\n    'Netherlands': '9. Netherlands',\n    'USA': '10. USA'\n}\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store the graph and anchor nodes\n    anchor_nodes = list(filtered_df['anc_id'].unique())\n    graphs[country] = {\n        'graph': G_filtered,\n        'anchor_nodes': anchor_nodes\n    }\n    \n    print(f\"{country}:\")\n    print(f\"  Number of nodes: {G_filtered.number_of_nodes()}\")\n    print(f\"  Number of edges: {G_filtered.number_of_edges()}\")\n    print()\n\n\n\n\nThis block computes and displays key network statistics for each country’s kinship network. We calculate the network density (how interconnected the network is), the mean degree (average number of connections per node), and the gender distribution among anchor nodes.\n\nfor country, data in graphs.items():\n    G = data['graph']\n    anchor_nodes = data['anchor_nodes']\n    \n    # Density\n    density = nx.density(G)\n    \n    # Mean degree\n    degrees = [deg for node, deg in G.degree()]\n    mean_degree = np.mean(degrees)\n    \n    # Gender distribution\n    female_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '2. Female')\n    male_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '1. Male')\n    other_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '3. Other gender or no gender')\n    \n    # Degree centrality (mean and top 5)\n    deg_centrality = nx.degree_centrality(G)\n    mean_deg_centrality = np.mean(list(deg_centrality.values()))\n    top5_deg_centrality = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n    \n    #Show our findings\n    print(f\"{country}:\")\n    print(f\"  Density: {density:.4f}\")\n    print(f\"  Mean degree: {mean_degree:.2f}\")\n    print(f\"  Female nodes: {female_count}\")\n    print(f\"  Male nodes: {male_count}\")\n    print(f\"  Other/No gender nodes: {other_count}\")\n    print(f\"  Mean degree centrality: {mean_deg_centrality:.4f}\")\n    print(f\"  Top 5 degree centrality nodes: {top5_deg_centrality}\")\n    print()\n\n\n\n\nHere we have enough information to do a visualization of our findings. We will use the Pyvis package. The code loads the NetworkX graph into Pyvis, cleans any missing attribute values, and provides interactive controls for exploring the network.\nNote (not for final version) Issues: I have also tried for Poland (35,000 nodes) - Very Laggy, but can be ran will need more guidance here as to how to go along with this in the class enviorment. - Tried using an external tool like Gephi, but did not really help. Just too laggy to display properly. - Seems that we need to use a small network of ~2000 nodes otherwise the display does not work properly\nMuch easier for a smaller one like Norway. My PC is also significantly better than most school laptops so we will need some kind of solution for displaying these visualizations.\nTest for Labels - Working\n\n# Retrieve the Norway Network\nG_Norway = graphs['Norway']['graph']\nanchor_nodes_norway = graphs['Norway']['anchor_nodes']\n\n# Clean None attributes\nfor n, attrs in G_Norway.nodes(data=True):\n    for k, v in attrs.items():\n        if v is None:\n            G_Norway.nodes[n][k] = \"NA\"\n\nfor u, v, attrs in G_Norway.edges(data=True):\n    for k, val in attrs.items():\n        if val is None:\n            G_Norway.edges[u, v][k] = \"NA\"\n\n# Create a new Pyvis Network\nnet = Network(height='800px', width='100%', notebook=True)\n\n# Create simple anchor mapping\nanchor_to_label = {}\nfor i, anchor in enumerate(anchor_nodes_norway, 1):\n    anchor_to_label[anchor] = f\"Anchor-{i}\"\n\n# Add nodes manually \nfor node in G_Norway.nodes():\n    if node in anchor_nodes_norway:\n        # Anchor nodes with labels\n        net.add_node(node, label=anchor_to_label[node], color='#4ECDC4', size=15)\n    else:\n        # Kin nodes\n        net.add_node(node, color='#4ECDC4', size=8)\n\n# Add edges manually\nfor u, v in G_Norway.edges():\n    net.add_edge(u, v)\n\nfor node in net.nodes:\n    if node['id'] not in anchor_nodes_norway:\n        node['label'] = '' \n\n# Minimal settings\nnet.show('norway_updated_network.html')\n\nprint(\"All blue nodes version saved as 'norway_all_blue_nodes.html'\")\nprint(f\"Total nodes: {G_Norway.number_of_nodes()}\")\nprint(f\"Total edges: {G_Norway.number_of_edges()}\")\nprint(f\"Anchor nodes labeled: {len(anchor_nodes_norway)}\")\n\n\n\n\n\nThis code block constructs kinship network graphs for each country in the KINMATRIX dataset and then analyzes how the density of family networks varies by age and gender within each country. This analysis helps reveal patterns and differences in family network structure across demographic groups and between countries, providing insight into how kinship connectivity varies by age and gender in different countries.\n\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store only the graph\n    graphs[country] = G_filtered\n\n\ndef analyze_family_density_by_demographics(G_filtered):\n    density_by_age = {}\n    density_by_gender = {}\n\n    for anchor_id in [n for n in G_filtered.nodes() if 'anc_' in str(n) or isinstance(n, (int, float))]:\n        ego_net = nx.ego_graph(G_filtered, anchor_id)\n        density = nx.density(ego_net)\n        age = G_filtered.nodes[anchor_id].get('age')\n        gender = G_filtered.nodes[anchor_id].get('gender')\n\n        if age:\n            density_by_age.setdefault(age // 10 * 10, []).append(density)\n        if gender:\n            density_by_gender.setdefault(gender, []).append(density)\n\n    print(\"Density by Age Group:\")\n    for age_group, densities in sorted(density_by_age.items()):\n        print(f\"Age {age_group}s: Mean Density = {np.mean(densities):.4f}, Count = {len(densities)}\")\n\n    print(\"\\nDensity by Gender:\")\n    for gender, densities in density_by_gender.items():\n        print(f\"Gender {gender}: Mean Density = {np.mean(densities):.4f}, Count = {len(densities)}\")\n\n# Loop through all countries\nfor country in country_map:\n    if country in graphs:\n        print(f\"\\n--- {country} ---\")\n        analyze_family_density_by_demographics(graphs[country])\n    else:\n        print(f\"No graph available for {country}\")\n\n\n\nInteractive part: Have a look at our findings from sections 1 and 2, based on frameworks taught in class explain a reason for them.\nSome example findings are listed below, but look for your own and try to link it to a concept from class.\n\nIn all countries, individuals in their 30s higher mean density than those in their 20s (Germany: 20s = 0.1355, 30s = 0.1468, USA: 20s = 0.1058, 30s = 0.1065)\nMen tend to have a higher mean density than women (Poland: Male = 0.1343, Female = 0.1130)\nThe USA has the lowest overall density but has the largest sample size? What are the reasons behind this, immigration and disconection from family, or maybe a smaller cultural focus on family?\n\nFrameworks (from syllabus 2019 can change here): - Social Exchange framework - Symbolic Interaction framework - Life Course Developmental framework - Family Stress Theory - Systems framework - Conflict and Critical Theories framework - Feminist framework - Ecological framework - Future of family theory"
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "",
    "text": "CHANGED HEADER FOR RENDER\nModule adapted from UBC COMET, prepared originally by Anneke Dresselhuis and Irene Berezin, by Irene Berezin, Anna Kovtunenko, Jalen Faddick and the prAxIs UBC team."
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#loading-and-installing-packages",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#loading-and-installing-packages",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Loading and Installing Packages",
    "text": "Loading and Installing Packages\nBefore continuing, run the code cell below to install the necessary packages.\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n\nimport os\nfrom pathlib import Path\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom datasets import Dataset\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nimport matplotlib.pyplot as plt\nimport seaborn as sns"
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#disinformation-in-the-information-age",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#disinformation-in-the-information-age",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Disinformation in the Information Age",
    "text": "Disinformation in the Information Age\nDisinformation is not new, but with the rise of digital platforms and generative AI, its scale, speed, and sophistication have grown exponentially. From elections and pandemics to social justice movements and international conflicts, false or misleading content is being spread online to manipulate emotions and polarize public opinion.\nThe challenge today is not just the volume of disinformation, but how convincing and targeted it has become. Former U.S. Director of National Intelligence Avril Haines describes how state-sponsored campaigns, like Russia’s Kremlin, now operate using “a vast multimedia influence apparatus,” including bots, cyber-actors, fake news websites, and social media trolls. Large language models (LLMs) can now generate human-like tweets, comments, and articles at scale. Combined with deepfakes, doppelgänger sites, and AI-generated personas, these tools allow bad actors to craft propaganda that appears authentic, emotionally resonant, and difficult to detect.\nIn this notebook, we’ll use machine learning — specifically, pretrained large language models — to study the language of disinformation in a real dataset of English and Russian-language tweets. These tweets include both propagandist and non-propagandist content."
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#learning-outcomes",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#learning-outcomes",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this module, you will be able to better understand disinformation and propaganda techniques using the following computational tools:\n\nSentiment analysis to detect emotional tone (positive, negative, neutral)\nToxicity analysis to identify harmful or aggressive language (e.g., insults, threats)\nStatistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian)\n\nYou’ll learn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than others?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\nThrough this analysis, we’ll explore various dimmensions of AI applications, critically examining how it can be used to better understand and detect the patterns of disinformation when working with large amounts of social data."
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#key-terms-and-concepts",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#key-terms-and-concepts",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Key Terms and Concepts",
    "text": "Key Terms and Concepts\nDisinformation: Disinformation is generally understood as the intentional spreading of false, misleading, or incorrect information, in order to influence the public or create an outcome, usually political.\nPropaganda: Propaganda is similar to disinformation in it’s intent to spread a cause or doctrine, but can differ in how systematically or overtly it is speread. The two concepts are both forms of misinformation, with propaganda generally being employed to promote a cause.\nLarge Language Model (LLM): A Large Language Model is a langauge model trained on a very large set of text data, accessing the features of the text by converting units of text into quanitative representations that can be used for various tasks, such as chatbotting, or in the case of this notebook, Natural Language Processing.\nNatural Language Processing (NLP): Natural Langauge Processing encompasses a wide variety of techniques and practices wtih the goal of enabling computers to understand, interpret, and produce human language. The key NLP techniques in this notebook are sentiment analysis, a technique that analayzes the relative positivity or negativity of langauge, and toxitcity analyis which analyzes the relative aggressiveness of language.\nBERT: to enable these analyses we will be using two BERT models. BERT is an open-source framework for machine learning, whcih is used for NLP. BERT is well-suited to understanding langauge, rather than generating it, which is what this notebook is concerned with. The specific BERT models we are using are a multi-lingual model, which can analyze tweets in different languages, and a model trained for analyzing tweet sentiments.\nFine Tuning: The multi-lingual model is fine-tuned, or trained specifically on the Russian Disinformation tweet dataset. This is done by inputting a training subset of the data, where the tweets are labeled as Disinformation, into the BERT model to create a model familiar with our data, and well-suited to producing analyses."
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#hashtag-analysis",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#hashtag-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "0.1 Hashtag analysis",
    "text": "0.1 Hashtag analysis\nThe next step in our data analysis is going to explore the most frequent hashtags from each account type. The code below finds the 10 most frequent hashtags from the control accounts and the disinformation accounts and displays them with their frequency. As you look at the tables below, pay attention to the content and the language of the hashtags.\nFrom our langauge analysis we can reasonable predict disinformation is likely to be in the English language, but the components of langauge that are frequently being used can be found by looking at the hashtags coming from these accounts.\n🧠 Key Questions\n\nWith reference to the timeline of tweets, and the hashtags below, describe some of the main targets of Russian disinformation.\nGiven what you know about disinformation, what are the intentions of these accounts, and what outcomes are they attempting to create?\nWhat do the hashtags not tell us about the disinformation accounts? Where might our ability to conclude the intentions and outcomes of these accounts be limited by the data we have examined?\nWhat additional data could we collect to better understand this type of disinformation.\n\n\ntweets_hs = tweets.explode('hashtags').rename(columns={'hashtags': 'hashtag'})\n\ntweets_hs = tweets_hs[tweets_hs['hashtag'].notna() & (tweets_hs['hashtag'] != '')]\ntweets_hs['hashtag'] = tweets_hs['hashtag'].str.lower()\n\ntop_io = (\n    tweets_hs[tweets_hs['account_type'] == 'IO']\n    .groupby('hashtag').size()\n    .nlargest(10)\n    .reset_index(name='count')\n)\n\ntop_control = (\n    tweets_hs[tweets_hs['account_type'] == 'Control']\n    .groupby('hashtag').size()\n    .nlargest(10)\n    .reset_index(name='count')\n)\n\nprint(\"Top 10 Hashtags - IO Accounts\")\ndisplay(top_io)\n\nprint(\"\\nTop 10 Hashtags - Control Accounts\")\ndisplay(top_control)\n\n\n🛑 Stop and Reflect\nNow that we have gone through the dataset and examined a variety of its features, take a few minutes and disucss the following questions with a partner or small group.\n\nWhat are the conclusions we have come to regarding disinformation? How are they influenced or limited by the dataset?\nSo far we have only looked at statistical aspects of the data without using machine learning techniques. Make some predictions on how the machine learning techniques we will use next might change our understanding of online disinformation. How might machine learning, specifically NLP be used to enrich our understanding of disinformation?\nHas your understanding of online disinformation changed after looking at this data? Write down a few questions you have about online disinformation, the dataset, or the computational methods we have been using."
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#can-we-tune-models-to-detect-online-disinformation-campaigns-classifying-current-tweets-with-a-model-finetuned-on-the-russian_disinformation_tweets-dataset",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#can-we-tune-models-to-detect-online-disinformation-campaigns-classifying-current-tweets-with-a-model-finetuned-on-the-russian_disinformation_tweets-dataset",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "1. Can we tune models to detect online disinformation campaigns? Classifying current tweets with a model finetuned on the russian_disinformation_tweets dataset",
    "text": "1. Can we tune models to detect online disinformation campaigns? Classifying current tweets with a model finetuned on the russian_disinformation_tweets dataset\n\nExploring Our Model\nNow that we have examined our data and looked at some of the key features in the Russian Disinformation Dataset, we can start thinking about ways to use machine learning to answer questions, classify features, and make predictions about our dataset. To do any of these tasks we first require a way to interpret the text data and assign numeric qualities to our tweets.\nThe model we are using to do this is a multilingual model which maps sentences and paragraphs into multi-dimensional vector space. In other words, it takes the sentences and paragraphs of our tweets and assigns them a position associated with their meaning. This is done based on the context of the token (the unit of text, like a word or sentence). The model we are using is capable of interpreting multiple languages and is fine-tuned, or specifically trained, on the data we are examining. The code below is going to call upon a pre-built classifier which uses this fine-tuned model to predict whether a tweet is likely Russian propaganda. The two sample tweets are:\n\n“#qanon #trump Hunter Biden is a Ukranian Shill”\n“What great weather we have today”\n\nThe model is going to take these text inputs, represent them in vector space, and then report whether their respective values are similar to those of disinformation tweets.\n\n\nClassification and It’s Discontents\nBefore we explore the possibilities of our model to classify, we should first consider some of the main concerns and limitations regarding classification. Classification is an essential element of how machine learning operates. At its core it is the method of finding features that are central to a class and assigning units to that class based on those features. As you may already see, this “in-or-out” framework necessarily flattens some of the richness of human life, in order to effectively create these incredibly useful classes.\n\nExample:\nYou might say a cat and a dog are really easy to classify. Most people know what a dog looks like and that it looks different than a cat. But if all I tell you is that there are two animals that commonly live with humans, that have a tail and paws, and make noise you might have a hard time classifying them, because they share common features.\nIt is important to think deeply about how we are classifying, especially as many datasets are labeled by people, who carry their own understandings of what belongs to each class.\nAny class or classifier will be informed by the balance of abstraction to specificity, and we should always keep this in mind when we are classifying. It is important to be specific enough to ascertain the qualities we are interested in, but not so specific we end up with thousands of classes.\n\nNow that we have explored some of the trickiness of classification as a concept, we can look at how machine learning can help us work through some of these challenges. By using data that is labeled as disinformation our model can be trained to associated certain numerical feautres with disinformation, and when we give it text data that is similar to what it knows to be disinformation, it will classify it as such.\n\nfrom inference_code.py import classify_texts\n\ntexts = [\"#qanon #trump Hunter Biden is a ukrainian shill\", \"What great weather we have today\"]\nfor text in texts:\n    preds, probs = classify_texts([text])\n\n    label_map = {0: \"Control\", 1: \"IO\"}  \n    pred_label = label_map[preds[0]]\n    pred_prob  = probs[0, preds[0]]\n\n    print(f\"Text: {text}\")\n    print(f\"Predicted class: {pred_label}  (confidence: {pred_prob:.2f})\")\n\nadd more stuff related to model evaluation, ipywidget for entering text students found on twitter and running it against the model"
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#what-is-sentiment-analysis",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#what-is-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "1. What is Sentiment Analysis?",
    "text": "1. What is Sentiment Analysis?\n\n“Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text” (Hussein, 2018).\n\nAs this definition alludes, sentiment analysis is a part of natural language processing (NLP), a field at the intersection of human language and computation. Because humans are complex, emotional beings, the language we use is often shaped by our affective (emotional) dispositions. Sentiment analysis, sometimes referred to as “opinion mining”, is one way researchers can methodologically understand the emotional intentions, typically positive, negative, or neutral sentiments, that lie in textual datasets.\n\n🔎 Engage Critically\nAt the heart of sentiment analysis is the assumption that language reveals interior, affective states, and that these states can be codified and generalized to broader populations. AI scholar Kate Crawford, in her book Atlas of AI, explores how many assumptions found in contemporary sentiment research (i.e., that there are 7 universal emotions) are largely unsubstantiated notions that emerged from mid-20th century research funded by the US Department of Defense. Rather than maintaining that emotions can be universally categorized, her work invites researchers to think about how emotional expression is highly contextualized by social and cultural factors and the distinct subject positions of content makers.\n\n❓ Consider the research question for your sentiment analysis. How might the text you are working with be shaped by the distinct groups that have generated it?\n\n\n❓ Are there steps you can take to educate yourself around the unique language uses of your dataset (for example, directly speaking with someone from that group or learning from a qualified expert on the subject)?\n\nIf you’re interested, you can learn more about data justice in community research in a guide created by UBC’s Office for Regional and International Community Engagement.\n\nThe rise of web 2.0 has produced prolific volumes of user-generated content (UGC) on the internet, particularly as people engage in a variety of social platforms and forums to share opinions, ideas and express themselves. Maybe you are interested in understanding how people feel about a particular political candidate by examining tweets around election time, or you wonder what people think about a particular bus route on reddit. UGC is often unstructured data, meaning that it isn’t organized in a recognizable way.\nStructured data for opinions about a political candidate might look like this:\n\n\n\n\n\n\n\n\nPro\nCon\nNeutral\n\n\n\n\nSupports climate action policies\nNo plan for lowering the cost of living\nUBC Graduate\n\n\nExpand mental health services\n\n\n\n\n\nWhile unstructured data might look like this:\n\nlove that she’s trying to increase mental health services + actually cares abt the climate 👏 but what’s up w rent n grocieries?? i dont wanna go broke out here 😭 a ubc alum too like i thought she’d understand\n\nIn the structured data example above, the reviewer defines which parts of the feedback are positive, negative or neutral. In the unstructured example on the other hand, there are many typos and a given sentence might include a positive and a negative review as well as more nuanced contextual information (i.e. mentioning being a UBC alum when discussing cost of living). While messy, this contextual information often carries valuable insights that can be very useful for researchers.\nThe task of sentiment analysis is to make sense of these kinds of nuanced textual data - often for the purpose of understanding people, predicting human behaviour, or even in some cases, manipulating human behaviour.\nDisinformation campaigns often aim to sway public opinion by influencing the emotional tone of online conversations. Sentiment analysis allows us to detect and understand these patterns by identifying whether large volumes of text express positive, negative, or neutral sentiment.\nOur model is pretrained, meaning it has already learnt from millions of labelled examples how to distinguish different sentiments. Specifically, because the model we’ll be using was trained on English tweets, it’s tuned to the language and syntax common on Twitter/X, and is limited to analyzing English-language text.\nLanguage is complex and always changing.\nIn the English language, for example, the word “present” has multiple meanings which could have positive, negative or neutral connotations. Further, a contemporary sentiment lexicon might code the word “miss” as being associated with negative or sad emotional experiences such as longing; if such a lexicon were applied to a 19th century novel which uses the word “miss” to describe single women, then, it might incorrectly associate negative sentiment where it shouldn’t be. While sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\nNow, we’re ready to get back to our analysis. Below, we’ll load in our model and tokenizer and start playing around with identifying the sentiment of different phrases.\n\nfrom transformers import pipeline\n\nlocal_dir = \"../twitter-roberta-base-sentiment-latest\"\n\nsentiment = pipeline(\n    \"sentiment-analysis\",\n    model=local_dir,\n    tokenizer=local_dir,\n)\n\nprint(sentiment(\"I HATE JOE BIDEN\"))\n\nLet’s breakdown this output. There are two parts to what the model returns:\n\nLabel → a classification labelling the text as either having positive, negative, or neutral sentiment\nScore → the model’s confidence in it’s classification\n\n\n🔎 Engage Critically\nTry using the interactive tool below to explore how a machine learning model detects sentiment in short texts like tweets. The model classifies each input as positive, neutral, or negative, and assigns a probability score to each label. Type a sentence (like a tweet or short message) into the box below and click “Analyze” to see how the model interprets its emotional tone.\n\n\nfrom transformers import pipeline\nsentiment = pipeline(\n    \"sentiment-analysis\",\n    model=\"../twitter-roberta-base-sentiment-latest\",\n    tokenizer=\"../twitter-roberta-base-sentiment-latest\",\n    return_all_scores=True  \n)\n\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nimport matplotlib.pyplot as plt\n\n#  widget components\ntext_input = widgets.Text(\n    value=\"I HATE JOE BIDEN\",\n    placeholder=\"Type a sentence here\",\n    description=\"Input:\",\n    layout=widgets.Layout(width=\"70%\")\n)\nanalyze_btn = widgets.Button(description=\"Analyze\", button_style=\"primary\")\noutput_area = widgets.Output()\n\n# what happens on button click\ndef on_analyze_clicked(b):\n    with output_area:\n        clear_output(wait=True)\n        scores = sentiment(text_input.value)[0]\n        labels = [item[\"label\"] for item in scores]\n        probs  = [item[\"score\"] for item in scores]\n        fig, ax = plt.subplots(figsize=(6,4))\n        bars = ax.bar(labels, probs)\n        ax.set_ylim(0, 1)\n        ax.set_ylabel(\"Probability\")\n        ax.set_title(\"Sentiment Probability Distribution\")\n        for bar, prob in zip(bars, probs):\n            height = bar.get_height()\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,  \n                height + 0.02,                    \n                f\"{prob:.2f}\",                   \n                ha=\"center\", va=\"bottom\",\n                color=\"red\", fontsize=12\n            )\n        plt.show()\n\nanalyze_btn.on_click(on_analyze_clicked)\n\ndisplay(widgets.VBox([text_input, analyze_btn, output_area]))\n\n\ntweets_small = tweets.sample(n=10000, random_state=42).reset_index(drop=True)\ntweets_small[\"sentiment\"] = \"\" \n\n\nBatch Sentiment Analysis\nNow, let’s start running sentiment analysis on our dataset. The general steps to run our analysis include:\n\nLoading a pretrained model and tokenizer\nWe load a RoBERTa model that has been fine-tuned for sentiment analysis on tweets, along with its corresponding tokenizer.\nCreating sentiment analysis pipeline\nWe set up a Hugging Face pipeline that handles finer steps in our sentiment analysis, such as tokenization (breaking up text into smaller units, called tokens), batching (processing multiple texts at once for efficiency), and prediction (predicting the overall sentiment).\nRunning batch sentiment analysis on the dataset\nTo efficiently analyze large numbers of tweets, we split the dataset into batches of 1,000 tweets and process them one batch at a time. To store the predictions, we extract the predicted sentiment labels and save them in a column named sentiment.\nPreviewing the results\n\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n\n# Step 1\nmodel_path = \"../twitter-roberta-base-sentiment-latest\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_path)\n\ntokenizer.model_max_length = tokenizer.model_max_length if tokenizer.model_max_length &lt;= 512 else 512\n\n# Step 2\nsentiment_pipeline = pipeline(\n    \"sentiment-analysis\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0,\n    truncation=True,\n    padding=True,\n)\n\n# Step 3\nbatch_size = 1000\nfor start in range(0, len(tweets_small), batch_size):\n    end = start + batch_size\n    batch_texts = tweets_small[\"text\"].iloc[start:end].tolist()\n    results = sentiment_pipeline(batch_texts)           \n    labels  = [res[\"label\"] for res in results]\n    tweets_small.loc[start:end-1, \"sentiment\"] = labels \n\n# Step 4\nprint(tweets_small[[\"text\", \"sentiment\"]].head())\n\nWe can see the first 5 tweets and their predicted sentiment above.\nNow that we know how to run sentiment analysis to identify the overarching sentiment of a tweet, we are now in good position to ask and investigate whether emotionally charged language is more common in propaganda. Let’s explore this by forming a hypothesis and testing it statistically.\n\n🔎 Engage Critically\nHypothesis: Propagandist tweets (is_control == 1) are more emotionally charged — that is, they are more likely to be classified as Positive or Negative (non-neutral) compared to non-propagandist tweets (is_control == 0).\nWe will test whether the difference in sentiment category frequencies between the two groups is statistically significant.\n\nFirst, let’s examine the sentiment distribution for each group:\n\ndist = pd.crosstab(tweets_small['sentiment'], tweets_small['labels'], normalize='columns')\ndist.columns = ['Non-propagandist', 'Propagandist']\nprint(dist * 100)\n\nReading the table, we can see that the majority of non-propagandist tweets are either negative (~43%) or neutral (~44%), while the majority of propagandist tweets (~63%) express neutral sentiment.\n\nprint(tweets_small['sentiment'])\n\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency\n\n# We define 'charged' sentiment as Positive or Negative\ntweets_small['charged'] = tweets_small['sentiment'].isin(['positive', 'pegative']).astype(int)\n\n# Constructing a  contingency table: rows = propagandist/non propagandist group, columnss = charged vs neutral\ncontingency = pd.crosstab(tweets_small['labels'], tweets_small['charged'])\nprint(\"Contingency table:\\n\", contingency)\n\n# Chi-squared test\nchi2, p, dof, expected = chi2_contingency(contingency)\nprint(f\"p-value = {p:.3e}\")\n\n\n🔎 Engage Critically\nTry interpreting the output. What are our results telling us? Based on the p-value, what can we conclude about our hypothesis?"
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#multilingual-sentiment-analysis",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#multilingual-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "3. Multilingual Sentiment Analysis",
    "text": "3. Multilingual Sentiment Analysis\nOur dataset of tweets (russian_disinformation_tweets.csv) isn’t entirely in English — many of the tweets are written in Russian. Could this be skewing our results? How is our model actually handling Russian-language tweets compared to English ones?\n\n🔎 Engage Critically\nRecall our discussion on sentiment lexicons in Section 1:\n\nWhile sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\n\n❓ How might the use of a monolingual sentiment model introduce bias into our results? Are non-English tweets being misclassified as neutral, negative, or positive when they shouldn’t be?\n\nWith this in mind, let’s explore below. We’ll use the Unicode values of Cyrillic characters to identify Russian-language tweets, and run sentiment analysis seperately on Russiand and English tweets.\n\nimport re\ntweets_lang = tweets_small.copy()\n\n# Identify Russian-language tweets\ntweets_lang['language'] = tweets_lang['text'].apply(\n     lambda txt: 'ru' if re.search('[\\u0400-\\u04FF]', str(txt)) else 'en'\n)\n\n# Filter English and Russian tweets\neng_tweets = tweets_lang[tweets_lang['language'] == 'en']\nru_tweets  = tweets_lang[tweets_lang['language'] == 'ru']\n\n# Sentiment distribution within each language group\neng_dist = pd.crosstab(eng_tweets['sentiment'], eng_tweets['labels'], normalize='columns') * 100\nru_dist  = pd.crosstab(ru_tweets['sentiment'],  ru_tweets['labels'],  normalize='columns') * 100\neng_dist.columns = ['Non-propagandist', 'Propagandist']\nru_dist.columns  = ['Non-propagandist', 'Propagandist']\n\nprint(\"English Tweets Sentiment (%):\\n\", eng_dist, \"\\n\")\nprint(\"Russian Tweets Sentiment (%):\\n\", ru_dist)\n\n\n🔎 Engage Critically\nTake a moment to interpret the results before continuing. What do they tell us about the performance of our model on Russian-language tweets? Why do you think that is?\n\nFrom the table above, we can see that our model is performing very poorly on Russian-language tweets, as nearly all of the Russian tweets are being marked as neutral regardless of if they are propagandist or not. This means that the pretrained model we were using before is not an appropriate choice based on the characteristics of our data, namely that a significant portion of the tweets are written in Russian, a language the model was not trained to make reliable predictions on.\nLet’s try re-running our sentiment analysis using a different model. This time, we’ll use a model trained on 198 million tweets that were not filtered by language. As a result, the training data reflects the most commonly used languages on the platform at the time of collection, with Russian conveniently ranking as the 11th most frequent.\nWe’ll follow the same steps for batch sentiment analysis that we did in Section 2:\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport tweetnlp\n\n# Step 1: Load the pretrained multilingual model and tokenizer\nmodel_path = \"../twitter-xlm-roberta-base-sentiment-multilingual\"\ntokenizer_multi = AutoTokenizer.from_pretrained(model_path)\nmodel_multi = AutoModelForSequenceClassification.from_pretrained(model_path)\n\ntokenizer_multi.model_max_length = min(tokenizer_multi.model_max_length, 512)\n\n# Step 2: Create the sentiment analysis pipeline\nsentiment_multi_pipeline = pipeline(\n    \"sentiment-analysis\",\n    model=model_multi,           \n    tokenizer=tokenizer_multi,   \n    device=0,\n    truncation=True,\n    padding=True,\n)\n\n# Step 3: Run batch sentiment analysis\nbatch_size = 1000\nfor start in range(0, len(tweets_small), batch_size):\n    end = start + batch_size\n    batch_texts = tweets_small[\"text\"].iloc[start:end].tolist()\n    results = sentiment_multi_pipeline(batch_texts)           \n    labels  = [res[\"label\"] for res in results]\n    tweets_small.loc[start:end-1, \"sentiment\"] = labels \n\n# Step 4: Preview results\nprint(tweets_small[[\"text\", \"sentiment\"]].head())\n\nTo make the results easier to visualize, let’s create a table that shows the percentage distribution of sentiment labels (positive, neutral, negative) within propagandist and non-propagandist tweets.\n\nmulti_dist = pd.crosstab(tweets_small['sentiment'], tweets['labels'], normalize='columns') * 100\nmulti_dist.columns = ['Non-propagandist', 'Propagandist']\nprint(\"Multilingual Model Sentiment (%):\\n\", multi_dist)\n\nThe sentiment distribution between propagandist and non-propagandist tweets is quite similar when using the multilingual model. Both groups are predominantly neutral (around 50%), with roughly equal proportions of negative and positive sentiment.\nNow, let’s run a statistical test to see if there’s a meaningful difference in sentiment between propagandist and non-propagandist tweets. Specifically, we want to know:\n\nAre propagandist tweets more likely to be emotionally charged (positive or negative) than neutral, compared to non-propagandist tweets?\n\nTo answer this, we’ll use a chi-squared test, which helps us check whether the differences we see in the data are likely due to chance or if they’re statistically significant.\n\ntweets_small['charged_multi'] = tweets_small['sentiment'].isin(['positive','negative']).astype(int)\ncontingency_multi = pd.crosstab(tweets_small['labels'], tweets_small['charged_multi'])\nchi2_multi, p_multi, *_ = chi2_contingency(contingency_multi)\nprint(f\"Chi-squared p-value with multilingual model: {p_multi:.3e}\")\n\n\n🔎 Engage Critically\nTake a moment to interpret the results before continuing. What does our p-value tell us about our inital research question above?\n\nOur p-value (0.00003727) is much smaller than the common significance level of 0.05, indicating that the difference in how emotionally charged tweets are distributed between propagandist and non-propagandist groups is very unlikely to be due to random chance.\nThis means there is strong evidence that propagandist tweets are more likely to be emotionally charged compared to non-propagandist tweets, according to the multilingual model’s sentiment analysis."
  },
  {
    "objectID": "docs/SOCI-280/SOCI280-notebook-1.html#introduction-to-toxicity-analysis",
    "href": "docs/SOCI-280/SOCI280-notebook-1.html#introduction-to-toxicity-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "4. Introduction to Toxicity Analysis",
    "text": "4. Introduction to Toxicity Analysis\nWarning: this section contains examples of potentially offensive or profane text\nToxicity analysis is another type of classification task that uses machine learning to detect whether a piece of text contains toxic speech. Jigsaw, a Google subsidary and leader in technological solutions for threats to civil society, uses the following definition for “toxic speech” proposed by Dixon et al. (2018):\n\n”[R]ude, disrespectful, or unreasonable language that is likely to make someone leave a discussion”\n\n\n🔎 Engage Critically\nThis definition is widely considered by the NLP community to be ill-defined and vague. Why do you think? What issues could potentially arise from this definition, and how could they impact (for example) a comment flagging tool that gives warnings to social media users whose comments meet this definition of toxic speech?\n\nA core issue defined by Berezin, Farahbakhsh, and Crespi (2023) is that the definition “gives no quantitative measure of the toxicity and operates with highly subjective cultural terms”, yet still remains widely used by researchers and developers in the field. We’ll explore some of the ways this definition is influencing toxicity analysis briefly below.\n\n4.1 Positive profanity\n\n\n\nFuck dude, nurses are the shit (Mauboussin, 2022)\n\n\nConsider the Reddit post above. Is the comment an example of toxic speech? Probably not, right?\nNow imagine you are Perspective API, Google’s AI toxicity moderation tool, with your scope of “toxic speech” limited solely to the definition of ”rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion”. Because of your architecture, you are limited in the way you can understand a message in context. You process the comment and immediately detect two profanities that meet your requirement for being rude language, and assign it a subsequent toxicity score:\n\n\n\nFuck dude, nurses are the shit with Toxicity Score (98.62%) (Mauboussin, 2022)\n\n\nThis is where, in the NLP community, there has been a growing discussion to ensure toxicity analysis tools, especially detectors used in online discussion and social media platforms, are more robust than simply being ‘profanity detectors’. They must be able to interpret a word in context.\n\n\n4.2 In-group language\nConsider in-group words used by distinct communities. Many of these words, once used as derogatory slurs against a group of people (such as Black or LGBTQ+ folk), have now largely been reclaimed and are prevalent in the lexicons of individuals identifying within these communities, no longer considered offensive when used by the in-group. However, if human annotators label textual data that ML models then are trained on, biases can permeate the models and lead to the classification of non-toxic, in-group language as harmful or offensive. Notably, African-American Vernacular English (AAVE) has been found to be flagged as toxic due to linguistic bias. XX frames how the challenge impacts toxicity detectors well:\n\n🔎 Engage Critically\nHow do you think this challenge can impact toxicity detectors? Resende et al. (2024) underscore this tension, noting that:\n\n…[S]uch a fine line between causal speaking and offensive discourse is problematic from a human and computational perspective. In that case, these interpretations are confounding to automatic content moderation tools. In other words, toxicity/sentiment analysis tools are usually developed using manual rules or supervised ML techniques that employ human-labeled data to extract patterns. The disparate treatment embodied by machine learning models usually replicates discrimination patterns historically practiced by humans when interacting with processes in the real world. Due to biases in this process, a lack of context leads both rule-based and machine learning-based models to a concerning scenario where minorities do not receive equal treatment.  Resende et al., 2024, p. 2\n\n\nResende et al. (2024) also conducted a comparison analysis of toxicity models, including Google’s Perspective API and Detoxify (the model we’ll be using for our own analysis soon).\n\n\n\nComparing the Toxicity Scoring Models (Resende et al., 2024)\n\n\nThis bias shown in this model’s performance can come from many factors in its structure, from data provenance and annotation to model architecture and processing, to a combination of many.\n\n🔎 Engage Critically\nIf you could, what questions would you want to ask the people who build these models?\n\n\n\nToxicity analysis using Detoxify\nThe model we’ll be using is called Detoxify (you can read more about it here). It was trained on large datasets of online comments across seven languages, including English and Russian. Detoxify provides an overall toxicity score for each text and can also detect five specific subtypes of toxicity: identity_attack, insult, obscene, sexual_explicit, and threat.\nIn the context of our dataset, propagandist tweets often aim to provoke strong emotions, spread hate, or stir conflict. Running toxicity analysis can help us investigate questions like:\n\nAre propagandist tweets more toxic than non-propagandist ones?\nWhat types of toxic language are most common?\nAre there patterns in how toxicity is used to influence or manipulate public discourse?\n\nToxicity analysis gives us another lens to understand how language and emotion are used in disinformation campaigns. Let’s begin by importing the necessary libraries and tools:\n\nimport pandas as pd\nimport numpy as np\nfrom detoxify import Detoxify\nfrom scipy.stats import ttest_ind, chi2_contingency\n\n\ndetox = Detoxify(\"original\")"
  },
  {
    "objectID": "docs/SOCI-217/SOCI217-notebook-2.html",
    "href": "docs/SOCI-217/SOCI217-notebook-2.html",
    "title": "SOCI217 Notebook 2",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec interdum porttitor lectus in scelerisque. Praesent euismod turpis in purus laoreet, at semper metus mattis. In hac habitasse platea dictumst. Aliquam euismod, metus quis tincidunt aliquet, magna felis posuere magna, et scelerisque massa sem non odio. Vestibulum justo eros, tempor quis nisl in, sagittis dictum est. Cras non ultrices est. Sed imperdiet venenatis felis pulvinar ultrices. Vestibulum lacinia id arcu in ultrices. Pellentesque lacinia odio in diam suscipit eleifend. Praesent non massa placerat sem luctus tincidunt. Nullam egestas sed dui quis elementum. Morbi eget lacus elit. Nulla sollicitudin, orci id viverra tincidunt, lacus dolor rutrum metus, nec bibendum felis nibh sit amet nisl.\nSuspendisse potenti. Morbi quis dignissim erat. Integer volutpat sem erat, sit amet luctus metus lacinia vel. Maecenas ut nunc sit amet ipsum commodo accumsan. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Mauris lorem odio, faucibus at mi eu, pretium hendrerit erat. Vestibulum pulvinar molestie lacus, vel facilisis dui porta eu. Vestibulum iaculis finibus hendrerit. Sed elementum enim justo, ac hendrerit elit tristique vitae. Phasellus velit ipsum, commodo semper mi vel, posuere fermentum felis. Proin vestibulum ligula lacus, eget facilisis eros ultrices eget.\nNulla eu posuere lacus, eget interdum magna. Donec finibus pretium eros, et tincidunt turpis ultricies eu. Duis facilisis pulvinar est id viverra. Nunc mollis velit non orci rutrum hendrerit. Maecenas pretium id velit at lobortis. Praesent mollis ipsum eros, nec hendrerit sem pretium nec. Cras suscipit, sapien nec dapibus molestie, libero urna hendrerit erat, eget gravida sapien odio at ante. Praesent mattis dui nunc, vel pharetra sem lacinia vitae. Nunc quis nibh orci. Quisque tincidunt finibus libero, a aliquam leo pellentesque eget. Suspendisse molestie libero a augue hendrerit consequat. Fusce quis mattis est, nec vulputate justo. Sed ut tempor risus. Aliquam commodo eget orci luctus lacinia. Nulla vulputate leo ut arcu tincidunt rhoncus.\nVivamus a ultricies sem, non finibus risus. Donec ac tempus ante. Donec lobortis purus vitae diam rhoncus, id tincidunt arcu scelerisque. Maecenas id est consectetur, consectetur ipsum vel, cursus nunc. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eu nisl hendrerit, sagittis orci in, lacinia turpis. Vestibulum eu tellus rhoncus, elementum mauris ornare, efficitur erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed gravida libero quis pulvinar fermentum. Vestibulum et nisi euismod nulla efficitur gravida in vitae nunc.\nFusce blandit suscipit laoreet. Ut convallis dapibus justo et iaculis. Etiam eu justo bibendum, viverra mi eget, tristique sapien. Etiam vel iaculis magna. Praesent quis mollis quam. In consequat, mi vitae scelerisque bibendum, diam ipsum tincidunt sapien, quis viverra nunc mauris ac turpis. Aliquam in diam consectetur, congue orci ac, blandit justo. Vestibulum et porttitor justo. Quisque ac molestie lacus. Proin sed commodo nunc, in ultrices tortor. Quisque et ornare enim. Donec sed mi bibendum, viverra massa in, finibus odio. Sed a massa eget nulla ornare finibus.\nAliquam nec orci nec tortor fermentum tincidunt. Aliquam blandit euismod mauris, et aliquet turpis molestie in. Phasellus sagittis, velit quis imperdiet porta, ipsum risus facilisis velit, ullamcorper tempus turpis lacus a diam. Aenean magna nunc, pulvinar et sollicitudin at, faucibus in elit. Cras congue, dolor et vulputate fermentum, lacus odio interdum ante, ut convallis risus lorem id augue. Nulla sodales orci vitae dolor sollicitudin, at gravida tellus aliquam. Phasellus eget semper urna, nec eleifend libero.\nCras luctus dolor eget quam laoreet aliquet. Praesent et lectus maximus, sagittis nibh at, eleifend lacus. Fusce nec metus sodales, pharetra nibh ac, maximus lorem. Maecenas imperdiet interdum ligula et imperdiet. Vestibulum ullamcorper quam id tempor consequat. Donec non nibh nec eros semper tristique nec nec mi. Fusce massa risus, venenatis at metus at, imperdiet elementum erat. Cras sit amet laoreet dui, non pulvinar ipsum. Sed condimentum sem ut ex rutrum, sed congue orci pellentesque. Donec venenatis ultrices tincidunt. Quisque scelerisque nulla non congue pretium. Maecenas rutrum sagittis ex at ullamcorper. Vestibulum non risus felis. Sed varius, mi placerat condimentum faucibus, sem ex bibendum neque, eu pulvinar tortor ipsum ac metus. Integer eu feugiat nibh, sit amet elementum erat. Aenean ac nunc felis.\nIn lacinia nec turpis nec vehicula. Donec sollicitudin suscipit risus, tincidunt lobortis odio cursus sit amet. Ut blandit convallis pharetra. Donec a lacus viverra, interdum dolor sed, vehicula neque. Cras in bibendum eros. Sed gravida commodo magna. Pellentesque nec metus sit amet diam gravida euismod. Cras iaculis odio eget consectetur lacinia.\nSuspendisse eu velit ultrices, imperdiet quam fermentum, auctor ante. Phasellus commodo, ipsum in facilisis interdum, odio tortor vehicula velit, quis varius erat ipsum in urna. Nam commodo sapien nulla, commodo vestibulum nisl feugiat non. Proin sit amet elit a elit ullamcorper aliquam at non urna. Vivamus non ullamcorper neque. Nam a erat fermentum, tempor urna finibus, molestie dolor. Sed mattis dolor sem, vitae volutpat lorem rutrum posuere. In vitae porttitor velit. Pellentesque pulvinar dolor at hendrerit suscipit.\nDonec at elit ex. Sed diam nibh, varius eget accumsan non, hendrerit vitae dolor. Integer at odio in ipsum luctus imperdiet. Vestibulum et massa vel magna commodo bibendum quis ut erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Integer condimentum nunc quis lorem sollicitudin, at tincidunt lorem pretium. In bibendum tincidunt augue, et auctor tortor tristique ut."
  },
  {
    "objectID": "docs/SOCI-217/SOCI217-notebook-2.html#notebook-2",
    "href": "docs/SOCI-217/SOCI217-notebook-2.html#notebook-2",
    "title": "SOCI217 Notebook 2",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec interdum porttitor lectus in scelerisque. Praesent euismod turpis in purus laoreet, at semper metus mattis. In hac habitasse platea dictumst. Aliquam euismod, metus quis tincidunt aliquet, magna felis posuere magna, et scelerisque massa sem non odio. Vestibulum justo eros, tempor quis nisl in, sagittis dictum est. Cras non ultrices est. Sed imperdiet venenatis felis pulvinar ultrices. Vestibulum lacinia id arcu in ultrices. Pellentesque lacinia odio in diam suscipit eleifend. Praesent non massa placerat sem luctus tincidunt. Nullam egestas sed dui quis elementum. Morbi eget lacus elit. Nulla sollicitudin, orci id viverra tincidunt, lacus dolor rutrum metus, nec bibendum felis nibh sit amet nisl.\nSuspendisse potenti. Morbi quis dignissim erat. Integer volutpat sem erat, sit amet luctus metus lacinia vel. Maecenas ut nunc sit amet ipsum commodo accumsan. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Mauris lorem odio, faucibus at mi eu, pretium hendrerit erat. Vestibulum pulvinar molestie lacus, vel facilisis dui porta eu. Vestibulum iaculis finibus hendrerit. Sed elementum enim justo, ac hendrerit elit tristique vitae. Phasellus velit ipsum, commodo semper mi vel, posuere fermentum felis. Proin vestibulum ligula lacus, eget facilisis eros ultrices eget.\nNulla eu posuere lacus, eget interdum magna. Donec finibus pretium eros, et tincidunt turpis ultricies eu. Duis facilisis pulvinar est id viverra. Nunc mollis velit non orci rutrum hendrerit. Maecenas pretium id velit at lobortis. Praesent mollis ipsum eros, nec hendrerit sem pretium nec. Cras suscipit, sapien nec dapibus molestie, libero urna hendrerit erat, eget gravida sapien odio at ante. Praesent mattis dui nunc, vel pharetra sem lacinia vitae. Nunc quis nibh orci. Quisque tincidunt finibus libero, a aliquam leo pellentesque eget. Suspendisse molestie libero a augue hendrerit consequat. Fusce quis mattis est, nec vulputate justo. Sed ut tempor risus. Aliquam commodo eget orci luctus lacinia. Nulla vulputate leo ut arcu tincidunt rhoncus.\nVivamus a ultricies sem, non finibus risus. Donec ac tempus ante. Donec lobortis purus vitae diam rhoncus, id tincidunt arcu scelerisque. Maecenas id est consectetur, consectetur ipsum vel, cursus nunc. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eu nisl hendrerit, sagittis orci in, lacinia turpis. Vestibulum eu tellus rhoncus, elementum mauris ornare, efficitur erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed gravida libero quis pulvinar fermentum. Vestibulum et nisi euismod nulla efficitur gravida in vitae nunc.\nFusce blandit suscipit laoreet. Ut convallis dapibus justo et iaculis. Etiam eu justo bibendum, viverra mi eget, tristique sapien. Etiam vel iaculis magna. Praesent quis mollis quam. In consequat, mi vitae scelerisque bibendum, diam ipsum tincidunt sapien, quis viverra nunc mauris ac turpis. Aliquam in diam consectetur, congue orci ac, blandit justo. Vestibulum et porttitor justo. Quisque ac molestie lacus. Proin sed commodo nunc, in ultrices tortor. Quisque et ornare enim. Donec sed mi bibendum, viverra massa in, finibus odio. Sed a massa eget nulla ornare finibus.\nAliquam nec orci nec tortor fermentum tincidunt. Aliquam blandit euismod mauris, et aliquet turpis molestie in. Phasellus sagittis, velit quis imperdiet porta, ipsum risus facilisis velit, ullamcorper tempus turpis lacus a diam. Aenean magna nunc, pulvinar et sollicitudin at, faucibus in elit. Cras congue, dolor et vulputate fermentum, lacus odio interdum ante, ut convallis risus lorem id augue. Nulla sodales orci vitae dolor sollicitudin, at gravida tellus aliquam. Phasellus eget semper urna, nec eleifend libero.\nCras luctus dolor eget quam laoreet aliquet. Praesent et lectus maximus, sagittis nibh at, eleifend lacus. Fusce nec metus sodales, pharetra nibh ac, maximus lorem. Maecenas imperdiet interdum ligula et imperdiet. Vestibulum ullamcorper quam id tempor consequat. Donec non nibh nec eros semper tristique nec nec mi. Fusce massa risus, venenatis at metus at, imperdiet elementum erat. Cras sit amet laoreet dui, non pulvinar ipsum. Sed condimentum sem ut ex rutrum, sed congue orci pellentesque. Donec venenatis ultrices tincidunt. Quisque scelerisque nulla non congue pretium. Maecenas rutrum sagittis ex at ullamcorper. Vestibulum non risus felis. Sed varius, mi placerat condimentum faucibus, sem ex bibendum neque, eu pulvinar tortor ipsum ac metus. Integer eu feugiat nibh, sit amet elementum erat. Aenean ac nunc felis.\nIn lacinia nec turpis nec vehicula. Donec sollicitudin suscipit risus, tincidunt lobortis odio cursus sit amet. Ut blandit convallis pharetra. Donec a lacus viverra, interdum dolor sed, vehicula neque. Cras in bibendum eros. Sed gravida commodo magna. Pellentesque nec metus sit amet diam gravida euismod. Cras iaculis odio eget consectetur lacinia.\nSuspendisse eu velit ultrices, imperdiet quam fermentum, auctor ante. Phasellus commodo, ipsum in facilisis interdum, odio tortor vehicula velit, quis varius erat ipsum in urna. Nam commodo sapien nulla, commodo vestibulum nisl feugiat non. Proin sit amet elit a elit ullamcorper aliquam at non urna. Vivamus non ullamcorper neque. Nam a erat fermentum, tempor urna finibus, molestie dolor. Sed mattis dolor sem, vitae volutpat lorem rutrum posuere. In vitae porttitor velit. Pellentesque pulvinar dolor at hendrerit suscipit.\nDonec at elit ex. Sed diam nibh, varius eget accumsan non, hendrerit vitae dolor. Integer at odio in ipsum luctus imperdiet. Vestibulum et massa vel magna commodo bibendum quis ut erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Integer condimentum nunc quis lorem sollicitudin, at tincidunt lorem pretium. In bibendum tincidunt augue, et auctor tortor tristique ut."
  },
  {
    "objectID": "docs/SOCI-217/SOCI217-notebook-2.html#testing-the-render",
    "href": "docs/SOCI-217/SOCI217-notebook-2.html#testing-the-render",
    "title": "SOCI217 Notebook 2",
    "section": "TESTING THE RENDER",
    "text": "TESTING THE RENDER"
  },
  {
    "objectID": "docs/HIST-414/pyLDAvis/pyLDAvis.html",
    "href": "docs/HIST-414/pyLDAvis/pyLDAvis.html",
    "title": "Using pyLDAvis for Analysis for HIST-414 Alex R",
    "section": "",
    "text": "Legal judgments are complex documents that draw upon many facets of legal reasoning, including the interpretation of applicable law, evaluation of evidence presented in court, and consideration of prior precedents. This makes them difficult to systemically analyze as they often defy single membership classification. Rice (2019) introduces Latent Dirichlet Allocation as a method of unsupervised structured topic modelling (STM), allowing for underlying themes in legal texts to be uncovered via a computational approach and capture the proportionate attention given to multiple legal dimensions within each judgement. A continuation of notebook 2 the goal of this notebook is to create interactive visualizations in order to explore and understand these relationships.\n#Import packages \nimport pandas as pd\nimport json\nimport pathlib\nimport re, string\nimport multiprocessing\nimport requests\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom gensim import corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.corpora import Dictionary\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom gensim.models import CoherenceModel\nfrom gensim.models import LdaMulticore\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport multiprocessing\nExplanation of LDA from notebook 2 or if this is attached to notebook 2 then this is unnecessary"
  },
  {
    "objectID": "docs/HIST-414/pyLDAvis/pyLDAvis.html#references",
    "href": "docs/HIST-414/pyLDAvis/pyLDAvis.html#references",
    "title": "Using pyLDAvis for Analysis for HIST-414 Alex R",
    "section": "References",
    "text": "References\nbmabey , and marksusol . “Pyldavis.” PyPI, 23 Apr. 2023, pypi.org/project/pyLDAvis/.\nMattingly, W.J.B. “Introduction to Topic Modeling and Text Classification.” 1. Introduction to Topic Modeling - Introduction to Topic Modeling and Text Classification, Feb. 2021, topic-modeling.pythonhumanities.com/01_01_introduction_to_topic_modeling.html.\n“Supreme Court of Canada Bulk Decisions Dataset.” Refugee Law Lab - Refugee Law Lab, 27 June 2024, refugeelab.ca/bulk-data/scc/.\nTran, Khuyen. “Pyldavis: Topic Modeling Exploration Tool.” Neptune.Ai, 20 May 2025, neptune.ai/blog/pyldavis-topic-modeling-exploration-tool.\n“What Is Pyldavis Library in Python?” HowDev, how.dev/answers/what-is-pyldavis-library-in-python. Accessed 9 June 2025."
  },
  {
    "objectID": "docs/ECON-227/ECON227-notebook-1.html",
    "href": "docs/ECON-227/ECON227-notebook-1.html",
    "title": "ECON 227 Notebook 1",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec interdum porttitor lectus in scelerisque. Praesent euismod turpis in purus laoreet, at semper metus mattis. In hac habitasse platea dictumst. Aliquam euismod, metus quis tincidunt aliquet, magna felis posuere magna, et scelerisque massa sem non odio. Vestibulum justo eros, tempor quis nisl in, sagittis dictum est. Cras non ultrices est. Sed imperdiet venenatis felis pulvinar ultrices. Vestibulum lacinia id arcu in ultrices. Pellentesque lacinia odio in diam suscipit eleifend. Praesent non massa placerat sem luctus tincidunt. Nullam egestas sed dui quis elementum. Morbi eget lacus elit. Nulla sollicitudin, orci id viverra tincidunt, lacus dolor rutrum metus, nec bibendum felis nibh sit amet nisl.\nSuspendisse potenti. Morbi quis dignissim erat. Integer volutpat sem erat, sit amet luctus metus lacinia vel. Maecenas ut nunc sit amet ipsum commodo accumsan. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Mauris lorem odio, faucibus at mi eu, pretium hendrerit erat. Vestibulum pulvinar molestie lacus, vel facilisis dui porta eu. Vestibulum iaculis finibus hendrerit. Sed elementum enim justo, ac hendrerit elit tristique vitae. Phasellus velit ipsum, commodo semper mi vel, posuere fermentum felis. Proin vestibulum ligula lacus, eget facilisis eros ultrices eget.\nNulla eu posuere lacus, eget interdum magna. Donec finibus pretium eros, et tincidunt turpis ultricies eu. Duis facilisis pulvinar est id viverra. Nunc mollis velit non orci rutrum hendrerit. Maecenas pretium id velit at lobortis. Praesent mollis ipsum eros, nec hendrerit sem pretium nec. Cras suscipit, sapien nec dapibus molestie, libero urna hendrerit erat, eget gravida sapien odio at ante. Praesent mattis dui nunc, vel pharetra sem lacinia vitae. Nunc quis nibh orci. Quisque tincidunt finibus libero, a aliquam leo pellentesque eget. Suspendisse molestie libero a augue hendrerit consequat. Fusce quis mattis est, nec vulputate justo. Sed ut tempor risus. Aliquam commodo eget orci luctus lacinia. Nulla vulputate leo ut arcu tincidunt rhoncus.\nVivamus a ultricies sem, non finibus risus. Donec ac tempus ante. Donec lobortis purus vitae diam rhoncus, id tincidunt arcu scelerisque. Maecenas id est consectetur, consectetur ipsum vel, cursus nunc. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eu nisl hendrerit, sagittis orci in, lacinia turpis. Vestibulum eu tellus rhoncus, elementum mauris ornare, efficitur erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed gravida libero quis pulvinar fermentum. Vestibulum et nisi euismod nulla efficitur gravida in vitae nunc.\nFusce blandit suscipit laoreet. Ut convallis dapibus justo et iaculis. Etiam eu justo bibendum, viverra mi eget, tristique sapien. Etiam vel iaculis magna. Praesent quis mollis quam. In consequat, mi vitae scelerisque bibendum, diam ipsum tincidunt sapien, quis viverra nunc mauris ac turpis. Aliquam in diam consectetur, congue orci ac, blandit justo. Vestibulum et porttitor justo. Quisque ac molestie lacus. Proin sed commodo nunc, in ultrices tortor. Quisque et ornare enim. Donec sed mi bibendum, viverra massa in, finibus odio. Sed a massa eget nulla ornare finibus.\nAliquam nec orci nec tortor fermentum tincidunt. Aliquam blandit euismod mauris, et aliquet turpis molestie in. Phasellus sagittis, velit quis imperdiet porta, ipsum risus facilisis velit, ullamcorper tempus turpis lacus a diam. Aenean magna nunc, pulvinar et sollicitudin at, faucibus in elit. Cras congue, dolor et vulputate fermentum, lacus odio interdum ante, ut convallis risus lorem id augue. Nulla sodales orci vitae dolor sollicitudin, at gravida tellus aliquam. Phasellus eget semper urna, nec eleifend libero.\nCras luctus dolor eget quam laoreet aliquet. Praesent et lectus maximus, sagittis nibh at, eleifend lacus. Fusce nec metus sodales, pharetra nibh ac, maximus lorem. Maecenas imperdiet interdum ligula et imperdiet. Vestibulum ullamcorper quam id tempor consequat. Donec non nibh nec eros semper tristique nec nec mi. Fusce massa risus, venenatis at metus at, imperdiet elementum erat. Cras sit amet laoreet dui, non pulvinar ipsum. Sed condimentum sem ut ex rutrum, sed congue orci pellentesque. Donec venenatis ultrices tincidunt. Quisque scelerisque nulla non congue pretium. Maecenas rutrum sagittis ex at ullamcorper. Vestibulum non risus felis. Sed varius, mi placerat condimentum faucibus, sem ex bibendum neque, eu pulvinar tortor ipsum ac metus. Integer eu feugiat nibh, sit amet elementum erat. Aenean ac nunc felis.\nIn lacinia nec turpis nec vehicula. Donec sollicitudin suscipit risus, tincidunt lobortis odio cursus sit amet. Ut blandit convallis pharetra. Donec a lacus viverra, interdum dolor sed, vehicula neque. Cras in bibendum eros. Sed gravida commodo magna. Pellentesque nec metus sit amet diam gravida euismod. Cras iaculis odio eget consectetur lacinia.\nSuspendisse eu velit ultrices, imperdiet quam fermentum, auctor ante. Phasellus commodo, ipsum in facilisis interdum, odio tortor vehicula velit, quis varius erat ipsum in urna. Nam commodo sapien nulla, commodo vestibulum nisl feugiat non. Proin sit amet elit a elit ullamcorper aliquam at non urna. Vivamus non ullamcorper neque. Nam a erat fermentum, tempor urna finibus, molestie dolor. Sed mattis dolor sem, vitae volutpat lorem rutrum posuere. In vitae porttitor velit. Pellentesque pulvinar dolor at hendrerit suscipit.\nDonec at elit ex. Sed diam nibh, varius eget accumsan non, hendrerit vitae dolor. Integer at odio in ipsum luctus imperdiet. Vestibulum et massa vel magna commodo bibendum quis ut erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Integer condimentum nunc quis lorem sollicitudin, at tincidunt lorem pretium. In bibendum tincidunt augue, et auctor tortor tristique ut."
  },
  {
    "objectID": "docs/ECON-227/ECON227-notebook-1.html#testing-the-render",
    "href": "docs/ECON-227/ECON227-notebook-1.html#testing-the-render",
    "title": "ECON 227 Notebook 1",
    "section": "TESTING THE RENDER",
    "text": "TESTING THE RENDER"
  },
  {
    "objectID": "docs/AMNE-376/development/cosine_demo.html",
    "href": "docs/AMNE-376/development/cosine_demo.html",
    "title": "Cosine Similarity Between Two Embeddings",
    "section": "",
    "text": "Vec1 X: \n  Vec1 Y: \n  Vec2 X: \n  Vec2 Y:"
  },
  {
    "objectID": "docs/AMNE-376/notebook/amne376_image_embedding.html",
    "href": "docs/AMNE-376/notebook/amne376_image_embedding.html",
    "title": "AMNE 376: A Study of Richter’s Kouroi Through Image Embedding",
    "section": "",
    "text": "Before you start, make sure you have the required libraries installed, if not, simply uncomment the lines below (use Ctrl + /) and run the cell to install them:\n\n\nCode\n# !pip install matplotlib\n# !pip install numpy\n# !pip install pandas\n# !pip install opencv-python\n# !pip install sklearn\n# !pip install torch\n# !pip install torchvision\n# !pip install transformers\n# !pip install datasets\n# !pip install grad-cam\n# !pip install tensorflow\n# !pip install keras\n\n\n\n1. Introduction: How Computers See Visual Art?\nIn this notebook, we explore a dataset of photographs collected from Gisela Richter’s Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942).\nGisela Richter’s 1942 book was one of the first systematic efforts to catalog and classify kouroi based on their stylistic evolution. Her work combined archaeological evidence with visual comparison, laying the foundation for how we study ancient sculpture today.\nIn this project, we aim to apply computer vision techniques to digitally analyze and group images from this dataset. Just as Richter used her trained eye to identify patterns and typologies, we’ll explore how machines can “see” these sculptures through their eyes (image embeddings, clustering, and convolutional neural networks).\nHave you ever wondered how images are stored in computers, how computers see them and distinguish the difference between them?\nMany of you probably know digital images are stored based on pixels as a grid of figures, but when we are doing image searches using a search engine or uploading them to a Generative AI model, how are computers interpret them, distinguish the difference and process them exactly? Here is a brief introduction that introduce you to some of the basic forms and methods.\n\n1.1 Digital Representations of Images\nHave you ever heard of the RGB primary color model? For those who are unfamiliar with the concept, the model uses numbers in a range 0 ~ 255 to represent the color intensity of red, green and blue and add up the three color channels to generate any color that’s visible to human. In a colorful digital image, each pixel is characterized by its color stored in the form (R, G, B), so knowing the distribution of color intensity gives you a lot of information about the image.\nHowever, for monochrome images, there is only one color channel, the grayscale. We can still use the distribution of grayscale intensities to represent the image. Since all of our images (cropped from scanned pdf books) are printed in monochrome, we can represent them using a grayscale color histogram.\nLet’s start with a three-view of the New York Kouros, here we read in the images and present them together.\n\n\nCode\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n# Define the folder path where the images are stored\nimage_path = 'data/richter_kouroi_filtered_photos' \n\nfig, axes = plt.subplots(1, 3, figsize=(8, 5))\n\n# List of specific image filenames\nimage_names = {'page188_img01_photo12.jpg': \"Left\", 'page188_img01_photo13.jpg': \"Front\", 'page189_img01_photo3.jpg': \"Back\"}\n\n# Display the images side by side\naxes = axes.flatten()\nfor i, img_name in enumerate(image_names):\n    img_path = f\"{image_path}/{img_name}\"\n    image = Image.open(img_path)\n    axes[i].imshow(image)\n    axes[i].set_title(image_names[img_name])\n    axes[i].axis('off')\n\nplt.suptitle(\"Selected Images from Richter's Kouroi Dataset\")\nplt.tight_layout()\nplt.show()\n\n\nEach of these images, when loaded into the computer, becomes a 2D array of numbers representing intensity values. We then plot the color histogram for each image representing the distribution of grayscale intensity.\n\nDiscussion: What do you notice by looking at the three histograms?\n\n\n\nCode\n# Generate and plot greyscale histograms for the selected images\nfig, axes = plt.subplots(1, 3, figsize=(7, 4))\n\nfor i, img_name in enumerate(image_names):\n    img_path = f\"{image_path}/{img_name}\"\n    image = Image.open(img_path)\n    histogram = image.histogram()\n\n    axes[i].plot(histogram, color='black')\n    axes[i].set_title(f'{image_names[img_name]}')\n    axes[i].set_xlim([0, 255])\n    axes[i].set_xlabel(\"Intensity\")\n    if i == 0:\n        axes[i].set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\nThey look very similar! This result is not surprising given that the three images were taken at the same time with the same equipment of the same Kouros. The above example shows us that comparing the similarity of color distributions is one way that computers understand the similarity of images.\nHowever, one can quickly realize the drawbacks of this approach. First, it relies on the correct representation of color, so two identical images with color differences may not be recognized as similar. Second, since it focuses only on color, it ignores the fundamental information for object recognition such as spatial, shape and texture in the image. Last but not least, there may exist two completely different images with exactly the same color distribution. Therefore, we need better methods to consider the similarity between images.\nBag of Visual Words (BoVW) is a more practical method for recognizing similarity. The rationale behind this is very complicated, but to put it simply, it treats a “feature” in an image as a “word” (a set of numbers containing information about the feature) and calculates how often each word appears in the image. Here, we created a visual vocabulary containing 20 “words” using three-view photos of the New York Kouros, and visualized what a visual word represents on the left-view image. Here, we pick the visual word with ID 6:\n\n\nCode\nimport cv2\nimport numpy as np\nfrom sklearn.cluster import KMeans\nfrom collections import defaultdict\n\n# Define the number of clusters for KMeans\nn_clusters   = 20\nword_to_show = 6\nmax_patches  = 30\n\n# Initialize ORB detector\norb = cv2.ORB_create(nfeatures=500)\nall_descriptors = []      # for stacking\nimage_data      = []      # (img_name, kps, descs)\n\n# Detect and describe all images\nfor img_name in image_names:\n    img_path = os.path.join(image_path, img_name)\n    img      = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    keypoints, descriptors = orb.detectAndCompute(img, None)\n\n    if descriptors is None:\n        descriptors = np.zeros((0, orb.descriptorSize()), dtype=np.uint8)\n\n    all_descriptors.append(descriptors)\n    image_data.append((img_name, keypoints, descriptors))\n\n# Build the visual vocabulary\nall_descriptors_stacked = np.vstack(all_descriptors)\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nkmeans.fit(all_descriptors_stacked)\n\n# Compute BoVW histograms\nhistograms = []\nfor img_name, _, descriptors in image_data:\n    if descriptors.shape[0] &gt; 0:\n        words = kmeans.predict(descriptors)\n        hist, _ = np.histogram(words, bins=np.arange(n_clusters + 1))\n    else:\n        hist = np.zeros(n_clusters, dtype=int)\n    histograms.append((img_name, hist))\n\n# Find the locations matching visual word ID = 6\nlocations = []\nfor img_idx, (_, keypoints, descriptors) in enumerate(image_data):\n    if descriptors.shape[0] == 0:\n        continue\n    assignments = kmeans.predict(descriptors)\n    for kp, w in zip(keypoints, assignments):\n        if w == word_to_show:\n            x, y = map(int, kp.pt)\n            locations.append((img_idx, x, y))\n            if len(locations) &gt;= max_patches:\n                break\n    if len(locations) &gt;= max_patches:\n        break\n\n# Group by image and visualize\nimgs = defaultdict(list)\nfor idx, x, y in locations:\n    imgs[idx].append((x, y))\n\nif imgs:\n    img_idx, pts = next(iter(imgs.items()))\n    fname = image_data[img_idx][0]\n    img   = cv2.imread(os.path.join(image_path, fname), cv2.IMREAD_GRAYSCALE)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    for x, y in pts:\n        cv2.circle(img_rgb, (x, y), radius=25, color=(0,255,0), thickness=2)\n\n    plt.figure(figsize=(6,6))\n    plt.imshow(img_rgb)\n    plt.title(f\"Word {word_to_show} Keypoints on the Left-View Photo\")\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\nWe note that the word 6 could represent the beads in the beadwork worn by the Kouros.\n\nDiscussion: Based on your knowledge of the various Kouros, do you think this visual word can be the key to differentiating between different Kouros, or even different sculptural subjects?\n\n\n\n1.2 Measurement of Similarity\nAs you may have realized, the visual word frequency distributions of different images are not exactly the same, so how can we determine if these images are similar? More importantly, what can we use as a criterion to categorize different images based on visual words? Here, we will use something called cosine similarity to make a measurement. \nYou can think of the visual word frequency histogram for each image as an arrow in space, and cosine similarity is a measure of how much those arrows are pointing in the same direction. The criterion is very intuitive: the closer the cosine similarity of two images is to 1, the more similar the two images are; the closer the cosine similarity is to 0, the less similar the two images are.\nHere, we perform pairwise cosine similarity measurements on left-view, front-view, and back-view photographs of New York Kouros, and show the results.\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nhist_list = []\nfor img_name, _, descriptors in image_data:\n    if descriptors.shape[0] &gt; 0:\n        words = kmeans.predict(descriptors)\n        hist, _ = np.histogram(words, bins=np.arange(n_clusters + 1))\n    else:\n        hist = np.zeros(n_clusters, dtype=int)\n    hist = hist.astype(float)\n    if hist.sum() &gt; 0:\n        hist /= hist.sum()\n    hist_list.append(hist)\n    \nhistograms = np.array(hist_list)\n\nsim_matrix = cosine_similarity(histograms)\n\nimage_keys = list(image_names.keys())\nimage_labels = list(image_names.values())\n\n# Display the similarity matrix\nfig, ax = plt.subplots(figsize=(5, 5))\ncax = ax.imshow(sim_matrix, interpolation='nearest', cmap='viridis')\nax.set_title('BoVW Cosine Similarity between Images')\nax.set_xticks(np.arange(len(image_labels)))\nax.set_yticks(np.arange(len(image_labels)))\nax.set_xticklabels(image_labels, rotation=45, ha='right')\nax.set_yticklabels(image_labels)\nfig.colorbar(cax, ax=ax, label='Cosine Similarity')\nplt.tight_layout()\nplt.show()\n\nprint(\"Pairwise Cosine Similarity Matrix:\")\nfor i in range(len(image_labels)):\n    for j in range(i + 1, len(image_labels)):\n        print(f\"{image_labels[i]} vs {image_labels[j]}: {sim_matrix[i, j]:.3f}\")\n\n\nAs you can see, the pairwise cosine similarities are all very high, even though the BoVW histograms look very different! This is good evidence that they are photographs of the same object, and computers can understand this by setting appropriate threshold.\nHowever, would this also work for photos of different objects? Let’s find out by calculating the cosine similarity between existing images and another Kouros currently exhibited in Piraeus Archaeological Museum.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_image_path = '../data/richter_kouroi_filtered_photos/page312_img01_photo4.jpg'\nnew_image_label = 'A Kouros from Athens' # Suppose this is a new artifact we just discovered\n\nimg_new = cv2.imread(new_image_path, cv2.IMREAD_GRAYSCALE)\norb = cv2.ORB_create(nfeatures=500)\nkp_new, desc_new = orb.detectAndCompute(img_new, None)\n\nif desc_new is not None and len(desc_new) &gt; 0:\n    words_new = kmeans.predict(desc_new)\n    hist_new, _ = np.histogram(words_new, bins=np.arange(kmeans.n_clusters + 1))\nelse:\n    hist_new = np.zeros(kmeans.n_clusters, dtype=int)\n    \nhist_new = hist_new.astype(float)\nif hist_new.sum() &gt; 0:\n    hist_new /= hist_new.sum()\n\nsims = cosine_similarity(histograms, hist_new.reshape(1, -1)).flatten() \n\n# Print the cosine similarity of the new image with existing images\nprint(f\"\\nCosine Similarity of '{new_image_label}' with existing images:\")\nfor i, label in enumerate(image_labels):\n    print(f\"{label} vs {new_image_label}: {sims[i]:.3f}\")\n\n# Show the new image\nplt.figure(figsize=(5, 5))\nplt.imshow(img_new, cmap='gray')\nplt.title(f\"{new_image_label}\")\nplt.axis('off')\nplt.show()\n\n\nBy looking at the results, we see that it has a lower but still relatively high cosine similarity to the previous images, albeit with different textures and poses. Although computers do not understand what “Kouroi” are simply by collecting visual words, they can still see the similarity! To support this view, let’s look at an example that is also a standing figure, but from a different culture (China, Sanxingdui). If our conjecture is correct, its cosine similarity to the previous images will decrease significantly.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_image_path2 = '../data/example_images/sanxingdui.jpeg'\nnew_image_label2 = 'A Bronze Figure from Sanxingdui' # Suppose this is a new artifact we just discovered\n\nimg_new2 = cv2.imread(new_image_path2, cv2.IMREAD_GRAYSCALE)\norb = cv2.ORB_create(nfeatures=500)\nkp_new, desc_new = orb.detectAndCompute(img_new2, None)\n\nif desc_new is not None and len(desc_new) &gt; 0:\n    words_new = kmeans.predict(desc_new)\n    hist_new, _ = np.histogram(words_new, bins=np.arange(kmeans.n_clusters + 1))\nelse:\n    hist_new = np.zeros(kmeans.n_clusters, dtype=int)\n    \nhist_new = hist_new.astype(float)\nif hist_new.sum() &gt; 0:\n    hist_new /= hist_new.sum()\n\nsims = cosine_similarity(histograms, hist_new.reshape(1, -1)).flatten()  \n\n# Print the cosine similarity of the new image with existing images\nprint(f\"\\nCosine Similarity of '{new_image_label2}' with existing images:\")\nfor i, label in enumerate(image_labels):\n    print(f\"{label} vs {new_image_label2}: {sims[i]:.3f}\")\n\n# Show the new image\nplt.figure(figsize=(5, 5))\nplt.imshow(img_new2, cmap='gray')\nplt.title(f\"{new_image_label2}\")\nplt.axis('off')\nplt.show()\n\n\nThe results were exactly as we expected. The cosine similarity measured for different artistic styles, different poses and different angles of the Sanxingdui sculpture is significantly lower.\nThis provides us with a hint on how to build an automatic image-based classifier for art and artifacts of different genres, cultures, and textures. Although BoVW also has some obvious limitations (lack of spatial relationships, lack of ability to detect specific objects in complex images), the examples above demonstrate the fundamentals of computer vision, and with the help of more advanced techniques we can do much more in analyzing artwork based on digitized images.\n\n\n\n2. Convolutions on Images\n\n2.1 What are convolutions?\nBefore diving into applying a convolutional neural network, let’s first make an intuitive introduction to the concept convolution.\nImagine sliding a tiny image over an image as a filter to make the actual image appear the same as the filter. Convolution is the mathematical operation to achieve such an effect.\nBelow is one of such filters, or in professional term, a kernel, how do you think it will filter an image to make the image look like it?\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Kernel\nkernel = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\n# Map: 1 -&gt; 1.0 (white), 0 -&gt; 0.0 (black), -1 -&gt; 1.0 (white)\ndisplay_kernel = np.where(kernel == 0, 0, 1)\n\nfig, ax = plt.subplots()\ncax = ax.matshow(display_kernel, cmap='gray', vmin=0, vmax=1)\nplt.colorbar(cax)\n\n# Annotate the kernel values\n\nax.set_title('Example of a Kernel')\nplt.show()\n\n\nThis is how an actual image Convolved with the filter:\n\n\nCode\nfrom scipy.ndimage import convolve\nfrom PIL import Image\n\n# Load the image as grayscale\nimg_path = \"../data/richter_kouroi_filtered_photos/page300_img01_photo8.jpg\"\nimage = Image.open(img_path).convert('L')\nimg_array = np.array(image)\n\n# Define the horizontal edge detection kernel\nkernel = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\n# Convolve the image with the kernel\nconvolved = convolve(img_array, kernel, mode='reflect')\n\n# Display the original and convolved images\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nax[0].imshow(img_array, cmap='gray')\nax[0].set_title(\"Original Image\")\nax[0].axis('off')\nax[1].imshow(convolved, cmap='gray')\nax[1].set_title(\"Convolved with Kernel\")\nax[1].axis('off')\nplt.show()\n\n\nThe above is just one example of a convolutional kernel that extracts horizontal edges in an image. In fact, there are many different kernels with different effects. For example, here is a filter that blurs all images:\n\n\nCode\nimg_path = \"../data/richter_kouroi_filtered_photos/page300_img01_photo8.jpg\"\nimg = np.array(Image.open(img_path).convert('L'))\n\ndef gaussian_kernel(size=21, sigma=5):\n    ax = np.linspace(-(size-1)//2, (size-1)//2, size)\n    xx, yy = np.meshgrid(ax, ax)\n    kernel = np.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n    return kernel / np.sum(kernel)\n\nkernel = gaussian_kernel(21, 5)\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"21x21 Gaussian Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Heavily Blurred\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nBelow is a kernel that preserves the input image as it is; it is also known as the identity kernel:\n\n\nCode\nimg_path = \"../data/richter_kouroi_filtered_photos/page300_img01_photo8.jpg\"\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Identity kernel (3x3)\nkernel = np.zeros((3, 3))\nkernel[1, 1] = 1\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray', vmin=0, vmax=1); ax[1].set_title(\"Identity Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nThere is also a kernel that sharpens the images, known as the sharpening filter:\n\n\nCode\nimg_path = \"../data/richter_kouroi_filtered_photos/page300_img01_photo8.jpg\"\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Sharpen kernel\nkernel = np.array([[ 0, -1,  0],\n                   [-1,  5, -1],\n                   [ 0, -1,  0]])\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"Sharpen Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nOther than sharpening, there is even a filter to emboss the image:\n\n\nCode\nimg_path = \"../data/richter_kouroi_filtered_photos/page300_img01_photo8.jpg\"\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Emboss kernel\nkernel = np.array([[-2, -1, 0],\n                   [-1,  1, 1],\n                   [ 0,  1, 2]])\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"Emboss Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nOther than only detecting vertical or horizontal edges, a filter named after Laplace was discovered to detect all edges:\n\n\nCode\nimg_path = \"../data/richter_kouroi_filtered_photos/page300_img01_photo8.jpg\"\nimg = np.array(Image.open(img_path).convert('L'))\n\nlaplacian = np.array([[0,-1,0],[-1,8,-1],[0,-1,0]])\nedge = convolve(img, laplacian)\n\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off'\n)\nax[1].imshow(laplacian, cmap='gray'); ax[1].set_title(\"Laplacian Kernel\"); ax[1].axis('off')\nax[2].imshow(edge, cmap='gray'); ax[2].set_title(\"Edges\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\n\n\n2.2 What can Machine Learning do?\nOver the years, people have discovered these tiny images or “kernels” or “filters”. In machine learning, we discover or learn these potentially useful filters directly from the data, rather than through mathematical derivation. In a word, machine learning can “learn” the filters from data what would be useful for downstream tasks like classifying images or identifying things in an image.\n\n\nCode\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\n# 1. Load a pretrained conv model (VGG16 without top)\nmodel = VGG16(weights='imagenet', include_top=False)\n\n# 2. Choose three conv layers: early, middle, late\nearly_layer = model.get_layer('block1_conv2')\nmid_layer   = model.get_layer('block3_conv3')\nlate_layer  = model.get_layer('block5_conv3')\n\n# 3. Extract one kernel from each layer\n# Each kernel has shape (k, k, in_channels, out_channels)\nkernel_early = early_layer.get_weights()[0][:, :, 0, 0]\nkernel_mid   = mid_layer.get_weights()[0][:, :, 0, 0]\nkernel_late  = late_layer.get_weights()[0][:, :, 0, 0]\n\n# 4. Load and preprocess an image\ndef load_and_gray(path, target_size=(224,224)):\n    img = image.load_img(path, target_size=target_size)\n    img_arr = image.img_to_array(img)\n    # convert to grayscale\n    gray = cv2.cvtColor(img_arr.astype('uint8'), cv2.COLOR_RGB2GRAY)\n    # normalize\n    gray = gray.astype('float32') / 255.0\n    return gray\n\nimg_path = '../data/richter_kouroi_filtered_photos/page300_img01_photo8.jpg'\n\ngray = load_and_gray(img_path)\n\n# 5. Convolve the image with each kernel\ndef apply_filter(img, kernel):\n    # Flip kernel for convolution\n    k = kernel.shape[0]\n    # OpenCV uses correlation; flip kernel to perform convolution\n    flipped = np.flipud(np.fliplr(kernel))\n    filtered = cv2.filter2D(img, -1, flipped)\n    return filtered\n\nout_early = apply_filter(gray, kernel_early)\nout_mid   = apply_filter(gray, kernel_mid)\nout_late  = apply_filter(gray, kernel_late)\n\n# 6. Visualize\nplt.figure(figsize=(8, 8))\n\nplt.subplot(2, 2, 1)\nplt.title('Original Gray')\nplt.imshow(gray, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 2)\nplt.title('Early Layer Kernel')\nplt.imshow(out_early, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 3)\nplt.title('Mid Layer Kernel')\nplt.imshow(out_mid, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 4)\nplt.title('Late Layer Kernel')\nplt.imshow(out_late, cmap='gray')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()  # In Colab this will display inline\n\n\nDifferent parts of the model highlight different thing. The importnat thing to note is that no one wrote the filters themselves. The network learned that the features highlighted by these filters are useful.We simply wrote the learning algorithm, then the model learned from data by itself.\nTypically, a model used for image classification can (on its own) learn filters to highlight things the model needs, such as edges and lines, and more importantly, in addition to these simple features, the model can learn filters to detect heads, eyes, ears, and other abstract concepts, and this is exactly how convolution makes it possible to detect, characterize, and categorize complex objects in complex images. In order to achieve these amazing features, we usually need to employ models such as Convolutional Neural Networks.\n\n\n\n3. Data Exploration\n\n3.1 Exploring the Metadata\nFor the rest of the notebook, we will use a small selection of photographs from Richter’s Kouroi (1942), which contain frontal shots of Kouroi with a full torso and recognizable facial features. We have also prepared a labeled metadata that shows information about which group and era these Kouroi belong to and what materials they are made of. We can begin by looking at some basic information from the metadata:\n\n\nCode\nimport pandas as pd\n\n# Read in the metadata CSV file\n# Note that we are only going to investigate a subset of the full dataset\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')\n\ndf = df.drop(columns = 'page')\n\nprint(df.head())\n\n\n\n\nCode\nprint(\"Information of the dataset:\")\nprint(f\"Number of images: {df.shape[0]}\")\nprint(f\"Number of distinct eras: {df['era'].nunique()}\")\nprint(f\"Number of distinct materials: {df['material'].nunique()}\")\n\n\nWe can also see the distribution of each label by plotting histograms:\n\n\nCode\ndef bar_plot(df, column1, column2):\n    # Calculate counts of each value in the specified columns\n    label_counts1 = df[column1].value_counts()\n    label_counts2 = df[column2].value_counts()\n\n    # Create a figure with a fixed size\n    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n    \n    # Plot the bar chart\n    label_counts1.plot(kind='bar', ax=axes[0], color='steelblue')\n    axes[0].set_title(f'Distribution of {column1.capitalize()}')\n    axes[0].set_xlabel(column1.capitalize())\n    axes[0].set_ylabel('Count')\n    axes[0].tick_params(axis='x', rotation=45)\n    label_counts2.plot(kind='bar', ax=axes[1], color='darkorange')\n    axes[1].set_title(f'Distribution of {column2.capitalize()}')\n    axes[1].set_xlabel(column2.capitalize())\n    axes[1].set_ylabel('Count')\n    axes[1].tick_params(axis='x', rotation=45)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot the distribution of labels in the dataset\nbar_plot(df, 'era', 'material')\n\n\n\n\n3.2 Exploring the Images\nTo get a direct idea of the general characteristics of this subset of photographs, we read the photographs from the image directory and show the first 4 images in this dataset.\n\n\nCode\n# Read in the images as a list \nfrom pathlib import Path\n\ndata_dir = Path(\"../data/richter_kouroi_complete_front_only\")\nimage_paths = sorted(data_dir.glob(\"*.jpg\"))\n\nimages = []\nfor p in image_paths:\n    img = Image.open(p).convert(\"RGB\")   # ensure 3‑channel\n    img_arr = np.array(img)\n    images.append(img_arr)\n    \nfig, axes = plt.subplots(1, 4, figsize=(6, 4))\nfor ax, img in zip(axes, images[:4]):\n    ax.imshow(img)\n    ax.axis(\"off\")\nplt.suptitle(\"First 4 images in the dataset\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n4. Image Embedding Using ConvNeXt V2\n\n4.1 CNN and Models See Images\nWhat we’ll do next is we will bring in ConvNeXt V2, it is a CNN model trained on millions of images based on ImageNet.\nI’m sure everyone has used some Large Language Models (LLMs) and gotten a feel for how these models mimic the way humans think. This imitation of the human way of thinking comes from Artificial Neural Networks (ANN). Just as LLMs process and generate natural language, CNN models process visual images, examining the features in those images through convolutional layers and generating their own digital representations of the images.\nHere, we will create a visualization that shows the early, middle, and late feature layers of the 4 images after convolution, what commonality and difference did you notice from the extracted features in these layers?\n\n\nCode\nmodel = VGG16(weights='imagenet', include_top=False)\n\n# Grab kernels from the first and last conv layers\nearly_layer = model.get_layer('block1_conv2')\nmid_layer = model.get_layer('block3_conv3')\nlate_layer  = model.get_layer('block5_conv3')\n\n# choose the (0,0) filter for each\nkernel_early = early_layer.get_weights()[0][:, :, 0, 0]\nkernel_mid  = mid_layer.get_weights()[0][:, :, 0, 0]\nkernel_late  = late_layer.get_weights()[0][:, :, 0, 0]\n\n# Convolution helper (flip kernel for true conv)\ndef apply_filter(img, kernel):\n    flipped = np.flipud(np.fliplr(kernel))\n    return cv2.filter2D(img, -1, flipped)\n\nfig, axes = plt.subplots(3, 4, figsize=(8, 8))\n\nfor col, img in enumerate(images[:4]):\n    gray = (\n        cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2GRAY)\n          .astype('float32') / 255.0\n    )\n    out_early = apply_filter(gray, kernel_early)\n    out_early = (out_early - out_early.min()) / (out_early.max() - out_early.min())\n\n    out_mid = apply_filter(gray, kernel_mid)\n    out_mid = (out_mid - out_mid.min()) / (out_mid.max() - out_mid.min())\n    \n    out_late = apply_filter(gray, kernel_late)\n    out_late = (out_late - out_late.min()) / (out_late.max() - out_late.min())\n\n    # plot\n    axes[0, col].imshow(out_early, cmap='gray')\n    axes[0, col].set_title(f'Early Layer #{col+1}')\n    axes[0, col].axis('off')\n\n    axes[1, col].imshow(out_mid, cmap='gray')\n    axes[1, col].set_title(f'Mid Layer #{col+1}')\n    axes[1, col].axis('off')\n\n    axes[2, col].imshow(out_late, cmap='gray')\n    axes[2, col].set_title(f'Late Layer #{col+1}')\n    axes[2, col].axis('off')\n\nfig.suptitle('First, Middle vs. Last Conv Layer Responses', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n4.2 Creating Image Embeddings\nImage embedding is a process where images are transformed into numerical representations, specifically, lists of numbers that carry informations about the images. While this sounds somewhat similar to the idea of visual words, they are not the same. Think of BoVW as counting how many times specific words appear in a book without caring about grammar or sentence structure, this can identify simple patterns, but cannot summarize the big picture of the book. Image embeddings, on the other hand, are like reading the entire book and summarizing its meaning in a well crafted passage, they capture the bigger picture, context, and nuance.\nWe can build a vocabulary of visual words quite easily, but creating image embeddings usually require using deep neural networks pretrained on millions of images. These networks process the entire image and learn hierarchical, abstract features that are more semantically meaningful.\nHere, we will load the pre-trained ConvNeXt V2 model, pass the image folder to generate embeddings, and save the embeddings in a grid of numbers.\n\n\nCode\n# Read in the pre-trained ConvNeXtV2 model\nfrom transformers import AutoImageProcessor\nfrom transformers import ConvNextV2Model\nfrom torchvision.models import convnext_base,  ConvNeXt_Base_Weights\nimport torch\nfrom tqdm.notebook import tqdm\n\n# Load the pre-trained ConvNeXtV2 model and image processor\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-base-22k-224\") \n\n# Move the model to the appropriate device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = ConvNextV2Model.from_pretrained(\"facebook/convnextv2-base-22k-224\")\n\n# Move the model to the appropriate device (GPU or CPU)\n_ = model.to(device)\n\n\n\n\nCode\nbatch_size = 16\nembeddings = []\nvalid_filenames = []\n\nimage_directory = \"../data/richter_kouroi_complete_front_only\"\n\nfilenames = df['filename'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i : i + batch_size]\n    images = []\n    for filename in batch_filenames:\n        path = os.path.join(image_directory, filename)\n        try:\n            img = Image.open(path).convert(\"RGB\")\n            images.append(img)\n        except FileNotFoundError:\n            print(f\"Missing: {path}\")\n        except Exception as e:\n            print(f\"Error with {filename}: {e}\")\n\n    if not images:\n        continue\n\n    # Prepare inputs\n    inputs = processor(images=images, return_tensors=\"pt\")\n    pixel_values = inputs[\"pixel_values\"].to(device)\n\n    # Forward through feature extractor\n    with torch.no_grad():\n        outputs = model(pixel_values)   \n    # Global average pool\n    if isinstance(outputs, torch.Tensor):\n        hidden = outputs           \n    else:\n        hidden = outputs.last_hidden_state \n\n    batch_emb = hidden.mean(dim=(2, 3)).cpu().numpy()\n\n    embeddings.extend(batch_emb)   \n    \nembeddings = np.stack(embeddings, axis=0) \n\nnp.save('../data/embeddings/convnextv2_image_embeddings.npy', embeddings)\n\n\n\n\nCode\nprint(embeddings)\n\nembeddings.shape\n\n\nWe printed the embedded results above to see what they look like, and we also displayed the shape of the grid. As you can see, it contains 62 rows representing the 62 images in the dataset, and each row has 1024 numbers representing all the information extracted from each image. From now on, we will use this embedded data instead of the original image data.\n\n\n\n5. Analysis of Image Embeddings\n1024 is way too many numbers for us to examine and understand with our brains. So, we use something techniques called dimensionality reduction to squish the data down to just 2 or 3 dimensions, making it easy to visualize in 2D.\nPrincipal Component Analysis (PCA) is one of such techniques, it is essentially finding the two or three major axes through our huge data along which the data has the most variations. By PCA we decompose our data with 1024 dimensions (number of cells in each row) to 2 dimensions and represent each image as a point on our scatterplots. We then color the data with “era” and “material” respectively.\nHere, we will use plotly to create interactive visualizations, feel free to play with it and discuss what pattern did you notice?\n\n\nCode\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\nimport plotly.io as pio\n\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n# load embeddings and metadata\nembeddings = np.load('../data/embeddings/convnextv2_image_embeddings.npy')\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')   \n\n# PCA to 2 components\npca = PCA(n_components=2)\npc2 = pca.fit_transform(embeddings)\n\n# build DataFrame\npc_df = pd.DataFrame(pc2, columns=['PC1','PC2'])\npc_df['filename'] = df['filename'].values\npc_df['era'] = df['era'].values\n\n# interactive scatter\nfig = px.scatter(\n        pc_df,\n        x='PC1',\n        y='PC2',\n        color='era',\n        hover_data=['filename'], \n        title='Interactive PCA of Image Embeddings Colored by Era',\n        width=700, height=500\n    )\n\nfig.show()\n\n\n\n\nCode\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\npc_df['material'] = df['material'].values\n\nfig = px.scatter(\n        pc_df,\n        x='PC1',\n        y='PC2',\n        color='material',\n        hover_data=['filename'], \n        title='Interactive PCA of Image Embeddings Colored by Material',\n        width=700, height=500\n    )\n\nfig.show()\n\n\n\nDiscussion: Did you see any clear patterns of distributions by looking at the visualizations above? How can you interpret the results? What primary “features” do you think the PCA embedding captured in the first two dimensions?\n\n\n\n6. Classification of Kouroi\nArchaeological classification has always been an important issue in archaeology and artifact research. This problem is especially challenging when faced with a large amount of artifact data, or when faced with new artifacts with insufficient information. With the development of machine learning and image recognition technology, the use of computer technology to assist classification has become a trend in the new era of information archaeology. In this section, we would like to provide an example of classifying Kouroi by visual element for your reference.\n\n6.1 Traditional Approach\nIn addition to observing how the labels are clustered based on the embeddings, we can train classifiers to categorize objects into appropriate labels based on the image embeddings directly. A traditional approach is through a technique called logistic regression.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nX = embeddings\ny1 = df['era'].tolist()\ny2 = df['material'].tolist()\n\ny1 = np.array(y1)\ny2 = np.array(y2)\n\n\nNote that here we use image embeddings and labels as training data and test data respectively, this is because we want to evaluate the effectiveness of the classifier when dealing with unseen data. After training the classifier for eras, we use it to predict the labels of the test data and compare the results with the real labels. The report is printed below:\n\n\nCode\n# Perform the classification of eras\n# Split the data into training and testing sets\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y1, test_size=0.25, random_state=42, stratify=y1)\n\n# Create a logistic regression model\nclf1 = LogisticRegression(max_iter=1000, random_state=42)\n\nclf1.fit(X_train1, y_train1)\n\ny_pred1 = clf1.predict(X_test1)\n\n# Calculate classification metrics\nclassification_report1 = metrics.classification_report(y_test1, y_pred1, zero_division=0)\n\n# Print the classification report \nprint(\"Classification Report for Eras:\")\nprint(classification_report1)\n\n\nThe key metrics we especially care about here is the accuracy of our classifier, it is defined by\n\\[\n\\text{Accuracy} = \\frac{\\text{True Predictions}}{\\text{True Predictions} + \\text{Flase Predictions}}\n\\]\nIt reflects the proportion of true predictions out of all predictions made using the classifier. However, as shown above in the report, the accuracy of predicting era based on image embedding is not satisfactory. Still, we can visualize the decision boundary of this classifier on our 2D PCA of image embeddings to see what went wrong:\n\n\nCode\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import LabelEncoder\n\n# Prepare your 2D data + labels\nX_pca = pc_df[['PC1','PC2']].values\ny_era = pc_df['era'].values\n\n# Encode eras as integers\nle = LabelEncoder()\ny_enc = le.fit_transform(y_era)\n\n# Train the logistic on the encoded labels\nclf_2d = LogisticRegression(max_iter=1000, random_state=42)\nclf_2d.fit(X_pca, y_enc)\n\n# Build a mesh grid over the plotting area\nx_min, x_max = X_pca[:,0].min() - 1, X_pca[:,0].max() + 1\ny_min, y_max = X_pca[:,1].min() - 1, X_pca[:,1].max() + 1\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\n\n# Predict integer labels on the mesh\nZ = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(7,7))\n\n# light colors for regions\ncmap_light = ListedColormap([\n    '#FFCCCC', '#CCFFCC', '#CCCCFF', '#FFE5CC', '#E5CCFF', '#FFFFCC', '#CCFFFF'\n][:len(le.classes_)])\n# bold colors for points\ncmap_bold  = ListedColormap([\n    '#FF0000', '#00AA00', '#0000FF', '#FF8000', '#8000FF', '#FFFF00', '#00FFFF'\n][:len(le.classes_)])\n\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n\n# scatter original points, mapping back to string labels in the legend\nfor class_int, era_label in enumerate(le.classes_):\n    mask = (y_enc == class_int)\n    plt.scatter(\n        X_pca[mask,0], X_pca[mask,1],\n        color=cmap_bold(class_int),\n        label=era_label,\n        edgecolor='k', s=40\n    )\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Logistic Decision Boundary on 2D PCA Embedding\\n(colored by era)')\nplt.legend(title='Era')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.grid(True)\nplt.show()\n\n\n\nDiscussion: What can you say about this decision boundary?\n\nSimilarly, we can use the same approach to classify material of Kouroi. We first perform a train-test split, then train the classifier, use the trained classifier to predict the labels of the test set, and print out the classification report for quality evaluation.\n\n\nCode\n# Perform the classification of materials\n# Split the data into training and testing sets\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2, test_size=0.25, random_state=42, stratify=y2)\n\n# Create a logistic regression model\nclf2 = LogisticRegression(max_iter=1000, random_state=42)\n\nclf2.fit(X_train2, y_train2)\n\ny_pred2 = clf2.predict(X_test2)\n\n# Calculate classification metrics\nclassification_report2 = metrics.classification_report(y_test2, y_pred2, zero_division=0)\n\n# Print the classification report\nprint(\"Classification Report for Materials:\")\nprint(classification_report2)\n\n\nAs you can see, the accuracy is much higher now, but does that mean the classifier is good? You may have noticed that bronze and marble are classified almost perfectly, but other materials are not. This means that the classifier, while having a high accuracy, may have low precision or recall, as defined below:\n\nPrecision: The ratio of the number of true positives to the number of positive predictions. Precision tells us how often the model predicts correctly.\nRecall: The ratio of the number of true positives to the number of actual positives. Recall answers the question, “What percentage of positive results did we correctly predict?”\n\nWe can also visualize the decision boundary on the 2D PCA of materials\n\n\nCode\n# Prepare 2D data and labels\nX_pca = pc_df[['PC1','PC2']].values\ny_era = pc_df['material'].values\n\n# Encode eras as integers\nle = LabelEncoder()\ny_enc = le.fit_transform(y_era)\n\n# Train the logistic on the encoded labels\nclf_2d = LogisticRegression(max_iter=1000, random_state=42)\nclf_2d.fit(X_pca, y_enc)\n\n# Build a mesh grid over the plotting area\nx_min, x_max = X_pca[:,0].min() - 1, X_pca[:,0].max() + 1\ny_min, y_max = X_pca[:,1].min() - 1, X_pca[:,1].max() + 1\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\n\n# Predict integer labels on the mesh\nZ = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(7,7))\n\n# light colors for regions\ncmap_light = ListedColormap(['#FFCCCC','#CCFFCC','#CCCCFF','#FFE5CC','#E5CCFF'][:len(le.classes_)])\n# bold colors for points\ncmap_bold  = ListedColormap(['#FF0000','#00AA00','#0000FF','#FF8000','#8000FF'][:len(le.classes_)])\n\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n\n# scatter original points, mapping back to string labels in the legend\nfor class_int, era_label in enumerate(le.classes_):\n    mask = (y_enc == class_int)\n    plt.scatter(\n        X_pca[mask,0], X_pca[mask,1],\n        color=cmap_bold(class_int),\n        label=era_label,\n        edgecolor='k', s=40\n    )\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Logistic Decision Boundary on 2D PCA Embedding\\n(colored by material)')\nplt.legend(title='Era')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.grid(True)\nplt.show()\n\n\n\nDiscussion: Now, going back to the classification reports shown above, do you think the classifier trained on era is a good chronological classifier? What about materials?\n\n\n\n6.2 CNN Classification of Materials\nThe last classifier we’re going to visit today is a neural network classifier, using a Multi-layer Perceptron (MLP) network architecture, which means we’re going to add a classification layer to the ConvNeXt V2 model to classify the material. All the model does here is act as a backbone to observe and extract features of interest in the input image. The MLP process, on the other hand, can be visualized as a number of experts examining different features on an image, then discussing them with each other, and finally voting to reach a final conclusion.\nWe begin by creating the data loader and load the Kouroi data directly from the folder:\n\n\nCode\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Map the materials to integers\nMAT2IDX = {\n    'Marble': 0,\n    'Bronze': 1,\n    'Other': 2\n}\n\n# Create a custom dataset class for the Kouroi dataset\nclass KouroiDataset(Dataset):\n    def __init__(self, df, img_dir, processor, mat2idx):\n        self.df = df\n        self.img_dir = image_directory\n        self.processor = processor\n        self.mat2idx = MAT2IDX\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(os.path.join(self.img_dir, row.filename)).convert(\"RGB\")\n\n        inputs = self.processor(images=img, return_tensors=\"pt\")\n\n        for k,v in inputs.items():\n            inputs[k] = v.squeeze(0)\n        label = self.mat2idx[row.material]\n        return inputs, label\n\n\n\n\nCode\n# Tain-test split\nfrom torch.utils.data import random_split\n\ntrain_df, test_df = train_test_split(\n    df,\n    test_size=0.25,                 # 20% held out for testing\n    stratify=df[\"material\"],       # preserve class proportions\n    random_state=42\n)\n\n# Initialize the dataset and dataloader\ntrain_ds = KouroiDataset(\n    df=train_df,\n    img_dir=image_directory,\n    processor=processor,\n    mat2idx=MAT2IDX\n)\ntest_ds  = KouroiDataset(\n    df=test_df,\n    img_dir=image_directory,\n    processor=processor,\n    mat2idx=MAT2IDX\n)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)\n\n\nAs mentioned above, here we freeze the model so that training does not change the way it understands the input image.\n\n\nCode\n# Build the model with convnextv2 as the backbone and a linear layer for classification\nclass Classifier(nn.Module):\n    def __init__(self, backbone_name, num_classes):\n        super().__init__()\n        # Load the ConvNeXtV2 backbone correctly and freeze it\n        self.backbone = ConvNextV2Model.from_pretrained(\n            backbone_name,\n            output_hidden_states=False,\n            output_attentions=False\n        )\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n        embed_dim = self.backbone.config.hidden_sizes[-1]\n\n        # Build a simple 2-layer MLP head\n        self.head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(embed_dim // 2, num_classes)\n        )\n\n    def forward(self, pixel_values):\n        # Forward through the frozen backbone\n        outputs = self.backbone(pixel_values=pixel_values)\n        x = outputs.pooler_output\n\n        # Classification head\n        logits = self.head(x)\n        return logits\n\n# Instantiate and move to device\nmodel = Classifier(\n    backbone_name=\"facebook/convnextv2-base-22k-224\",\n    num_classes=len(MAT2IDX)\n).to(device)\n\n\nHere, after setting up the new model, we will define the training loop and train the model. Please note that this process may take some time, especially when running on devices without a GPU. This also suggests that the high computational power requirement is a drawback when using CNNs for classification.\n\n\nCode\n# Set up the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.head.parameters(), lr=1e-4, weight_decay=0.01)\nepochs = 20  \n\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n        inputs, labels = batch\n        # move to device\n        inputs = {k:v.to(device) for k,v in inputs.items()}\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        loss   = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * labels.size(0)\n\n    avg_loss = total_loss / len(train_ds)\n    print(f\" Epoch {epoch} avg loss: {avg_loss:.4f}\")\n\n\nAfter training, we set the model to evaluation mode and assessed the classification quality by printing the confusion matrix and classification report. It is clear that the accuracy does improve with the MLP architecture. However, we still lacked samples of materials other than bronze and marble, which undoubtedly harmed the quality of our training.\n\n\nCode\n# Run one pass over your data in eval mode\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        if inputs is None: continue\n        inputs = {k:v.to(device) for k,v in inputs.items()}\n        logits = model(**inputs)\n        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\nreport = classification_report(all_labels, all_preds, target_names=list(MAT2IDX.keys()), zero_division=0)\nprint(\"Classification Report for Materials:\")\nprint(report)\n\n\n\nDiscussion: What difference did you notice in the result? Does this mean that MLP is not applicable to classifying materials?\n\nWhile the MLP classifier via CNN has advantages in dealing with more complex data structures (especially high-dimension nonlinear data), we note that it has two serious drawbacks: 1. it is more susceptible to the randomness of the training-testing split, with greater model variability; and 2. it is more susceptible to overfitting, whereas logistic regression is more susceptible to underfitting (for more detailed information on these concepts discussion can be found in Appendix B). This tells us that no one model is perfect for all situations. We must remain cautious in our choice of models.\n\n\n6.3 Example: Predicting the Material of Unseen Kouroi Photos\nNow, let’s imagine a scenario where we find a new Kouros, but we are not sure what it is made of, and we want to use our trained classifier to classify it based on its image features.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_artifact_path = '../data/NAMA_3938_Aristodikos_Kouros.jpeg'\nnew_artifact_label = 'A Kouros from Athens' # Suppose this is a new artifact we just discovered\n\nart_new = cv2.imread(new_artifact_path, cv2.IMREAD_GRAYSCALE)\n\n# Show the new image\nplt.figure(figsize=(6, 6))\nplt.imshow(art_new, cmap='gray')\nplt.title(f\"{new_artifact_label}\")\nplt.axis('off')\nplt.show()\n\n\n\nDiscussion: Looking at this photo, how would you classify the era and material of this Kouros?\n\nWe pass it into our trained CNN classifier and see how it would be classified:\n\n\nCode\nfrom PIL import Image\nimport torch\nfrom torchvision import transforms\n# Resize the model to an appropriate size\npreprocess = transforms.Compose([\n    transforms.Resize(256),                \n    transforms.CenterCrop(224),            \n    transforms.ToTensor(),\n    transforms.Normalize(                   \n        mean=[0.485, 0.456, 0.406],\n        std= [0.229, 0.224, 0.225]\n    ),\n])\n\nidx2mat = {idx: mat for mat, idx in MAT2IDX.items()}\n\ndef predict_image(image_path, model, device):\n    # Load\n    img = Image.open(image_path).convert(\"RGB\")\n    # Preprocess\n    x = preprocess(img)\n    x = x.unsqueeze(0).to(device)\n    # Inference\n    model.eval()\n    with torch.no_grad():\n        logits = model(**{\"pixel_values\": x}      \n                       if isinstance(x, torch.Tensor) else x)\n        pred_idx = logits.argmax(dim=1).item()\n\n    return idx2mat[pred_idx]\n\nmodel.to(device)\n\npredicted_material = predict_image(new_artifact_path, model, device)\nprint(\"Predicted Material:\", predicted_material)\n\n\n\nDiscussion: Do you think the prediction is correct?\n\n\n\n6.4 Additional Note on Fine-tuning\nWhat we didn’t include here is an advanced application called fine-tuning. Fine-tuning refers to the process of taking a pre-trained model (like ConvNeXt V2) and continuing its training on your specific dataset, allowing the model to adapt its learned features to better suit your task. Why we did not cover fine-tuning because it requires more computational resources, careful hyperparameter tuning, and a larger dataset to avoid overfitting. Additionally, fine-tuning can be time-consuming and is often not practical in an introductory or resource-limited setting.\nHowever, fine-tuning has the potential to significantly improve classification quality. By allowing the model to update its internal representations based on the unique characteristics of your images, it can learn more relevant features for distinguishing between subtle differences in style, era, or material. For research or production applications with sufficient data and compute, fine-tuning is a powerful next step to achieve higher accuracy and more robust results.\n\n\n\n7. Conclusion\nThrough this notebook, you’ve taken a journey with the example of Richter’s Kouroi from the basics of how computers “see” images to advanced techniques for analyzing and classifying images. You’ve explored how simple pixel values can be transformed into powerful representations using convolution, image embeddings, and neural networks. Along the way, you learned to visualize, cluster, and classify artworks…… these are all skills that are at the heart of modern computer vision.\nRemember, the tools and concepts you’ve practiced here are not just limited to art history or archaeology: they are widely used in fields ranging from medicine to astronomy, and beyond. As you continue your studies, keep experimenting, stay curious, and don’t be afraid to explore new datasets or try more advanced models. The intersection of technology and the humanities is full of exciting possibilities, and your creativity is the key to unlocking them.\nCongratulations on completing this exploration, and we hope you feel inspired to keep learning and discovering the field of Machine Learning!\n\n\nKey Takeaways\n\nThere are multiple different ways for computers to represent and understand content in images, including intensity histograms, BoVW distribution and image embeddings.\nConvolution is a common technique to process and extract different features in input images.\nConvolutional Neural Networks (CNN) are trained models that mimic the way people view images and understand them through convolutional layers.\nImage embeddings produced by a pretrained CNN map each kouros image into a high‑dimensional feature space. We can then apply dimensionality‑reduction techniques such as Principal Component Analysis to visualize clusters among different archaic sculptures.\nFor formal classification of kouroi, such as distinguishing groups/eras or identifying materials, both logistic regression and neural-network classification provide robust methods to assign style labels based on extracted image features, but we should always be aware of issues such as underfitting and overfitting.\nDifferent models may yield different results for image embedding, and sometimes the features recognized from an image are not exactly what we want. We should always be cautious about our results.\n\n\n\nGlossary\n\nComputer Vision: Computer Vision is a field of artificial intelligence that enables computers to “see” and interpret images and videos, mimicking human vision.\nConvolution: In the context of Computer Vision, Convolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\nConvolutional Neural Network (CNN): Convolutional Neural Network is a type of feedforward neural network that learns features via filter (or kernel) optimization. It is distinguished from other neural networks by its superior performance with image, speech or audio signal inputs.\nImage Embedding: Image Embedding is a process where images are transformed into numerical representations, called vectors, that capture the semantic meaning of the image.\nPrincipal Component Analysis (PCA): Principal Component Analysis is a statistical technique that simplifies complex data sets by reducing the number of variables while retaining key information. It does so by finding the major axes where the data sets vary the most.\nLogistic Regression: Logistic Regression is a statistical model used for binary or multiclass classification tasks. It estimates the probability that an input belongs to a particular class by applying the logistic (sigmoid) function to a weighted sum of the input features, making it well-suited for problems where outputs are discrete categories.\nMultilayer Perceptron (MLP): A Multilayer Perceptron is a class of feedforward artificial neural network composed of an input layer, one or more hidden layers of nonlinear activation units, and an output layer. It learns complex patterns by adjusting the weights of connections through backpropagation and is versatile for both classification and regression tasks.\nUnderfitting: Underfitting in machine learning occurs when a model is too simple to capture the underlying patterns in the training data.\nOverfitting: Overfitting in machine learning occurs when a model learns the training data too well, including its noise and random fluctuations, leading to poor performance on new, unseen data.\n\n\n\nAppendix A: Image Data Collection and Preprocessing from .pdf Files\nThis part provides a brief overview of how the data was collected and preprocessed for the analysis, typically how we cropped the images and prepared the metadata.\nWe used the following python script to convert a scanned pdf of Richter (1942) to image files in .jpg format.\nimport fitz  \n\n# Change the filename here if you want to reuse the script for your own project\ndoc = fitz.open(\"kouroiarchaicgre0000rich_1.pdf\") \n\nimport os\nout_dir = \"extracted_images\"\nos.makedirs(out_dir, exist_ok=True)\n\n# Iterate pages\nfor page_index in range(len(doc)):\n    page = doc[page_index]\n    image_list = page.get_images(full=True)  # get all images on this page\n\n    # Skip pages without images\n    if not image_list:\n        continue\n\n    # Extract each image\n    for img_index, img_info in enumerate(image_list, start=1):\n        xref = img_info[0]                   \n        base_image = doc.extract_image(xref)  \n        image_bytes = base_image[\"image\"]     \n        image_ext   = base_image[\"ext\"]      \n\n        # Write to file\n        out_path = os.path.join(\n            out_dir,\n            f\"page{page_index+1:03d}_img{img_index:02d}.{image_ext}\"\n        )\n        with open(out_path, \"wb\") as f:\n            f.write(image_bytes)\n\nprint(f\"Saved all images to {out_dir}\")\nThe following script cropped the photos by applying convolution.\nimport cv2\nimport glob\nimport os\n\n# Folder containing your page images\ninput_folder = \"extracted_images\"\noutput_folder = \"cropped_photos\"\nos.makedirs(output_folder, exist_ok=True)\n\ndef extract_photos_from_page(image_path, min_area=5000):\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # Blur and threshold to get binary image\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY_INV)\n    \n    # Dilate to merge photo regions\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n    dilated = cv2.dilate(thresh, kernel, iterations=2)\n    \n    # Find contours\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    crops = []\n    for cnt in contours:\n        x, y, w, h = cv2.boundingRect(cnt)\n        area = w * h\n        # Filter by area to remove small artifacts\n        if area &gt; min_area:\n            crop = img[y:y+h, x:x+w]\n            crops.append((crop, (x, y, w, h)))\n    return crops\n\n# Process all pages\nfor img_path in glob.glob(os.path.join(input_folder, \"*.*\")):\n    base = os.path.splitext(os.path.basename(img_path))[0]\n    photos = extract_photos_from_page(img_path)\n    for idx, (crop, (x, y, w, h)) in enumerate(photos, start=1):\n        out_file = os.path.join(output_folder, f\"{base}_photo{idx}.jpg\")\n        cv2.imwrite(out_file, crop)\n\nprint(f\"Saved all images at {output_folder}\")\nThe following script employed tesseract OCR engine to detect pure text images and filter out photos of Kouros. You may want to visit the GitHub repository https://github.com/tesseract-ocr/tesseract to see how to install and setup appropriately.\nimport os\nimport shutil\nfrom PIL import Image\nimport pytesseract\n# Ensure you have Tesseract installed and pytesseract configured correctly.\n# On Windows, you might need:\n# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n\n# Folders\ninput_folder = \"cropped_photos\"\ntext_folder = \"text_crops\"\nphoto_folder = \"filtered_photos\"\nos.makedirs(text_folder, exist_ok=True)\nos.makedirs(photo_folder, exist_ok=True)\n\n# Threshold for text length to consider as \"text-only\"\n# You can also adjust this threshold based on your specific needs.\nTEXT_CHAR_THRESHOLD = 2 # Be careful with this threshold, do remember to check the results manually\n\n\nfor filename in os.listdir(input_folder):\n    path = os.path.join(input_folder, filename)\n    img = Image.open(path)\n\n    # Perform OCR to extract text\n    extracted_text = pytesseract.image_to_string(img)\n\n    # Classify based on length of extracted text\n    if len(extracted_text.strip()) &gt;= TEXT_CHAR_THRESHOLD:\n        dest = os.path.join(text_folder, filename)\n    else:\n        dest = os.path.join(photo_folder, filename)\n\n    shutil.move(path, dest)\n    print(f\"Moved {filename} -&gt; {os.path.basename(dest)}\")\n\nprint(\"Filtering complete\")\nThis script creates a .csv file for mannual labelling.\nimport os, re\nimport pandas as pd\n\n# Scan your filtered_photos folder\nrecords = []\n\n# Updated regex to match \"page&lt;number&gt;_img&lt;number&gt;_photo&lt;number&gt;.&lt;ext&gt;\"\npattern = re.compile(r\"page(\\d+)_img\\d+_photo(\\d+)\\.(?:png|jpe?g)\", re.IGNORECASE)\n\nfor fn in os.listdir(\"richter_kouroi_head_front_only\"):\n    m = pattern.match(fn)\n    if not m:\n        continue\n    page = int(m.group(1))\n    photo_idx = int(m.group(2))\n    records.append({\n        \"filename\": fn,\n        \"page\": page,\n        \"group\": \"\",    # blank for manual entry\n        \"era\": \"\",  # blank for manual entry\n        \"material\": \"\"  # blank for manual entry\n    })\n\n# Build DataFrame\ndf = pd.DataFrame(records)\n\ndf.sort_values([\"page\", \"filename\"], inplace=True)\n\n# Save out to CSV for manual labeling\ndf.to_csv(\"label_template.csv\", index=False)\nYou can try out the scripts with your interested pdf files yourself by running them in a python environment.\n\n\nAppendix B: The Risk of Underfitting and Overfitting\nIn the context of machine learning, two common pitfalls are underfitting and overfitting.\nUnderfitting occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test sets. This is often seen when using models like logistic regression on complex, non-linear datasets, as shown in the left panel above, where the decision boundary fails to separate the classes effectively. On the other hand, overfitting happens when a model is excessively complex, such as a deep neural network with many layers, and learns not only the true patterns but also the noise in the training data. This leads to excellent performance on the training set but poor generalization to new, unseen data, as illustrated in the right panel where the decision boundary is overly intricate.\nLet’s examine the two problems with a simulated two-class data:\n\n\nCode\nfrom sklearn.datasets     import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model  import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\n# Generate a simulater two-class data\nX, y = make_moons(n_samples=500, noise=0.40, random_state=0)\n\n# Visualize the simulated data\nplt.figure(figsize=(6, 4))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolor='k')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Simulated Two-Class Data')\nplt.show()\n\n\nWe can clearly see that the data is very non-linear, which gives us a hint that the right model should be able to account for non-linear relationships. However, for demonstration purposes, I will be using logistic regression to generate an underfitting classifier; and while MLP is suitable for use here, I will let it generate an overfitting classifier by significantly oversizing the neurons and iterations.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=1\n)\nunder = LogisticRegression()\n\nover  = MLPClassifier(hidden_layer_sizes=(200,200,200),\n                      max_iter=2000,\n                      random_state=1)\n\n# Train both models\nunder.fit(X_train, y_train)\nover.fit(X_train, y_train)\n\n\nThen, we get the decision boundaries of the two classifiers and visualize them with the test data, what do you find about their accuracy?\n\n\nCode\n# Build grid\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Get decision boundary\nZu = under.predict_proba(grid)[:,1].reshape(xx.shape)\nZo = over.predict_proba(grid)[:,1].reshape(xx.shape)\n\n# Plot side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n\nfor ax, Z, title in [\n    (ax1, Zu, 'Extreme Underfit'),\n    (ax2, Zo, 'Extreme Overfit')\n]:\n    ax.contourf(xx, yy, Z&gt;0.5, alpha=0.3)\n    # draw precise decision boundary P=0.5\n    ax.contour(   xx, yy, Z, levels=[0.5], colors='k', linewidths=1.5)\n\n    ax.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor='k')\n    ax.set_title(title)\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n\nfig.suptitle('Underfitting vs. Overfitting in Classification', fontsize=16)\nplt.tight_layout(rect=[0,0.03,1,0.95])\nplt.show()\n\n\nWe can tell easily from the above visualization that both scenarios are harmful: underfitting prevents the model from making meaningful predictions, while overfitting results in unreliable predictions on real-world data. The example above also highlights the necessity to validate classifier quality using test data.\nWhile underfitting and overfitting can seem scary, there are many tools that have been developed to address this issue. We can adjust the complexity of the model, use regularization techniques, collect more data, or employ cross-validation to find a balance that generalizes well to new examples. These are left for you to explore on your own.\n\n\nReference\n\nRichter, G. M. A. (1970). Kouroi: Archaic Greek youths: A study of the development of the Kouros type in Greek sculpture. Phaidon. Accessed through Internet Archive https://archive.org/details/kouroiarchaicgre0000rich.\nPinecone. Embedding Methods for Image Search. Accessed through Pinecone https://www.pinecone.io/learn/series/image-search/.\nIBM. What are convolutional neural networks? https://www.ibm.com/think/topics/convolutional-neural-networks\nHugging Face. Image Classification. https://huggingface.co/docs/transformers/tasks/image_classification\nColeman, C., Lyon, S., & Perla, J. (2020). Introduction to Economic Modeling and Data Science. QuantEcon. Retrieved from https://datascience.quantecon.org/\nWoo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I. S., & Xie, S. (2024). ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders. arXiv preprint arXiv:2301.00808v1."
  },
  {
    "objectID": "docs/HIST-414/NLI/NLI.html",
    "href": "docs/HIST-414/NLI/NLI.html",
    "title": "Text Embeddings for Regina V Wing Chong (1885)",
    "section": "",
    "text": "# Data Wrangling\nimport os\nimport numpy as np\nimport pandas\nfrom nltk.tokenize import word_tokenize\n\n\nwith open('data/Regina_V_Wing_Chong.txt', encoding='utf-8') as f:\n    full_text = f.read()\nprint(full_text)\n\n\nBERT Word Embeddings\n\nimport re\n\ndef clean_text(text):\n    \n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    \n    return text.strip()\n\ntext_cleaned = clean_text(full_text)\nprint(text_cleaned[:500])  # Print the first 500 characters of the cleaned text\n\n\n# Load pre-trained BERT tokenizer and model\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\nbert_model = BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')\n\n\n# Create the word embeddings\n# Tokenize the cleaned text into words\ntokens = word_tokenize(text_cleaned)\n\ntoken_frequencies = {}\n\nfor token in tokens:\n    token_frequencies[token] = token_frequencies.get(token, 0) + 1\n\n\nsorted_tokens = sorted(token_frequencies.items(), key=lambda x: x[1], reverse=True)\n\n# Example: print top 10 most frequent tokens\nprint(\"Most frequent tokens:\")\nfor token, freq in sorted_tokens[:20]:\n    print(f\"{token}: {freq}\")\n\n\nimport re\n# Build ethnicity vocabulary\nethnicities = [\n    \"chinese\", \"japanese\", \"black\", \"white\", \"yellow\", \"chinamans\", \"hong kong\",\n    \"canada\", \"american\", \"americans\", \"european\", \"china\", \"chinaman\", \"britain\",\n    \"canadian\", \"latino\", \"mongolian\", \"asian\", \"indian\", \"india\", \"english\",\n    \"british\", \"america\", \"columbia\", \"ontario\", \"australia\", \"australian\",\n    \"germans\", \"german\", \"chinamen\", \"italian\", \"italy\", \"french\", \"france\"\n]\n\npattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, ethnicities)) + r\")\\b\", flags=re.IGNORECASE)\n\n# Mask in any string\ndef mask_ethnicity(tokens):\n    masked = []\n    for tok in tokens:\n        masked.append(pattern.sub(\"[MASK]\", tok))\n        \n    return masked\n\n\nexample_word = [\"chinaman\", \"chinese women\"]\n\nmask_ethnicity(example_word)\n\n\ntokens = mask_ethnicity(tokens)\n\n# Get unique words to avoid redundant computation\nunique_tokens = list(set(tokens))\n\n\n# Include the word \"chinese\" as our target\nunique_tokens.append(\"chinese\")\n\n# Print the shape of unique tokens\nprint(f'There are {len(unique_tokens)} unique tokens in this corpus.')\n\n\n# Prepare a dictionary to store word embeddings\nbert_word_embeddings = {}\n\n# For each word, get its BERT embedding by feeding it as a single-token input\nfor word in unique_tokens:\n    word_inputs = tokenizer(word, return_tensors='pt', truncation=True, max_length=10)\n    with torch.no_grad():\n        word_outputs = bert_model(**word_inputs)\n        # Use the [CLS] token embedding as the word embedding\n        word_embedding = word_outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n        bert_word_embeddings[word] = word_embedding\n\n\n# Print embedding for the word of interest 'chinese'\n\nprint(f\"BERT embedding for 'chinese':\\n{bert_word_embeddings.get('chinese')}\")\n\n\n# Compute cosine similarity between all words with Chinese in the model\nfrom scipy.spatial.distance import cosine\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    if other_word != \"chinese\":\n        similarity = 1 - cosine(bert_word_embeddings[\"chinese\"], bert_word_embeddings[other_word])\n        similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'chinese':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    if other_word != \"commerce\":\n        similarity = 1 - cosine(bert_word_embeddings[\"commerce\"], bert_word_embeddings[other_word])\n        similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'commerce':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\nemd = np.array(bert_word_embeddings.get('chinese')) - np.array(bert_word_embeddings.get('alien'))\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    similarity = 1 - cosine(emd, bert_word_embeddings[other_word])\n    similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'chinese - alien':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\n# Generate a 2D PCA for visualiaztion\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\n\nword_embeddings = np.array(list(bert_word_embeddings.values()))\npca_results = pca.fit_transform(word_embeddings)\n\n\nimport plotly.express as px\ndf_pca = pandas.DataFrame(pca_results, columns = ['x', 'y'])\ndf_pca['word'] = list(bert_word_embeddings.keys())\n# Highlight the word 'chinese' in the plot\ndf_pca['highlight'] = df_pca['word'].apply(lambda x: 'chinese' if x == 'chinese' else '')\n\nfig = px.scatter(\n    df_pca,\n    x='x',\n    y='y',\n    title=' Visualization of 2D PCA of the legal-BERT Word Embeddings',\n    color='highlight',                        \n    hover_data=['word'], \n    text= 'highlight'\n)\n\nfig.show()\n\n\n# Generate a t-SNE plot for visualization\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=42)\n\ntsne_results = tsne.fit_transform(word_embeddings)\n\n\n# Create a DataFrame for visualization\ndf_tsne = pandas.DataFrame(tsne_results, columns=['x', 'y'])\ndf_tsne['word'] = list(bert_word_embeddings.keys())\n# Highlight the word 'chinese' in the plot\ndf_tsne['highlight'] = df_tsne['word'].apply(lambda x: 'chinese' if x == 'chinese' else '')\n\nfig = px.scatter(\n    df_tsne,\n    x='x',\n    y='y',\n    title='t-SNE Visualization of legal-BERT Word Embeddings',\n    color='highlight',                        \n    hover_data=['word'], \n    text= 'highlight'\n)\n\nfig.show()\n\n\n\nSentence Embeddings\n\nfrom pathlib import Path\n\n# Read the txt file as lines\nlines = Path(\"data/Regina_V_Wing_Chong.txt\").read_text(encoding=\"utf-8\").splitlines()\n\n# Extract line 67 as the target\ntarget = lines[91]\nprint(\"Line 91:\", target)\n\n\nparagraphs = [p.strip() for p in full_text.split(\"\\n\\n\") if p.strip()]\n\nfor paragraph in paragraphs[:5]:\n    print(paragraph)\n\n\nfrom sentence_transformers import SentenceTransformer\n\n# Import the sentence transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Calculate embeddings by calling model.encode()\nparagraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)\nprint(paragraph_embeddings.shape)\n\n\n# We also want to encode the target separately\ntarget_embedding = model.encode(target, convert_to_tensor=True)\n\n\nimport torch\nfrom torch.nn.functional import cosine_similarity\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), paragraph_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\ntop_paragraphs = []\n\nfor score, idx in zip(topk.values, topk.indices):\n    top_paragraphs.append(paragraphs[idx])\n    print(f\"{score:.4f}\\t{paragraphs[idx]}\")\n\n\nimport spacy\n\n# Tokenize the text into sentences\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(full_text)\nsentences = [sent.text.strip() for sent in doc.sents]\n\nprint(sentences)\n\n\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\nprint(sentence_embeddings.shape)\n\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), sentence_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\nfor score, idx in zip(topk.values, topk.indices):\n    print(f\"{score:.4f}\\t{sentences[idx]}\")\n\nWe apply a trained model to mask key words related to ethnicity and nationality identities.\n\nfrom transformers import pipeline\nimport numpy\n\nner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n\ndef mask_ethnicity_hf(text):\n    entities = ner(text)\n    spans_to_mask = [e for e in entities if e[\"entity_group\"] == \"MISC\" or e[\"entity_group\"] == \"ORG\" or e[\"entity_group\"] == \"PER\" or e[\"entity_group\"] == \"LOC\" or e[\"entity_group\"] == \"NORP\"]\n    # typically nationality is in MISC or NORP depending on the model\n    masked = text\n    for ent in sorted(spans_to_mask, key=lambda e: e[\"start\"], reverse=True):\n        masked = masked[:ent[\"start\"]] + \"[MASK]\" + masked[ent[\"end\"]:]\n    return masked\n\n\ndef mask_ethnicity(texts):\n    masked_list = []\n    for sent in texts:\n        sent = mask_ethnicity_hf(sent)\n        masked_list.append(sent)\n        \n    return masked_list\n\n\n# Example output applying this pre-trained model\nexample_text = \"\"\"And when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, \nfrom every state and every city, we will be able to speed up that day when all of God's children, Black men and white men, \nJews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. \nFree at last. Thank God almighty, we are free at last.\"\"\"\n\nmasked_example = mask_ethnicity_hf(example_text)\n\nprint(masked_example)\n\n\nmasked_paragraphs = mask_ethnicity(paragraphs)\n\nmasked_paragraphs[40]\n\n\n# Calculate embeddings by calling model.encode()\nmasked_paragraph_embeddings = model.encode(masked_paragraphs, convert_to_tensor=True)\nprint(masked_paragraph_embeddings.shape)\n\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), masked_paragraph_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\ntop_masked_paragraphs = []\n\nfor score, idx in zip(topk.values, topk.indices):\n    top_masked_paragraphs.append(masked_paragraphs[idx])\n    print(f\"{score:.4f}\\t{masked_paragraphs[idx]}\")\n\n\n\nNatural Language Inference\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n\n# Choose a strong NLI model\nmodel_name = \"lexlms/legal-roberta-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel     = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Create an NLI pipeline\nnli = pipeline(\n    \"text-classification\",\n    model=model,\n    tokenizer=tokenizer,\n    device=-1,                   \n    return_all_scores=True        \n)\n\n# Define the premise\npremise = \"Chinese immigrants should enjoy equal rights and legal protections.\"\n\nresults = []\nfor sent in top_masked_paragraphs:\n    inputs = tokenizer.encode_plus(premise, sent, return_tensors=\"pt\", truncation=True)\n    out = model(**inputs).logits.softmax(dim=-1).tolist()[0]\n    label_idx = out.index(max(out))\n    label = [\"FAVOR\", \"NEUTRAL\", \"AGAINST\"][label_idx]\n    results.append((sent, label, dict(zip([\"favor\",\"neutral\",\"against\"], out))))\n\n# Print stance results\nfor sent, label, probs in results:\n    print(f\"{label.lower():&gt;12}  {probs[label.lower()]:.2f}  -&gt;  {sent}\")\n\n\nfrom transformers import pipeline\nimport pandas as pd\n\n# Load the MNLI‑based zero‑shot classifier\nclassifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"facebook/bart-large-mnli\",\n    device=-1\n)\n\n# Use the NLI labels as your “candidate labels”\ncandidate_labels = [\"entailment\", \"neutral\", \"contradiction\"]\n\nrecords = []\nfor para in paragraphs:\n    out = classifier(\n        sequences=para,\n        candidate_labels=candidate_labels,\n        hypothesis_template=\"Given the context that the texts for classification are from a legal ruling in 1885, this paragraph is {} of the premise 'Chinese immigrants should enjoy equal rights and legal protections'.\"\n    )\n    # out['labels'] is sorted by score descending\n    scores = dict(zip(out[\"labels\"], out[\"scores\"]))\n    pred = out[\"labels\"][0]\n\n    records.append({\n        \"paragraph\": para,\n        \"entailment\":   scores.get(\"entailment\", 0.0),\n        \"neutral\":      scores.get(\"neutral\",    0.0),\n        \"contradiction\":scores.get(\"contradiction\", 0.0),\n        \"predicted\":    pred\n    })\n\n#  Build a DataFrame\ndf_nli = pd.DataFrame(records)\n\n# Inspect the first few rows\nprint(df_nli.head())\n\n\ndf_nli.shape\n\n\ncounts = df_nli['predicted'].value_counts()\n\nproportions = df_nli['predicted'].value_counts(normalize=True)\n\nresult = pd.DataFrame({\n    'count': counts,\n    'proportion': proportions\n})\n\nprint(result)\n\n\n\nTopic Modelling Through BERTopic\n\nfrom bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\nvectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1,2), max_df=0.85, min_df=2)\n\ntopic_model = BERTopic(\n    vectorizer_model=vectorizer, \n    ctfidf_model= ctfidf_model\n)\n\ntopics, probs = topic_model.fit_transform(paragraphs)\n\ndf_topic = topic_model.get_topic_info()\nprint(df_topic)\n\n\nfor para in df_topic[\"Representative_Docs\"][1]:\n    print(para)\n\n\nrep_list = []\n\nfor list in df_topic[\"Representation\"]:\n    rep_list.extend(list)\n    \nprint(rep_list)\n\n\ntopic_labels = topic_model.generate_topic_labels(\n    nr_words=3,       \n    separator=\" \",     \n    topic_prefix=False \n)\n\ntopic_list = []\n\nfor label in topic_labels:\n    phrases = label.split(\" \")\n    topic_list.extend(phrases)\n    \nprint(topic_list)\n\n\nfrom umap import UMAP\nimport pandas as pd\nimport plotly.express as px\n\n# Get your topic-term embeddings\nembeddings = topic_model.c_tf_idf_.toarray()\n\n# 1) Build a UMAP reducer with random init\numap_model = UMAP(\n    n_neighbors=15,\n    n_components=2,\n    metric=\"cosine\",\n    init=\"random\",\n    random_state=42\n)\nreduced_embeddings = umap_model.fit_transform(embeddings)\n\n# 2) Build a DataFrame and scatter\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\ndf[\"topic\"] = topic_model.get_topic_info()[\"Topic\"].values\n\nfig = px.scatter(\n    df,\n    x=\"x\",\n    y=\"y\",\n    text=\"topic\",\n    title=\"Topic visualization\"\n)\nfig.show()\n\n\nfrom sentence_transformers import SentenceTransformer\n\nembedding_model = SentenceTransformer(\"nlpaueb/legal-bert-base-uncased\")\n\n\nfrom bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\n\ntopic_model = BERTopic(embedding_model=embedding_model,\n                       vectorizer_model= vectorizer,\n                       ctfidf_model= ctfidf_model)\n\ntopics, probs = topic_model.fit_transform(masked_paragraphs)\n\ndf_topic = topic_model.get_topic_info()\nprint(df_topic)\n\n\nfor para in df_topic['Representative_Docs']:\n    print(para)\n\n\ntopic_labels = topic_model.generate_topic_labels(\n    nr_words=5,       \n    separator=\" \",     \n    topic_prefix=False \n)\n\ntopic_labels\n\n\nfrom umap import UMAP\nimport pandas as pd\nimport plotly.express as px\n\n# Get your topic-term embeddings\nembeddings = topic_model.c_tf_idf_.toarray()\n\n# 1) Build a UMAP reducer with random init\numap_model = UMAP(\n    n_neighbors=15,\n    n_components=2,\n    metric=\"cosine\",\n    init=\"random\",\n    random_state=42\n)\nreduced_embeddings = umap_model.fit_transform(embeddings)\n\n# 2) Build a DataFrame and scatter\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\ndf[\"topic\"] = topic_model.get_topic_info()[\"Topic\"].values\n\nfig = px.scatter(\n    df,\n    x=\"x\",\n    y=\"y\",\n    text=\"topic\",\n    title=\"Topic visualization\"\n)\nfig.show()"
  },
  {
    "objectID": "docs/SOCI-217/SOCI217-notebook-1.html",
    "href": "docs/SOCI-217/SOCI217-notebook-1.html",
    "title": "SOCI 217 Notebook 1",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec interdum porttitor lectus in scelerisque. Praesent euismod turpis in purus laoreet, at semper metus mattis. In hac habitasse platea dictumst. Aliquam euismod, metus quis tincidunt aliquet, magna felis posuere magna, et scelerisque massa sem non odio. Vestibulum justo eros, tempor quis nisl in, sagittis dictum est. Cras non ultrices est. Sed imperdiet venenatis felis pulvinar ultrices. Vestibulum lacinia id arcu in ultrices. Pellentesque lacinia odio in diam suscipit eleifend. Praesent non massa placerat sem luctus tincidunt. Nullam egestas sed dui quis elementum. Morbi eget lacus elit. Nulla sollicitudin, orci id viverra tincidunt, lacus dolor rutrum metus, nec bibendum felis nibh sit amet nisl.\nSuspendisse potenti. Morbi quis dignissim erat. Integer volutpat sem erat, sit amet luctus metus lacinia vel. Maecenas ut nunc sit amet ipsum commodo accumsan. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Mauris lorem odio, faucibus at mi eu, pretium hendrerit erat. Vestibulum pulvinar molestie lacus, vel facilisis dui porta eu. Vestibulum iaculis finibus hendrerit. Sed elementum enim justo, ac hendrerit elit tristique vitae. Phasellus velit ipsum, commodo semper mi vel, posuere fermentum felis. Proin vestibulum ligula lacus, eget facilisis eros ultrices eget.\nNulla eu posuere lacus, eget interdum magna. Donec finibus pretium eros, et tincidunt turpis ultricies eu. Duis facilisis pulvinar est id viverra. Nunc mollis velit non orci rutrum hendrerit. Maecenas pretium id velit at lobortis. Praesent mollis ipsum eros, nec hendrerit sem pretium nec. Cras suscipit, sapien nec dapibus molestie, libero urna hendrerit erat, eget gravida sapien odio at ante. Praesent mattis dui nunc, vel pharetra sem lacinia vitae. Nunc quis nibh orci. Quisque tincidunt finibus libero, a aliquam leo pellentesque eget. Suspendisse molestie libero a augue hendrerit consequat. Fusce quis mattis est, nec vulputate justo. Sed ut tempor risus. Aliquam commodo eget orci luctus lacinia. Nulla vulputate leo ut arcu tincidunt rhoncus.\nVivamus a ultricies sem, non finibus risus. Donec ac tempus ante. Donec lobortis purus vitae diam rhoncus, id tincidunt arcu scelerisque. Maecenas id est consectetur, consectetur ipsum vel, cursus nunc. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eu nisl hendrerit, sagittis orci in, lacinia turpis. Vestibulum eu tellus rhoncus, elementum mauris ornare, efficitur erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed gravida libero quis pulvinar fermentum. Vestibulum et nisi euismod nulla efficitur gravida in vitae nunc.\nFusce blandit suscipit laoreet. Ut convallis dapibus justo et iaculis. Etiam eu justo bibendum, viverra mi eget, tristique sapien. Etiam vel iaculis magna. Praesent quis mollis quam. In consequat, mi vitae scelerisque bibendum, diam ipsum tincidunt sapien, quis viverra nunc mauris ac turpis. Aliquam in diam consectetur, congue orci ac, blandit justo. Vestibulum et porttitor justo. Quisque ac molestie lacus. Proin sed commodo nunc, in ultrices tortor. Quisque et ornare enim. Donec sed mi bibendum, viverra massa in, finibus odio. Sed a massa eget nulla ornare finibus.\nAliquam nec orci nec tortor fermentum tincidunt. Aliquam blandit euismod mauris, et aliquet turpis molestie in. Phasellus sagittis, velit quis imperdiet porta, ipsum risus facilisis velit, ullamcorper tempus turpis lacus a diam. Aenean magna nunc, pulvinar et sollicitudin at, faucibus in elit. Cras congue, dolor et vulputate fermentum, lacus odio interdum ante, ut convallis risus lorem id augue. Nulla sodales orci vitae dolor sollicitudin, at gravida tellus aliquam. Phasellus eget semper urna, nec eleifend libero.\nCras luctus dolor eget quam laoreet aliquet. Praesent et lectus maximus, sagittis nibh at, eleifend lacus. Fusce nec metus sodales, pharetra nibh ac, maximus lorem. Maecenas imperdiet interdum ligula et imperdiet. Vestibulum ullamcorper quam id tempor consequat. Donec non nibh nec eros semper tristique nec nec mi. Fusce massa risus, venenatis at metus at, imperdiet elementum erat. Cras sit amet laoreet dui, non pulvinar ipsum. Sed condimentum sem ut ex rutrum, sed congue orci pellentesque. Donec venenatis ultrices tincidunt. Quisque scelerisque nulla non congue pretium. Maecenas rutrum sagittis ex at ullamcorper. Vestibulum non risus felis. Sed varius, mi placerat condimentum faucibus, sem ex bibendum neque, eu pulvinar tortor ipsum ac metus. Integer eu feugiat nibh, sit amet elementum erat. Aenean ac nunc felis.\nIn lacinia nec turpis nec vehicula. Donec sollicitudin suscipit risus, tincidunt lobortis odio cursus sit amet. Ut blandit convallis pharetra. Donec a lacus viverra, interdum dolor sed, vehicula neque. Cras in bibendum eros. Sed gravida commodo magna. Pellentesque nec metus sit amet diam gravida euismod. Cras iaculis odio eget consectetur lacinia.\nSuspendisse eu velit ultrices, imperdiet quam fermentum, auctor ante. Phasellus commodo, ipsum in facilisis interdum, odio tortor vehicula velit, quis varius erat ipsum in urna. Nam commodo sapien nulla, commodo vestibulum nisl feugiat non. Proin sit amet elit a elit ullamcorper aliquam at non urna. Vivamus non ullamcorper neque. Nam a erat fermentum, tempor urna finibus, molestie dolor. Sed mattis dolor sem, vitae volutpat lorem rutrum posuere. In vitae porttitor velit. Pellentesque pulvinar dolor at hendrerit suscipit.\nDonec at elit ex. Sed diam nibh, varius eget accumsan non, hendrerit vitae dolor. Integer at odio in ipsum luctus imperdiet. Vestibulum et massa vel magna commodo bibendum quis ut erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Integer condimentum nunc quis lorem sollicitudin, at tincidunt lorem pretium. In bibendum tincidunt augue, et auctor tortor tristique ut."
  },
  {
    "objectID": "docs/SOCI-217/SOCI217-notebook-1.html#testing-the-render",
    "href": "docs/SOCI-217/SOCI217-notebook-1.html#testing-the-render",
    "title": "SOCI 217 Notebook 1",
    "section": "TESTING THE RENDER",
    "text": "TESTING THE RENDER"
  },
  {
    "objectID": "docs/SOCI-217/SOCI217-notebook-3.html",
    "href": "docs/SOCI-217/SOCI217-notebook-3.html",
    "title": "SOCI217 Notebook 3",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec interdum porttitor lectus in scelerisque. Praesent euismod turpis in purus laoreet, at semper metus mattis. In hac habitasse platea dictumst. Aliquam euismod, metus quis tincidunt aliquet, magna felis posuere magna, et scelerisque massa sem non odio. Vestibulum justo eros, tempor quis nisl in, sagittis dictum est. Cras non ultrices est. Sed imperdiet venenatis felis pulvinar ultrices. Vestibulum lacinia id arcu in ultrices. Pellentesque lacinia odio in diam suscipit eleifend. Praesent non massa placerat sem luctus tincidunt. Nullam egestas sed dui quis elementum. Morbi eget lacus elit. Nulla sollicitudin, orci id viverra tincidunt, lacus dolor rutrum metus, nec bibendum felis nibh sit amet nisl.\nSuspendisse potenti. Morbi quis dignissim erat. Integer volutpat sem erat, sit amet luctus metus lacinia vel. Maecenas ut nunc sit amet ipsum commodo accumsan. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Mauris lorem odio, faucibus at mi eu, pretium hendrerit erat. Vestibulum pulvinar molestie lacus, vel facilisis dui porta eu. Vestibulum iaculis finibus hendrerit. Sed elementum enim justo, ac hendrerit elit tristique vitae. Phasellus velit ipsum, commodo semper mi vel, posuere fermentum felis. Proin vestibulum ligula lacus, eget facilisis eros ultrices eget.\nNulla eu posuere lacus, eget interdum magna. Donec finibus pretium eros, et tincidunt turpis ultricies eu. Duis facilisis pulvinar est id viverra. Nunc mollis velit non orci rutrum hendrerit. Maecenas pretium id velit at lobortis. Praesent mollis ipsum eros, nec hendrerit sem pretium nec. Cras suscipit, sapien nec dapibus molestie, libero urna hendrerit erat, eget gravida sapien odio at ante. Praesent mattis dui nunc, vel pharetra sem lacinia vitae. Nunc quis nibh orci. Quisque tincidunt finibus libero, a aliquam leo pellentesque eget. Suspendisse molestie libero a augue hendrerit consequat. Fusce quis mattis est, nec vulputate justo. Sed ut tempor risus. Aliquam commodo eget orci luctus lacinia. Nulla vulputate leo ut arcu tincidunt rhoncus.\nVivamus a ultricies sem, non finibus risus. Donec ac tempus ante. Donec lobortis purus vitae diam rhoncus, id tincidunt arcu scelerisque. Maecenas id est consectetur, consectetur ipsum vel, cursus nunc. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eu nisl hendrerit, sagittis orci in, lacinia turpis. Vestibulum eu tellus rhoncus, elementum mauris ornare, efficitur erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed gravida libero quis pulvinar fermentum. Vestibulum et nisi euismod nulla efficitur gravida in vitae nunc.\nFusce blandit suscipit laoreet. Ut convallis dapibus justo et iaculis. Etiam eu justo bibendum, viverra mi eget, tristique sapien. Etiam vel iaculis magna. Praesent quis mollis quam. In consequat, mi vitae scelerisque bibendum, diam ipsum tincidunt sapien, quis viverra nunc mauris ac turpis. Aliquam in diam consectetur, congue orci ac, blandit justo. Vestibulum et porttitor justo. Quisque ac molestie lacus. Proin sed commodo nunc, in ultrices tortor. Quisque et ornare enim. Donec sed mi bibendum, viverra massa in, finibus odio. Sed a massa eget nulla ornare finibus.\nAliquam nec orci nec tortor fermentum tincidunt. Aliquam blandit euismod mauris, et aliquet turpis molestie in. Phasellus sagittis, velit quis imperdiet porta, ipsum risus facilisis velit, ullamcorper tempus turpis lacus a diam. Aenean magna nunc, pulvinar et sollicitudin at, faucibus in elit. Cras congue, dolor et vulputate fermentum, lacus odio interdum ante, ut convallis risus lorem id augue. Nulla sodales orci vitae dolor sollicitudin, at gravida tellus aliquam. Phasellus eget semper urna, nec eleifend libero.\nCras luctus dolor eget quam laoreet aliquet. Praesent et lectus maximus, sagittis nibh at, eleifend lacus. Fusce nec metus sodales, pharetra nibh ac, maximus lorem. Maecenas imperdiet interdum ligula et imperdiet. Vestibulum ullamcorper quam id tempor consequat. Donec non nibh nec eros semper tristique nec nec mi. Fusce massa risus, venenatis at metus at, imperdiet elementum erat. Cras sit amet laoreet dui, non pulvinar ipsum. Sed condimentum sem ut ex rutrum, sed congue orci pellentesque. Donec venenatis ultrices tincidunt. Quisque scelerisque nulla non congue pretium. Maecenas rutrum sagittis ex at ullamcorper. Vestibulum non risus felis. Sed varius, mi placerat condimentum faucibus, sem ex bibendum neque, eu pulvinar tortor ipsum ac metus. Integer eu feugiat nibh, sit amet elementum erat. Aenean ac nunc felis.\nIn lacinia nec turpis nec vehicula. Donec sollicitudin suscipit risus, tincidunt lobortis odio cursus sit amet. Ut blandit convallis pharetra. Donec a lacus viverra, interdum dolor sed, vehicula neque. Cras in bibendum eros. Sed gravida commodo magna. Pellentesque nec metus sit amet diam gravida euismod. Cras iaculis odio eget consectetur lacinia.\nSuspendisse eu velit ultrices, imperdiet quam fermentum, auctor ante. Phasellus commodo, ipsum in facilisis interdum, odio tortor vehicula velit, quis varius erat ipsum in urna. Nam commodo sapien nulla, commodo vestibulum nisl feugiat non. Proin sit amet elit a elit ullamcorper aliquam at non urna. Vivamus non ullamcorper neque. Nam a erat fermentum, tempor urna finibus, molestie dolor. Sed mattis dolor sem, vitae volutpat lorem rutrum posuere. In vitae porttitor velit. Pellentesque pulvinar dolor at hendrerit suscipit.\nDonec at elit ex. Sed diam nibh, varius eget accumsan non, hendrerit vitae dolor. Integer at odio in ipsum luctus imperdiet. Vestibulum et massa vel magna commodo bibendum quis ut erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Integer condimentum nunc quis lorem sollicitudin, at tincidunt lorem pretium. In bibendum tincidunt augue, et auctor tortor tristique ut."
  },
  {
    "objectID": "docs/SOCI-217/SOCI217-notebook-3.html#notebook-3",
    "href": "docs/SOCI-217/SOCI217-notebook-3.html#notebook-3",
    "title": "SOCI217 Notebook 3",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec interdum porttitor lectus in scelerisque. Praesent euismod turpis in purus laoreet, at semper metus mattis. In hac habitasse platea dictumst. Aliquam euismod, metus quis tincidunt aliquet, magna felis posuere magna, et scelerisque massa sem non odio. Vestibulum justo eros, tempor quis nisl in, sagittis dictum est. Cras non ultrices est. Sed imperdiet venenatis felis pulvinar ultrices. Vestibulum lacinia id arcu in ultrices. Pellentesque lacinia odio in diam suscipit eleifend. Praesent non massa placerat sem luctus tincidunt. Nullam egestas sed dui quis elementum. Morbi eget lacus elit. Nulla sollicitudin, orci id viverra tincidunt, lacus dolor rutrum metus, nec bibendum felis nibh sit amet nisl.\nSuspendisse potenti. Morbi quis dignissim erat. Integer volutpat sem erat, sit amet luctus metus lacinia vel. Maecenas ut nunc sit amet ipsum commodo accumsan. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Mauris lorem odio, faucibus at mi eu, pretium hendrerit erat. Vestibulum pulvinar molestie lacus, vel facilisis dui porta eu. Vestibulum iaculis finibus hendrerit. Sed elementum enim justo, ac hendrerit elit tristique vitae. Phasellus velit ipsum, commodo semper mi vel, posuere fermentum felis. Proin vestibulum ligula lacus, eget facilisis eros ultrices eget.\nNulla eu posuere lacus, eget interdum magna. Donec finibus pretium eros, et tincidunt turpis ultricies eu. Duis facilisis pulvinar est id viverra. Nunc mollis velit non orci rutrum hendrerit. Maecenas pretium id velit at lobortis. Praesent mollis ipsum eros, nec hendrerit sem pretium nec. Cras suscipit, sapien nec dapibus molestie, libero urna hendrerit erat, eget gravida sapien odio at ante. Praesent mattis dui nunc, vel pharetra sem lacinia vitae. Nunc quis nibh orci. Quisque tincidunt finibus libero, a aliquam leo pellentesque eget. Suspendisse molestie libero a augue hendrerit consequat. Fusce quis mattis est, nec vulputate justo. Sed ut tempor risus. Aliquam commodo eget orci luctus lacinia. Nulla vulputate leo ut arcu tincidunt rhoncus.\nVivamus a ultricies sem, non finibus risus. Donec ac tempus ante. Donec lobortis purus vitae diam rhoncus, id tincidunt arcu scelerisque. Maecenas id est consectetur, consectetur ipsum vel, cursus nunc. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Donec eu nisl hendrerit, sagittis orci in, lacinia turpis. Vestibulum eu tellus rhoncus, elementum mauris ornare, efficitur erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed gravida libero quis pulvinar fermentum. Vestibulum et nisi euismod nulla efficitur gravida in vitae nunc.\nFusce blandit suscipit laoreet. Ut convallis dapibus justo et iaculis. Etiam eu justo bibendum, viverra mi eget, tristique sapien. Etiam vel iaculis magna. Praesent quis mollis quam. In consequat, mi vitae scelerisque bibendum, diam ipsum tincidunt sapien, quis viverra nunc mauris ac turpis. Aliquam in diam consectetur, congue orci ac, blandit justo. Vestibulum et porttitor justo. Quisque ac molestie lacus. Proin sed commodo nunc, in ultrices tortor. Quisque et ornare enim. Donec sed mi bibendum, viverra massa in, finibus odio. Sed a massa eget nulla ornare finibus.\nAliquam nec orci nec tortor fermentum tincidunt. Aliquam blandit euismod mauris, et aliquet turpis molestie in. Phasellus sagittis, velit quis imperdiet porta, ipsum risus facilisis velit, ullamcorper tempus turpis lacus a diam. Aenean magna nunc, pulvinar et sollicitudin at, faucibus in elit. Cras congue, dolor et vulputate fermentum, lacus odio interdum ante, ut convallis risus lorem id augue. Nulla sodales orci vitae dolor sollicitudin, at gravida tellus aliquam. Phasellus eget semper urna, nec eleifend libero.\nCras luctus dolor eget quam laoreet aliquet. Praesent et lectus maximus, sagittis nibh at, eleifend lacus. Fusce nec metus sodales, pharetra nibh ac, maximus lorem. Maecenas imperdiet interdum ligula et imperdiet. Vestibulum ullamcorper quam id tempor consequat. Donec non nibh nec eros semper tristique nec nec mi. Fusce massa risus, venenatis at metus at, imperdiet elementum erat. Cras sit amet laoreet dui, non pulvinar ipsum. Sed condimentum sem ut ex rutrum, sed congue orci pellentesque. Donec venenatis ultrices tincidunt. Quisque scelerisque nulla non congue pretium. Maecenas rutrum sagittis ex at ullamcorper. Vestibulum non risus felis. Sed varius, mi placerat condimentum faucibus, sem ex bibendum neque, eu pulvinar tortor ipsum ac metus. Integer eu feugiat nibh, sit amet elementum erat. Aenean ac nunc felis.\nIn lacinia nec turpis nec vehicula. Donec sollicitudin suscipit risus, tincidunt lobortis odio cursus sit amet. Ut blandit convallis pharetra. Donec a lacus viverra, interdum dolor sed, vehicula neque. Cras in bibendum eros. Sed gravida commodo magna. Pellentesque nec metus sit amet diam gravida euismod. Cras iaculis odio eget consectetur lacinia.\nSuspendisse eu velit ultrices, imperdiet quam fermentum, auctor ante. Phasellus commodo, ipsum in facilisis interdum, odio tortor vehicula velit, quis varius erat ipsum in urna. Nam commodo sapien nulla, commodo vestibulum nisl feugiat non. Proin sit amet elit a elit ullamcorper aliquam at non urna. Vivamus non ullamcorper neque. Nam a erat fermentum, tempor urna finibus, molestie dolor. Sed mattis dolor sem, vitae volutpat lorem rutrum posuere. In vitae porttitor velit. Pellentesque pulvinar dolor at hendrerit suscipit.\nDonec at elit ex. Sed diam nibh, varius eget accumsan non, hendrerit vitae dolor. Integer at odio in ipsum luctus imperdiet. Vestibulum et massa vel magna commodo bibendum quis ut erat. Interdum et malesuada fames ac ante ipsum primis in faucibus. Integer condimentum nunc quis lorem sollicitudin, at tincidunt lorem pretium. In bibendum tincidunt augue, et auctor tortor tristique ut."
  },
  {
    "objectID": "docs/SOCI-217/SOCI217-notebook-3.html#testing-the-render",
    "href": "docs/SOCI-217/SOCI217-notebook-3.html#testing-the-render",
    "title": "SOCI217 Notebook 3",
    "section": "TESTING THE RENDER",
    "text": "TESTING THE RENDER"
  },
  {
    "objectID": "docs/SOCI-415/SOCI415-notebook-1.html",
    "href": "docs/SOCI-415/SOCI415-notebook-1.html",
    "title": "SOCI 415 Network Analysis",
    "section": "",
    "text": "This notebook introduces key concepts in network analysis pertaining to sociology and provides a hands-on tutorial to using the NetworkX Python library."
  },
  {
    "objectID": "docs/SOCI-415/SOCI415-notebook-1.html#what-is-network-analysis",
    "href": "docs/SOCI-415/SOCI415-notebook-1.html#what-is-network-analysis",
    "title": "SOCI 415 Network Analysis",
    "section": "1. What is Network analysis?",
    "text": "1. What is Network analysis?\nNetwork Analysis is a set of techniques used to study the structure and dynamics of networks. Networks are collections of objects/locations/entities (called nodes) connected by relationships (called edges). Network analysis has applications in many fields, including sociology, biology, economics, computer science, and more.\n\n\n\nRegional networks of ceramic similarity across time in the greater Arizona/New Mexico area of the US (From Mills et al. 2013, Fig. 2)\n\n\nNetwork science in Sociology Network analysis in sociology is the systematic study of social structure through the mapping and measurement of relationships among individuals, groups, or organizations. It is not a discipline on its own, but rather a methodological and theoretical approach within sociology that helps conceptualize, describe, and model society as interconnected sets of actors linked by specific relationships. Network analysis in sociology is a core tool for understanding how relationships and social structures impact individuals and groups,\n\n1.1 Key terms\n\n\n\n\n\nAn example of a graph with nodes and edges.\n\n\n\nNode: A node is a representation of an individual entity or actor in a network. In different contexts, nodes can be people, organizations, cities, or any other unit of analysis.\nEdge: An edge represents the relationship or connection between two nodes. Edges can be directed (having a specific direction from one node to another) or undirected (no direction, implying a mutual relationship).\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nDegree: The degree of a node is the number of edges connected to it. In directed networks, this can be further divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges).\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nThe network above is an example of a Undirected graph, a graph with no direction. This means that if there is a connection between node A and node B, it is bidirectional - A is connected to B, and B is connected to A.\nThe example to the left is a directed graph: the edges between nodes have a specific direction. This means that if there is an edge from node A to node B, it does not imply there is an edge from B to A unless explicitly stated.\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nDensity: Density is a measure that indicates how closely connected the nodes in a network are. Specifically, it refers to the ratio of the number of actual edges in the network to the maximum possible number of edges between nodes.\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nCentrality: Centrality measures the importance, influence, or prominence of nodes (entities) within a network. The centrality of a node tells us how “important” a node is to the aggregate network. There are many different kinds of centrality, but the four most well-known ones are degree, betweenness, closeness, and eigenvector centrality. This notebook will primarily focus on the first three.\n\n\n\n\n1.2 NetworkX\nNetworkX is a Python library that is used for the creation, manipulation, and visualization of complex networks. It provides tools to work with both undirected and directed networks, perform network-related calculations, and visualize the results.\nA library in Python is a collection of code that makes everyday tasks more efficient. In this case working with networks becomes much simpler when using network X.\nIf you want to read the NetworkX documentation you can follow the NetworkX documentation link. This link shows what kind of commands exist within the NetworkX library.\n\n1.2.1 Importing NetworkX\nWe can import NetworkX using the import command. At the same time, we’ll also import the matplotlib.pyplot library, for plotting graphs. Additionally, we’ll import pandas for basic data wrangling, and numpy for math. The as command allows us to use networkx commands without needing to type out networkx each time. Additionally, we’ll import the community_louvain package for the louvain clustering algorithm. Along with some other libraries for our code to function.\n\nimport matplotlib.pyplot as plt #allows us to call the matplotlib.pyplot library as 'plt'\nimport matplotlib.patches as mpatches #imports mpatches matplotlib subpackage \nimport networkx as nx #allows us to call the networkx library as 'nx'\nimport pandas as pd #allows us to call the pandas library as 'pd'\nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\n\n\n\n1.2.2 Creating simple networks using NetworkX\nWe’ll start by creating a simple graph:\nBelow in the code we choose our nodes and edges between them.\n\nG = nx.Graph() #creates an empty network graph\n\nnodes = (1, 2, 3, 4, 5, 6) #our nodes, labeled 1,2,3,4,5,6.\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\n        #the connections between our nodes are stored in an array, containing pairs of numbers called tuples.\nG.add_edges_from(edges) #the `add_edges_from()` command adds edges to the network\nG.add_nodes_from(nodes) #the `add_nodes_from()` command adds nodes to the network\n\nnx.draw(G, with_labels = True) #renders the graph in the notebook\n        #the `with_labels = True` argument specifies that we want labels on the nodes.\n\nLet’s create a directed graph using nx.DiGraph(). We’ll also set our node positions using a seed: this will ensure that each time the nodes are rendered they hold the same position on the graph. You can set the seed to any number.\n\nG = nx.DiGraph() #creates an empty directed graph object\nnodes = (1, 2, 3, 4, 5, 6) #our nodes\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)] #our tuples stored in an array which represent our nodes\nG.add_edges_from(edges) #connects edges to nodes\nG.add_nodes_from(nodes) #connects edges to nodes\n\nposition = nx.spring_layout(G, seed=100)\n\n#nx.draw plots our network\nnx.draw(G, pos = position, with_labels = True) # `pos` argument assigns a position to each node\n\n\n\n\n1.3 Creating Random Graphs\nInstead of creating a graph with predetermined positions of nodes and edges we can also generate a random graph with a set amount of nodes and edges. Below you can change the amount of nodes and edges by changing n and d which correspond to the number of nodes and the degree (number of edges) that each node has. Creating a random graph could be more helpful for testing or when you want to try something and don’t wish to spend time plotting a real network and determining paths for all edges and nodes.\nThe first most basic command we will use is the nx.random_regular_graph command. Which generates a random regular graph.\n\n# Set a seed for reproducibility so that everytime the code runs we get the same random graph\nrandom.seed(42)\n\n# Parameters\nn = 20  # number of nodes\nd = 3   # degree of each node\n\n# Generate the random regular graph\nrr_graph = nx.random_regular_graph(d, n)\n\n# Visualize the graph, you can change the size, color, font and node size. \nplt.figure(figsize=(8, 6)) \nnx.draw(rr_graph, with_labels=True, node_color='lightgreen', node_size=500, font_size=10, font_weight='bold')\nplt.title(\"Random Regular Graph\")\nplt.show()\n\n# Print some basic information about the graph\nprint(f\"Number of nodes: {rr_graph.number_of_nodes()}\")\nprint(f\"Number of edges: {rr_graph.number_of_edges()}\")\nprint(f\"Degree of each node: {d}\")\n\nAnother option is using the Erdős-Rényi model which can be accessed using the nx.erdos_renyi_graph(n, p) command. This command has two inputs n and p. N is the number of nodes and p is the probability of edge creation to each node.\nThe Erdős–Rényi model refers to one of two closely related models for generating random graphs or the evolution of a random network. These models are named after Hungarian mathematicians Paul Erdős and Alfréd Rényi, who introduced one of the models in 1959\n\n# Set a seed for reproducibility so that everytime the code runs we get the same random graph\nrandom.seed(43)\n\n# Parameters\nn = 20  # number of nodes\np = 0.2  # probability of edge creation\n\n# Generate the Erdős-Rényi random graph\ner_graph = nx.erdos_renyi_graph(n, p)\n\n# Visualize the graph\nplt.figure(figsize=(8, 6))\nnx.draw(er_graph, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_weight='bold')\nplt.title(\"Erdős-Rényi Random Graph\")\nplt.show()\n\n# Print some basic information about the graph\nprint(f\"Number of nodes: {er_graph.number_of_nodes()}\")\nprint(f\"Number of edges: {er_graph.number_of_edges()}\")\nprint(f\"Average degree: {sum(dict(er_graph.degree()).values()) / n:.2f}\")\n\nThere are more commands in NetworkX to generate random graphs, but the two above demonstrate two common methods of random graph generations. The first being a set number of nodes and edges and the second being a set number of nodes and a probability of edge creation between them."
  },
  {
    "objectID": "docs/SOCI-415/SOCI415-notebook-1.html#degrees-density-and-weights",
    "href": "docs/SOCI-415/SOCI415-notebook-1.html#degrees-density-and-weights",
    "title": "SOCI 415 Network Analysis",
    "section": "2. Degrees, Density and Weights",
    "text": "2. Degrees, Density and Weights\n\n2.1 Degrees\nThe degree of a node is the number of edges that are connected to a node. The degree of a node \\(N\\) is denoted as \\(deg(N)\\). The maximum degree of a network \\(G\\) is denoted by \\(\\Delta(G)\\) and is the degree of the node with the highest degree in the network. Conversely, the minimum degree is denoted as \\(\\delta(G)\\).\n\nIf a node on a graph with \\(n\\) nodes has degree \\(n-1\\) it is called a dominating vertex. Not every graph has a dominating vertex.\n\nWe can see the degree of each node by running dict(G.degree()). This create a dictionary of key-value pairs for our network, where each key is the name of the node and the value is it’s respective degree.\n\ndegrees = dict(G.degree())\n\nIf we want to see the degree of node \\(n\\), we can do so by running print(degrees[n]). For instance:\n\nprint(degrees[1])\n\nLet’s color the nodes of our graph based on their degree. We’ll create a function called get_node_colors which takes in the degree dictionary of each node and returns a color. We’ll then create a for-loop that iterates over each nodes in the list of nodes, gets the color of each node using the get_node_colors function we defined earlier, and appends it to an empty list called color_map.\n\ndegrees = dict(G.degree())\nnodes = list(G.nodes())\n\ndef get_node_colors(degree):\n    if degree in [1, 2]:\n        return 'blue'\n    elif degree in [3, 4]:\n        return 'green'\n    elif degree in [5, 6]:\n        return 'yellow'\n    else:\n        return 'red' \n\ncolor_map = [] #`color_map` is an empty list\n\nfor node in nodes:\n  color = get_node_colors(degrees[node]) # get color of current node using node_colors according to degree of node\n  color_map.append(color) # appends color of each node to color_map for each node in nodes\n\nprint(degrees)\nprint(nodes)\nprint(color_map)\n\nThe \\(n\\)-th entry in color_map corresponds to the \\(n\\)-th node in nodes. For instance, color_map[0] returns the color of the first node (1).\n\ncolor_map[0]\n\nWe can now color the nodes of our graph, using the color map we defined above. The node_color argument takes in an array or list of colors that it uses to color each node.\n\nG = nx.DiGraph() # creates an empty directed graph object\nnodes = (1, 2, 3, 4, 5, 6) \nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nG.add_edges_from(edges) \nG.add_nodes_from(nodes) \n\nposition = nx.spring_layout(G, seed=100)\n\nnx.draw(G, pos = position, node_color=color_map, with_labels=True) \n    # node_color argument colors the nodes based on a given list or array of colors, \n    # with the first color corresponding to the first node, second to the second node, etc.\n\nLet’s also add a legend to our graph, which gives information about the meaning of each color. We’ll do this using the mpatches subpackage we imported earlier.\n\nblue_patch = mpatches.Patch(color='blue', label='1-2 edges') \ngreen_patch = mpatches.Patch(color='green', label='3-4 edges')\nyellow_patch = mpatches.Patch(color='yellow', label='5-6 edges')\nplt.legend(handles=[blue_patch, green_patch, yellow_patch]) #adds legend to the plot\n\nnx.draw(G, pos = position, node_color=color_map, with_labels=True)\n\n\n\n2.2 Density\nDensity is defined as:\n\\[\n\\text{Density} = \\frac{\\text{Number of Possible Edges}}{\\text{Number of Actual Edges}}\n​\\]\nIn an undirected graph, the total number of edges is \\(\\frac{V\\times(V-1)}{2}\\), where V is the total number of nodes. In a directed graph, the total number of edges is \\(V\\times(V-1)\\), because a connection between point A and point B can either be from point A to point B, or to point A from point B (hence multiplying by 2).\n\nNote that self-loops (edges from and to the same node) are counted in the total number of edges but not in the maximum number of edges so graphs can have a density greater than 1.\n\nThe formula for undirected graph density is:\n\\[\n\\frac{2E}{V(V-1)}\n\\]\nAnd for directed graphs, it is:\n\\[\n\\frac{E}{V(V-1)}\n\\]\nWhere \\(E\\) is the number of edges in our graph and \\(V\\) is the number of nodes.\nWe can calculate the density of our graph:\n\nnx.density(G)\n\n\n\n2.3 Weights\nOften times, you may end up working with weighted graphs: for instance, these weights could correspond to popularity of roads in road networks, or the size of pipes in a sewage network.\nWe’ll standardize our weights to be between 1 and 2 (as otherwise the results are messy). We’ll do this using a for-loop, like we did with the degrees.\n\nG_weights = nx.DiGraph() #creating a new graph object called G_weights\nnodes = [1, 2, 3, 4, 5, 6]\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nweights = [100, 50, 75, 50, 60, 100, 100, 75, 40, 50, 50, 100, 100] #add list of weights\nG_weights.add_edges_from(edges) \nG_weights.add_nodes_from(nodes) \n\nadjusted_weights = []\nfor weight in weights:\n    adjusted_weight = 1+ (max(weights)-weight)/(max(weights)-min(weights)) #standardizes weights to be between 1 and 2\n    adjusted_weights.append(adjusted_weight)\n\nposition = nx.spring_layout(G, seed=100)\n\nprint(adjusted_weights)\nnx.draw(G_weights, pos = position, width = adjusted_weights, with_labels = True) \n    # width argument take in a list or array of numbers corresponding to weights\n\nThis is great, but the results aren’t very clear. Let’s add a color gradient to the edges to represent different weights.\n\nnorm = plt.Normalize(min(weights), max(weights), clip=False) \n    #`plot.normalizes` normalizes the weights such that they are evenly distributed across the gradient spectrum\nedge_colors = plt.cm.Greys(norm(weights)) \n    # norm(weights) normalizes the weights \n    # plot.cm.greys() assigns the weights to color values\n    # edge_colors is a multidimensional array of RGBA color values corresponding to each edge\n\nfig, ax = plt.subplots() #explicitly specifying figure and axes in order to create a color bar\n\nnx.draw(G_weights, pos=position, edge_color=edge_colors, width=adjusted_weights, with_labels=True, ax=ax) \n    #ax = ax argument needed for color bar\n\n# Adding color bar\nsm = plt.cm.ScalarMappable(cmap=\"Greys\", norm=norm) # creates a scalarmappable object which acts \n                                                    # as a bridge between the numerical weight values and color map\nplt.colorbar(sm, ax=ax) #plotting color bar\n\n\n\n3. Adjacency matrices\nAn Adjacency matrix is a method of representing graphs in matrix form. In an adjacency matrix, the rows and columns correspond to the vertices (or nodes) of the graph. The entries of the matrix indicate whether pairs of vertices are adjacent or not in the graph. Normally, a value of 1 is assigned to entries where an edge is present, and 0 is assigned to entries where an edge is not. For a weighed graph, the weight of the edge is represented as a numerical value for entries where an edge is present.\nWe can convert our simple graph to an adjacency matrix:\n\nnx.to_pandas_adjacency(G)\n\nIf we want to use our weighted graph, we can use the following code:\n\n# len(edges) returns the total number of entries in the list of edges.\n# range(len(edges)): This generates a sequence of numbers from 0 to n-1 where n is len(edges), \n    #so the for-loop will run n times with i taking each value in that range, one at a time.\n\nfor i in range(len(edges)):\n    edge = edges[i] # retrieves the edge at position i in the list of edges\n    weight = weights[i] # retrieves the weight at position i in the list of weights\n    G_weights.add_edge(edge[0], edge[1], weight=weight) # adds an edge with a weight to the graph \n    \nnx.to_pandas_adjacency(G_weights, nodelist=nodes, weight='weight') #converts to pandas adjacency matrix with the weights in place\n\nWe can visualize our matrix using the code below. Note that instead of using nx.to_pandas_adjacency we use nx.to_numpy_array: this allows us to store the matrix in the form of an array.\n\nadj_matrix = nx.to_numpy_array(G_weights, nodelist=nodes, weight='weight')\n\n\nplt.figure(figsize=(8, 8)) #displays data as an image on a 2d raster; in our case, a numpy array\n\nplt.imshow(adj_matrix, cmap='gray_r')\n\nfor i in range(adj_matrix.shape[0]): #loops through each row of the matrix\n    for j in range(adj_matrix.shape[1]): #for each row, loops through each column of the matrix\n        plt.text(j, i, int(adj_matrix[i, j]),\n                 ha='center', va='center', color='red', size=30) #prints the value at that position in the matrix on the graph\n\nplt.title('Adjacency Matrix Visualization')\nplt.xlabel('Node Index')\nplt.ylabel('Node Index')"
  },
  {
    "objectID": "docs/SOCI-415/SOCI415-notebook-3.html",
    "href": "docs/SOCI-415/SOCI415-notebook-3.html",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "",
    "text": "The China Biographical Database Abstract: The China Biographical Database is a freely accessible relational database with biographical information about approximately 641,568 individuals as of August 2024, currently mainly from the 7th through 19th centuries. With both online and offline versions, the data is meant to be useful for statistical, social network, and spatial analysis as well as serving as a kind of biographical reference. The image below shows the spatial distribution of a cross dynastic subset of 190,000 people in CBDB by basic affiliations\nDisplay values within the dataset\n\nimport sqlite3\nimport pandas as pd\n\ndb_path = r'C:\\Users\\alexr\\OneDrive\\Desktop\\WORK\\Summer2025\\latest.db'\nconn = sqlite3.connect(db_path)\ncursor = conn.cursor()\n\n# List all tables\ncursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\ntables = cursor.fetchall()\n\nprint(\"Tables in database:\", tables)\n\nconn.close()\n\nLoad the one we want\n\ndb_path = r'C:\\Users\\alexr\\OneDrive\\Desktop\\WORK\\Summer2025\\latest.db'\n\n# Connect to the database\nconn = sqlite3.connect(db_path)\n\n# Replace 'person' with the actual table name you want to load\ndf = pd.read_sql_query(\"SELECT * FROM KIN_DATA\", conn)\n\n# Show the first few rows\nprint(df.head())\n\nconn.close()\n\nBuild the NetworkX Graph\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Create an empty graph\nG = nx.Graph()\n\n# Add edges with kinship type as edge attribute\nfor _, row in df.iterrows():\n    person = row['c_personid']\n    kin = row['c_kin_id']\n    kin_type = row['c_kin_code']\n    G.add_edge(person, kin, kinship=kin_type)\n\nprint(f\"Number of nodes: {G.number_of_nodes()}\")\nprint(f\"Number of edges: {G.number_of_edges()}\")\n\nVisualize the network\n\nplt.figure(figsize=(12, 12))\npos = nx.spring_layout(G, k=0.15)  # Layout for better spacing\n\n# Draw nodes and edges\nnx.draw_networkx_nodes(G, pos, node_size=50, node_color='skyblue')\nnx.draw_networkx_edges(G, pos, alpha=0.5)\n\nplt.title(\"Kinship Network from CBDB KIN_DATA\")\nplt.axis('off')\nplt.show()"
  },
  {
    "objectID": "docs/intro_to_convolution/intro_to_convolution.html",
    "href": "docs/intro_to_convolution/intro_to_convolution.html",
    "title": "Introduction to Convolutions",
    "section": "",
    "text": "Figure 1. An example of convolution operation (Created by the author)\n\n\nBefore you start, make sure you have the required libraries installed, if not, simply uncomment the lines below and run the cell to install them:\n\n\nCode\n# !pip install opencv-python\n# !pip install numpy\n# !pip install matplotlib\n# !pip install pandas\n# !pip install scikit-learn\n\n\n\nWhy Convolutions?\nMathematically speaking, Convolution is an operation that combines two functions to produce a third function, which has a variety of applications in signal processing, image analysis and more. While this may sound complex, we can minimize the math behind it and explain it in a less harmful and vivid way.\nLet’s begin by imagining a simple scenario: you took a picture of a cute dog, and you want to apply a filter to it so that it looks more vibrant and colorful. Now that you input the image into a computer, how does a computer “see” it? The computer would see the image as a grid of numbers, where the combination of 3 numbers (R, G, B) in a grid represents the color of a pixel, and with all the colored pixels combined, it forms the image. Given the numeric nature of a computer image, we say that the image is digitalized.\nHere, let’s read in the image into Python using OpenCV and display it using matplotlib:\n\n\nCode\n# Load necessary libraries\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Read the image in BGR format\n# Note this is the default input format of Python OpenCV\ndoge_bgr = cv2.imread('images/Original_Doge_meme.jpg')\n\n# Therefore we need to convert the color mapping to RGB\ndoge_rgb = cv2.cvtColor(doge_bgr, cv2.COLOR_BGR2RGB)\n\n# Display the image using matplotlib\nplt.figure(figsize=(4, 4))\nplt.imshow(doge_rgb)\nplt.axis('off')  \nplt.title('Input Image \\n Sato (2010).\"Kabosu the Shiba Inu\"')\nplt.show()\n\n\nThe image doesn’t look like numbers, does it? In fact, if we zoom in close enough, we can clearly see that the image is made up of pixels. But we still don’t see the numbers, this is because the numerical information is decoded by the computer and displayed as a colored pixel. However, we can easily convert the image into a numerical representation.\nFor demonstration purpose, the image is rescaled to a 10 \\(\\times\\) 10 grid, where within each cell, the numbers represents the R, G, B values (0-255) of each pixel.\nNote that compressing images to a smaller size is always easier comparing to enhancing images to a larger size, as compression can be done by going through the image pixel by pixel and averaging the color values in each cell, while enhancement usually requires more complex operations. This gives us a hint on why convolution is important.\n\n\nCode\nimg_rgb = cv2.resize(doge_rgb, (10, 10), interpolation=cv2.INTER_NEAREST)\n\n# Plot the RGB matrix\nfig, ax = plt.subplots(figsize=(6, 6))\n\n\nh, w = img_rgb.shape[:2]\nax.imshow(img_rgb, extent=[0, w, h, 0]) \n\n# Set ticks to show grid at pixel boundaries\nax.set_xticks(np.arange(0, w + 1, 1))\nax.set_yticks(np.arange(0, h + 1, 1))\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.grid(color='black', linewidth=1)\n\nfor i in range(h):\n    for j in range(w):\n        r, g, b = img_rgb[i, j]\n        brightness = int(r) + int(g) + int(b)\n        color = 'white' if brightness &lt; 380 else 'black'\n        ax.text(j + 0.5, i + 0.5, f'({r},{g},{b})',\n                ha=\"center\", va=\"center\", fontsize=6, color=color)\n        \n        \n# Display the Grid\nax.set_title(\"RGB Value Grid of Doge Image Resized to 10x10\")\nplt.tight_layout()\nplt.show()\n\n\nThe image has become a little abstract. Can you still identify the original image out of it?\nWhile the resized image looked significantly different, it still contains the necessary information, and same ideas also applies to larger images. That is, all images can be represented as a grid of numbers, where the 3 numbers in each cell corresponds to the color of a pixel. Computers can’t see colors like we do, the way they see colors is as if they were mixing colors using a palette that only has red, green and blue (RGB), where each color has an “amount” of intensity between 0 and 255. With the 3 values for red, green and blue, computers can create any color we see in the world.\nBack to the dog picture, it is easy to see that resizing the image to a smaller grid loses a lot of details, especially the rich color that makes the image vibrant. If we want to keep the complete color information, alternatively, we can plot out the distribution of the RGB values and frequencies in the image using a histogram. While this gives us a good idea of the color distribution, it does not tell us much about the spatial relationships between the pixels.\n\n\nCode\n# Compute and plot the color histogram\ncolors = ('r', 'g', 'b')\nplt.figure(figsize=(6, 4))\n\nfor i, col in enumerate(colors):\n    hist = cv2.calcHist(\n        images=[doge_bgr],       # source image (still in BGR)\n        channels=[i],           # which channel: 0=B, 1=G, 2=R\n        mask=None,              \n        histSize=[256],         \n        ranges=[0, 256]         \n    )\n    plt.plot(hist, color=col)\n    plt.xlim([0, 256])\n    \n# Display the histogram\nplt.title('RGB Histogram')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n\nCan you still identify the original image out of it?\nThe example above shows us what images are like in the eyes of a computer. Computers do not understand images in the same way that humans do, they can only see them as a collection of numbers. It thus make sense that we need to apply some math to these numbers to either change the image or extract some useful information from it, and that’s where convolution comes in.\n\n\nHow Does Convolution Work?\nBefore we dive into the application of it, let’s first understand how convolution operate on an image in a more intuitive way:\nThink of copying a painting by first sketching its outline at the same size. To add your own flair, you use a patterned brush: wherever the brush touches, it brightens the paint. You move this brush methodically across your sketch, from left to right, top to bottom so every spot is stamped with the pattern, tweaking the original image into something familiar yet distinctly styled.\nHere, the brush you used is called a kernel in the context of convolution, and the process of applying the brush is what we call convolution operation. We would define the kernel as a small matrix of numbers that represents the pattern of the brush, and the convolution operation as the process of transforming the original image by applying the kernel to it.\nHere is a gif illustrating how our filter (the kernel) will work on the image mathematically. You can see it as the small brush that slides over the image, operating on a small region of the image at a time, and eventually producing a new image that was completely transformed by the filter.\n\n\n\nFigure 2. Visual explanation of sharpening filter (Michael Plotke, Wikipedia, 2013)\n\n\nNow, let’s return to the example of the cute dog picture. What we are going to apply is a brush called sharpening filter, it is a 3 \\(\\times\\) 3 matrix that looks like this:\n\\[\n\\text{Sharpening Filter} = \\begin{bmatrix}\n0 &-1 & 0 \\\\\n-1 & 5 & -1 \\\\\n0 & -1 & 0\n\\end{bmatrix}\n\\]\nDon’t panic as we are not going to do any math here, we will just let computer do the math for us. The only thing you need to know is that this kernel will enhance the edges of the image, making it look sharper and more defined. For better understanding, let’s visualize it to see what this kernel look like in grey-scale:\n\n\nCode\nsharp_kernel = np.array([\n    [ 0, -1,  0],\n    [-1,  5, -1],\n    [ 0, -1,  0]\n], dtype=np.float32)\n\nfig, ax = plt.subplots(figsize=(4, 4))\n\n# Display with true gray-scale between kernel's min and max\nim = ax.imshow(sharp_kernel, cmap='gray_r',\n               vmin=sharp_kernel.min(), vmax=sharp_kernel.max())\nax.set_title('Sharpening Kernel')\nax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nIf you take a closer look at the kernel, you will see that it has a positive value in the center and negative values around it, and it has 0 values on the corners. This exactly looks like a brush that enhances the center of a region while reducing the intensity of the surrounding pixels, which is exactly what we want to achieve with the filter.\n\n\nCode\n# Define the sharpening filter\nkernel = np.array([\n    [ 0, -1,  0],\n    [-1,  5, -1],\n    [ 0, -1,  0]\n], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nfiltered = cv2.filter2D(doge_rgb, ddepth=-1, kernel=kernel)\n\nfiltered = np.clip(filtered, 0, 255).astype(np.uint8)\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\n# Input\nax1.imshow(doge_rgb)\nax1.set_title('Input Image \\n Sato (2010).\"Kabosu the Shiba Inu\"')\nax1.axis(\"off\")\n\n# Output\nax2.imshow(filtered)\nax2.set_title(\"Filtered Image (Sharper & More Vibrant)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nThe difference is quite obvious now. You can clearly see that the sharpened image (R) has more contrast and the edges are more defined, making it look more vibrant and colorful. This is the power of convolution, it allows us to apply filters to images and transform them in a way that is not possible with simple pixel manipulation. Similarly, we can blur an image quite easily, for which the “brush” we are going to use looks like this\n\\[\\text{Box Blur Filter} = \\begin{bmatrix}\n\\frac{1}{9} &\\frac{1}{9} & \\frac{1}{9} \\\\\n\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9} \\\\\n\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9}\n\\end{bmatrix}\n\\]\nVisually, this kernel is a filter that looks like:\n\n\nCode\nid_kernel = np.array([\n  [ 1, 1, 1],\n  [ 1, 1, 1],\n  [ 1, 1, 1]\n])\n\nblur_kernel = np.array([\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9]\n], dtype=np.float32)\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor ax, kernel, title in zip(axes, [id_kernel, blur_kernel],\n                             ['Input Image', 'Box Blur Kernel']):\n    # Show actual values in gray-scale\n    im = ax.imshow(kernel, cmap='gray_r', vmin=0, vmax=1)\n    ax.set_title(title)\n    ax.axis('off')\n\n    # Draw horizontal and vertical boundary lines at half‐integer positions\n    for i in range(1, kernel.shape[0]):\n        ax.axhline(i - .5, color='white', linewidth=1.5, zorder=1)\n        ax.axvline(i - .5, color='white', linewidth=1.5, zorder=1)\n\nplt.tight_layout()\nplt.show()\n\n\nLet’s see what it will make on our input image.\n\n\nCode\n# Define the box blur filter\nkernel = np.array([\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9]\n], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nfiltered = cv2.filter2D(doge_rgb, ddepth=-1, kernel=kernel)\n\nfiltered = np.clip(filtered, 0, 255).astype(np.uint8)\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\n# Input\nax1.imshow(doge_rgb)\nax1.set_title('Input Image\\n Sato (2010).\"Kabosu the Shiba Inu\"')\nax1.axis(\"off\")\n\n# Output\nax2.imshow(filtered)\nax2.set_title(\"Filtered Image (Blurred)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nIt worked as we expected, now the dog image becomes more blurry.\nHowever, convolution on image is not limited to filtering, it can also be used to extract features from the image. For example, we can use a kernel to detect edges, lines and texts in images, we can even use specific kernels to detect specific shapes or patterns in images, such as a kernel that detects anything that looks like a dog. As you can imagine, this is a very powerful tool in many applications, in fact, you most likely have already used it in your daily life. For example, when you use a photo editing app to apply a filter to your picture, the app is using convolution. When you use a search engine to search do image searches, the search engine is using convolution to extract features from the images and match them with your search query. Let’s say, the thing you are trying to find is eyes, the gif below shows how a convolution kernel detects “eyes” in an image:\n\n\n\nFigure 3. Eye detection kernel (Neuromatch Academy, 2025)\n\n\nThis is a very simple example, and it is implemented exactly the same way as we did with the sharpening filter. The only difference is that to extract a specific features, we need to use a “brush” designed to detect that feature, which usually requires some knowledge of the feature we want to extract. For example, if we want to detect eyes in the painting, we would need our “brush” to understand what eyes look like and what typical colors they have. This could be way too complicated for a single “brush”, so we often use multiple brushes to detect different features when it comes to the task of feature extraction.\nTo demonstrate how convolution extracts a specific feature from an image, let’s take a look at a different “art tool”. Let’s say this time you don’t want to color the painting differently, but rather you want to sketch a line art based on the original painting. You would use a fineliner pen that detects the edges of a painting and draw a line along them. In the eyes of a computer, these are the tools it is going to use:\n\\[\\text{Horizontal Sobel} = \\begin{bmatrix}\n1 & 0 & -1 \\\\\n2 & 0 & -2 \\\\\n1 & 0 & -1\n\\end{bmatrix}\n\\text{, }\n\\text{Vertical Sobel} = \\begin{bmatrix}\n1 & 2 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -2 & -1\n\\end{bmatrix}\n\\]\nStill, don’t panic, we won’t do any math in this notebook. All you need to know is these two kernels together are called Sobel filter, and what they do is highlighting the edges in the image, making them more visible. We can also visualize them as follows:\n\n\nCode\nh_kernel = np.array([\n    [ 1, 0, -1],\n    [ 2, 0, -2],\n    [ 1, 0, -1]\n])\n\nv_kernel = np.array([\n    [ 1, 2, 1],\n    [ 0, 0, 0],\n    [ -1, -2, -1]\n])\n\n# Plot the kernels\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor ax, kernel, title in zip(axes, [np.abs(h_kernel), np.abs(v_kernel)],\n                             ['Horizontal Kernel', 'Vertical Kernel']):\n    # Show actual values in gray-scale\n    im = ax.imshow(kernel, cmap='gray_r', vmin=0, vmax=2)\n    ax.set_title(title)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nIf you take a closer look at the kernels, you will see that the first one has positive values on the left and negative values on the right, while the second one has positive values on the top and negative values on the bottom. This pattern intuitively tells us that the first “pen” will scan through the image horizontally and extract the horizontal edges, while the second “pen” will scan through the image vertically and extract the vertical edges.\nLet’s now look at a different example and see what happens when we apply the Sobel filter.\n\n\nCode\n# Generate a greyscale version of the image\nhill_bgr = cv2.imread('images/xiangbishan.jpg')\n\nhill_rgb = cv2.cvtColor(hill_bgr, cv2.COLOR_BGR2RGB)\n\nhill_gray = cv2.cvtColor(hill_bgr, cv2.COLOR_BGR2GRAY)\n\n# Define the Sobel filter kernel\nsb_kernel_h = np.array([\n    [ 1, 0, -1],\n    [ 2, 0, -2],\n    [ 1, 0, -1]\n], dtype=np.float32)\n\nsb_kernel_v = np.array([\n    [ 1, 2, 1],\n    [ 0, 0, 0],\n    [-1, -2, -1]], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nhoriz = cv2.filter2D(hill_gray, -1, sb_kernel_h)\n\nvert = cv2.filter2D(hill_gray, -1, sb_kernel_v)\n\ncombined = cv2.convertScaleAbs(np.sqrt(vert.astype(np.float32)**2 + horiz.astype(np.float32)**2))\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n\n# Input\nax1.imshow(hill_rgb)\nax1.set_title('Input Image \\n xiquinhosilva (2018). \"Elephant Trunk Hill\"')\nax1.axis(\"off\")\n\n# Output\nax2.imshow(combined, cmap = 'gray')\nax2.set_title(\"Filtered Image (Edges Highlighted)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nAs we can see in the example above, the Sobel filter detects the edges in the image and highlights them like a fineliner pen. This is a very useful technique in image processing, as it allows us to extract features from the image that can be used for further analysis or classification. Let’s do more explorations with the example above:\n\n\nCode\n# Compute the magnitude and orientation of the gradient.\n# To put it simpler, they represents the length and direction of the edges at each pixel.\n# For those who are familiar with pythagorean theorem, the magnitude is exactly the length of the\"long edge\" calculated with pythagorean theorem.\nmagnitude = np.sqrt(vert.astype(np.float32)**2 + horiz.astype(np.float32)**2)\norientation = np.arctan2(vert.astype(np.float32), horiz.astype(np.float32))\n\n# Generate an edge binary map\nmag_norm = cv2.normalize(magnitude, None, 0,255, cv2.NORM_MINMAX).astype(np.uint8)\n_, edge_binary = cv2.threshold(mag_norm, 50, 255, cv2.THRESH_BINARY)\n\n# Create the Contour plot\ncontours, _ = cv2.findContours(edge_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncontour_img = hill_rgb.copy()\ncv2.drawContours(contour_img, contours, -1, (255,0,0), 1)\n\n# Create an edge density heatmap\nblock = 32\nh, w = edge_binary.shape\nint_img = cv2.integral(edge_binary)\n\nheatmap = np.zeros_like(edge_binary, dtype=np.float32)\n\nfor y in range(0, h - block + 1):\n    for x in range(0, w - block + 1):\n        y1, x1 = y,       x\n        y2, x2 = y + block, x + block\n        total = (int_img[y2, x2]\n               - int_img[y1, x2]\n               - int_img[y2, x1]\n               + int_img[y1, x1])\n        # center the block’s density back into the heatmap\n        heatmap[y + block//2, x + block//2] = total\n        \nheatmap = cv2.normalize(heatmap, None, 0,255, cv2.NORM_MINMAX).astype(np.uint8)\n\n# Plot out the visualizations\nfig, axs = plt.subplots(2, 2, figsize=(8, 5))\nax1, ax2, ax3, ax4 = axs.flatten()\n\n# Original Image\nax1.imshow(hill_rgb)\nax1.set_title(\"Input Image\")\nax1.axis(\"off\")\n\n# Contours on Original Image\nax2.imshow(contour_img)\nax2.set_title(\"Contours on Original Image\")\nax2.axis(\"off\")\n\n# Edge Density Heatmap\nax3.imshow(heatmap, cmap=\"hot\")\nax3.set_title(\"Edge Density Heatmap\")\nax3.axis(\"off\")\n\n# Edge Orientation Histogram\nangles_deg = np.degrees(orientation[magnitude &gt; 50].flatten())\n\nax4.hist(angles_deg, bins=36, range=(0, 90), color='purple')\nax4.set_title(\"Edge Orientation Histogram\")\nax4.set_xlabel(\"Angle (degrees)\")\nax4.set_ylabel(\"Frequency\")\n\n\nplt.tight_layout()\nplt.show()\n\n\nHere, we have generated three visualizations based on the extracted edge data.\n\nThe first visualization is a Contour Map on the original image. It circles contours from the image that are distinguishable from others. In many cases, this implies the circled regions have different visual patterns or belong to different objects, which is very useful in tasks such as image recognition.\nThe second visualization is the Edge Density Heatmap that assigns corresponding heat values to regions on the image based on the density distribution of the edges. The lighter the color, the higher the edge density of the region. This helps us to understand which area of the image carries the most information that may be of interest to us.\nThe third visualization is a histogram showing the distribution of edge orientations. edge orientation is the direction of the edge in the graph, expressed as the angle between the edge and the x-axis (horizontal line). Understanding the distribution of edge orientation can help us better recognize the graphical features of objects in an image for tasks such as classification. For example, based on this image featuring a hill and a water surface, we now know that the edges of these two objects typically have either a horizontal or vertical orientation.\n\nThe examples above demonstrated the power of convolution in both image processing and image analysis, and more importantly, convolution is very efficient, as it is easy for computers to understand and process, and can be applied to images of any size without losing information. This is why convolution has become a fundamental operation in computer vision and image processing.\nBut as you have probably aware, implementing convolution from scratch can be quite tedious, especially when we need to perform more specific tasks such as detecting texts or some specific shapes. A more advanced, adaptable and efficient way of applying convolution has been developed, which is called Convolutional Neural Network (CNN), and we will discuss it in the next notebook.\n\n\nKey takeaways:\n\nComputers see images as grids of numbers, where each grid cell contains the RGB values of a pixel. With the spatial relationships and the color information, computers can understand images without losing information.\nConvolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\nThe kernel that sharpens an image is called a sharpening filter, which enhances the edges of the image and makes it look more vibrant. The kernel that detects edges is called Sobel filter, which highlights the edges in the image and makes them more visible.\nConvolution is a powerful tool that can be used in many applications, such as photo editing, image search and feature extraction. It is efficient and can be applied to images of any size without losing information.\n\n\n\nGlossary\n\nComputer Vision: Computer vision is a field of artificial intelligence that enables computers to “see” and interpret images and videos in a way that is similar to human vision.\nConvolution: Convolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\nKernel/Filter/Mask: A convolution kernel, also known as a filter or mask, is a small matrix (array of numbers) used in image processing and computer vision to perform operations like blurring, sharpening, edge detection, and more.\n\n\n\nAdditional Resources\n\n3Blue1Brown. (2018, August 15). But what is a convolution? [Video]. YouTube. https://www.youtube.com/watch?v=KuXjwB4LzSA This video graphically explains the mathematical operation of convolution.\n\n\n\n\nReferences\n\nWikipedia contributors. (2025, June 18). Convolution. In Wikipedia, The Free Encyclopedia. Retrieved June 18, 2025, from https://en.wikipedia.org/wiki/Convolution\nSato, A. (2010, September 13). Kabosu the Shiba Inu (“Doge”) [Photograph]. Flickr. This photo is cited and used under   https://creativecommons.org/licenses/by-sa/2.0/\nxiquinhosilva. (2018, June 14), Elephant Trunk Hill [Photograph]. Flickr. This photo is cited and used under   https://creativecommons.org/licenses/by-sa/2.0/\nGeeksforGeeks. (n.d.). Types of Convolution Kernels. Retrieved July 16, 2025, from https://www.geeksforgeeks.org/deep-learning/types-of-convolution-kernels\nOpenCV.org. (n.d.). Image Processing in OpenCV. Retrieved July 16, 2025, from https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html"
  },
  {
    "objectID": "pages/amne170.html",
    "href": "pages/amne170.html",
    "title": "AMNE170 Placeholder",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In scelerisque lorem vel sollicitudin blandit. Donec et purus vitae metus posuere rutrum. Nunc id elit vitae libero blandit ultrices sed quis quam. Praesent suscipit nisl et tempor ornare. Phasellus laoreet mattis porttitor. Integer eu congue enim, vel auctor ligula. Curabitur gravida hendrerit lorem quis dictum.\nNunc pulvinar felis lectus, non finibus velit placerat id. Curabitur dignissim egestas lacinia. Nam ut urna non ante ultrices vulputate. Fusce malesuada venenatis porttitor. Mauris vulputate erat quis leo pharetra semper. Etiam semper arcu sit amet volutpat cursus. Mauris nec molestie tellus, sit amet consectetur risus. Nunc non nulla et velit elementum vulputate. Curabitur lectus tellus, consequat ut aliquam id, sodales sit amet velit. Praesent ac lacus urna.\nSuspendisse rutrum iaculis eleifend. Donec id ex accumsan, condimentum nisl blandit, lacinia metus. Proin fermentum facilisis placerat. Nullam auctor ex nisl, condimentum ornare metus cursus sed. Nullam ut nisl sed lorem dignissim consectetur sit amet id sem. Nam dolor nunc, lacinia iaculis nulla sit amet, scelerisque malesuada nibh. Praesent et nulla ac tellus tristique ultricies. Quisque condimentum libero felis, eget dapibus leo sodales vel.\nQuisque vel sapien nec nisi luctus dignissim. Proin id nisl sit amet enim semper sodales et ut est. Pellentesque pretium leo sapien, non viverra justo iaculis nec. Mauris congue tortor et eros gravida, quis imperdiet ante efficitur. Suspendisse sapien libero, dignissim suscipit nisi at, sodales porta elit. Sed et eros id turpis mattis rhoncus. Sed non arcu dui. Donec fermentum condimentum lectus. Donec sit amet felis vitae ex interdum dictum. Suspendisse vulputate neque quis semper mattis. Nulla imperdiet dolor odio, at porttitor risus volutpat quis. Nullam eu dictum risus. Aliquam erat volutpat. Suspendisse potenti. Integer volutpat tempus dui, in auctor massa dapibus a. Vestibulum facilisis velit leo, vitae accumsan tortor convallis id.\nSed vitae elit quis ligula luctus mattis eget eget nunc. Ut hendrerit rutrum condimentum. Nulla facilisi. Morbi pretium venenatis posuere. Integer tellus quam, porta vel massa et, fringilla tristique nisl. Sed dolor arcu, dignissim a tellus sit amet, interdum ornare quam. Etiam ullamcorper neque libero, vitae egestas velit maximus in. Proin ut blandit ante, ut faucibus nulla. Maecenas auctor malesuada augue. Duis auctor tincidunt lorem vitae viverra. Suspendisse ante neque, faucibus ut imperdiet ac, ornare eget urna. Nunc faucibus, dolor ut maximus consectetur, enim felis semper mi, sit amet luctus dolor erat sed urna. Morbi libero orci, aliquam et hendrerit id, pharetra ut ligula. Etiam ut elit est."
  },
  {
    "objectID": "pages/copyright.html",
    "href": "pages/copyright.html",
    "title": "Copyright Information",
    "section": "",
    "text": "This project uses data from a variety of sources, most available under an open data license. All other material is published under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License..\n\n\n\nOur suggested attribution for this project is:\n\n\n\n\n\n\nNelson, L., Graves, J., and other prAxIs Contributors. 2023. ‘The prAxIs Project: Creating Online Materials for Econometric Teaching’. https://comet.arts.ubc.ca/.\n\n\n\nADD DATA HERE Below as an example\n\nThe 2016 Census Data was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. This does not constitute an endorsement by Statistics Canada of this product.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product. ​\n\nThe Penn World Table was provided by Feenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), “The Next Generation of the Penn World Table” American Economic Review, 105(10), 3150-3182, available for download at www.ggdc.net/pwt\n\nThe Penn World Table Penn World Table 10.0 by Robert C. Feenstra, Robert Inklaar and Marcel P. Timmer is licensed under a Creative Commons Attribution 4.0 International License. This research received support through grants from the National Science Foundation, the Sloan Foundation and the Transatlantic Platform’s Digging into Data program.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\nWe also use data from the World Bank’s World Development Indicators and Quarterly Public Sector Debt databases.\n\nBoth of these are licensed under the CC by 4.0 open license.\n\nAll of the citations for the data used in the GEOG 374 notebooks are cited in the specific notebooks.\nThe data used in the projects modules is simulated, as is the salmon data in the prep modules and was created for this project. It falls under the license for the project in general (see above).\n\nThe simulation for the salmon data was based on data from the Pacific Salmon Foundation’s salmon watersheds program"
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html",
    "href": "pages/documentation/writing_self_tests.html",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "",
    "text": "An important part of notebook development is to design them so they give users formative feedback. Formative feedback helps students check if they understand a concept or skill.\nWe prefer to use immediate formative feedback, by integrating tests into the notebooks. These self-tests are run by the students and provide them with instant feedback about whether they have something correct or not.\nThis can be accomplished through the following process:\nIt is also very important to follow best practices when developing these notebooks and tests, since even small mistakes can create a great deal of confusion for users."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#general-framework",
    "href": "pages/documentation/writing_self_tests.html#general-framework",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "1 General Framework",
    "text": "1 General Framework\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nEarly in the notebook, usually in the first executed cell, include a source function call to the test scripts file:\nsource(tests.r)\nThis file should include the tests, as outlined below in Section 3. In this example, they are of the form test().\n\n\n\n\nPython Kernels\nEarly in the notebook, usually in the first executed cell, import the Tests class from the test scripts file:\nfrom .tests.py import *\nThis file should include the tests, as outlined below in Section 4. In this example, they are of the form test(), and are called like Tests.test().\n\n\n\n\n\n1.1 Use in Jupyter Notebooks (.ipynb)\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nIn the notebook, ask the students to fill in the object requested, then call the test function. Add a comment to explain what needs to be changed, if it’s not clear.\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\n\nTry to make the test function is a void call; avoid passing parameters.\nClearly indicate what to change using #comments.\nBe as specific as possible when giving directions.\n\n\n\n\n\nPython Kernels\nIn the notebook, ask the students to fill in the object requested, then call the test function. Add a comment to explain what needs to be changed, if it’s not clear.\nanswer_1 = #fill in the correct value here\n\nTests.test()\n\nTry to make the test function is a void call; avoid passing parameters.\nClearly indicate what to change using #comments.\nBe as specific as possible when giving directions."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#answers-in-.qmd-notebooks",
    "href": "pages/documentation/writing_self_tests.html#answers-in-.qmd-notebooks",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "2 Answers in .qmd notebooks",
    "text": "2 Answers in .qmd notebooks\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nEarly in the notebook, usually in the first executed cell, include a source link to the test scripts file:\nsource(tests.r)\nThis file should include the tests, as outlined below in Section 3. In this example, they are of the form test().\nIn .qmd notebooks, when you write a test include two versions: one with the answers, and one without. Include meta class tags to help tell them apart, and avoid evaluation. The cell should look like:\n#| eval: false\n#| classes: \"question\"\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nfor the question, and like:\n#| eval: false\n#| classes: \"answer\"\n\nanswer_1 &lt;- the_right_answer(stuff)\n\ntest_1()\nfor the answer. This will help debug questions easily.\n\n\n\n\n\n\nTip\n\n\n\nIt’s usually easiest to write the answer first, then debug and test.\n\n\n\n\n\n\nPython Kernels\nEarly in the notebook, usually in the first executed cell, import the Tests class from the test scripts file:\nfrom .tests.py import Tests\nThis file should include the tests, as outlined below in Section 4. In this example, they are of the form test(), and are called like Tests.test().\nIn .qmd notebooks, when you write a test, include two versions: one with the answers, and one without. Include meta class tags to help tell them apart, and avoid evaluation. The cell should look like:\n#| eval: false\n#| classes: \"question\"\n\nanswer_1 = #fill in the correct value here\n\nTests.test_1()\nfor the question, and like:\n#| eval: false\n#| classes: \"answer\"\n\nanswer_1 = the_right_answer(stuff)\n\nTests.test_1()\nfor the answer. This will help debug questions easily.\n\n\n\n\n\n\nTip\n\n\n\nIt’s usually easiest to write the answer first, then debug and test."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#sec-r-tests",
    "href": "pages/documentation/writing_self_tests.html#sec-r-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "3 Writing R Self-Tests",
    "text": "3 Writing R Self-Tests\nSelf-test scripts are R files (.r) which supply the testing functions. They use two libraries:\n\nlibrary(testthat): a test assertion library, which provides functions to check if something is correct and give feedback.\nlibrary(digest): a hash library, which computes and check hash functions.\n\nHere is an example of the first function of a file and the library headers:\nlibrary(testthat)\nlibrary(digest)\n\ntest_1 &lt;- function() {\n  test_that(\"Solution is incorrect\", {\n    expect_equal(digest(answer1), \"dbc09cba9fe2583fb01d63c70e1555a8\")\n  })\n  print(\"Success!\")\n}\nThis creates a function (test1()) that when called in the Jupyter notebook:\n\nFinds the object answer1.\nComputes the hash of it (digest(answer)) and compares it to the string dbc09cba9fe2583fb01d63c70e1555a8 (the correct answer’s hash).\nIf they match, it prints “Success!” otherwise it throws an error.\n\nIn order to develop the test, you can use this template:\n\nCreate a new cell to contain the test. If this a .qmd test, make it the answer version of the test.\nCreate a new function in the script file with a unique name (test_n()) and the answer (answer_n) to test in the testing script.\nCompute digest(answer_n) to get the correct has value.\nAdd it to the expect_equal element in the script.\nIf a .qmd copy the answer, and change it to a question. Then, replace the correct answer with a comment.\n\nNote that you may not want to test the entire object, but rather some particular part of it, such as answer_n$coefs; see Section 3.2 for details.\n\n3.1 Richer Feedback\nThe previous method only tests if an answer exactly matches the correct answer. If there are common errors you may want to give a hint about what is wrong. For example, in a multiple-choice question, answers A and B reflect common misconceptions.\nYou can use tests to give this kind of feedback with a more complex test function. Use the case_when function to give varied responses depending on the answer given by the student. For example:\ntest_1 &lt;- function(answer_1) {\n    ans &lt;- digest(answer_1)\n    case_when(ans == \"dbc09cba9fe2583fb01d63c70e1555a8\" ~ test_that(TRUE),\n             ans == \"dd531643bffc240879f11278d7a360c1\" ~ \n              \"This is a common misconception, remember that...\",\n              TRUE ~ test_that(FALSE))\n}\nYou can adapt this framework for more complex tests, as necessary.\n\n\n\n\n\n\nA Note on Feedback\n\n\n\nIt is important to provide feedback that will guide the student towards the right answer and a greater understanding of the topic at hand. Try not to give feedback along the lines of “That is correct, congratulations!” or “I’m sorry, that is incorrect!.” Feedback should point out the error that students are making and guide them to the correct answer.\n\n\n\n\n3.2 Important Notes\nHere are some common pitfalls and notes about creating tests. The main idea is that hash functions are exact: the objects must be exactly the same. This means you should:\n\nAlways round numbers to 3 or 4 decimal places using the round() function. Do this in the testing function, rather than making students do it.\nNever test objects that include arbitrary elements, such as names or sequences.\nOnly test the simplest object necessary, not the easiest one to test.\n\nFor example, the following objects will return different hashes:\nd1 &lt;- data.frame(age = \"12\")\nd2 &lt;- data.frame(Age = \"12\")\n\ndigest(d1) # == d2da0d698613f4cafa7d6fe5af762294\ndigest(d2) # == cfe4cbf9291d5705b2c61422098db883\nHere are some examples of arbitrary elements that you can miss:\n\nObject or variable names (Age != age)\nRegression models (y ~ x1 + x2 != y ~ x2 + x1)\nFloating point numbers (1.222222222222 != 1.222222222222)\nMethods that us randomization (e.g., Monte Carlo methods)\n\nBottom line: only test mathematical or textual objects, not programming objects unless you are very, very explicit about them."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#sec-py-tests",
    "href": "pages/documentation/writing_self_tests.html#sec-py-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "4 Writing Python Self-Tests",
    "text": "4 Writing Python Self-Tests\nPython self-test scripts are Python files (.py) which supply the testing function in a test class. They use two libraries:\n\nunittest: a test assertion library, which provides functions to check if something is correct and give feedback.\nhashlib: a hash library, which computes and check hash functions, and report the hexdigest of one.\n\nHere is an example of the first function of a file and the library headers:\n\nfrom hashlib import blake2b\nimport unittest import TestCase as t\n\n# Don't change this one\ndef hash(data):\n    h = blake2b(digest_size=20)\n    h.update(data)\n    return h.hexdigest()\n\n\nclass Test():\n\n  def test1():\n    t.assertEqual(hash(answer1), \"dbc09cba9fe2583fb01d63c70e1555a8\")\nSee Section 3.1 and Section 3.2 for guidelines above writing richer tests, and some common mistakes. The issues and advice applies to Python as well."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#other-uses-for-tests",
    "href": "pages/documentation/writing_self_tests.html#other-uses-for-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "5 Other Uses for Tests",
    "text": "5 Other Uses for Tests\nYou can also write “hidden” tests for developers; this is recommended when you have a complex example with interdependent parts. Try to make these as hidden as possible from the main notebook; hide them in a supplemental file which is included at runtime."
  },
  {
    "objectID": "pages/hist414.html",
    "href": "pages/hist414.html",
    "title": "Hist414 Placeholder",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In scelerisque lorem vel sollicitudin blandit. Donec et purus vitae metus posuere rutrum. Nunc id elit vitae libero blandit ultrices sed quis quam. Praesent suscipit nisl et tempor ornare. Phasellus laoreet mattis porttitor. Integer eu congue enim, vel auctor ligula. Curabitur gravida hendrerit lorem quis dictum.\nNunc pulvinar felis lectus, non finibus velit placerat id. Curabitur dignissim egestas lacinia. Nam ut urna non ante ultrices vulputate. Fusce malesuada venenatis porttitor. Mauris vulputate erat quis leo pharetra semper. Etiam semper arcu sit amet volutpat cursus. Mauris nec molestie tellus, sit amet consectetur risus. Nunc non nulla et velit elementum vulputate. Curabitur lectus tellus, consequat ut aliquam id, sodales sit amet velit. Praesent ac lacus urna.\nSuspendisse rutrum iaculis eleifend. Donec id ex accumsan, condimentum nisl blandit, lacinia metus. Proin fermentum facilisis placerat. Nullam auctor ex nisl, condimentum ornare metus cursus sed. Nullam ut nisl sed lorem dignissim consectetur sit amet id sem. Nam dolor nunc, lacinia iaculis nulla sit amet, scelerisque malesuada nibh. Praesent et nulla ac tellus tristique ultricies. Quisque condimentum libero felis, eget dapibus leo sodales vel.\nQuisque vel sapien nec nisi luctus dignissim. Proin id nisl sit amet enim semper sodales et ut est. Pellentesque pretium leo sapien, non viverra justo iaculis nec. Mauris congue tortor et eros gravida, quis imperdiet ante efficitur. Suspendisse sapien libero, dignissim suscipit nisi at, sodales porta elit. Sed et eros id turpis mattis rhoncus. Sed non arcu dui. Donec fermentum condimentum lectus. Donec sit amet felis vitae ex interdum dictum. Suspendisse vulputate neque quis semper mattis. Nulla imperdiet dolor odio, at porttitor risus volutpat quis. Nullam eu dictum risus. Aliquam erat volutpat. Suspendisse potenti. Integer volutpat tempus dui, in auctor massa dapibus a. Vestibulum facilisis velit leo, vitae accumsan tortor convallis id.\nSed vitae elit quis ligula luctus mattis eget eget nunc. Ut hendrerit rutrum condimentum. Nulla facilisi. Morbi pretium venenatis posuere. Integer tellus quam, porta vel massa et, fringilla tristique nisl. Sed dolor arcu, dignissim a tellus sit amet, interdum ornare quam. Etiam ullamcorper neque libero, vitae egestas velit maximus in. Proin ut blandit ante, ut faucibus nulla. Maecenas auctor malesuada augue. Duis auctor tincidunt lorem vitae viverra. Suspendisse ante neque, faucibus ut imperdiet ac, ornare eget urna. Nunc faucibus, dolor ut maximus consectetur, enim felis semper mi, sit amet luctus dolor erat sed urna. Morbi libero orci, aliquam et hendrerit id, pharetra ut ligula. Etiam ut elit est."
  },
  {
    "objectID": "pages/index/index_AMNE-170.html",
    "href": "pages/index/index_AMNE-170.html",
    "title": "Temples, Tombs, and Tyrants: The Archaeology of the Middle East, Greece, and Rome (AMNE 170)",
    "section": "",
    "text": "This section contains material to support UBC’s AMNE 170. The rise of civilizations, cultural interconnections, and power dynamics in the ancient Middle East (including Egypt), Greece, and Rome (10,000 BCE - 300 CE). Archaeological methods and interpretation, and analysis of ancient artifacts in UBC collections. Credit will be granted for only one of CNRS_V 104 or AMNE_V 170. Equivalency: CNRS _V 104.\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision. CHANGE THIS PART\nThey can also be used for self-study, with some additional effort\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_CNN.html",
    "href": "pages/index/index_CNN.html",
    "title": "CNN",
    "section": "",
    "text": "The modules in this unit are for the topics with CNN’s. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_CNN.html#modules",
    "href": "pages/index/index_CNN.html#modules",
    "title": "CNN",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nIntroduction to Convolutional Neural Networks (CNNs)\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_HIST-414.html",
    "href": "pages/index/index_HIST-414.html",
    "title": "Constitutions in Canadian History: From Pre-Contact to the Charter of Rights (HIST 414)",
    "section": "",
    "text": "This section contains material to support UBC’s Constitutions in Canadian History: From Pre-Contact to the Charter of Rights (HIST 414). This course covers European precedents, Colonial self-government, Canadian Confederation, and issues such as gay rights, abortion, and First Nations land rights.\nCHANGE\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision\nThey can also be used for self-study, with some additional effort\n\nThese materials are a review of basic probability, which contains overlap with AP and IB Statistics from high school.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nText Embeddings for Regina V Wing Chong (1885)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing pyLDAvis for Analysis for HIST-414 Alex R\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_SOCI280.html",
    "href": "pages/index/index_SOCI280.html",
    "title": "SOCI280 - Data and Society",
    "section": "",
    "text": "The modules in this unit are for the classes under SOCI 280 - Data and Society. Impacts of changing information and communication technologies on societies and social interactions. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_SOCI280.html#modules",
    "href": "pages/index/index_SOCI280.html#modules",
    "title": "SOCI280 - Data and Society",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nIntroduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_convolution.html",
    "href": "pages/index/index_convolution.html",
    "title": "Convolution",
    "section": "",
    "text": "The modules in this unit are for the topics with convolution. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_convolution.html#modules",
    "href": "pages/index/index_convolution.html#modules",
    "title": "Convolution",
    "section": "Modules",
    "text": "Modules"
  },
  {
    "objectID": "pages/installation/installing_locally.html",
    "href": "pages/installation/installing_locally.html",
    "title": "Install and Use COMET",
    "section": "",
    "text": "We have designed COMET to be easy to use and install for a wide range of students. If there’s one thing we know about teaching data and econometrics, it’s that everyone finds their own routine and tools that work best for them. We encourage you to explore and try different things - see what works best for you!\n\nFor most students, we recommend using Jupyter Notebooks via a cloud server, called a JupyterHub which is the easiest to use, and the simplest to get started, since you don’t have to install anything. See Accessing COMET using a JupyterHub for a in-depth explanation.\nAlternatively, you can run JupyterLab directly on your device as a stand-alone application. See Running Via Jupyter Desktop.\nIf you have experience with VSCode, or prefer using a general-purpose IDE, you can install Comet using VSCode.\nIf you have experience with R Studio, you can check out our guide Using RStudio.\n\nIf you’re not sure, start with the cloud-based option, then get more sophisticated in the future.\n\n\n\n\n\n\n\n\n\n\nAccessing COMET using a JupyterHub\n\n\n\n\n\n\n\n\n\n6 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall and Use COMET locally Through the Terminal\n\n\n\n\n\n\n\n\n\n28 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunning Via Jupyter Desktop\n\n\n\n\n\n\n\n\n\n28 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing RStudio\n\n\n\n\n\n\n\n\n\n28 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing VSCode\n\n\n\n\n\n\n\n\n\n28 Jul 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html",
    "href": "pages/installation/jupyter_desktop_setup.html",
    "title": "Running Via Jupyter Desktop",
    "section": "",
    "text": "One alternative to running on a JupyterHub is running Jupyter locally though Jupyter Desktop. This is particularly helpful if you have a machine with a good CPU/GPU and would like to use it to it’s full abilities, or when you need to run commands that require more memory than UBC’s JupyterHub options offer.\nIn 2021, the developers of JupyterLab launched JupyterLab Desktop, a desktop application for JupyterLab, which this tutorial will demonstrate how to install and use."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-and-launching-jupyterlab",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-and-launching-jupyterlab",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing and Launching JupyterLab",
    "text": "Installing and Launching JupyterLab\nTo install JupyterLab on your machine, head to github.com/jupyterlab/jupyterlab-desktop and scroll down to installations. Then, select the version of the installer that you require. From there, open the installer and follow the instructions.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are on a Windows device, running the installer may prompt you with a warning. To bypass it, press Learn More &gt; Run Anyway.\n\n\nTo launch JupyterLab, simply open the application."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-r",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-r",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing R",
    "text": "Installing R\nAdditionally, we’ll need to install R and Rtools.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-the-r-package-compiler",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-the-r-package-compiler",
    "title": "Running Via Jupyter Desktop",
    "section": "3. Installing the R package compiler",
    "text": "3. Installing the R package compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-conda-and-required-packages",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-conda-and-required-packages",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing conda and required packages",
    "text": "Installing conda and required packages\nAdditionally, we’ll need to install minconda, a python environment manager, in order to create an envrionment in which we will use our required packages.\n\nHead to anaconda.com and select the installer that meets your computer requirements.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that, during the installation process, you select the following options:\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\n\n\n\n\nOnce installed, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet_env jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env.\n\nInstalling the R kernel\nWe’ll need to install the R kernel in order to use the R programming language in Jupyter. To do so, in the miniconda terminal, run r. This should result in the following output:\n\n\n\n\n\nIf you recieved this output, it means that everything has been set up correctly. Lastly, run the following lines of code in the terminal:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThis will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, this would be “Canada (MB)”.)\n\n\nSelecting our environment in Jupyter Desktop\nLastly, we’ll need to select our environment. To do so, open jupyter desktop and press on the two blue bars at the top right of the application. This will prompt you to select your new environment. If the previous installation steps were successfull, you should see the COMET environment.\nOnce pressed, this will restart your current session. To check that everything is been set up properly, select the R kernel and run installed.packages() in a new notebook. If the output gives a list of R packages, the installation has been successful."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#additional-resources",
    "href": "pages/installation/jupyter_desktop_setup.html#additional-resources",
    "title": "Running Via Jupyter Desktop",
    "section": "Additional resources",
    "text": "Additional resources\nIf you would like to learn more about how to use Jupyter Desktop, consider checking out the Jupyter github page found here."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html",
    "href": "pages/installation/local_terminal_setup.html",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "",
    "text": "Some students may prefer to use the local version of Jupyter that is accessible via browser, rather than through Jupyter desktop. The latter allows for more customizability at the expense of a more intuitive installation and activation process."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-r",
    "href": "pages/installation/local_terminal_setup.html#installing-r",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "1. Installing R",
    "text": "1. Installing R\nBefore we install Jupyter, we’ll need to install R and Rtools.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-the-r-package-compiler",
    "href": "pages/installation/local_terminal_setup.html#installing-the-r-package-compiler",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "2. Installing the R package Compiler",
    "text": "2. Installing the R package Compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-conda-and-jupyter",
    "href": "pages/installation/local_terminal_setup.html#installing-conda-and-jupyter",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "3. Installing Conda and Jupyter",
    "text": "3. Installing Conda and Jupyter\nWe can now install Jupyter. To do so, we’ll need to install minconda, a python environment manager, in order to create an envrionment in which we will use our required packages and activate Jupyter.\n\nHead to anaconda.com and select the installer that meets your computer requirements.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that, during the installation process, you select the following options:\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\n\n\n\n\nOnce installed, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet_env jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-the-r-kernel",
    "href": "pages/installation/local_terminal_setup.html#installing-the-r-kernel",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "4. Installing the R kernel",
    "text": "4. Installing the R kernel\nWe’ll need to install the R kernel in order to use the R programming language in Jupyter. To do so, in the miniconda terminal, run r. This should result in the following output:\n\n\n\n\n\nIf you recieved this output, it means that everything has been set up correctly. Lastly, run the following lines of code in the terminal:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThis will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, this would be “Canada (MB)”.)"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#opening-jupyter",
    "href": "pages/installation/local_terminal_setup.html#opening-jupyter",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "5. Opening Jupyter",
    "text": "5. Opening Jupyter\nFinally, to open Jupyter, open a new miniconda terminal. In the terminal, run the following command: jupyter lab. This will open up Jupyter as a local copy on your search engine.\n\n\n\n\n\n\nWarning\n\n\n\nThis terminal acts as your local Jupyter server. Closing it will shut down your server!"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#opening-the-comet-modules",
    "href": "pages/installation/local_terminal_setup.html#opening-the-comet-modules",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "6. Opening the COMET modules",
    "text": "6. Opening the COMET modules\nLastly, you’ll want to download the COMET files. In the COMET website, press launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer. Extract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. Within Jupyter, find this directory, and open it."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html",
    "href": "pages/installation/rstudio_setup.html",
    "title": "Using RStudio",
    "section": "",
    "text": "This page explains how to set up RStudio in order to access and use the COMET notebooks."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-r",
    "href": "pages/installation/rstudio_setup.html#installing-r",
    "title": "Using RStudio",
    "section": "1. Installing R",
    "text": "1. Installing R\nThe first thing we’ll need to do is install R onto our computer.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and `associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-rstudio",
    "href": "pages/installation/rstudio_setup.html#installing-rstudio",
    "title": "Using RStudio",
    "section": "2. Installing RStudio",
    "text": "2. Installing RStudio\nWe’ll now install RStudio, our IDE of choice.\n\nWindowsMacOS\n\n\n\nHead to RStudio and press Download RStudio Desktop for windows.\nOpen the RStudio setup and press Next &gt; Next &gt; Install.\n\n\n\n\n\nRStudio, scroll down to “OS”, and select the Mac installer.\nOpen the Rstudio setup and install."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-package-compilers",
    "href": "pages/installation/rstudio_setup.html#installing-package-compilers",
    "title": "Using RStudio",
    "section": "3. Installing package compilers",
    "text": "3. Installing package compilers\nWe’ll need to install a package compiler to activate the renv.lock, a package bundle made specifically for the COMET notebooks.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages from source.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#downloading-and-opening-the-comet-notebooks",
    "href": "pages/installation/rstudio_setup.html#downloading-and-opening-the-comet-notebooks",
    "title": "Using RStudio",
    "section": "4. Downloading and opening the COMET notebooks",
    "text": "4. Downloading and opening the COMET notebooks\n\n4.1. Downloading the COMET notebooks\n\nOpen comet.arts.ubc.ca and, in the top right corner, select Launch Comet &gt; Launch Locally. This will download a .zip file to your computer.\nOpen the file in file explorer and extract the zip folder to your preferred destination."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#importing-the-comet-notebooks-into-rstudio",
    "href": "pages/installation/rstudio_setup.html#importing-the-comet-notebooks-into-rstudio",
    "title": "Using RStudio",
    "section": "4.2. Importing the COMET notebooks into RStudio",
    "text": "4.2. Importing the COMET notebooks into RStudio\nWe can now import the COMET notebooks into RStudio. When we say “import”, what we really mean is setting the working directory. The working directory is the location in your computer where you are working in. Unlike VSCode and other high-level IDEs, setting our working directory is done manually by specifiying our location using the R console.\n\nLocate the downloaded COMET notebooks and copy the absolute file path by right-clicking on the folder and pressing copy as path. If you are on a Mac, hold the option key and select Copy (file name) as Pathname.\nLocate the current working directory in RStudio by entering getwd() in the R console (bottom left of the screen).\n\n\n\n\nThe output should look something like this on windows, but won’t necessarily give the same file path.\n\n\n\nWe’ll now set our working directory to the COMET folder. To do so, enter the following command into the console:\n\nsetwd(\"C:/your/file/path/goes/here\")\nWhere “C:/your/file/path/goes/here” is the absolute file path you copied earlier, with the backshashes (\\) set to forward slashes (/). For example, the file path (on windows) C:\\users\\i_love_econ will be changed to C:/users/i_love_econ.\n\n\n\n\n\n\nWarning\n\n\n\nRStudio requires file paths to have a forward slash instead of a back slash. If you don’t adjust the absolute file path accordingly, you won’t be able to set/change your working directory.\n\n\nTo check that you’ve got the right working directory, run getwd() again.\n\n4.3. Changing your working directory in the files tab.\nOn the right-hand bottom side of Rstudio there is a display that shows your files. If you’d like to change it to your current working directory, press the small button ... button called “go to directory”. This will allow you to pick the COMET modules folder from your file system and navigate it from within R."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#activating-the-comet-environment",
    "href": "pages/installation/rstudio_setup.html#activating-the-comet-environment",
    "title": "Using RStudio",
    "section": "5. Activating the COMET environment",
    "text": "5. Activating the COMET environment\nWe’re now ready to activate the COMET environment in RStudio.\n\nIn the R console, enter the following line of code: install.packages(\"renv\"). This will install the renv package, which will allow RStudio to read the custom environment file for the notebooks.\nIn the R console, run renv::restore(). You should get a message that reads: “It looks like you’ve called renv::restore() in a project that hasn’t been activated yet. How would you like to proceed?”. Press 1. This should restart R. If it doesn’t, run renv::activate().\nTo check that everything is installed properly, run renv::status(). This command will give you a the list of packages in your environment (Note that this might take some time to run)."
  },
  {
    "objectID": "pages/quickstart.html",
    "href": "pages/quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "Want to get started using prAxIs as soon as possible? It’s as easy as 1-2-3!\nAnd select your JupyterHub\nYou’re all ready to go! You can see more details in our using prAxis guide."
  },
  {
    "objectID": "pages/quickstart.html#using-locally",
    "href": "pages/quickstart.html#using-locally",
    "title": "Quickstart Guide",
    "section": "Using Locally",
    "text": "Using Locally\nWant to run prAxIs on your own computer? Don’t have a reliable internet connection? Want to be a power user or have a favourite IDE? You don’t have to rely on a JupyterHub to use prAxIs.\n\nInstall Jupyter or RStudio locally on your own computer, including R and any other packages necessary.\nSelect “Launch Locally” from the “Launch prAxIs” menu and download the repository files.\nUnzip the files and open them in your local IDE.\n\nYou’re all ready to go!"
  },
  {
    "objectID": "pages/soci280.html",
    "href": "pages/soci280.html",
    "title": "SOCI280 Placeholder",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. In scelerisque lorem vel sollicitudin blandit. Donec et purus vitae metus posuere rutrum. Nunc id elit vitae libero blandit ultrices sed quis quam. Praesent suscipit nisl et tempor ornare. Phasellus laoreet mattis porttitor. Integer eu congue enim, vel auctor ligula. Curabitur gravida hendrerit lorem quis dictum.\nNunc pulvinar felis lectus, non finibus velit placerat id. Curabitur dignissim egestas lacinia. Nam ut urna non ante ultrices vulputate. Fusce malesuada venenatis porttitor. Mauris vulputate erat quis leo pharetra semper. Etiam semper arcu sit amet volutpat cursus. Mauris nec molestie tellus, sit amet consectetur risus. Nunc non nulla et velit elementum vulputate. Curabitur lectus tellus, consequat ut aliquam id, sodales sit amet velit. Praesent ac lacus urna.\nSuspendisse rutrum iaculis eleifend. Donec id ex accumsan, condimentum nisl blandit, lacinia metus. Proin fermentum facilisis placerat. Nullam auctor ex nisl, condimentum ornare metus cursus sed. Nullam ut nisl sed lorem dignissim consectetur sit amet id sem. Nam dolor nunc, lacinia iaculis nulla sit amet, scelerisque malesuada nibh. Praesent et nulla ac tellus tristique ultricies. Quisque condimentum libero felis, eget dapibus leo sodales vel.\nQuisque vel sapien nec nisi luctus dignissim. Proin id nisl sit amet enim semper sodales et ut est. Pellentesque pretium leo sapien, non viverra justo iaculis nec. Mauris congue tortor et eros gravida, quis imperdiet ante efficitur. Suspendisse sapien libero, dignissim suscipit nisi at, sodales porta elit. Sed et eros id turpis mattis rhoncus. Sed non arcu dui. Donec fermentum condimentum lectus. Donec sit amet felis vitae ex interdum dictum. Suspendisse vulputate neque quis semper mattis. Nulla imperdiet dolor odio, at porttitor risus volutpat quis. Nullam eu dictum risus. Aliquam erat volutpat. Suspendisse potenti. Integer volutpat tempus dui, in auctor massa dapibus a. Vestibulum facilisis velit leo, vitae accumsan tortor convallis id.\nSed vitae elit quis ligula luctus mattis eget eget nunc. Ut hendrerit rutrum condimentum. Nulla facilisi. Morbi pretium venenatis posuere. Integer tellus quam, porta vel massa et, fringilla tristique nisl. Sed dolor arcu, dignissim a tellus sit amet, interdum ornare quam. Etiam ullamcorper neque libero, vitae egestas velit maximus in. Proin ut blandit ante, ut faucibus nulla. Maecenas auctor malesuada augue. Duis auctor tincidunt lorem vitae viverra. Suspendisse ante neque, faucibus ut imperdiet ac, ornare eget urna. Nunc faucibus, dolor ut maximus consectetur, enim felis semper mi, sit amet luctus dolor erat sed urna. Morbi libero orci, aliquam et hendrerit id, pharetra ut ligula. Etiam ut elit est."
  },
  {
    "objectID": "pages/teaching_with_comet.html",
    "href": "pages/teaching_with_comet.html",
    "title": "Teaching with Jupyter and COMET",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\nThis guide is an introduction to how you can use COMET and Jupyter notebooks for teaching. We will mainly discuss Jupyter Notebooks, for reasons explained in Section 2, but most of this advice is easy to adapt to other forms of notebook-based instruction such as R Markdown or Quarto.\nIn this guide, we will discuss:\n\nThe advantages and disadvantages of different notebook-based systems for classroom instruction.\nHow to use COMET-style notebooks in different classroom settings, including an outline of how to plan a lesson.\nHow to develop interactive learning activities to accompany a COMET-style notebooks, including some classroom-tested suggestions.\nAn introduction to developing your own COMET-style notebooks for classroom instruction.\n\n\n\n\n\n\n\nWant the Basics?\n\n\n\nJust looking for a quick overview of how Jupyter notebooks work? Try out getting started introduction to Jupyter notebook, then come back here.\n\n\nAs most of our experience is in post-secondary education, this guide is intended for educators teaching students at the university level. However, it is probably also suitable for upper-year high school students, with sufficient experience1."
  },
  {
    "objectID": "pages/teaching_with_comet.html#disclaimer",
    "href": "pages/teaching_with_comet.html#disclaimer",
    "title": "Teaching with Jupyter and COMET",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\nThis guide is an introduction to how you can use COMET and Jupyter notebooks for teaching. We will mainly discuss Jupyter Notebooks, for reasons explained in Section 2, but most of this advice is easy to adapt to other forms of notebook-based instruction such as R Markdown or Quarto.\nIn this guide, we will discuss:\n\nThe advantages and disadvantages of different notebook-based systems for classroom instruction.\nHow to use COMET-style notebooks in different classroom settings, including an outline of how to plan a lesson.\nHow to develop interactive learning activities to accompany a COMET-style notebooks, including some classroom-tested suggestions.\nAn introduction to developing your own COMET-style notebooks for classroom instruction.\n\n\n\n\n\n\n\nWant the Basics?\n\n\n\nJust looking for a quick overview of how Jupyter notebooks work? Try out getting started introduction to Jupyter notebook, then come back here.\n\n\nAs most of our experience is in post-secondary education, this guide is intended for educators teaching students at the university level. However, it is probably also suitable for upper-year high school students, with sufficient experience1."
  },
  {
    "objectID": "pages/teaching_with_comet.html#sec-why",
    "href": "pages/teaching_with_comet.html#sec-why",
    "title": "Teaching with Jupyter and COMET",
    "section": "2 Why Jupyter Notebooks?",
    "text": "2 Why Jupyter Notebooks?\nWhy are Jupyter Notebooks a valuable tool for teaching? There are two main reasons:\n\nFirst, there are the advantages of Notebooks for teaching.\nSecond, there are the advantages of Jupyter for teaching.\n\nCombining these advantages creates a very valuable tool.\n\n2.1 Why Notebooks?\nA notebook refers to a digital document which combines rich text (including hyperlinks, formatting, and images) with cells that can perform computations. A user is able to change the content of the notebook, such as performing a computation or changing the text.\nNotebooks teach students three important skills, useful for data science and applied social science research:\n\nFirst, they teach students how to perform literate coding. Literate programming dates back to Knuth (1984), and has become extremely popular in sciences that use data. As Kery et al. (2018) explains, combining notes and context with code creates a self-documenting research notebook that addresses many common problems novice (and experienced) researchers face when analyzing data.\nSecond, they encourage replicable and reproducible data analysis. The non-reproducability of empirical results (see Camerer et al. (2018)) has reached crisis-levels in some fields. Because notebooks need to be run from the top-down, they naturally encourage students to make their analyses replicable. The structure of a notebook also encourage transparency when experimenting with analyses. This makes the work more likely to be reproducible.\nThird, they teach industry-relevant skills. Notebooks are extensively used by employers who conduct data science research, or who use data science in their work. Understanding how to write and use notebooks is a valuable skill in itself.\n\nThese properties make notebooks ideal to teach to students. Creating notebooks for classroom instruction turns them from a research tool into a pedagogical tool.\n\n\n2.2 Why Jupyter?\nJupyter is not the only option for notebooks (see Section 2.3). However, it has some advantages for teaching not shared by alternatives:\n\nNo installation necessary: when used through a JupyterHub, Jupyter notebooks do not require students to install any software or have a powerful computer. Even students with just a Chromebook or tablet can use Jupyter notebooks.\n\nThis eliminates many of the most time-consuming and frustrating parts of teaching student data science, including: installing software, troubleshooting package conflicts, issues sharing files and data, and computer problems.\n\nSimple Github integration: through nbgitpuller it is easy to share notebooks directly into a JupyterHub. This means that starting a class using notebooks is as easy as sharing a link with your students.\nLanguage independence: although the Jupyter framework is written in Python, it uses kernels to perform computation. There are dozens of kernels available, including those for popular languages such as R, Julia, Java, C, STATA, and Python itself.\n\nThe biggest strength of Jupyter is its hub-based design. This is also its biggest weakness, since it relies on an internet connection and someone to manage the hub. However, there are many free, well-maintained, hubs online such as:\n\nUBC OpenJupyter\nSyzygy\nGoogle Colab\nGitHub Codespces\n\nYou can also set up your own, or work with non-profits like 2i2c to develop your own hub.\n\n\n2.3 What are the Alternatives?\nJupyter Notebooks are not the only option for teaching using notebooks. In fact, there are significant advantages to other notebook styles, which may be more effective for certain kinds of teaching.\n\nJupyterQuartoR MarkdownObservable\n\n\n\nJupyter Notebooks\nJupyter notebooks are the most widely-used framework for notebook-based content, and are the easiest to use for students. However, they are not always the easiest to develop or maintain. We recommend Jupyter for online and student use.\n\nAdvantages\n\nWidely used, many tutorials and guides online.\nNo software installation needed for users.\nMany public, free, hubs (including Google Collab).\nLarge, open-source community.\n\n\n\nDisadvantages\n\nComplex and difficult local set-up.\nJSON-based file type; hard to maintain.\nLimited display and render options.\n\nLearn more about Jupyter Notebooks.\n\n\n\n\n\nQuarto Notebooks\nQuarto is a strong improvement over R Markdown and supports multiple languages. However, it is still in development and is more complicated. We recommend Quarto for development and offline use2.\n\nAdvantages\n\nInteroperable with R Markdown, but not R specific.\nVery rich output and render options.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\n\n\n\nDisadvantages\n\nNo easy-to-use free hubs available.\nMore complex than comparable notebook formats.\nNew, still in development.\n\nLearn more about Quarto.\n\n\n\n\n\nR Markdown Notebooks\nR Markdown is an excellent alternative to Jupyter for offline-only applications that only use R.\n\nAdvantages\n\nVery widely-used, many tutorials and guides online.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\nMany render options for output, rich output.\n\n\n\nDisadvantages\n\nIdiosyncratic syntax.\nNo easy-to-use free hubs available.\nOnly supports R coding, no other languages.\n\nLearn more about R Markdown.\n\n\n\n\n\nObservable Notebooks\nObservable is the newest format on the market, and looks very professional. It is designed for enterprise clients, and is the most complex of the alternatives.\n\nAdvantages\n\nNon-language specific framework.\nExtremely rich output formats.\nStrong dashboarding and interactive support.\nLarge enterprise developer.\n\n\n\nDisadvantages\n\nLarge enterprise developer, no free hubs.\nMost complex of the alternatives.\nNew, still in development.\n\nLearn more about Observable and D3.js."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "href": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "title": "Teaching with Jupyter and COMET",
    "section": "3 Teaching with Jupyter Notebooks",
    "text": "3 Teaching with Jupyter Notebooks\nWe have tried several ways of using Jupyter Notebooks when teaching, and found that they fit most teaching styles. We have found them particularly effective in:\n\nLecture.\nFlipped classrooms.\nWorkshops or labs.\n\nYou can find advice on lesson planning in these formats in the tabs below.\n\nLecturesFlipped ClassroomWorkshops or Labs\n\n\n\nDirect Instruction or Lectures\nJupyter notebooks are most effective in lecture when you use them as a demonstration tool which students can follow along with.\n\nThe power of a Jupyter notebook is the interactive nature of the different cells.\nThis allows you to enhance your lecture content by immediately and interactively demonstrating principles by running cells and changing their values.\n\nEssentially, you make can your slides or visualizations interactive by running or editing cells.\n\nYou can either do this directly, by showing the notebook, or you can turn the notebook into a slideshow using either nbcovert or RISE, which create a RevealJS presentation from your notebook.\n\nRevealJS is a powerful HTML-based presentation framework, widely used on the web and in computation.\n\nThere are also powerful libraries for interactive visualization, such as plotly and ggplotly.3\n\nSee Section 4.1 for a guide to creating presentations using Jupyter notebooks.\nWe have found it is usually best to give students the Jupyter notebook of the presentation, as a kind of “hand-out,” while you demonstrate using the presentation display of the notebook. This avoids the problem of having to make sure students have a suitable presentation display tool installed.\n\n\nSuggestions for Teaching\n\nTry demonstrating a cell, then asking students to predict what happen when you make a different change. Then do it!\n\nThis works great with classroom response systems such as iClicker or TopHat.\n\nSpend time thinking about how interacting with the cell can show the concept more effectively than a static visualization.\n\nWe have found this to be particularly useful for dynamics in visualizations, such as showing a change.\n\nSpend time on each interactive part of your presentation, and walk through the changes.\nUse encapsulation by placing code in auxillary files to make the demonstrations easier to follow.\n\nIf students don’t need to know how it works, only what it does, consider re-writing the code to hide the details.\n\n\n\n\n\n\nFlipped Classrooms\nA flipped classroom refers to a teaching model where activities traditional done in the classroom are done at while, while activities done at home are done in the classroom (Akçayır and Akçayır (2018)). “Flipping” the lecture demonstration, outlined above, using Jupyter Notebooks is a natural fit.\nMost flipped classroom experiences tend to use videos (see Akçayır and Akçayır (2018)), and this is quite feasible with Jupyter Notebooks. Record yourself demonstrating the notebook, and have students follow along. We can done this for some of the COMET notebooks (see our project notebooks for example).\nHowever, the interactivity of notebooks makes them ideal for doing as “pre-reading” assignment instead, or in addition, to videos. The active learning created by interacting with the notebook, and completing self-test exercises, makes them more effective than just doing a reading.\nA good flipped-classroom notebook:\n\nIntroduces the topic in a narrative, systematic way, and does not require any significant external references to follow along.\nIncludes regularly-spaced interactive cells, which require students to evaluate and inspect the results.\nHas a series of self tests (see Section 4.2) at regular intervals, to check and reinforce student understanding.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nFor example, you could assign students a pre-reading of classification and clustering before class. In the class, you could then introduce a new dataset and have them explore it using clustering methods.\nMany of our COMET notebooks, especially the intermediate ones are built with this structure, where one “Part” of the notebook can be turned into the classroom part of the flipped classroom.4\n\n\n\n\n\nWorkshops or Labs\nWe have also found it effective to teach using Jupyter notebooks in small group settings, such as workshops. A typical Jupyter workshop in our experience:\n\nDivide the students into groups of about 4-5, and have them physically move so that they are seated next to one another.\n\nThis also works well online, using a feature like Zoom’s breakout rooms.\n\nOnce they are settled, or before moving people into their breakout rooms, introduce the purpose of the workshop. Identify what students are supposed to do, and how they will interact with the Jupyter Notebooks.\nAllow students to work together on the notebooks, while you move around the room discussing with the groups.\n\nIt is often effective to design your notebooks so that they have several identifiable “tasks” or stopping points, where you can bring the workshop back together.\n\nMany of our COMET notebooks, especially the intermediate ones are built with this structure.\nMake sure you build in time for students to introduce themselves to one another, if this is their first time meeting.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nMany of the tools and techniques for flipped classroom instruction work well in a workshop format too. Just make sure there is sufficient support so students can follow the notebooks together.\nEncourage students to work together to troubleshoot problems if they encounter them, so you are not running around too often.\nAn effective strategy is to have the students collaborate on a single “final” version of the notebook together, while experimenting on their own. Nominating one student as the “scribe” is a good way to keep this organized.\n\nAt the end of the workshop, having students hand in their Notebook is an effective way of measuring participation, and encourages participation."
  },
  {
    "objectID": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "href": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "title": "Teaching with Jupyter and COMET",
    "section": "4 Developing your own Notebooks for Teaching",
    "text": "4 Developing your own Notebooks for Teaching\nIt is easier than you might expect to develop notebooks for teaching. Some of our favourite examples are:\n\nWord Embeddings: an advanced, workshop-style, .ipynb format notebook.\nVisualization: a basic, lecture-style, introduction with time for experimentation.\nExporting Output: a flipped-classroom style book or workshop for STATA output.\n\nPoke around and see some more!\n\n4.1 Presenting and Slideshows\nWhen teaching with Jupyter, effective presentation skills require a little planning. There are several options, depending on what kind of presentation you want to give.\n\n4.1.1 Presenting a Notebook\nPresenting a Notebook as a notebook, such as in a demonstration or workshop, is easy.\n\nIn JupyterLab, the easiest way is use the View menu:\n\nUncheck all of the un-necessary bars, such as the top bar and status bar.\nTurn on “Simple View”, which only shows your activate notebook tab.\nTurn on “Presentation Mode.”\n\n\nThis will create a large-format, interactive, version of your notebook suitable for presenting on an overhead projector or monitor.\n\n\n4.1.2 Presenting a Slideshow\nIf you want to turn your notebook into a slideshow, things are more complicated depending on whether you want it to be interactive or not. However, in general you create a slideshow by designating individual cells are either whole slides, or fragments of slides:\n\nA slide is a single higher-level slide. When the presentation advances from one slide to another, it will “slide” right-to-left.\nA sub-slide is like a lower-level slide. When the presentations advances from to a sub-slide, it will “slide” from up-to-down.\nA fragment is part of a slide. It appears by sliding up, into the slide, keep the previous content visible. This is how you can reveal information or advance content.\n\nYou designate cells as the different part of a presentation by clicking on the gear icon, then selecting the cell. A dropdown menu that says “Slide type” will be visible. Use this to set up your presentation.\nIf you don’t care about interactivity, at this point you can go to “File &gt; Save and Export Notebook as…” then select “Reveal.js Slides.” This will download an .html file with your presentation in it. Learn more about Reveal.js to see how this file works in more detail.\nIf you want to run code in your presentation and edit it as you present, things are more complicated. To make your presentation editable, you need to install a JupyterLab extension called RISE.\n\nRISE is easiest to install on your own computer, not on a JupyterHub unless you have administrator privileges.\nIn the terminal, run pip install jupyterlab_rise then re-launch your server.\nYou can read more about RISE above; it’s still in development so things might change.\n\n\n\n\n4.2 Writing Self-Tests\nWriting self-tests is an important part of providing formative feedback to students. It can be somewhat complicated, but the basic idea is to write cells in your notebooks that look like:\n#an R self test\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nThe function test_1() is stored in an auxillary file, and performs a test on the answer. It also gives feedback to the student, such as whether the answer was correct.\nThis requires some set-up, and is slightly different for different languages. To make this easier, we provide a detailed guide for R and Python in our writing self-tests documentation.\n\n\n4.3 Development Tips\nDeveloping notebooks as a one-off is straightforward if you author them in JupyterLab: what you see is what you get. However, if you have a more complex project some planning helps. This includes multiple notebooks, or notebooks you need to collaborate on over time.\n\nWe strongly recommend not developing directly in .ipynb notebooks long term. Draft your initial notebook in .ipynb, then switch to another framework for longer-term development.\n\nThe reason is because editing an .ipynb edits the state of the program, making it easy to accidentally evaluate or delete something.\nIt’s also hard to maintain and doesn’t play nicely with version control systems like git because the documents are very complicated in structure.\n\nOur recommended format is .qmd which can render .ipynb notebooks from the source code. The underlying document is just text, which makes it easy to edit and maintain.\n\nUsing .qmd notebooks is much easier: think of these as the “source code” and the .ipynb as the “output.” This also has an advantage of begin able to create other output formats, like PDFs, websites, or presentations directly from the source code."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-offline",
    "href": "pages/teaching_with_comet.html#teaching-offline",
    "title": "Teaching with Jupyter and COMET",
    "section": "5 Teaching Offline",
    "text": "5 Teaching Offline\nWhile we think that teaching using a JupyterHub is the best option, that may not always be desired or possible. If you want to use COMET notebooks, or similar teaching tools, without a JupyterHub you have two main options:\n\nOption 1: have students install one of the alternative frameworks.\nOption 2: have students install Jupyter locally, on their own computers.\n\nBe prepared to troubleshoot installation issues."
  },
  {
    "objectID": "pages/teaching_with_comet.html#further-reading",
    "href": "pages/teaching_with_comet.html#further-reading",
    "title": "Teaching with Jupyter and COMET",
    "section": "6 Further Reading",
    "text": "6 Further Reading\nYou can see some of our other publications on our dissemination page."
  },
  {
    "objectID": "pages/teaching_with_comet.html#references",
    "href": "pages/teaching_with_comet.html#references",
    "title": "Teaching with Jupyter and COMET",
    "section": "7 References",
    "text": "7 References\n\n\nAkçayır, Gökçe, and Murat Akçayır. 2018. ‘The Flipped Classroom: A Review of Its Advantages and Challenges’. Computers & Education 126: 334–45.\n\n\nCamerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. ‘Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015’. Nature Human Behaviour 2 (9): 637–44.\n\n\nGraves, Jonathan L, Emrul Hasan, and Trish L Varao-Sousa. 2024. ‘Understanding the Hybrid Classroom in Economics: A Case Study’. International Review of Economics Education 45: 100282.\n\n\nKery, Mary Beth, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A Myers. 2018. ‘The Story in the Notebook: Exploratory Data Science Using a Literate Programming Tool’. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–11.\n\n\nKnuth, Donald Ervin. 1984. ‘Literate Programming’. The Computer Journal 27 (2): 97–111."
  },
  {
    "objectID": "pages/teaching_with_comet.html#footnotes",
    "href": "pages/teaching_with_comet.html#footnotes",
    "title": "Teaching with Jupyter and COMET",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe have taught several of these notebooks to 1st year students during intake events, who were fresh out of high school.↩︎\nWe use Quarto to develop the COMET project.↩︎\nPlotly comes in several flavours for different languages, such as `plotly-r↩︎\nThis was actually our original use for the notebooks, before COMET! See Graves, Hasan, and Varao-Sousa (2024)↩︎"
  },
  {
    "objectID": "pages/using_comet.html",
    "href": "pages/using_comet.html",
    "title": "Using COMET",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\n\nThe COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module.\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\n\n\n\n\nAs we mentioned, all of the Jupyter Notebooks are interactive; to access a notebook, simply launch the appropriate hub from the top menu of this website (where it says, “LAUNCH COMET”). The notebooks can also be downloaded for local use (“LAUNCH COMET &gt; LAUNCH LOCALLY”).\n\nThe interactive element will redirect you to the JupyterHub of your choice, and then synch the modules over.\nThe synching process uses a Jupyter extension called nbgitpuller to perform this operation.\n\n\n\n\n\n\n\nJupyter Troubleshooting Tip\n\n\n\nOccasionally, you can run into problems working with Jupyter in the cloud. If this occurs, you can try (a) deleting all of the COMET modules from your JupyterHub account, then trying to launch again, or (b) downloading the files manually, then uploading them. Option (b) will always work, even if it is a bit time exhuastive.\n\n\n\n\n\nThere are two ways to include a module in Canvas (or another LMS):\n\nEmbed it in a Canvas page or assignment\nInclude it as an external link\n\nThe advantage of option (1) is that you can include other material around the link (such as instructions). The advantage of option (2) is that it is easier.\n\n\nYou can see a visualization below:\n\n\n\nGIF of Embedding COMET into Canvas\n\n\n\nFirst, create the page (or assignment) that you want to embed the page in.\nThen, edit the page, and switch the HTML edit mode\nCopy the following text into the page:\n\n&lt;p&gt;&lt;iframe src=\"PASTE URL HERE\" width=\"800\" height=\"3200\" style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\nReplace the PASTE URL HERE text with the URL of the COMET page you want\nOptionally edit the width and height options to change the size of the embed\nSave the page; you should see the embedded page\n\nYou can now edit this page normally - for instance, by adding other material. You could also just add a URL as hyperlink instead of an embed. It’s up to you!\n\n\n\n\n\n\nCanvas Tip\n\n\n\nYou can find more information about embedding webpages in canvas on this page.\n\n\n\n\n\nYou can also just add a link directly to a Canvas module.\n\nOn the Canvas module, click the (+) to add an item, then select “External URL”.\nEnter the URL of the COMET page and customize the name; this is what will show up on Canvas.\nYou can choose to select the “load in a new tab” buttonif that is your preference.\n\n\n\n\n\nIf you run into issues with a COMET module (say, a bug) you can create an issue to our GitHub directory using the “Report an issue” button at the top right of this page.\nIf you need other support, please contact comet-project at ubc dot ca\n\n\n\n\n\n\nImportant\n\n\n\nIf the issue is about a JupyterHub, and not the notebook specifically, we unfortunately cannot help. Contact your hub maintainer for information.\n\nAt UBC, this is the LTHub"
  },
  {
    "objectID": "pages/using_comet.html#using-comet-for-teaching",
    "href": "pages/using_comet.html#using-comet-for-teaching",
    "title": "Using COMET",
    "section": "",
    "text": "The COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module.\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down."
  },
  {
    "objectID": "pages/using_comet.html#interactive-modules",
    "href": "pages/using_comet.html#interactive-modules",
    "title": "Using COMET",
    "section": "",
    "text": "As we mentioned, all of the Jupyter Notebooks are interactive; to access a notebook, simply launch the appropriate hub from the top menu of this website (where it says, “LAUNCH COMET”). The notebooks can also be downloaded for local use (“LAUNCH COMET &gt; LAUNCH LOCALLY”).\n\nThe interactive element will redirect you to the JupyterHub of your choice, and then synch the modules over.\nThe synching process uses a Jupyter extension called nbgitpuller to perform this operation.\n\n\n\n\n\n\n\nJupyter Troubleshooting Tip\n\n\n\nOccasionally, you can run into problems working with Jupyter in the cloud. If this occurs, you can try (a) deleting all of the COMET modules from your JupyterHub account, then trying to launch again, or (b) downloading the files manually, then uploading them. Option (b) will always work, even if it is a bit time exhuastive."
  },
  {
    "objectID": "pages/using_comet.html#using-with-canvas",
    "href": "pages/using_comet.html#using-with-canvas",
    "title": "Using COMET",
    "section": "",
    "text": "There are two ways to include a module in Canvas (or another LMS):\n\nEmbed it in a Canvas page or assignment\nInclude it as an external link\n\nThe advantage of option (1) is that you can include other material around the link (such as instructions). The advantage of option (2) is that it is easier.\n\n\nYou can see a visualization below:\n\n\n\nGIF of Embedding COMET into Canvas\n\n\n\nFirst, create the page (or assignment) that you want to embed the page in.\nThen, edit the page, and switch the HTML edit mode\nCopy the following text into the page:\n\n&lt;p&gt;&lt;iframe src=\"PASTE URL HERE\" width=\"800\" height=\"3200\" style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\nReplace the PASTE URL HERE text with the URL of the COMET page you want\nOptionally edit the width and height options to change the size of the embed\nSave the page; you should see the embedded page\n\nYou can now edit this page normally - for instance, by adding other material. You could also just add a URL as hyperlink instead of an embed. It’s up to you!\n\n\n\n\n\n\nCanvas Tip\n\n\n\nYou can find more information about embedding webpages in canvas on this page.\n\n\n\n\n\nYou can also just add a link directly to a Canvas module.\n\nOn the Canvas module, click the (+) to add an item, then select “External URL”.\nEnter the URL of the COMET page and customize the name; this is what will show up on Canvas.\nYou can choose to select the “load in a new tab” buttonif that is your preference."
  },
  {
    "objectID": "pages/using_comet.html#problems-and-support",
    "href": "pages/using_comet.html#problems-and-support",
    "title": "Using COMET",
    "section": "",
    "text": "If you run into issues with a COMET module (say, a bug) you can create an issue to our GitHub directory using the “Report an issue” button at the top right of this page.\nIf you need other support, please contact comet-project at ubc dot ca\n\n\n\n\n\n\nImportant\n\n\n\nIf the issue is about a JupyterHub, and not the notebook specifically, we unfortunately cannot help. Contact your hub maintainer for information.\n\nAt UBC, this is the LTHub"
  },
  {
    "objectID": "docs/AMNE-376/development/convnext_embeddings.html",
    "href": "docs/AMNE-376/development/convnext_embeddings.html",
    "title": "Praxis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport torch\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport timm\nfrom tqdm.notebook import tqdm # Import tqdm for progress bar\n\n\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')\ndf.head()\n\n\n\n\n\n\n\n\nfilename\npage\ngroup\nera\nmaterial\n\n\n\n\n0\npage188_img01_photo13.jpg\n188\nSOUNION GROUP\n615 - 590 BC\nMarble\n\n\n1\npage202_img01_photo3.jpg\n202\nSOUNION GROUP\n615 - 590 BC\nMarble\n\n\n2\npage202_img01_photo4.jpg\n202\nSOUNION GROUP\n615 - 590 BC\nMarble\n\n\n3\npage205_img01_photo4.jpg\n205\nSOUNION GROUP\n615 - 590 BC\nMarble\n\n\n4\npage211_img01_photo12.jpg\n211\nSOUNION GROUP\n615 - 590 BC\nLead\n\n\n\n\n\n\n\n\nmodel_name = 'convnextv2_tiny' \nmodel = timm.create_model(model_name, pretrained=True)\nmodel.eval()  # set to eval mode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kaiyan Zhang\\.cache\\huggingface\\hub\\models--timm--convnextv2_tiny.fcmae_ft_in22k_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\nTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n  warnings.warn(message)\n\n\nConvNeXt(\n  (stem): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n    (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n  )\n  (stages): Sequential(\n    (0): ConvNeXtStage(\n      (downsample): Identity()\n      (blocks): Sequential(\n        (0): ConvNeXtBlock(\n          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=96, out_features=384, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=384, out_features=96, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (1): ConvNeXtBlock(\n          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=96, out_features=384, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=384, out_features=96, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (2): ConvNeXtBlock(\n          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=96, out_features=384, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=384, out_features=96, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n      )\n    )\n    (1): ConvNeXtStage(\n      (downsample): Sequential(\n        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n      )\n      (blocks): Sequential(\n        (0): ConvNeXtBlock(\n          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=192, out_features=768, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=768, out_features=192, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (1): ConvNeXtBlock(\n          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=192, out_features=768, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=768, out_features=192, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (2): ConvNeXtBlock(\n          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=192, out_features=768, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=768, out_features=192, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n      )\n    )\n    (2): ConvNeXtStage(\n      (downsample): Sequential(\n        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n      )\n      (blocks): Sequential(\n        (0): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (1): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (2): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (3): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (4): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (5): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (6): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (7): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (8): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n      )\n    )\n    (3): ConvNeXtStage(\n      (downsample): Sequential(\n        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n      )\n      (blocks): Sequential(\n        (0): ConvNeXtBlock(\n          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (1): ConvNeXtBlock(\n          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (2): ConvNeXtBlock(\n          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n      )\n    )\n  )\n  (norm_pre): Identity()\n  (head): NormMlpClassifierHead(\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n    (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n    (pre_logits): Identity()\n    (drop): Dropout(p=0.0, inplace=False)\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n  )\n)\n\n\n\n# Option A: if model.forward_features exists:\ndef extract_backbone_features(x):\n    # x: tensor (B,3,H,W)\n    feat_map = model.forward_features(x)  # tensor shape: (B, C, H', W')\n    # Usually, to get a single vector per image, apply global pooling:\n    # Many timm models have model.global_pool or model.fc (depending on arch).\n    # For ConvNeXt, there's often model.global_pool (AdaptiveAvgPool).\n    # Check if model.global_pool exists:\n    if hasattr(model, 'global_pool'):\n        # Apply global pool: \n        pooled = model.global_pool(feat_map)  # shape: (B, C, 1, 1)\n        return pooled.view(pooled.size(0), -1)  # shape: (B, C)\n    else:\n        # Fallback: adaptive avg pool:\n        import torch.nn.functional as F\n        pooled = F.adaptive_avg_pool2d(feat_map, 1)  # (B, C, 1, 1)\n        return pooled.view(pooled.size(0), -1)\n\n# Option B: reset classifier to output feature vectors directly\n# This depends on timm model API; many have model.reset_classifier.\ntry:\n    model.reset_classifier(0)  # replace head so output is feature vector\n    # Now model(x) returns features of shape (B, C)\n    def extract_backbone_features(x):\n        return model(x)\nexcept AttributeError:\n    # fallback to forward_features approach above\n    pass\n\n\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\n# ImageNet normalization (mean/std) :contentReference[oaicite:4]{index=4}\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std  = [0.229, 0.224, 0.225]\n\n# Decide target input size: many ConvNeXt V2 variants use 224×224 or larger (e.g., 384).\ninput_size = 224  # adjust if your variant expects 384: set 384 accordingly.\n\npreprocess = transforms.Compose([\n    transforms.Resize(int(input_size * 1.14)),  # resize shorter side\n    transforms.CenterCrop(input_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n])\n\ndef load_and_preprocess(image_path):\n    img = Image.open(image_path).convert('RGB')\n    return preprocess(img)\n\n\nimport torch\nimport numpy as np\n\ndef extract_features_from_folder(folder_path, batch_size=16):\n    # Collect image file paths\n    exts = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n    image_paths = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path)\n                   if fname.lower().endswith(exts)]\n    image_paths.sort()\n    features_list = []\n    filenames = []\n    model.eval()\n    with torch.no_grad():\n        for i in range(0, len(image_paths), batch_size):\n            batch_paths = image_paths[i:i+batch_size]\n            batch_imgs = []\n            for p in batch_paths:\n                try:\n                    tensor = load_and_preprocess(p)\n                    batch_imgs.append(tensor)\n                except Exception as e:\n                    print(f\"Warning: could not process {p}: {e}\")\n            if not batch_imgs:\n                continue\n            batch_tensor = torch.stack(batch_imgs, dim=0).to(device)  # (B,3,H,W)\n            feats = extract_backbone_features(batch_tensor)  # (B, feat_dim)\n            # Convert to CPU numpy\n            feats_np = feats.cpu().numpy()\n            features_list.append(feats_np)\n            filenames.extend([os.path.basename(p) for p in batch_paths])\n    if features_list:\n        features = np.concatenate(features_list, axis=0)  # shape (N, feat_dim)\n    else:\n        features = np.zeros((0,))  # empty\n    return filenames, features\n\n# Example usage:\nfolder = '../data/richter_kouroi_complete_front_only'\nfilenames, features = extract_features_from_folder(folder, batch_size=8)\nprint(f\"Extracted features for {len(filenames)} images; feature dimension: {features.shape[1]}\")\n\nExtracted features for 62 images; feature dimension: 768\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef plot_pca_by_metadata(features, filenames, df, category_col='era', title=None, annotate=False):\n    \"\"\"\n    Plot PCA of `features`, coloring and optionally annotating points by metadata category.\n    \n    Args:\n        features: numpy array of shape (N, D) - feature vectors.\n        filenames: list of length N - filenames corresponding to each feature vector.\n        df: pandas DataFrame containing at least columns ['filename', category_col].\n        category_col: str - column name in df to color by (e.g., 'era' or 'material').\n        title: str or None - plot title.\n        annotate: bool - whether to annotate each point with filename or metadata label.\n    \"\"\"\n    # Ensure DataFrame has the necessary columns\n    if 'filename' not in df.columns or category_col not in df.columns:\n        raise ValueError(f\"DataFrame must contain 'filename' and '{category_col}' columns.\")\n    \n    # Run PCA\n    pca = PCA(n_components=2, random_state=0)\n    feats_2d = pca.fit_transform(features)  # shape (N, 2)\n    \n    # Prepare mapping from filename to category\n    # We'll create a dict for faster lookup\n    # If DataFrame has duplicates for the same filename, last one will be used\n    mapping = dict(zip(df['filename'], df[category_col].astype(str)))\n    \n    # Get unique categories for colormap\n    unique_cats = sorted(df[category_col].astype(str).unique())\n    # Create a consistent color map\n    cmap = plt.get_cmap('tab10')\n    # If more categories than cmap colors, colors will cycle; for many categories, consider a larger colormap\n    era_color_map = {cat: cmap(i % cmap.N) for i, cat in enumerate(unique_cats)}\n    \n    # Build color list and optional labels\n    colors = []\n    annotations = []\n    for fn in filenames:\n        cat = mapping.get(fn, None)\n        if cat is None:\n            # If missing metadata, assign a default color and label\n            colors.append('gray')\n            annotations.append(f\"{fn}\\n(Unknown {category_col})\")\n        else:\n            colors.append(era_color_map[cat])\n            annotations.append(f\"{fn}\\n{cat}\")\n    \n    # Plot\n    plt.figure(figsize=(8, 6))\n    scatter = plt.scatter(feats_2d[:, 0], feats_2d[:, 1], c=colors, s=50, alpha=0.7)\n    \n    # Create legend manually\n    handles = [plt.Line2D([], [], marker=\"o\", ls=\"\", color=era_color_map[cat], label=cat)\n               for cat in unique_cats]\n    plt.legend(handles=handles, title=category_col, bbox_to_anchor=(1.05, 1), loc='upper left')\n    \n    plt.xlabel(\"PC1\")\n    plt.ylabel(\"PC2\")\n    if title is None:\n        title = f\"PCA colored by {category_col}\"\n    plt.title(title)\n    plt.tight_layout()\n    \n    # Annotate points if requested\n    if annotate:\n        for i, text in enumerate(annotations):\n            plt.annotate(text, (feats_2d[i, 0], feats_2d[i, 1]),\n                         textcoords=\"offset points\", xytext=(3, 3),\n                         fontsize=7, color=colors[i])\n    \n    plt.show()\n\n# Example usage (assuming you have `features`, `filenames`, and `df` defined):\nplot_pca_by_metadata(features, filenames, df, category_col='era', title=\"PCA by Era\", annotate=False)\nplot_pca_by_metadata(features, filenames, df, category_col='material', title=\"PCA by Material\", annotate=False)\n\n\n\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import convnext_base\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# Load model and image\nmodel = convnext_base(pretrained=True)\nmodel.eval()\ntarget_layers = [model.features[-1]]\n\n# Load and preprocess image\nimg = Image.open(\"../data/richter_kouroi_filtered_photos/page312_img01_photo4.jpg\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = transform(img).unsqueeze(0)\n\n# GradCAM\nfrom pytorch_grad_cam import GradCAM\ncam = GradCAM(model=model, target_layers=target_layers)\ntargets = [ClassifierOutputTarget(0)]  # Class index to visualize\ngrayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n\n# Overlay CAM on image\nrgb_img = np.array(img.resize((224, 224))) / 255.0\nvisualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n\nplt.imshow(visualization)\nplt.axis(\"off\")\nplt.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Base_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Base_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\" to C:\\Users\\Kaiyan Zhang/.cache\\torch\\hub\\checkpoints\\convnext_base-6075fbad.pth\n100%|██████████| 338M/338M [00:09&lt;00:00, 37.6MB/s] \n\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import convnext_base\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# 1. Load and record original size\norig_img = Image.open(\"../data/richter_kouroi_filtered_photos/page312_img01_photo4.jpg\").convert(\"RGB\")\norig_w, orig_h = orig_img.size\nrgb_orig = np.array(orig_img) / 255.0\n\n# 2. Prepare the resized tensor for the model\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = transform(orig_img).unsqueeze(0)\n\n# 3. Load model & CAM setup\nmodel = convnext_base(pretrained=True)\nmodel.eval()\ntarget_layers = [model.features[-1]]\ncam = GradCAM(model=model, target_layers=target_layers)\n\n# 4. Compute the CAM at 224×224\ngrayscale_cam = cam(input_tensor=input_tensor,\n                    targets=[ClassifierOutputTarget(0)])[0]\n\n# 5. Upsample CAM to original size\ngrayscale_cam_orig = cv2.resize(grayscale_cam, (orig_w, orig_h),\n                                interpolation=cv2.INTER_LINEAR)\n\n# 6. Overlay on the original image\nvisualization = show_cam_on_image(rgb_orig,\n                                  grayscale_cam_orig,\n                                  use_rgb=True)\n\n# 7. Display\nplt.figure(figsize=(8,8))\nplt.imshow(visualization)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import convnext_base\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# 1. Load and record original size\norig_img = Image.open(\"../data/richter_kouroi_filtered_photos/page188_img01_photo13.jpg\").convert(\"RGB\")\norig_w, orig_h = orig_img.size\nrgb_orig = np.array(orig_img) / 255.0\n\n# 2. Prepare the resized tensor for the model\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = transform(orig_img).unsqueeze(0)\n\n# 3. Load model & CAM setup\nmodel = convnext_base(pretrained=True)\nmodel.eval()\ntarget_layers = [model.features[-1]]\ncam = GradCAM(model=model, target_layers=target_layers)\n\n# 4. Compute the CAM at 224×224\ngrayscale_cam = cam(input_tensor=input_tensor,\n                    targets=[ClassifierOutputTarget(0)])[0]\n\n# 5. Upsample CAM to original size\ngrayscale_cam_orig = cv2.resize(grayscale_cam, (orig_w, orig_h),\n                                interpolation=cv2.INTER_LINEAR)\n\n# 6. Overlay on the original image\nvisualization = show_cam_on_image(rgb_orig,\n                                  grayscale_cam_orig,\n                                  use_rgb=True)\n\n# 7. Display\nplt.figure(figsize=(8,8))\nplt.imshow(visualization)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import convnext_base\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# 1. Load sculpture image\norig_img = Image.open(\"sculpture.jpg\").convert(\"RGB\")\norig_w, orig_h = orig_img.size\nrgb_orig = np.array(orig_img) / 255.0\n\n# 2. Detect parts with YOLOv8 (assumes classes: 0=face,1=torso,2=hand)\nfrom ultralytics import YOLO\nyolo = YOLO(\"path/to/your/face_torso_hand_yolov8.pt\")\ndetections = yolo.predict(source=\"sculpture.jpg\")[0]  # first image\n\n# 3. Prepare ConvNeXt + GradCAM\nmodel = convnext_base(pretrained=True)\nmodel.eval()\ncam = GradCAM(model=model, target_layers=[model.features[-1]])\n\n# 4. Preprocess transform\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])\ninput_tensor = transform(orig_img).unsqueeze(0)\n\n# 5. Compute base CAM once (for class-agnostic) or per class\n#    Here we’ll do per class—map YOLO class to a classifier output index\nclass_to_idx = {0: 0, 1: 1, 2: 2}  # dummy mapping\n\n# Precompute CAM heatmap at 224×224 for each class\ncams = {}\nfor yolo_cls, idx in class_to_idx.items():\n    grayscale_cam = cam(input_tensor=input_tensor,\n                        targets=[ClassifierOutputTarget(idx)])[0]\n    # upsample to original size\n    cams[yolo_cls] = cv2.resize(grayscale_cam, (orig_w, orig_h),\n                                interpolation=cv2.INTER_LINEAR)\n\n# 6. Overlay per-detection\nvis = rgb_orig.copy()\nfor box, score, cls in zip(detections.boxes.xyxy, detections.boxes.conf,\n                           detections.boxes.cls):\n    x1, y1, x2, y2 = map(int, box.cpu().numpy())\n    part_cam = cams[int(cls)]\n\n    # zero-out CAM outside the box\n    mask = np.zeros_like(part_cam)\n    mask[y1:y2, x1:x2] = part_cam[y1:y2, x1:x2]\n    \n    # overlay just that masked CAM\n    vis = show_cam_on_image(vis, mask, use_rgb=True)\n\n    # draw bounding box\n    cv2.rectangle(vis, (x1,y1), (x2,y2), (255,255,255), 2)\n    cv2.putText(vis, f\"{['face','torso','hand'][int(cls)]} {score:.2f}\",\n                (x1,y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (1,1,1), 2)\n\n# 7. Display full-res result\nplt.figure(figsize=(8,8))\nplt.imshow(vis)\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "docs/HIST-414/NLI/development/text_embeddings.html",
    "href": "docs/HIST-414/NLI/development/text_embeddings.html",
    "title": "Text Embedding Analysis Through Legal-BERT",
    "section": "",
    "text": "Dataset Reading\n\nimport pandas as pd\nimport numpy as np\n\n# Load the dataset\ndf = pd.read_csv(\"../data/metadata.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\nfilename\nauthor\ntype\ntext\n\n\n\n\n0\nregina_v_wing_chong.txt\nCrease\ncase\nCREASE, J. 1885. REGINA v. WING CHONG. \\r\\n\\r\\...\n\n\n1\nwong_hoy_woon_v_duncan.txt\nCrease\ncase\nCREASE, J.\\r\\n\\r\\nWONG HOY WOON v. DUNCAN.\\r\\n...\n\n\n2\nregina_v_mee_wah.txt\nBegbie\ncase\nBRITISH COLUMBIA REPORTS.\\r\\n\\r\\nREGINA v. MEE...\n\n\n3\nregina_v_victoria.txt\nBegbie\ncase\nOF BRITISH COLUMBIA.\\r\\n\\r\\nREGINA r, CORPORAT...\n\n\n4\nquong_wing_v_the_king.txt\nFitzpatrick\ncase\nQUONG WING v. THE KING. CAN. \\r\\n\\r\\nSupreme ...\n\n\n\n\n\n\n\n\n\nNaive Word Embedding Analysis\n\n# Define a function to clean the text\nimport re\n\ndef clean_text(text):\n    \n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    \n    return text.strip()\n\n\nfrom nltk.tokenize import word_tokenize\n\n# Create the large corpus by joining all text from all authors\nall_text = \" \".join(df[\"text\"].tolist())\n\nclean_text = clean_text(all_text)\n\n\n# Load the tokenizer and model from Hugging Face\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\n\n# We will use the Legal-BERT model for this task\ntokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\nmodel = AutoModel.from_pretrained('nlpaueb/legal-bert-base-uncased')\n\n# set the model to evaluation mode\nmodel.eval()\n\nBertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0-11): 12 x BertLayer(\n        (attention): BertAttention(\n          (self): BertSdpaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)\n\n\n\n# Create the word embeddings\n# Tokenize the cleaned text into words\ntokens = word_tokenize(clean_text)\n\n# Get unique words to avoid redundant computation\nunique_tokens = list(set(tokens))\n\n# Print the shape of unique tokens\nprint(f'There are {len(unique_tokens)} unique tokens in this corpus.')\n\nThere are 4905 unique tokens in this corpus.\n\n\n\n# Prepare a dictionary to store word embeddings\nword_embeddings = {}\n\n# Batch processing for efficiency\nbatch_size = 32\nbatches = [unique_tokens[i:i + batch_size] for i in range(0, len(unique_tokens), batch_size)]\n\nfor batch in batches:\n    # Tokenize the batch\n    batch_inputs = tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=10)\n    with torch.no_grad():\n        batch_outputs = model(**batch_inputs)\n        # Use the [CLS] token embedding as the word embedding\n        for i, word in enumerate(batch):\n            word_embedding = batch_outputs.last_hidden_state[i, 0, :].numpy()\n            word_embeddings[word] = word_embedding\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n  return forward_call(*args, **kwargs)\n\n\n\n# Print embedding for the word of interest 'chinese'\n\nprint(f\"Embedding for 'chinese':\\n{word_embeddings.get('chinese')}\")\n\nEmbedding for 'chinese':\n[-6.20929956e-01 -1.41669914e-01  6.38973951e-01  5.66694997e-02\n  2.49504417e-01  3.55757445e-01 -9.64457169e-02  3.54798347e-01\n -2.72701204e-01 -6.37608767e-01  1.72130004e-01  5.87600231e-01\n  5.80052547e-02 -1.98573306e-01 -6.22220993e-01  6.23441458e-01\n -2.84138501e-01 -2.01131850e-01 -1.16010487e-01  3.39486867e-01\n -1.49681792e-01  4.16030079e-01  4.64206189e-01 -4.62918758e-01\n  3.87408853e-01  6.31607652e-01  6.86673462e-01  2.19447494e-01\n -3.76842201e-01  1.29364863e-01 -2.28453383e-01 -2.85087019e-01\n  3.50299895e-01  4.33139503e-01 -4.69817609e-01  2.95414388e-01\n  5.21553159e-02 -2.85932124e-02  4.41666007e-01  2.89368480e-01\n  3.54161978e-01 -7.48490632e-01  7.74222836e-02 -1.15736835e-01\n -1.74301475e-01  1.22696489e-01 -2.15352607e+00 -3.29317093e-01\n  1.01303458e-02 -3.54923382e-02 -1.23484582e-01  6.59714282e-01\n -8.31694156e-03  6.29764497e-01  6.69251561e-01 -4.71155584e-01\n  7.91467354e-02 -6.24099374e-01 -4.18075711e-01 -6.55549914e-02\n -2.87118733e-01 -7.29588091e-01 -1.68598548e-01  1.50030822e-01\n  5.73546231e-01 -3.64863038e-01 -1.75638422e-01 -1.08312464e+00\n -8.72777045e-01  5.20283341e-01 -4.19047356e-01 -1.03994772e-01\n -4.50686663e-01  3.96263808e-01  4.59026873e-01  1.49249226e-01\n -9.73936677e-01  2.40058377e-01  5.98394834e-02  2.12372527e-01\n -5.36928058e-01 -2.63254791e-01 -1.00200519e-01 -2.81153798e-01\n -8.00029695e-01 -2.07510263e-01  1.99834585e-01  4.22091097e-01\n -4.92548853e-01  4.73332137e-01  2.41993725e-01  2.99992532e-01\n -6.62101805e-02  4.97656286e-01  6.22859597e-01 -1.12074003e-01\n  2.90469170e-01 -7.17187077e-02  1.00461102e+00 -4.98112887e-01\n -1.79656237e-01  1.90938503e-01 -2.20559061e-01 -1.83572412e-01\n  3.41635972e-01  3.36762846e-01  3.71979505e-01  2.52298228e-02\n -2.27320552e-01 -9.94918868e-02 -3.79481077e-01  1.15087226e-01\n -5.67131303e-02 -1.43991798e-01  2.93666482e-01 -2.44778529e-01\n -1.55897692e-01  3.21055874e-02  1.62579507e-01 -2.96471626e-01\n  3.47169191e-01  2.43933558e-01  3.24788690e-01  5.33173680e-01\n  7.26383626e-01 -2.29867995e-01 -3.03071707e-01 -2.68376768e-01\n -6.95385337e-01 -4.25722748e-01  4.37337130e-01  6.58007935e-02\n  3.43149225e-03  3.50311279e-01 -6.52863622e-01  8.04584146e-01\n -7.14763224e-01 -3.13350976e-01 -4.31784183e-01 -1.15973637e-01\n  5.64839691e-02  3.03973496e-01  9.12815481e-02  4.21348840e-01\n  1.21128201e-01  1.19176537e-01 -1.22158878e-01  8.35701168e-01\n -4.48836505e-01  2.71129847e-01  3.85445319e-02  9.48291063e-01\n  2.80741602e-01  5.03640592e-01  2.05556616e-01  2.82688320e-01\n  3.95921826e-01  2.34585017e-01 -4.72099602e-01  4.53259498e-02\n  8.21071938e-02  3.81634474e-01  2.54405200e-01  2.08593875e-01\n  2.25974157e-01 -1.33065522e-01 -4.37187612e-01  2.77443707e-01\n  3.75223011e-01  8.46717730e-02  6.87633157e-02 -6.03798091e-01\n  4.42020833e-01  1.27284899e-01 -1.15679063e-01  2.35863045e-01\n -1.95469931e-01  2.60744274e-01  2.57575095e-01 -9.72114503e-04\n -7.11989403e-03  2.02457681e-02 -3.01856250e-01 -3.68264824e-01\n -5.51048756e-01 -3.21638107e-01  6.82744801e-01  3.11158597e-01\n  6.41475201e-01  4.92584072e-02  3.97575051e-01  1.49251878e-01\n -6.21115148e-01  1.83911607e-01  9.18093994e-02 -3.95073116e-01\n  4.48636059e-03 -4.84976768e-01 -5.95896304e-01  4.45791148e-02\n -1.56988084e-01 -3.84183414e-02  5.43418348e-01 -8.11479092e-01\n -6.62249386e-01  3.90995383e-01  6.33010343e-02  5.95393836e-01\n  5.37799112e-02 -1.34044200e-01  7.78971165e-02  5.71762860e-01\n  7.11518079e-02 -5.18555701e-01 -6.77722543e-02 -1.98609218e-01\n  3.19237523e-02 -1.81521446e-01  7.71615386e-01  4.34639424e-01\n -3.50789547e-01 -6.15721047e-01  7.47690797e-01  1.04829222e-01\n -3.23299944e-01  4.18362141e-01 -4.87017900e-01  3.74183536e-01\n  1.56449273e-01 -5.77521548e-02  5.83280206e-01 -5.13220251e-01\n -2.18854100e-03 -2.09312946e-01  3.91384870e-01  3.79638702e-01\n  9.49216425e-01 -1.29741237e-01  5.69063306e-01 -4.44785506e-01\n -3.64393830e-01 -2.67691910e-01  1.04195893e-01  5.82311749e-01\n -1.04580969e-01 -3.69361848e-01 -2.09110811e-01 -4.69661951e-01\n  3.93494815e-02  1.20271072e-01 -4.09655809e-01  3.17582756e-01\n -4.20122355e-01 -1.83060527e-01 -1.00395709e-01  2.06323966e-01\n -1.38006851e-01 -4.76040810e-01  4.09804583e-01  3.76812190e-01\n  3.12000930e-01 -8.93614113e-01 -4.87620503e-01 -3.83996904e-01\n  1.66279912e-01 -4.89073694e-01  1.85237497e-01 -2.76643574e-01\n  2.62034088e-01  1.78466216e-01  3.37472558e-01  2.73906887e-01\n -2.62220383e-01 -3.90416145e-01 -4.20720190e-01  2.96501964e-02\n -6.88901782e-01  9.03950632e-02  4.92249221e-01 -1.56695545e-01\n -2.83631593e-01  3.26625034e-02 -6.74628764e-02  8.64049271e-02\n  9.33768451e-02  2.47449368e-01  1.16173588e-01 -4.40933466e-01\n -3.54221940e-01  4.36044246e-01 -4.43125516e-01 -1.34401396e-01\n -2.39204153e-01 -3.71705174e-01  4.45067547e-02 -1.50166914e-01\n  2.20735818e-01  8.04604143e-02 -4.51403528e-01 -3.34843814e-01\n -1.17283873e-01  9.54926550e-01  1.49068922e-01 -1.64130867e-01\n  2.71167696e-01  5.29190339e-02 -2.54569560e-01  5.92227936e-01\n -8.31328583e+00 -1.58776045e-01  2.74165452e-01  4.40410018e-01\n  3.51175398e-01 -1.75801247e-01 -4.99369167e-02 -3.76174986e-01\n  3.19188148e-01 -7.06144333e-01  8.99769068e-01  2.41788909e-01\n  4.14058030e-01  2.70290822e-01 -8.15664157e-02 -3.30294192e-01\n -1.47519931e-01 -6.20588362e-01 -4.30307984e-01 -9.92837697e-02\n  1.35646656e-01 -3.30278993e-01  8.26435164e-04 -3.72957140e-02\n -8.20025563e-01 -1.79837734e-01  2.61670411e-01  5.46909034e-01\n  2.47776508e-01 -9.14679348e-01  4.57372546e-01 -7.14825928e-01\n  2.61447102e-01  3.62202264e-02 -2.40433916e-01 -1.51181951e-01\n  5.14292456e-02  2.96004355e-01 -3.33741009e-02  7.91547775e-01\n -2.90189207e-01 -2.67834395e-01  1.55358255e-01 -3.04543316e-01\n -6.61973655e-03  5.82447290e-01 -8.84473324e-02 -2.95755386e-01\n -2.36821383e-01  2.25739777e-01  1.06621131e-01  4.18176949e-02\n  2.94192970e-01 -4.99745190e-01  6.70128882e-01 -4.35431786e-02\n  7.06657469e-01  7.02272058e-01 -1.19627364e-01 -7.91100383e-01\n -6.75487995e-01 -1.07149065e-01 -7.84962595e-01  4.35174704e-01\n -3.13441977e-02  2.44785011e-01 -8.73120725e-01  3.50022353e-02\n  2.09037378e-01 -1.19712971e-01 -1.83357909e-01  6.73410237e-01\n  6.20655060e-01 -4.46414083e-01 -4.69991937e-02 -2.27657735e-01\n -1.87436327e-01 -5.16896725e-01 -6.93054259e-01 -6.81385517e-01\n -7.97633290e-01 -5.03173769e-01  3.59278142e-01 -9.57255736e-02\n -4.03269604e-02 -4.18358952e-01  1.79500312e-01 -2.03033864e-01\n  4.86599207e-01  4.59470987e-01  8.80892158e-01 -8.17529619e-01\n -1.26835912e-01 -4.91106033e-01 -2.95128614e-01 -4.75888133e-01\n -5.21908216e-02  2.00935096e-01  3.80766988e-01  1.80223122e-01\n  4.48103130e-01 -3.33189778e-02 -4.20342296e-01 -7.12261915e-01\n -3.03021073e-01  1.31230146e-01 -6.22782826e-01  3.06484938e-01\n -6.40364215e-02 -3.02132308e-01  8.56923699e-01  1.15723848e-01\n  3.39410037e-01  5.69929004e-01  1.52576774e-01 -5.94419658e-01\n -1.86862931e-01 -5.08795083e-01  2.31838319e-02 -3.78885031e-01\n -1.16499260e-01  5.03729165e-01 -6.11288905e-01  4.04664040e-01\n -1.57213137e-01  2.46463999e-01 -8.03821445e-01  2.13734694e-02\n  1.84446663e-01  1.37956098e-01  4.44984883e-01  3.24014008e-01\n  5.92168689e-01  4.99686182e-01 -2.96122074e-01 -1.36218712e-01\n  3.44956547e-01  3.80547382e-02  2.13075086e-01 -1.07691221e-01\n -2.26502255e-01  4.00369644e-01  4.69331175e-01  4.01728719e-01\n  1.76137567e-01 -5.67294478e-01  9.60933790e-02  6.66244209e-01\n  5.86347163e-01 -1.36371404e-01  3.98506761e-01 -6.31231308e-01\n  1.67884022e-01  3.80158991e-01 -1.42207444e-02 -6.25287741e-03\n  8.95560384e-02 -3.62140477e-01  1.87743872e-01  4.58195776e-01\n -7.12240458e-01 -1.88387841e-01 -1.19760895e+00 -3.01312715e-01\n  1.09179556e-01  7.96130240e-01 -5.37170544e-02  1.74531769e-02\n  3.53715181e-01  1.95554882e-01  7.30575204e-01 -5.38034201e-01\n  4.53047007e-01  1.16199367e-01  1.04140759e-01  1.75456032e-02\n -1.89646021e-01 -4.50457111e-02 -2.96386540e-01  8.41702640e-01\n  1.34698525e-01  2.22343564e-01  3.41144174e-01  3.16154957e-01\n -1.96809396e-01 -1.29942954e-01  1.70184225e-01 -2.71234393e-01\n  3.57916534e-01  6.67839423e-02 -4.23583910e-02 -4.82594728e-01\n  1.66769296e-01  4.94927704e-01  4.79743063e-01  3.16114455e-01\n  2.30504364e-01  3.55769023e-02 -5.56551456e-01  3.27263087e-01\n  7.53074363e-02  4.10910100e-02  4.74388115e-02 -7.82817900e-02\n -3.72774750e-01  1.69891298e-01  7.65117764e-01 -2.97236681e-01\n  9.89364535e-02 -1.80658445e-01 -3.42060685e-01  2.08858326e-02\n -4.04214978e-01  2.49380052e-01  3.97761703e-01  6.12111926e-01\n  2.35218033e-01  5.57211339e-01 -2.22379059e-01  8.74022245e-02\n -1.54909298e-01  2.01315686e-01  5.51343322e-01  4.11243141e-01\n  1.35008991e-01 -7.33748376e-01  1.75054118e-01  9.85418335e-02\n -2.71403968e-01 -4.27066609e-02 -7.39626288e-02 -8.95793512e-02\n -6.73125267e-01  3.34185362e-03  4.13883299e-01 -8.63712072e-01\n -7.17986166e-01  4.02059108e-01 -5.30343503e-03 -4.63847704e-02\n -9.58980799e-01 -5.41762769e-01 -2.94652343e-01 -1.40150785e-01\n -5.82643487e-02 -3.00248891e-01  4.91785020e-01 -7.32115865e-01\n  7.90896490e-02  6.98811293e-01 -1.66678444e-01  2.70790845e-01\n -3.74785125e-01  5.75993583e-02  2.90283650e-01  2.36416310e-01\n -7.49478340e-01 -5.69795251e-01 -1.31442502e-01  6.77229762e-02\n  5.24142459e-02 -2.42648467e-01 -3.51804316e-01 -2.79660672e-01\n  2.92793125e-01  2.59541478e-02  1.31968647e-01 -1.84148461e-01\n -3.91101480e-01  3.43214124e-02  3.82385030e-02  3.72994915e-02\n -2.93730259e-01  5.79028249e-01  5.89166403e-01  2.75530010e-01\n  5.50004840e-03  2.74638981e-01 -1.79950923e-01  2.87223980e-02\n -1.35058343e-01 -4.60491091e-01 -8.06818187e-01  3.41826946e-01\n -4.05554414e-01  2.48564854e-01  2.85961777e-01 -8.20883363e-02\n -1.83619842e-01 -4.03567672e-01  4.51681554e-01  6.78313673e-01\n -2.61587054e-01 -6.08418941e-01 -7.37745225e-01  5.62529683e-01\n  2.67416030e-01  3.26011866e-01 -3.88305783e-01  1.19363032e-01\n  1.32869795e-01 -2.45447546e-01  2.76360720e-01  4.10816789e-01\n -1.66601807e-01 -4.75261152e-01  4.95802276e-02 -4.01173294e-01\n  4.47842479e-02  2.52735853e-01  4.99048799e-01 -2.71846294e-01\n -2.30236679e-01 -4.19545352e-01 -6.44784272e-01 -2.02411860e-02\n  1.57229856e-01  5.70324719e-01 -4.36489373e-01 -5.75618327e-01\n -3.09315890e-01  7.87956774e-01  1.20356046e-01  1.53235763e-01\n -3.91700119e-02 -6.28403127e-01 -5.95002592e-01  2.11278677e-01\n -1.00958347e-01 -6.25989020e-01 -1.93989336e-01  2.68235356e-01\n  6.01909697e-01 -1.00548573e-01 -7.74189308e-02 -6.57544732e-02\n  1.52519777e-01  2.57917996e-02  2.68099010e-02 -1.81484312e-01\n  9.75855887e-02 -3.44188064e-01 -4.13170643e-02  2.30506361e-01\n  4.41174924e-01 -4.94555026e-01 -5.63982010e-01 -3.73879462e-01\n -1.57981247e-01 -1.60825193e-01  6.00513339e-01  1.28841642e-02\n -1.66598916e-01  2.62471616e-01 -4.88434464e-01  1.59599692e-01\n  8.19031179e-01  3.17419261e-01  2.73416519e-01 -2.63669342e-01\n  4.64785457e-01 -6.24555834e-02  4.31151867e-01 -3.60418260e-02\n -2.06647277e-01  4.98503923e-01 -2.39020959e-01  2.77610064e-01\n -2.38970906e-01  2.57991642e-01 -4.60961968e-01 -6.56904638e-01\n -5.68882048e-01  3.67466897e-01  1.33221045e-01  6.14784420e-01\n -6.97932363e-01  1.25866085e-01 -7.44488120e-01  7.49795198e-01\n  1.52957171e-01 -6.04382813e-01 -5.13626337e-02 -1.60399359e-02\n -2.52286434e-01  2.35126168e-01  2.17649221e-01 -1.34040356e-01\n -3.57570648e-01  2.62209028e-01  8.41782987e-02  3.69732797e-01\n -1.52155936e-01 -1.49425477e-01  2.26673886e-01 -1.80154204e-01\n  3.90131682e-01 -1.35756254e-01  3.67597282e-01 -1.23757668e-01\n -5.05584955e-01 -1.76664218e-01  4.05613303e-01  2.53209800e-01\n -6.05255246e-01 -4.27263796e-01  1.87023267e-01 -7.18621790e-01\n -3.03840339e-01 -9.55155343e-02  5.01942158e-01  8.24584484e-01\n  2.96675533e-01 -5.18148124e-01  3.41293961e-01 -4.17495072e-01\n -5.55484295e-01  4.51619178e-01  2.32937664e-01  2.70074666e-01\n  2.09832400e-01 -5.74077368e-01  1.88415691e-01  5.79990566e-01\n -3.45205307e-01  6.84873685e-02 -1.38211906e-01  4.09526706e-01\n  6.40893579e-01  5.76397896e-01 -4.55712646e-01 -4.62623239e-01\n -2.42349565e-01  6.17270052e-01  2.84024298e-01  2.09292769e-03\n -3.66119117e-01  4.20001596e-01 -3.87326509e-01 -3.73405725e-01\n  2.78264396e-02  2.48537898e-01 -7.01753974e-01 -1.90863177e-01\n -4.03226405e-01  4.49352920e-01  1.93640038e-01  1.57222137e-01\n  5.84116727e-02 -2.18468346e-02 -7.76933804e-02  5.18231802e-02\n  2.96478182e-01 -5.23954153e-01  1.57790691e-01 -1.08946666e-01\n  5.36393166e-01 -1.97972674e-02  3.74067098e-01 -3.51414710e-01]\n\n\n\n# Compute cosine similarity between all words with Chinese in the model\nfrom scipy.spatial.distance import cosine\n\nsimilarity_scores = {}\n\nfor other_word in word_embeddings.keys():\n    if other_word != \"chinese\":\n        similarity = 1 - cosine(word_embeddings[\"chinese\"], word_embeddings[other_word])\n        similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'chinese':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\nTop 10 most similar words to 'chinese':\njapanese: 0.8790\nchong: 0.8652\nalien: 0.8581\nfourteen: 0.8564\njaw: 0.8557\nking: 0.8519\nhong: 0.8516\ncontradiction: 0.8485\ncousin: 0.8480\ninferior: 0.8472\n\n\n\n# Compute cosine similarity between all words with Chinaman in the model\n\nsimilarity_scores = {}\n\nfor other_word in word_embeddings.keys():\n    if other_word != \"chinaman\":\n        similarity = 1 - cosine(word_embeddings[\"chinaman\"], word_embeddings[other_word])\n        similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'chinaman'\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\nTop 10 most similar words to 'chinaman'\nprovei: 0.9922\nclamor: 0.9916\ntotally: 0.9916\ntoday: 0.9914\nchinamen: 0.9911\ninterred: 0.9910\nunequalled: 0.9909\nsurely: 0.9906\nsemitropical: 0.9904\nquickly: 0.9904\n\n\n\n# Generate a t-SNE plot for visualization\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=42)\n\nembeddings = np.array(list(word_embeddings.values()))\ntsne_results = tsne.fit_transform(embeddings)\n\n\n# Create a DataFrame for visualization\nimport plotly.express as px\n\ndf_tsne = pd.DataFrame(tsne_results, columns=['x', 'y'])\ndf_tsne['word'] = list(word_embeddings.keys())\n# Highlight the word 'chinese' in the plot\nkeywords = [\"chinese\", \"china\", \"chinaman\", \"chinamen\"]\ndf_tsne['highlight'] = df_tsne['word'].apply(lambda x: x if x in keywords else '')\n\nfig = px.scatter(\n    df_tsne,\n    x='x',\n    y='y',\n    title='t-SNE Visualization of legal-BERT Word Embeddings',\n    color='highlight',                        \n    hover_data=['word'], \n    text= 'highlight',\n    height=600,\n    width=800\n)\n\nfig.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  sf: grouped.get_group(s if len(s) &gt; 1 else s[0])\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nText Embedding and Analysis of Crease and Begbie Corpus\n\n# Compile the Crease texts into a single text list\ncrease_texts = df[df['author'] == 'Crease']['text'].tolist()\n\n# Compile the Begbie texts into a single text list\nbegbie_texts = df[df['author'] == 'Begbie']['text'].tolist()\n\n# Combine both lists in a dictionary\njudge_dict = {\n    'Crease': crease_texts,\n    'Begbie': begbie_texts\n}\n\n\n# Define a function to embed text using the model\nfrom typing import Union, List\n\ndef embed_text(\n    text: str,\n    focus_token: Union[str, List[str]] = None,\n    window: int = 5,\n    tokenizer=tokenizer,\n    model=model)-&gt; np.ndarray:\n    \"\"\"\n    text: the raw string\n    focus_token: either a single word, or a list of words to look for\n    window: how many tokens on each side to include\n    tokenizer: HuggingFace tokenizer\n    model: BERT model\n    \"\"\"\n    # Run the model once\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    hidden = outputs.last_hidden_state.squeeze(0) \n\n    if focus_token is None:\n        return hidden[0].cpu().numpy()\n    \n    # Normalize to list\n    keywords = (\n        [focus_token] if isinstance(focus_token, str)\n        else focus_token\n    )\n\n    # Pre-tokenize each keyword to its subtoken ids\n    kw_token_ids = {\n        kw: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(kw))\n        for kw in keywords\n    }\n\n    input_ids = inputs[\"input_ids\"].squeeze(0).tolist()\n    spans = []  # list of (start, end) index pairs\n\n    # find every match of every keyword\n    for kw, sub_ids in kw_token_ids.items():\n        L = len(sub_ids)\n        for i in range(len(input_ids) - L + 1):\n            if input_ids[i:i+L] == sub_ids:\n                spans.append((i, i+L))\n\n    if not spans:\n        # fallback on CLS vector\n        return hidden[0].cpu().numpy()\n\n    # For each span, grab the window around it\n    vecs = []\n    for (start, end) in spans:\n        lo = max(1, start - window)\n        hi = min(hidden.size(0), end + window)\n        # mean‑pool over all tokens in this extended window\n        span_vec = hidden[lo:hi].mean(dim=0).cpu().numpy()\n        vecs.append(span_vec)\n\n    # Average across all spans\n    return np.mean(np.stack(vecs, axis=0), axis=0)\n\n\nfrom nltk import sent_tokenize \n# Create a dictionary to hold the mentionings of \"Chinese\" by author\njudge_snippets = {}\n\nkeywords = [\"Chinese\", \"China\", \"Chinaman\", \"Chinamen\", \"immigrant\", \"immigrants\", \"immigration\"]\nfor auth, texts in judge_dict.items():\n    snippets = []\n    for txt in texts:\n        sentence = sent_tokenize(txt)\n        for sent in sentence:\n            if any(keyword in sent for keyword in keywords):\n                snippets.append(sent)\n    judge_snippets[auth] = snippets\n\n\n# Investigate the length of the snippets\nn_snippet = {auth: len(snippets) for auth, snippets in judge_snippets.items()}\n\nprint(\"Snippet size by author:\")\nfor auth, num in n_snippet.items():\n    print(f\"{auth}: {num}\")\n    \n\nSnippet size by author:\nCrease: 146\nBegbie: 108\n\n\n\n# Define an ethnicity anchor, not including \"chinese\"\nethnicities = [\n    \"Japanese\",   \"Korean\",    \"Vietnamese\",\n    \"Filipino\",    \"Thai\",       \"Malay\",     \"Indian\",\n    \"Pakistani\",   \"Bangladeshi\",\"Nepalese\",  \"Tibetan\",\n    \"Arab\",        \"Persian\",    \"Turkish\",   \"Slavic\",\n    \"Germanic\",    \"Celtic\",     \"Slavic\",    \"Romani\",\n    \"Jewish\",      \"Zulu\",       \"Xhosa\",     \"Maori\",\n    \"Sami\",        \"Berber\",     \"Tamil\",     \"Punjabi\",\n    \"Bengali\",     \"Kazakh\",     \"Uyghur\"\n]\n\n# Create embeddings\neth_vecs = []\nfor e in ethnicities:\n    eth_vecs.append(embed_text(e))\n    \neth_anchor = np.mean(eth_vecs, axis=0)\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning:\n\n`encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n\n\n\n\n# Create embeddings\nembeddings_dict = {'Crease': [], 'Begbie': []}\n\nfor auth, snippets in judge_snippets.items():\n    for snip in snippets:\n        v = embed_text(snip, focus_token=keywords, window=15)\n        embeddings_dict[auth].append(v)\n\n\n# Create embeddings that subtract the ethnicity anchor\nsubtracted_embeddings_dict = {'Crease': [], 'Begbie': []}\n\nfor auth, embeddings in embeddings_dict.items():\n    for emb in embeddings:\n        v = emb - eth_anchor\n        subtracted_embeddings_dict[auth].append(v)\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Compute per author mean and cosine similarity\nmean_crease = np.mean(embeddings_dict[\"Crease\"], axis=0, keepdims=True)  \nmean_begbie = np.mean(embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\n\n# Compute the pairwise cosine similarity\nsim_crease_begbie = cosine_similarity(mean_crease, mean_begbie)[0, 0] \n\nprint(f\"Cosine similarity between Crease and Begbie: {sim_crease_begbie:.4f}\")\n\nCosine similarity between Crease and Begbie: 0.9948\n\n\n\n# Compute per author mean and cosine similarity\nsubtracted_mean_crease = np.mean(subtracted_embeddings_dict[\"Crease\"], axis=0, keepdims=True)  \nsubtracted_mean_begbie = np.mean(subtracted_embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\n\n# Compute the pairwise cosine similarity\nsim_crease_begbie_sub = cosine_similarity(subtracted_mean_crease, subtracted_mean_begbie)[0, 0] \n\nprint(f\"Cosine similarity between Crease and Begbie with ethical axis removed: {sim_crease_begbie_sub:.4f}\")\n\nCosine similarity between Crease and Begbie with ethical axis removed: 0.9971\n\n\n\n# We check if the cosine similarity is lower when one subtracted the ethnicity anchor but not the other\nsim_crease_sub_begbie = cosine_similarity(mean_crease, subtracted_mean_begbie)[0, 0]\nsim_sub_crease_begbie = cosine_similarity(subtracted_mean_crease, mean_begbie)[0, 0]\n\nprint(f\"Cosine similarity between Crease and Begbie with Begbie removed ethnical axis: {sim_crease_sub_begbie:.4f}\")\nprint(f\"Cosine similarity between Crease and Begbie with Crease removed ethnical axis: {sim_sub_crease_begbie:.4f}\")\n\nCosine similarity between Crease and Begbie with Begbie removed ethnical axis: 0.6105\nCosine similarity between Crease and Begbie with Crease removed ethnical axis: 0.6063\n\n\n\n# We check the cosine similarity of max and min embeddings\nmax_crease = np.max(embeddings_dict[\"Crease\"], axis=0, keepdims=True)\nmin_crease = np.min(embeddings_dict[\"Crease\"], axis=0, keepdims=True)\nmax_begbie = np.max(embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\nmin_begbie = np.min(embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\n\nsim_crease_begbie_max = cosine_similarity(max_crease, max_begbie)[0, 0]\nsim_crease_begbie_min = cosine_similarity(min_crease, min_begbie)[0, 0]\n\nprint(f\"Cosine similarity between max Crease and max Begbie: {sim_crease_begbie_max:.4f}\")\nprint(f\"Cosine similarity between min Crease and min Begbie: {sim_crease_begbie_min:.4f}\")\n\nCosine similarity between max Crease and max Begbie: 0.9785\nCosine similarity between min Crease and min Begbie: 0.9843\n\n\n\nimport matplotlib.pyplot as plt\n# We can bootstrap the similarity score to get a confidence interval\ncrease_embeddings = np.array(embeddings_dict[\"Crease\"])\nbegbie_embeddings = np.array(embeddings_dict[\"Begbie\"])\n\n# Convert to arrays\ncrease_embeddings = np.vstack(crease_embeddings)\nbegbie_embeddings = np.vstack(begbie_embeddings)\n\nn_boot = 1000 \ncosine_scores = []\n\nfor _ in range(n_boot):\n    # Sample with replacement\n    crease_sample = crease_embeddings[np.random.choice(len(crease_embeddings), size=len(crease_embeddings), replace=True)]\n    begbie_sample = begbie_embeddings[np.random.choice(len(begbie_embeddings), size=len(begbie_embeddings), replace=True)]\n    \n    # Compute mean embeddings\n    mean_crease_boot = np.mean(crease_sample, axis=0, keepdims=True)\n    mean_begbie_boot = np.mean(begbie_sample, axis=0, keepdims=True)\n    \n    # Compute cosine similarity\n    cos_sim = cosine_similarity(mean_crease_boot, mean_begbie_boot)[0][0]\n    cosine_scores.append(cos_sim)\n\n# Convert to numpy array\ncosine_scores = np.array(cosine_scores)\n\n# Compute 95% confidence interval\nlower = np.percentile(cosine_scores, 2.5)\nupper = np.percentile(cosine_scores, 97.5)\nmean_sim = np.mean(cosine_scores)\n\n# Plot the bootstrap distribution\nplt.figure(figsize=(8, 4))\nplt.hist(cosine_scores, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Add lines for mean and confidence interval\nplt.axvline(mean_sim, color='black', linestyle='-', linewidth=2, label=f'Mean: {mean_sim:.4f}')\nplt.axvline(lower, color='red', linestyle='--', linewidth=2, label=f'2.5%: {lower:.4f}')\nplt.axvline(upper, color='green', linestyle='--', linewidth=2, label=f'97.5%: {upper:.4f}')\n\nplt.title('Bootstrap Distribution of Cosine Similarity Between Crease and Begbie')\nplt.xlabel('Cosine Similarity')\nplt.ylabel('Frequency')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Print the results\nprint(f\"Bootstrap mean cosine similarity between Crease and Begbie: {mean_sim:.4f}\")\nprint(f\"95% Confidence Interval: [{lower:.4f}, {upper:.4f}]\")\n\n\n\n\nBootstrap mean cosine similarity between Crease and Begbie: 0.9936\n95% Confidence Interval: [0.9919, 0.9951]\n\n\n\n# Similarly, bootstrap the similarity score to get a confidence interval\ncrease_embeddings = np.array(subtracted_embeddings_dict[\"Crease\"])\nbegbie_embeddings = np.array(subtracted_embeddings_dict[\"Begbie\"])\n\n# Convert to arrays\ncrease_embeddings = np.vstack(crease_embeddings)\nbegbie_embeddings = np.vstack(begbie_embeddings)\n\nn_boot = 1000 \nsubtracted_cosine_scores = []\n\nfor _ in range(n_boot):\n    # Sample with replacement\n    crease_sample = crease_embeddings[np.random.choice(len(crease_embeddings), size=len(crease_embeddings), replace=True)]\n    begbie_sample = begbie_embeddings[np.random.choice(len(begbie_embeddings), size=len(begbie_embeddings), replace=True)]\n    \n    # Compute mean embeddings\n    mean_crease_boot = np.mean(crease_sample, axis=0, keepdims=True)\n    mean_begbie_boot = np.mean(begbie_sample, axis=0, keepdims=True)\n    \n    # Compute cosine similarity\n    cos_sim = cosine_similarity(mean_crease_boot, mean_begbie_boot)[0][0]\n    subtracted_cosine_scores.append(cos_sim)\n\n# Convert to numpy array\nsubtracted_cosine_scores = np.array(subtracted_cosine_scores)\n\n# Compute 95% confidence interval\nlower = np.percentile(subtracted_cosine_scores, 2.5)\nupper = np.percentile(subtracted_cosine_scores, 97.5)\nmean_sim = np.mean(subtracted_cosine_scores)\n\n# Plot the bootstrap distribution\nplt.figure(figsize=(8, 4))\nplt.hist(subtracted_cosine_scores, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n\n# Add lines for mean and confidence interval\nplt.axvline(mean_sim, color='black', linestyle='-', linewidth=2, label=f'Mean: {mean_sim:.4f}')\nplt.axvline(lower, color='red', linestyle='--', linewidth=2, label=f'2.5%: {lower:.4f}')\nplt.axvline(upper, color='green', linestyle='--', linewidth=2, label=f'97.5%: {upper:.4f}')\n\nplt.title('Bootstrap Distribution of Cosine Similarity Between Crease and Begbie (Ethnical Axis Removed)')\nplt.xlabel('Cosine Similarity')\nplt.ylabel('Frequency')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n# Print the results\nprint(f\"Bootstrap mean cosine similarity between Crease and Begbie with ethnical axis removed: {mean_sim:.4f}\")\nprint(f\"95% Confidence Interval: [{lower:.4f}, {upper:.4f}]\")\n\n\n\n\nBootstrap mean cosine similarity between Crease and Begbie with ethnical axis removed: 0.9964\n95% Confidence Interval: [0.9953, 0.9972]\n\n\n\n# Testing if the ethnical axis removal has a significant effect on the similarity score\n# We can use a bootstrapped t-test to compare the means of the two distributions\nfrom scipy.stats import ttest_ind\n\n# Perform bootstrapped t-test\nt_stat, p_value = ttest_ind(subtracted_cosine_scores, cosine_scores, equal_var=False)\n\n# Print the results\nprint(f\"t-statistic: {t_stat:.4f}\")\nprint(f\"p-value: {p_value:.4f}\")\n\nif p_value &lt; 0.05:\n    print(\"The difference in mean cosine similarity is statistically significant.\")\nelse:\n    print(\"The difference in mean cosine similarity is not statistically significant.\")\n\nt-statistic: 88.7248\np-value: 0.0000\nThe difference in mean cosine similarity is statistically significant.\n\n\nWith the above test on embeddings with or without the ethnical axis, we confirmed that the difference between cosine similarities of Crease and Begbie are statistically significantly biased by the ethnical factors. However, with the small difference in real cosine similarity, we can conclude that the two characters are still very similar in terms of their text embeddings, thus the bias is not very significant in the real world.\n\n# Create UMAP projection for visualization\nimport umap \n\nall_vecs = np.vstack(embeddings_dict[\"Crease\"] + embeddings_dict[\"Begbie\"])\nlabels  = ([\"Crease\"] * len(embeddings_dict[\"Crease\"])) + ([\"Begbie\"] * len(embeddings_dict[\"Begbie\"]))\n\nreducer = umap.UMAP(n_neighbors=15, min_dist=0.1)\nproj = reducer.fit_transform(all_vecs) \n\n\n# plot using plotly to further explore\nimport plotly.express as px\nimport textwrap\n\ndef wrap_text(text, width=60):\n    return '&lt;br&gt;'.join(textwrap.wrap(text, width=width))\n\numap_df = pd.DataFrame(proj, columns=['UMAP 1', 'UMAP 2'])\numap_df['Author'] = labels\numap_df['Text'] = [snip for auth in judge_snippets for snip in judge_snippets[auth]]\numap_df['Text'] = umap_df['Text'].apply(lambda t: wrap_text(t, width=50))\n\nfig = px.scatter(umap_df, x='UMAP 1', y='UMAP 2', \n                 color='Author', hover_data=['Text'], \n                 width=800, height=500 )\nfig.update_traces(marker=dict(size=5))\nfig.update_layout(title='UMAP Projection of Word Embeddings by Author')\nfig.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Print out the 10 most similar embedding sentences to Crease's mean embedding\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ncrease_similarity_df = pd.DataFrame(columns=['Author', 'Text', 'Similarity Score'])\n\n# Iterate through the embeddings and their corresponding sentences\nfor auth, snippets in judge_snippets.items():\n    for snippet, emb in zip(snippets, embeddings_dict[auth]):\n        similarity = cosine_similarity(emb.reshape(1, -1), mean_crease)[0][0]\n        crease_similarity_df.loc[len(crease_similarity_df)] = [\n            auth, snippet, similarity\n        ]\n\n# Sort by similarity score\ncrease_sorted_similarity = crease_similarity_df.sort_values(by='Similarity Score', ascending=False)\n\nprint(\"Top 10 most similar sentences to Crease's mean embedding:\\n\")\n\nfor _, row in crease_sorted_similarity.head(10).iterrows():\n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    print(f\"Author: {row['Author']}\\nSentence: {wrapped_para}\\nSimilarity Score: {row['Similarity Score']:.4f}\\n\")\n\nTop 10 most similar sentences to Crease's mean embedding:\n\nAuthor: Crease\nSentence: The Act is found associated with another Act now disallowed, the express object of which is to\nprevent the Chinese altogether from coming to this country, and the principle \"noscitur a sociis\" is\nkept up by the preamble of the present Act, which describes the Chinese in terms which, I venture to\nthink, have never before in any other country found a place in an Act of Parliament.\nSimilarity Score: 0.9635\n\nAuthor: Crease\nSentence: In coming to British Columbia, and while here, the Chinese have no idea of interfering with the\nproperty of the white population in any way beyond the ordinary competition which they offer in the\nlabor market.\nSimilarity Score: 0.9630\n\nAuthor: Crease\nSentence: The strike of the Chinese in Victoria when resisting an intentionally discriminating and illegal tax\nof $30 a head on all Chinese-although it occurred a few years ago-is too fresh in the recollection\nto be forgotten.\nSimilarity Score: 0.9628\n\nAuthor: Crease\nSentence: In the case of the Chinese treaties, they were forced at the point of the bayonet on China, to\nobtain a right for us to enter China, and in return for a similar permission to us, full permission\nwas given for the Chinese to trade and reside in British dominions everywhere.\nSimilarity Score: 0.9621\n\nAuthor: Begbie\nSentence: Statutes were by their title and preamble REGINA v. MEE WAH expressly aimed at Chinamen by name;\nthat this distinction also renders inapplicable all the United States' cases cited; that this\nenactment is quite general extending to all laundries without exception and we must not look beyond\nthe words of the enactment to enquire what its object was; that there is in fact one laundry in\nVictoria not conducted by Chinamen on which the tax will fall with equal force so that it is\nimpossible to say that Chinamen are hereby exclusively selected for taxation; the circumstance that\nthey are chiefly affected being a mere coincidence; that the bylaw only imposes $100.00 per annum,\nkeeping far within the limit of $150.00 permitted by the Statute; that the tax clearly is calculated\nto procuring additional Municipal revenue and that no other object is hinted at.\nSimilarity Score: 0.9606\n\nAuthor: Crease\nSentence: Though possessed of all the qualities I have described, Chinamen do not make good settlers in the\nsense of raising up citizens of a free.\nSimilarity Score: 0.9605\n\nAuthor: Crease\nSentence: It has generally been supposed from the secrecy with which some murders were committed (they are\nconfined to two or three Chinese murders altogether in many years), which have occurred in British\nColumbia undiscovered, that the victims were executed by the decrees of some secret Chinese\ntribunal, like the Vehm-Gericht, having its centre in San Francisco, but I have not been able to\ndiscover a single fact which tended to corroborate that suspicion.\nSimilarity Score: 0.9596\n\nAuthor: Begbie\nSentence: When we find (1st) no other description of labour taxed at all ; (2nd) this description of labour\npractically quite abandoned to Chinamen alone ; (3rd) this description of labour taxed at fifteen\ntimes the rate permitted to be levied on any retail shop ; (4th) that a preliminary Provincial Act\nhas declared Chinamen incapable of the franchise which they formerly exercised.\nSimilarity Score: 0.9590\n\nAuthor: Crease\nSentence: The grounds alleged by the Health Officer for this arbitrary treatment is “that China is an infected\nlocality,” and all persons coming from China and especially in this instance Hongkong, and\nespecially also the natives of China, come from an infected locality, and that he has that authority\nunder the Health By-Law, 1893, which was passed at the time of the small pox panic in Victoria, and,\nin all he did, was simply doing his duty.\nSimilarity Score: 0.9588\n\nAuthor: Crease\nSentence: I do not say that all these evils whether white or Chinese should not be determinedly suppressed,\nbut there is such a manifest spirit of exaggeration in the complaints that are made, for the purpose\nI have described, as very materially to lessen in impartial eyes the accusations levelled against\nthe Chinese.\nSimilarity Score: 0.9586\n\n\n\n\n# Print out the 10 most similar embedding sentences to Begbie's mean embedding\nbegbie_similarity_df = pd.DataFrame(columns=['Author', 'Text', 'Similarity Score'])\n\n# Iterate through the embeddings and their corresponding sentences\nfor auth, snippets in judge_snippets.items():\n    for snippet, emb in zip(snippets, embeddings_dict[auth]):\n        similarity = cosine_similarity(emb.reshape(1, -1), mean_begbie)[0][0]\n        begbie_similarity_df.loc[len(begbie_similarity_df)] = [\n            auth, snippet, similarity\n        ]\n\n# Sort by similarity score\nbegbie_sorted_similarity = begbie_similarity_df.sort_values(by='Similarity Score', ascending=False)\n\nprint(\"Top 10 most similar sentences to Begbie's mean embedding:\\n\")\n\nfor _, row in begbie_sorted_similarity.head(10).iterrows():\n    \n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    \n    print(f\"Author: {row['Author']}\\nSentence: {wrapped_para}\\nSimilarity Score: {row['Similarity Score']:.4f}\\n\")\n\nTop 10 most similar sentences to Begbie's mean embedding:\n\nAuthor: Crease\nSentence: Though possessed of all the qualities I have described, Chinamen do not make good settlers in the\nsense of raising up citizens of a free.\nSimilarity Score: 0.9644\n\nAuthor: Begbie\nSentence: Whites who have evil communications with Chinese must themselves be lamentably depraved beforehand;\nand so, I should be disposed to say, immoral Chinese are not only not more injurious, but they are\nquite innocuous to the morals of the whites, in comparison with white people of similar or allied\nhabits.\nSimilarity Score: 0.9620\n\nAuthor: Crease\nSentence: The strike of the Chinese in Victoria when resisting an intentionally discriminating and illegal tax\nof $30 a head on all Chinese-although it occurred a few years ago-is too fresh in the recollection\nto be forgotten.\nSimilarity Score: 0.9610\n\nAuthor: Crease\nSentence: I do not say that all these evils whether white or Chinese should not be determinedly suppressed,\nbut there is such a manifest spirit of exaggeration in the complaints that are made, for the purpose\nI have described, as very materially to lessen in impartial eyes the accusations levelled against\nthe Chinese.\nSimilarity Score: 0.9595\n\nAuthor: Crease\nSentence: I know of retired officers and persons of settled incomes who would not have thought of coming here\nif they had not known that Chinese servants could he had here, though very indifferent compared with\nthose one can obtain in China itself.\nSimilarity Score: 0.9588\n\nAuthor: Begbie\nSentence: All the evils arising from opium in British Columbia in a year do not, probably, equal the damage,\ntrouble and expense occasioned to individuals and to the state by whiskey in a single month, or\nperhaps in some single night, As already observed, I do not remember ever to have seen a drunken\nChinaman; and the argument against Chinamen founded on opium appears to be analogous to the\ncomparison of the mote and the beam.\nSimilarity Score: 0.9585\n\nAuthor: Crease\nSentence: Good white labor is so far superior to Chinese, that it will of itself, when it can be contented\nwith reasonable prices as in the East, infallibly work Chinese manual labor out of the field.\nSimilarity Score: 0.9577\n\nAuthor: Begbie\nSentence: When we find (1st) no other description of labour taxed at all ; (2nd) this description of labour\npractically quite abandoned to Chinamen alone ; (3rd) this description of labour taxed at fifteen\ntimes the rate permitted to be levied on any retail shop ; (4th) that a preliminary Provincial Act\nhas declared Chinamen incapable of the franchise which they formerly exercised.\nSimilarity Score: 0.9572\n\nAuthor: Begbie\nSentence: The enormous consumption which this implies does not appear to prevent Chinamen from being the most\nprolific race, the most indefatigable laborers, and the keenest traders in the world.\nSimilarity Score: 0.9569\n\nAuthor: Crease\nSentence: In coming to British Columbia, and while here, the Chinese have no idea of interfering with the\nproperty of the white population in any way beyond the ordinary competition which they offer in the\nlabor market.\nSimilarity Score: 0.9562\n\n\n\nThe mixture occurrence of the two judges in the most similar sentences to the mean embeddings of the two also shows that the stance are very similar in terms of their text embeddings.\n\n\nThe Comparison of Crease’s and Begbie’s Rulings and the Chinese Regulation Act of 1884\n\ncrease_cases = df[(df['author'] == 'Crease') & (df['type'] == 'case')]['text'].tolist()\nbegbie_cases = df[(df['author'] == 'Begbie') & (df['type'] == 'case')]['text'].tolist()\nact_1884 = df[df['type'] == 'act']['text'].tolist()\n\nact_dict = {\n    'Crease': crease_cases,\n    'Begbie': begbie_cases,\n    'Act 1884': act_1884}\n\n\nact_snippets = {}\n\nkeywords = [\"Chinese\", \"China\", \"Chinaman\", \"Chinamen\"]\nfor auth, texts in act_dict.items():\n    snippets = []\n    for txt in texts:\n        sentence = sent_tokenize(txt)\n        for sent in sentence:\n            if any(keyword in sent for keyword in keywords):\n                snippets.append(sent)\n    act_snippets[auth] = snippets\n\n\n# Investigate the length of the snippets\nn_snippet = {auth: len(snippets) for auth, snippets in act_snippets.items()}\n\nprint(\"Snippet size by author:\")\nfor auth, num in n_snippet.items():\n    print(f\"{auth}: {num}\")\n\nSnippet size by author:\nCrease: 86\nBegbie: 18\nAct 1884: 24\n\n\n\n# Create embeddings\nembeddings_dict = {'Crease': [], 'Begbie': [], 'Act 1884': []}\n\nfor auth, snippets in act_snippets.items():\n    for snip in snippets:\n        v = embed_text(snip, focus_token=keywords, window=15)\n        embeddings_dict[auth].append(v)\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning:\n\n`encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n\n\n\n\n# Create embeddings that subtract the ethnicity anchor\nsubtracted_embeddings_dict = {'Crease': [], 'Begbie': [], 'Act 1884': []}\n\nfor auth, embeddings in embeddings_dict.items():\n    for emb in embeddings:\n        v = emb - eth_anchor\n        subtracted_embeddings_dict[auth].append(v)\n\n\n# Compute the pairwise cosine similarity\nmean_crease = np.mean(embeddings_dict[\"Crease\"], axis=0, keepdims=True)\nmean_begbie = np.mean(embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\nmean_act_1884 = np.mean(embeddings_dict[\"Act 1884\"], axis=0, keepdims=True)\n\nsim_crease_begbie = cosine_similarity(mean_crease, mean_begbie)[0, 0]\nsim_crease_act_1884 = cosine_similarity(mean_crease, mean_act_1884)[0, 0]\nsim_begbie_act_1884 = cosine_similarity(mean_begbie, mean_act_1884)[0, 0]\n\nprint(f\"Cosine similarity between Crease and Begbie: {sim_crease_begbie:.4f}\")\nprint(f\"Cosine similarity between Crease and Act 1884: {sim_crease_act_1884:.4f}\")\nprint(f\"Cosine similarity between Begbie and Act 1884: {sim_begbie_act_1884:.4f}\")\n\nCosine similarity between Crease and Begbie: 0.9893\nCosine similarity between Crease and Act 1884: 0.9757\nCosine similarity between Begbie and Act 1884: 0.9600\n\n\n\n# Compute the pairwise cosine similarity by max and min poolings\nmax_crease = np.max(embeddings_dict[\"Crease\"], axis=0, keepdims=True)\nmin_crease = np.min(embeddings_dict[\"Crease\"], axis=0, keepdims=True)\nmax_begbie = np.max(embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\nmin_begbie = np.min(embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\nmax_act_1884 = np.max(embeddings_dict[\"Act 1884\"], axis=0, keepdims=True)\nmin_act_1884 = np.min(embeddings_dict[\"Act 1884\"], axis=0, keepdims=True)\n\n# Compute the pairwise cosine similarity\nsim_crease_begbie_max = cosine_similarity(max_crease, max_begbie)[0, 0]\nsim_crease_begbie_min = cosine_similarity(min_crease, min_begbie)[0, 0]\nsim_crease_act_1884_max = cosine_similarity(max_crease, max_act_1884)[0, 0]\nsim_crease_act_1884_min = cosine_similarity(min_crease, min_act_1884)[0, 0]\nsim_begbie_act_1884_max = cosine_similarity(max_begbie, max_act_1884)[0, 0]\nsim_begbie_act_1884_min = cosine_similarity(min_begbie, min_act_1884)[0, 0]\n\n# Print the results\nprint(f\"Cosine similarity between max Crease and max Begbie: {sim_crease_begbie_max:.4f}\")\nprint(f\"Cosine similarity between min Crease and min Begbie: {sim_crease_begbie_min:.4f}\")\nprint(f\"Cosine similarity between max Crease and max Act 1884: {sim_crease_act_1884_max:.4f}\")\nprint(f\"Cosine similarity between min Crease and min Act 1884: {sim_crease_act_1884_min:.4f}\")\nprint(f\"Cosine similarity between max Begbie and max Act 1884: {sim_begbie_act_1884_max:.4f}\")\nprint(f\"Cosine similarity between min Begbie and min Act 1884: {sim_begbie_act_1884_min:.4f}\")\n\nCosine similarity between max Crease and max Begbie: 0.9540\nCosine similarity between min Crease and min Begbie: 0.9648\nCosine similarity between max Crease and max Act 1884: 0.9588\nCosine similarity between min Crease and min Act 1884: 0.9687\nCosine similarity between max Begbie and max Act 1884: 0.9401\nCosine similarity between min Begbie and min Act 1884: 0.9587\n\n\n\n# Compute the pairwise cosine similarity with subtracted embeddings\nsubtracted_mean_crease = np.mean(subtracted_embeddings_dict[\"Crease\"], axis=0, keepdims=True)\nsubtracted_mean_begbie = np.mean(subtracted_embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\nsubtracted_mean_act_1884 = np.mean(subtracted_embeddings_dict[\"Act 1884\"], axis=0, keepdims=True)\n\nsim_crease_begbie_sub = cosine_similarity(subtracted_mean_crease, subtracted_mean_begbie)[0, 0]\nsim_crease_act_1884_sub = cosine_similarity(subtracted_mean_crease, subtracted_mean_act_1884)[0, 0]\nsim_begbie_act_1884_sub = cosine_similarity(subtracted_mean_begbie, subtracted_mean_act_1884)[0, 0]\n\nprint(f\"Cosine similarity between Crease and Begbie with ethnical axis removed: {sim_crease_begbie_sub:.4f}\")\nprint(f\"Cosine similarity between Crease and Act 1884 with ethnical axis removed: {sim_crease_act_1884_sub:.4f}\")\nprint(f\"Cosine similarity between Begbie and Act 1884 with ethnical axis removed: {sim_begbie_act_1884_sub:.4f}\")\n\nCosine similarity between Crease and Begbie with ethnical axis removed: 0.9940\nCosine similarity between Crease and Act 1884 with ethnical axis removed: 0.9865\nCosine similarity between Begbie and Act 1884 with ethnical axis removed: 0.9776\n\n\n\n# Create UMAP projection for visualization\nimport umap \n\nall_vecs = np.vstack(embeddings_dict[\"Crease\"] + embeddings_dict[\"Begbie\"] + embeddings_dict[\"Act 1884\"])\nlabels  = ([\"Crease\"] * len(embeddings_dict[\"Crease\"])) + ([\"Begbie\"] * len(embeddings_dict[\"Begbie\"])) + (['Act 1884'] * len(embeddings_dict[\"Act 1884\"]))\n\nreducer = umap.UMAP(n_neighbors=15, min_dist=0.1)\nproj = reducer.fit_transform(all_vecs) \n\n\numap_df = pd.DataFrame(proj, columns=['UMAP 1', 'UMAP 2'])\numap_df['Author'] = labels\numap_df['Text'] = [snip for auth in act_snippets for snip in act_snippets[auth]]\numap_df['Text'] = umap_df['Text'].apply(lambda t: wrap_text(t, width=50))\n\nfig = px.scatter(umap_df, x='UMAP 1', y='UMAP 2', \n                 color='Author', hover_data=['Text'], \n                 width=800, height=500 )\nfig.update_traces(marker=dict(size=5))\nfig.update_layout(title='UMAP Projection of Word Embeddings by Author')\nfig.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nZero-shot Stance Classification with legal-BERT\n\n# Create the full snippets dictionary\nact_1884_full = \" \".join(act_1884)\ncrease_cases_full = \" \".join(crease_cases)\nbegbie_cases_full = \" \".join(begbie_cases)\n\n\nfull_cases = {\"act_1884\": act_1884_full, \"crease\": crease_cases_full, \"begbie\": begbie_cases_full}\n\nfull_snippets = {}\nfor author, text in full_cases.items():\n    sentence = sent_tokenize(text)\n    full_snippets[author] = sentence\n\n\nlen(full_snippets[\"act_1884\"]), len(full_snippets[\"crease\"]), len(full_snippets[\"begbie\"])\n\n(71, 356, 260)\n\n\n\n# Create pipeline for zero-shot classification\nfrom transformers import pipeline\n\nzero_shot = pipeline(\n    \"zero-shot-classification\",\n    model=\"facebook/bart-large-mnli\",\n    tokenizer=\"facebook/bart-large-mnli\",\n    hypothesis_template=\"This legal text {}.\"\n)\n\nlabels = [\n    \"advocates for equal legal treatment of Chinese immigrants compared to white or European settlers, opposing racial discrimination\",\n    \"describes the status or treatment of Chinese immigrants without expressing support or opposition to racial inequality\",\n    \"justifies or reinforces unequal legal treatment of Chinese immigrants relative to white or European settlers, supporting racially discriminatory policies\"\n]\n\ndef get_scores(snippet):\n    out = zero_shot(snippet, candidate_labels=labels)\n    return dict(zip(out[\"labels\"], out[\"scores\"]))\n\nDevice set to use cpu\n\n\n\n# Run zero-shot classification on the snippets from the Chinese Regulation Act 1884\nact_scores = {}\n\nfor auth, snippets in full_snippets.items():\n    scores = []\n    for snip in snippets:\n        score = get_scores(snip)\n        scores.append(score)\n    act_scores[auth] = scores\n\nrows = []\n\nfor auth, snippets in full_snippets.items():\n    for snip, score_dict in zip(snippets, act_scores[auth]):\n        row = {\n            \"Author\": auth,\n            \"Text\": snip,\n            \"Pro\": score_dict[labels[0]],\n            \"Neutral\": score_dict[labels[1]],\n            \"Cons\": score_dict[labels[2]]\n        }\n        rows.append(row)\n\n# Create long-form DataFrame\ndf_scores = pd.DataFrame(rows)\n\n# Group by author and calculate mean scores\nmean_scores = df_scores.groupby(\"Author\")[[\"Pro\", \"Neutral\", \"Cons\"]].mean()\n\nprint(\"Mean scores by author:\")\nprint(mean_scores)\n\nMean scores by author:\n               Pro   Neutral      Cons\nAuthor                                \nact_1884  0.331260  0.215666  0.453073\nbegbie    0.311259  0.295174  0.393567\ncrease    0.275872  0.272703  0.451425\n\n\n\ndf_scores['Text'] = df_scores['Text'].apply(lambda t: wrap_text(t, width = 50))\n\nfig = px.scatter(\n    df_scores,\n    x=\"Pro\",\n    y=\"Cons\",\n    color=\"Author\",\n    hover_data=[\"Text\"],\n    title=\"Pros vs Cons Scores by Author\",\n    width=800,\n    height=600\n)\n\nfig.update_traces(marker=dict(size=5))\nfig.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n# Plot histogram for Pro scores\nfig_pro = px.histogram(\n    df_scores,\n    x=\"Pro\",\n    color=\"Author\",\n    title=\"Distribution of Pro Scores by Author\",\n    nbins=30,\n    opacity=0.7,\n    labels={\"Pro\": \"Pro Score\"}\n)\nfig_pro.update_layout(\n    xaxis_title=\"Pro Score\",\n    yaxis_title=\"Frequency\",\n    width=800,\n    height=500\n)\nfig_pro.show()\n\n# Plot histogram for Cons scores\nfig_cons = px.histogram(\n    df_scores,\n    x=\"Cons\",\n    color=\"Author\",\n    title=\"Distribution of Cons Scores by Author\",\n    nbins=30,\n    opacity=0.7,\n    labels={\"Cons\": \"Cons Score\"}\n)\nfig_cons.update_layout(\n    xaxis_title=\"Cons Score\",\n    yaxis_title=\"Frequency\",\n    width=800,\n    height=500\n)\nfig_cons.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nDomain‐Adaptive Pretraining"
  }
]