[
  {
    "objectID": "pages/using_comet.html",
    "href": "pages/using_comet.html",
    "title": "Using COMET",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\n\nThe COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module.\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\n\n\n\n\nAs we mentioned, all of the Jupyter Notebooks are interactive; to access a notebook, simply launch the appropriate hub from the top menu of this website (where it says, “LAUNCH COMET”). The notebooks can also be downloaded for local use (“LAUNCH COMET &gt; LAUNCH LOCALLY”).\n\nThe interactive element will redirect you to the JupyterHub of your choice, and then synch the modules over.\nThe synching process uses a Jupyter extension called nbgitpuller to perform this operation.\n\n\n\n\n\n\n\nJupyter Troubleshooting Tip\n\n\n\nOccasionally, you can run into problems working with Jupyter in the cloud. If this occurs, you can try (a) deleting all of the COMET modules from your JupyterHub account, then trying to launch again, or (b) downloading the files manually, then uploading them. Option (b) will always work, even if it is a bit time exhuastive.\n\n\n\n\n\nThere are two ways to include a module in Canvas (or another LMS):\n\nEmbed it in a Canvas page or assignment\nInclude it as an external link\n\nThe advantage of option (1) is that you can include other material around the link (such as instructions). The advantage of option (2) is that it is easier.\n\n\nYou can see a visualization below:\n\n\n\nGIF of Embedding COMET into Canvas\n\n\n\nFirst, create the page (or assignment) that you want to embed the page in.\nThen, edit the page, and switch the HTML edit mode\nCopy the following text into the page:\n\n&lt;p&gt;&lt;iframe src=\"PASTE URL HERE\" width=\"800\" height=\"3200\" style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\nReplace the PASTE URL HERE text with the URL of the COMET page you want\nOptionally edit the width and height options to change the size of the embed\nSave the page; you should see the embedded page\n\nYou can now edit this page normally - for instance, by adding other material. You could also just add a URL as hyperlink instead of an embed. It’s up to you!\n\n\n\n\n\n\nCanvas Tip\n\n\n\nYou can find more information about embedding webpages in canvas on this page.\n\n\n\n\n\nYou can also just add a link directly to a Canvas module.\n\nOn the Canvas module, click the (+) to add an item, then select “External URL”.\nEnter the URL of the COMET page and customize the name; this is what will show up on Canvas.\nYou can choose to select the “load in a new tab” buttonif that is your preference.\n\n\n\n\n\nIf you run into issues with a COMET module (say, a bug) you can create an issue to our GitHub directory using the “Report an issue” button at the top right of this page.\nIf you need other support, please contact comet-project at ubc dot ca\n\n\n\n\n\n\nImportant\n\n\n\nIf the issue is about a JupyterHub, and not the notebook specifically, we unfortunately cannot help. Contact your hub maintainer for information.\n\nAt UBC, this is the LTHub"
  },
  {
    "objectID": "pages/using_comet.html#using-comet-for-teaching",
    "href": "pages/using_comet.html#using-comet-for-teaching",
    "title": "Using COMET",
    "section": "",
    "text": "The COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module.\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down."
  },
  {
    "objectID": "pages/using_comet.html#interactive-modules",
    "href": "pages/using_comet.html#interactive-modules",
    "title": "Using COMET",
    "section": "",
    "text": "As we mentioned, all of the Jupyter Notebooks are interactive; to access a notebook, simply launch the appropriate hub from the top menu of this website (where it says, “LAUNCH COMET”). The notebooks can also be downloaded for local use (“LAUNCH COMET &gt; LAUNCH LOCALLY”).\n\nThe interactive element will redirect you to the JupyterHub of your choice, and then synch the modules over.\nThe synching process uses a Jupyter extension called nbgitpuller to perform this operation.\n\n\n\n\n\n\n\nJupyter Troubleshooting Tip\n\n\n\nOccasionally, you can run into problems working with Jupyter in the cloud. If this occurs, you can try (a) deleting all of the COMET modules from your JupyterHub account, then trying to launch again, or (b) downloading the files manually, then uploading them. Option (b) will always work, even if it is a bit time exhuastive."
  },
  {
    "objectID": "pages/using_comet.html#using-with-canvas",
    "href": "pages/using_comet.html#using-with-canvas",
    "title": "Using COMET",
    "section": "",
    "text": "There are two ways to include a module in Canvas (or another LMS):\n\nEmbed it in a Canvas page or assignment\nInclude it as an external link\n\nThe advantage of option (1) is that you can include other material around the link (such as instructions). The advantage of option (2) is that it is easier.\n\n\nYou can see a visualization below:\n\n\n\nGIF of Embedding COMET into Canvas\n\n\n\nFirst, create the page (or assignment) that you want to embed the page in.\nThen, edit the page, and switch the HTML edit mode\nCopy the following text into the page:\n\n&lt;p&gt;&lt;iframe src=\"PASTE URL HERE\" width=\"800\" height=\"3200\" style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\nReplace the PASTE URL HERE text with the URL of the COMET page you want\nOptionally edit the width and height options to change the size of the embed\nSave the page; you should see the embedded page\n\nYou can now edit this page normally - for instance, by adding other material. You could also just add a URL as hyperlink instead of an embed. It’s up to you!\n\n\n\n\n\n\nCanvas Tip\n\n\n\nYou can find more information about embedding webpages in canvas on this page.\n\n\n\n\n\nYou can also just add a link directly to a Canvas module.\n\nOn the Canvas module, click the (+) to add an item, then select “External URL”.\nEnter the URL of the COMET page and customize the name; this is what will show up on Canvas.\nYou can choose to select the “load in a new tab” buttonif that is your preference."
  },
  {
    "objectID": "pages/using_comet.html#problems-and-support",
    "href": "pages/using_comet.html#problems-and-support",
    "title": "Using COMET",
    "section": "",
    "text": "If you run into issues with a COMET module (say, a bug) you can create an issue to our GitHub directory using the “Report an issue” button at the top right of this page.\nIf you need other support, please contact comet-project at ubc dot ca\n\n\n\n\n\n\nImportant\n\n\n\nIf the issue is about a JupyterHub, and not the notebook specifically, we unfortunately cannot help. Contact your hub maintainer for information.\n\nAt UBC, this is the LTHub"
  },
  {
    "objectID": "pages/teaching_with_comet.html",
    "href": "pages/teaching_with_comet.html",
    "title": "Teaching with Jupyter and Praxis",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\nThis guide is an introduction to how you can use Praxis and Jupyter notebooks for teaching. We will mainly discuss Jupyter Notebooks, for reasons explained in Section 2, but most of this advice is easy to adapt to other forms of notebook-based instruction such as R Markdown or Quarto.\nIn this guide, we will discuss:\n\nThe advantages and disadvantages of different notebook-based systems for classroom instruction.\nHow to use Praxis-style notebooks in different classroom settings, including an outline of how to plan a lesson.\nHow to develop interactive learning activities to accompany a Praxis-style notebooks, including some classroom-tested suggestions.\nAn introduction to developing your own Praxis-style notebooks for classroom instruction.\n\n\n\n\n\n\n\nWant the Basics?\n\n\n\nJust looking for a quick overview of how Jupyter notebooks work? Try out getting started introduction to Jupyter notebook, then come back here.\n\n\nAs most of our experience is in post-secondary education, this guide is intended for educators teaching students at the university level. However, it is probably also suitable for upper-year high school students, with sufficient experience1."
  },
  {
    "objectID": "pages/teaching_with_comet.html#disclaimer",
    "href": "pages/teaching_with_comet.html#disclaimer",
    "title": "Teaching with Jupyter and Praxis",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\nThis guide is an introduction to how you can use Praxis and Jupyter notebooks for teaching. We will mainly discuss Jupyter Notebooks, for reasons explained in Section 2, but most of this advice is easy to adapt to other forms of notebook-based instruction such as R Markdown or Quarto.\nIn this guide, we will discuss:\n\nThe advantages and disadvantages of different notebook-based systems for classroom instruction.\nHow to use Praxis-style notebooks in different classroom settings, including an outline of how to plan a lesson.\nHow to develop interactive learning activities to accompany a Praxis-style notebooks, including some classroom-tested suggestions.\nAn introduction to developing your own Praxis-style notebooks for classroom instruction.\n\n\n\n\n\n\n\nWant the Basics?\n\n\n\nJust looking for a quick overview of how Jupyter notebooks work? Try out getting started introduction to Jupyter notebook, then come back here.\n\n\nAs most of our experience is in post-secondary education, this guide is intended for educators teaching students at the university level. However, it is probably also suitable for upper-year high school students, with sufficient experience1."
  },
  {
    "objectID": "pages/teaching_with_comet.html#sec-why",
    "href": "pages/teaching_with_comet.html#sec-why",
    "title": "Teaching with Jupyter and Praxis",
    "section": "2 Why Jupyter Notebooks?",
    "text": "2 Why Jupyter Notebooks?\nWhy are Jupyter Notebooks a valuable tool for teaching? There are two main reasons:\n\nFirst, there are the advantages of Notebooks for teaching.\nSecond, there are the advantages of Jupyter for teaching.\n\nCombining these advantages creates a very valuable tool.\n\n2.1 Why Notebooks?\nA notebook refers to a digital document which combines rich text (including hyperlinks, formatting, and images) with cells that can perform computations. A user is able to change the content of the notebook, such as performing a computation or changing the text.\nNotebooks teach students three important skills, useful for data science and applied social science research:\n\nFirst, they teach students how to perform literate coding. Literate programming dates back to Knuth (1984), and has become extremely popular in sciences that use data. As Kery et al. (2018) explains, combining notes and context with code creates a self-documenting research notebook that addresses many common problems novice (and experienced) researchers face when analyzing data.\nSecond, they encourage replicable and reproducible data analysis. The non-reproducability of empirical results (see Camerer et al. (2018)) has reached crisis-levels in some fields. Because notebooks need to be run from the top-down, they naturally encourage students to make their analyses replicable. The structure of a notebook also encourage transparency when experimenting with analyses. This makes the work more likely to be reproducible.\nThird, they teach industry-relevant skills. Notebooks are extensively used by employers who conduct data science research, or who use data science in their work. Understanding how to write and use notebooks is a valuable skill in itself.\n\nThese properties make notebooks ideal to teach to students. Creating notebooks for classroom instruction turns them from a research tool into a pedagogical tool.\n\n\n2.2 Why Jupyter?\nJupyter is not the only option for notebooks (see Section 2.3). However, it has some advantages for teaching not shared by alternatives:\n\nNo installation necessary: when used through a JupyterHub, Jupyter notebooks do not require students to install any software or have a powerful computer. Even students with just a Chromebook or tablet can use Jupyter notebooks.\n\nThis eliminates many of the most time-consuming and frustrating parts of teaching student data science, including: installing software, troubleshooting package conflicts, issues sharing files and data, and computer problems.\n\nSimple Github integration: through nbgitpuller it is easy to share notebooks directly into a JupyterHub. This means that starting a class using notebooks is as easy as sharing a link with your students.\nLanguage independence: although the Jupyter framework is written in Python, it uses kernels to perform computation. There are dozens of kernels available, including those for popular languages such as R, Julia, Java, C, STATA, and Python itself.\n\nThe biggest strength of Jupyter is its hub-based design. This is also its biggest weakness, since it relies on an internet connection and someone to manage the hub. However, there are many free, well-maintained, hubs online such as:\n\nUBC OpenJupyter\nSyzygy\nGoogle Colab\nGitHub Codespces\n\nYou can also set up your own, or work with non-profits like 2i2c to develop your own hub.\n\n\n2.3 What are the Alternatives?\nJupyter Notebooks are not the only option for teaching using notebooks. In fact, there are significant advantages to other notebook styles, which may be more effective for certain kinds of teaching.\n\nJupyterQuartoR MarkdownObservable\n\n\n\nJupyter Notebooks\nJupyter notebooks are the most widely-used framework for notebook-based content, and are the easiest to use for students. However, they are not always the easiest to develop or maintain. We recommend Jupyter for online and student use.\n\nAdvantages\n\nWidely used, many tutorials and guides online.\nNo software installation needed for users.\nMany public, free, hubs (including Google Collab).\nLarge, open-source community.\n\n\n\nDisadvantages\n\nComplex and difficult local set-up.\nJSON-based file type; hard to maintain.\nLimited display and render options.\n\nLearn more about Jupyter Notebooks.\n\n\n\n\n\nQuarto Notebooks\nQuarto is a strong improvement over R Markdown and supports multiple languages. However, it is still in development and is more complicated. We recommend Quarto for development and offline use2.\n\nAdvantages\n\nInteroperable with R Markdown, but not R specific.\nVery rich output and render options.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\n\n\n\nDisadvantages\n\nNo easy-to-use free hubs available.\nMore complex than comparable notebook formats.\nNew, still in development.\n\nLearn more about Quarto.\n\n\n\n\n\nR Markdown Notebooks\nR Markdown is an excellent alternative to Jupyter for offline-only applications that only use R.\n\nAdvantages\n\nVery widely-used, many tutorials and guides online.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\nMany render options for output, rich output.\n\n\n\nDisadvantages\n\nIdiosyncratic syntax.\nNo easy-to-use free hubs available.\nOnly supports R coding, no other languages.\n\nLearn more about R Markdown.\n\n\n\n\n\nObservable Notebooks\nObservable is the newest format on the market, and looks very professional. It is designed for enterprise clients, and is the most complex of the alternatives.\n\nAdvantages\n\nNon-language specific framework.\nExtremely rich output formats.\nStrong dashboarding and interactive support.\nLarge enterprise developer.\n\n\n\nDisadvantages\n\nLarge enterprise developer, no free hubs.\nMost complex of the alternatives.\nNew, still in development.\n\nLearn more about Observable and D3.js."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "href": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "title": "Teaching with Jupyter and Praxis",
    "section": "3 Teaching with Jupyter Notebooks",
    "text": "3 Teaching with Jupyter Notebooks\nWe have tried several ways of using Jupyter Notebooks when teaching, and found that they fit most teaching styles. We have found them particularly effective in:\n\nLecture.\nFlipped classrooms.\nWorkshops or labs.\n\nYou can find advice on lesson planning in these formats in the tabs below.\n\nLecturesFlipped ClassroomWorkshops or Labs\n\n\n\nDirect Instruction or Lectures\nJupyter notebooks are most effective in lecture when you use them as a demonstration tool which students can follow along with.\n\nThe power of a Jupyter notebook is the interactive nature of the different cells.\nThis allows you to enhance your lecture content by immediately and interactively demonstrating principles by running cells and changing their values.\n\nEssentially, you make can your slides or visualizations interactive by running or editing cells.\n\nYou can either do this directly, by showing the notebook, or you can turn the notebook into a slideshow using either nbcovert or RISE, which create a RevealJS presentation from your notebook.\n\nRevealJS is a powerful HTML-based presentation framework, widely used on the web and in computation.\n\nThere are also powerful libraries for interactive visualization, such as plotly and ggplotly.3\n\nSee Section 4.1 for a guide to creating presentations using Jupyter notebooks.\nWe have found it is usually best to give students the Jupyter notebook of the presentation, as a kind of “hand-out,” while you demonstrate using the presentation display of the notebook. This avoids the problem of having to make sure students have a suitable presentation display tool installed.\n\n\nSuggestions for Teaching\n\nTry demonstrating a cell, then asking students to predict what happen when you make a different change. Then do it!\n\nThis works great with classroom response systems such as iClicker or TopHat.\n\nSpend time thinking about how interacting with the cell can show the concept more effectively than a static visualization.\n\nWe have found this to be particularly useful for dynamics in visualizations, such as showing a change.\n\nSpend time on each interactive part of your presentation, and walk through the changes.\nUse encapsulation by placing code in auxillary files to make the demonstrations easier to follow.\n\nIf students don’t need to know how it works, only what it does, consider re-writing the code to hide the details.\n\n\n\n\n\n\nFlipped Classrooms\nA flipped classroom refers to a teaching model where activities traditional done in the classroom are done at while, while activities done at home are done in the classroom (Akçayır and Akçayır (2018)). “Flipping” the lecture demonstration, outlined above, using Jupyter Notebooks is a natural fit.\nMost flipped classroom experiences tend to use videos (see Akçayır and Akçayır (2018)), and this is quite feasible with Jupyter Notebooks. Record yourself demonstrating the notebook, and have students follow along. We can done this for some of the COMET notebooks (see our project notebooks for example).\nHowever, the interactivity of notebooks makes them ideal for doing as “pre-reading” assignment instead, or in addition, to videos. The active learning created by interacting with the notebook, and completing self-test exercises, makes them more effective than just doing a reading.\nA good flipped-classroom notebook:\n\nIntroduces the topic in a narrative, systematic way, and does not require any significant external references to follow along.\nIncludes regularly-spaced interactive cells, which require students to evaluate and inspect the results.\nHas a series of self tests (see Section 4.2) at regular intervals, to check and reinforce student understanding.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nFor example, you could assign students a pre-reading of classification and clustering before class. In the class, you could then introduce a new dataset and have them explore it using clustering methods.\nMany of our COMET notebooks, especially the intermediate ones are built with this structure, where one “Part” of the notebook can be turned into the classroom part of the flipped classroom.4\n\n\n\n\n\nWorkshops or Labs\nWe have also found it effective to teach using Jupyter notebooks in small group settings, such as workshops. A typical Jupyter workshop in our experience:\n\nDivide the students into groups of about 4-5, and have them physically move so that they are seated next to one another.\n\nThis also works well online, using a feature like Zoom’s breakout rooms.\n\nOnce they are settled, or before moving people into their breakout rooms, introduce the purpose of the workshop. Identify what students are supposed to do, and how they will interact with the Jupyter Notebooks.\nAllow students to work together on the notebooks, while you move around the room discussing with the groups.\n\nIt is often effective to design your notebooks so that they have several identifiable “tasks” or stopping points, where you can bring the workshop back together.\n\nMany of our COMET notebooks, especially the intermediate ones are built with this structure.\nMake sure you build in time for students to introduce themselves to one another, if this is their first time meeting.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nMany of the tools and techniques for flipped classroom instruction work well in a workshop format too. Just make sure there is sufficient support so students can follow the notebooks together.\nEncourage students to work together to troubleshoot problems if they encounter them, so you are not running around too often.\nAn effective strategy is to have the students collaborate on a single “final” version of the notebook together, while experimenting on their own. Nominating one student as the “scribe” is a good way to keep this organized.\n\nAt the end of the workshop, having students hand in their Notebook is an effective way of measuring participation, and encourages participation."
  },
  {
    "objectID": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "href": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "title": "Teaching with Jupyter and Praxis",
    "section": "4 Developing your own Notebooks for Teaching",
    "text": "4 Developing your own Notebooks for Teaching\nIt is easier than you might expect to develop notebooks for teaching. Some of our favourite examples are:\n\nWord Embeddings: an advanced, workshop-style, .ipynb format notebook.\nVisualization: a basic, lecture-style, introduction with time for experimentation.\nExporting Output: a flipped-classroom style book or workshop for STATA output.\n\nPoke around and see some more!\n\n4.1 Presenting and Slideshows\nWhen teaching with Jupyter, effective presentation skills require a little planning. There are several options, depending on what kind of presentation you want to give.\n\n4.1.1 Presenting a Notebook\nPresenting a Notebook as a notebook, such as in a demonstration or workshop, is easy.\n\nIn JupyterLab, the easiest way is use the View menu:\n\nUncheck all of the un-necessary bars, such as the top bar and status bar.\nTurn on “Simple View”, which only shows your activate notebook tab.\nTurn on “Presentation Mode.”\n\n\nThis will create a large-format, interactive, version of your notebook suitable for presenting on an overhead projector or monitor.\n\n\n4.1.2 Presenting a Slideshow\nIf you want to turn your notebook into a slideshow, things are more complicated depending on whether you want it to be interactive or not. However, in general you create a slideshow by designating individual cells are either whole slides, or fragments of slides:\n\nA slide is a single higher-level slide. When the presentation advances from one slide to another, it will “slide” right-to-left.\nA sub-slide is like a lower-level slide. When the presentations advances from to a sub-slide, it will “slide” from up-to-down.\nA fragment is part of a slide. It appears by sliding up, into the slide, keep the previous content visible. This is how you can reveal information or advance content.\n\nYou designate cells as the different part of a presentation by clicking on the gear icon, then selecting the cell. A dropdown menu that says “Slide type” will be visible. Use this to set up your presentation.\nIf you don’t care about interactivity, at this point you can go to “File &gt; Save and Export Notebook as…” then select “Reveal.js Slides.” This will download an .html file with your presentation in it. Learn more about Reveal.js to see how this file works in more detail.\nIf you want to run code in your presentation and edit it as you present, things are more complicated. To make your presentation editable, you need to install a JupyterLab extension called RISE.\n\nRISE is easiest to install on your own computer, not on a JupyterHub unless you have administrator privileges.\nIn the terminal, run pip install jupyterlab_rise then re-launch your server.\nYou can read more about RISE above; it’s still in development so things might change.\n\n\n\n\n4.2 Writing Self-Tests\nWriting self-tests is an important part of providing formative feedback to students. It can be somewhat complicated, but the basic idea is to write cells in your notebooks that look like:\n#an R self test\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nThe function test_1() is stored in an auxillary file, and performs a test on the answer. It also gives feedback to the student, such as whether the answer was correct.\nThis requires some set-up, and is slightly different for different languages. To make this easier, we provide a detailed guide for R and Python in our writing self-tests documentation.\n\n\n4.3 Development Tips\nDeveloping notebooks as a one-off is straightforward if you author them in JupyterLab: what you see is what you get. However, if you have a more complex project some planning helps. This includes multiple notebooks, or notebooks you need to collaborate on over time.\n\nWe strongly recommend not developing directly in .ipynb notebooks long term. Draft your initial notebook in .ipynb, then switch to another framework for longer-term development.\n\nThe reason is because editing an .ipynb edits the state of the program, making it easy to accidentally evaluate or delete something.\nIt’s also hard to maintain and doesn’t play nicely with version control systems like git because the documents are very complicated in structure.\n\nOur recommended format is .qmd which can render .ipynb notebooks from the source code. The underlying document is just text, which makes it easy to edit and maintain.\n\nUsing .qmd notebooks is much easier: think of these as the “source code” and the .ipynb as the “output.” This also has an advantage of begin able to create other output formats, like PDFs, websites, or presentations directly from the source code."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-offline",
    "href": "pages/teaching_with_comet.html#teaching-offline",
    "title": "Teaching with Jupyter and Praxis",
    "section": "5 Teaching Offline",
    "text": "5 Teaching Offline\nWhile we think that teaching using a JupyterHub is the best option, that may not always be desired or possible. If you want to use COMET notebooks, or similar teaching tools, without a JupyterHub you have two main options:\n\nOption 1: have students install one of the alternative frameworks.\nOption 2: have students install Jupyter locally, on their own computers.\n\nBe prepared to troubleshoot installation issues."
  },
  {
    "objectID": "pages/teaching_with_comet.html#further-reading",
    "href": "pages/teaching_with_comet.html#further-reading",
    "title": "Teaching with Jupyter and Praxis",
    "section": "6 Further Reading",
    "text": "6 Further Reading\nYou can see some of our other publications on our dissemination page."
  },
  {
    "objectID": "pages/teaching_with_comet.html#references",
    "href": "pages/teaching_with_comet.html#references",
    "title": "Teaching with Jupyter and Praxis",
    "section": "7 References",
    "text": "7 References\n\n\nAkçayır, Gökçe, and Murat Akçayır. 2018. ‘The Flipped Classroom: A Review of Its Advantages and Challenges’. Computers & Education 126: 334–45.\n\n\nCamerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. ‘Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015’. Nature Human Behaviour 2 (9): 637–44.\n\n\nGraves, Jonathan L, Emrul Hasan, and Trish L Varao-Sousa. 2024. ‘Understanding the Hybrid Classroom in Economics: A Case Study’. International Review of Economics Education 45: 100282.\n\n\nKery, Mary Beth, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A Myers. 2018. ‘The Story in the Notebook: Exploratory Data Science Using a Literate Programming Tool’. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–11.\n\n\nKnuth, Donald Ervin. 1984. ‘Literate Programming’. The Computer Journal 27 (2): 97–111."
  },
  {
    "objectID": "pages/teaching_with_comet.html#footnotes",
    "href": "pages/teaching_with_comet.html#footnotes",
    "title": "Teaching with Jupyter and Praxis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe have taught several of these notebooks to 1st year students during intake events, who were fresh out of high school.↩︎\nWe use Quarto to develop the COMET project.↩︎\nPlotly comes in several flavours for different languages, such as `plotly-r↩︎\nThis was actually our original use for the notebooks, before COMET! See Graves, Hasan, and Varao-Sousa (2024)↩︎"
  },
  {
    "objectID": "pages/index/index_topical.html",
    "href": "pages/index/index_topical.html",
    "title": "Topical Index",
    "section": "",
    "text": "This page collects all modules by topics. You can search and filter these as necessary."
  },
  {
    "objectID": "pages/index/index_topical.html#modules",
    "href": "pages/index/index_topical.html#modules",
    "title": "Topical Index",
    "section": "Modules",
    "text": "Modules"
  },
  {
    "objectID": "pages/index/index_SOCI280.html",
    "href": "pages/index/index_SOCI280.html",
    "title": "Data and Society: Exploring Disinformation through Textual Analysis (SOCI 280)",
    "section": "",
    "text": "This section contains material to support UBC’s Data and Society (SOCI 280)."
  },
  {
    "objectID": "pages/index/index_SOCI280.html#soci-280-suggested-lesson-plan",
    "href": "pages/index/index_SOCI280.html#soci-280-suggested-lesson-plan",
    "title": "Data and Society: Exploring Disinformation through Textual Analysis (SOCI 280)",
    "section": "SOCI 280: Suggested Lesson Plan",
    "text": "SOCI 280: Suggested Lesson Plan\n110-minute lecture and tutorial session\n\nLearning Objectives\nBy the end of this lesson, students will:\n\nUnderstand and complete sentiment analysis to detect emotional tone (positive, negative, neutral), and understand and complete toxicity analysis to identify harmful or aggressive language (e.g., insults, threats).\nIntroduce concepts in statistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian).\nLearn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than normal political tweets?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\n\nThrough this analysis, we’ll explore various dimensions of AI applications, critically examining how it can better understand and detect the patterns of disinformation when working with large amounts of social data.\n\n\n\nMaterials and Technical Requirements\n\nJupyter notebook (hosted on the prAxIs UBC website)\nDevice with internet access (laptop preferred)\nNo coding experience required (familiarity with Python is an asset)\nStudents may pair up (groups of 2–3) if device access is limited\nGroup work is encouraged; students will compare findings\n\n\n\n\nPre-lesson Checklist\n\nStudents have completed the reading up to Section 0 (approx. 5–10 minutes)\nInstructor has loaded and can project the notebook\nStudents reminded to bring a device"
  },
  {
    "objectID": "pages/index/index_SOCI280.html#agenda",
    "href": "pages/index/index_SOCI280.html#agenda",
    "title": "Data and Society: Exploring Disinformation through Textual Analysis (SOCI 280)",
    "section": "Agenda",
    "text": "Agenda\n\n1. Pre-discussion and Brief Lecture (5–10 minutes)\n\nDiscuss misinformation, disinformation, and propaganda\nEmphasize the role of digital platforms in their spread\nHow researchers detect disinformation\nConsider influence of new technology and personal data\n\nExample:\nhttps://www.engadget.com/ai/researchers-secretly-experimented-on-reddit-users-with-ai-generated-comments-194328026.html\n\n\n\n2. Load the notebook (5 minutes)\n\nStudents open the Jupyter notebook\nPair students without devices\nInstruct all to complete Section 0\n\n\n\n\n3. Section 0 (20 minutes)\n\nWork through code/activities in Section 0\nUpon reaching the Screen Time activity, pause for a brief discussion to compare results\n\n\n\n\n4. Section 1 (15 minutes)\n\n2–3 min. explanation of classification and course connections\nStudents complete Section 1 at their own pace (~15 min.)\n\n\n\n\n5. Sections 3–4 (20 minutes)\n\nPause to discuss Section 1 findings in a group\nStudents complete the remainder of the notebook\n\n\n\n\n6. Takeaways and Activity (5–10 minutes)\n\nTime-dependent; includes:\n\nQuestions about methods\nBrief lecture summarizing key takeaways\nBegin participation activities (discussion posts, worksheets, etc.)"
  },
  {
    "objectID": "pages/index/index_SOCI280.html#activity-materials",
    "href": "pages/index/index_SOCI280.html#activity-materials",
    "title": "Data and Society: Exploring Disinformation through Textual Analysis (SOCI 280)",
    "section": "Activity Materials",
    "text": "Activity Materials\n\nDiscussion Post Questions\nRespond in 100–250 words for each:\n\nHow can sentiment analysis be useful in answering research questions? Can you think of any tasks it would be well suited to?\nDo you think the methods in the notebook were useful in understanding disinformation campaigns? What might be some of the limitations to these approaches?\nDiscuss the role AI plays in disinformation, both the detection and analysis of it, and the production of it.\n\n\n\n\nActivity: Identify Disinformation Online\nOn a social media platform of your choice, try to identify a post you believe to be disinformation.\nYou can search for specific topics or wait for something in your feed.\nLink to the content here: _______________________________________________________\n\nWhy do you think this is disinformation?\nWhat data/information are you using from the content and its features to come to this conclusion?\nExplain your reasoning in 150–300 words.\nCompare with the Classifier:\nThink back to the classifier in the notebook.\n\nWhat data was it using to classify text as disinformation?\n\nIs that process similar or different from how you identified your post?\n\nDo you think you are more likely to be correct? If so, why?\nRespond in 200–350 words."
  },
  {
    "objectID": "pages/index/index_SOCI280.html#modules",
    "href": "pages/index/index_SOCI280.html#modules",
    "title": "Data and Society: Exploring Disinformation through Textual Analysis (SOCI 280)",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nIntroduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP\n\n\nAn introduction to BERT-based NLP using online examples of disinformation and exploration of how embeddings and classification models can be used to analyze and detect…\n\n\n\n24 Jul 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_ECON227.html",
    "href": "pages/index/index_ECON227.html",
    "title": "Data in Economics (ECON 227)",
    "section": "",
    "text": "This section contains material to support UBC’s Data in Economics: Application-driven introduction to the analysis of economic data. Descriptive analysis, causality, experimental and observational data, hypothesis testing. Restricted to BIE students."
  },
  {
    "objectID": "pages/index/index_ECON227.html#modules",
    "href": "pages/index/index_ECON227.html#modules",
    "title": "Data in Economics (ECON 227)",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nECON 227 - How Do Large Language Models Predict?\n\n\n\n\n\n\n26 Aug 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/all.html",
    "href": "pages/index/all.html",
    "title": "All Modules",
    "section": "",
    "text": "This section contains all materials and case studies to support a variety of classes. Filter by topic using the categories on the right. Filter by class by choosing the appropriate class category."
  },
  {
    "objectID": "pages/index/all.html#modules",
    "href": "pages/index/all.html#modules",
    "title": "All Modules",
    "section": "Modules",
    "text": "Modules\n\n\n\n\n\n\n\n\n\n\nA Study of Richter’s Kouroi Through Image Embedding\n\n\nUsing examples from Richter’s Kouroi, this notebook introduces computer vision and image embeddings, showing how pre-trained convolutional neural networks can be applied…\n\n\n\n25 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nECON 227 - How Do Large Language Models Predict?\n\n\n\n\n\n\n26 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP\n\n\nAn introduction to BERT-based NLP using online examples of disinformation and exploration of how embeddings and classification models can be used to analyze and detect…\n\n\n\n24 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLesson Plan : Large Language Models and Stock Market Predictions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLesson Plan: Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP\n\n\n\n\n\n\n24 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - CBDB Dataset\n\n\nUsing Network Analysis to Analyze The China Biographical Dataset\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - KINMATRIX Dataset\n\n\nUsing the KINMATRIX Dataset to explore network analysis for SOCI 415\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - Lesson Plan\n\n\nLesson Plan for SOCI-415\n\n\n\n19 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI415 - Theories of Family and Kinship\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/copyright.html",
    "href": "pages/copyright.html",
    "title": "Copyright Information",
    "section": "",
    "text": "This project uses data from a variety of sources, most available under an open data license. All other material is published under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License..\n\n\n\nOur suggested attribution for this project is:\n\n\n\n\n\n\nNelson, L., Graves, J., and other prAxIs Contributors. 2023. ‘The prAxIs Project: Creating Online Materials for Econometric Teaching’. https://comet.arts.ubc.ca/.\n\n\n\nADD DATA HERE Below as an example\n\nThe 2016 Census Data was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. This does not constitute an endorsement by Statistics Canada of this product.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product. ​\n\nThe Penn World Table was provided by Feenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), “The Next Generation of the Penn World Table” American Economic Review, 105(10), 3150-3182, available for download at www.ggdc.net/pwt\n\nThe Penn World Table Penn World Table 10.0 by Robert C. Feenstra, Robert Inklaar and Marcel P. Timmer is licensed under a Creative Commons Attribution 4.0 International License. This research received support through grants from the National Science Foundation, the Sloan Foundation and the Transatlantic Platform’s Digging into Data program.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\nWe also use data from the World Bank’s World Development Indicators and Quarterly Public Sector Debt databases.\n\nBoth of these are licensed under the CC by 4.0 open license.\n\nAll of the citations for the data used in the GEOG 374 notebooks are cited in the specific notebooks.\nThe data used in the projects modules is simulated, as is the salmon data in the prep modules and was created for this project. It falls under the license for the project in general (see above).\n\nThe simulation for the salmon data was based on data from the Pacific Salmon Foundation’s salmon watersheds program"
  },
  {
    "objectID": "docs/text_analysis/text_analysis.html#section-1-traditional-qualitative-coding",
    "href": "docs/text_analysis/text_analysis.html#section-1-traditional-qualitative-coding",
    "title": "Text Analysis Overview",
    "section": "Section 1: Traditional Qualitative Coding",
    "text": "Section 1: Traditional Qualitative Coding\n\n\nQualitative research often begins with hand-coded analysis, where researchers create a codebook and apply categories to text manually\n\n\nThe convention\nImagine you have a giant pile of stories, surveys or interviews, researchers first make a list of buckets (this list is called a codebook). Then one by one, they read the text and put each piece into the right bucket.\nExamples of buckets might be emotions such as happy, angry, sad; or topics about money, about family, etc. Researchers can freely decide the buckets they need."
  },
  {
    "objectID": "docs/text_analysis/text_analysis.html#section-2-computational-text-analysis",
    "href": "docs/text_analysis/text_analysis.html#section-2-computational-text-analysis",
    "title": "Text Analysis Overview",
    "section": "Section 2: Computational Text Analysis",
    "text": "Section 2: Computational Text Analysis\n\n\nMachine Learning (ML) is a transformative field that combines statistics, computer science, and artificial intelligence to enable computers to learn from data and make predictions or decisions without being explicitly programmed.\n\n\nWhat is ML?\nML focuses on building algorithms that can identify patterns in data, adapt to new information, and improve performance over time.\nWhy it matters for data analysis:\nML automates tasks like classification, clustering, and regression, making it faster and more scalable to extract insights from large and complex datasets.\n\n\n\nML is the backbone of modern data analysis, enabling researchers and organizations to unlock the full potential of structured and unstructured data.\nWe can split up ML-based methods into two categories: Supervised and Unsupervised learning."
  },
  {
    "objectID": "docs/text_analysis/text_analysis.html#sources",
    "href": "docs/text_analysis/text_analysis.html#sources",
    "title": "Text Analysis Overview",
    "section": "3. Sources",
    "text": "3. Sources\n\nNational University Library. Coding Qualitative Data Guide. Accessed 2025-08-18.\nNational University Library. Dissertation Center — Analysis and Coding Example: Qualitative Data. Accessed 2025-08-18.\nAmazon. Amazon Mechanical Turk. Accessed 2025-08-18.\nProlific. Prolific for Academic Researchers. Accessed 2025-08-18.\nDelve. The Essential Guide to Coding Qualitative Data. Accessed 2025-08-18.\nChen, M., Aragon, C., et al. Using Machine Learning to Support Qualitative Coding in Social Science Research. ACM Transactions on Interactive Intelligent Systems, 2018.\nMattingly, W. J. B. Topic Modeling for the People. Accessed 2025-08-18.\nAntoniak, M. Topic Modeling for the People (Practical Guide). Accessed 2025-08-18.\nSpringer. Word Embeddings (Book Landing Page). Accessed 2025-08-18.\nIBM. What is Natural Language Processing?. Accessed 2025-08-18.\nAmazon Web Services. What is NLP?. Accessed 2025-08-18.\nTowards Data Science. Natural Language Processing: Dependency Parsing. Accessed 2025-08-18.\nIBM. What is Named Entity Recognition?. Accessed 2025-08-18.\nLINCS Project. Named Entity Recognition. Accessed 2025-08-18.\nIntellipaat. What is Parsing in NLP?. Accessed 2025-08-18.\nCogito Tech. Named Entity Recognition Overview. Accessed 2025-08-18.\nSánchez, Y. Zero-Shot Text Classification. Accessed 2025-08-18.\nModulai. Zero-Shot Learning in NLP. Accessed 2025-08-18.\nWikipedia. Zero-shot learning. Accessed 2025-08-18.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint, 2018.\nIBM. AI Ethics: Principles and Practices. Accessed 2025-08-18.\nReddit. Unauthorized Experiment on r/changemyview. Accessed 2025-08-18.\nScience.org. Unethical AI Research on Reddit Under Fire. Accessed 2025-08-18."
  },
  {
    "objectID": "docs/intro_to_deep_learning/intro_to_fundamental_ML.html",
    "href": "docs/intro_to_deep_learning/intro_to_fundamental_ML.html",
    "title": "A visual introduction to fundamental modern machine Learning",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\n# from graphviz import Digraph\nfrom IPython.display import display\nfrom ipywidgets import interact, FloatSlider\nnp.random.seed(19_750)\nWhat is Deep Learning, Machine Learning, and Artificial Intelligence?\nA Simple History of AI In the 1950s, scientists dreamed of building machines that could think and learn like humans. They created early models called neural networks, inspired by how the brain works. One of the first, called the Perceptron, was built by Frank Rosenblatt. But back then, computers were too slow, and there wasn’t enough data to make these systems work well. So, researchers moved on to simpler methods, largely ignoring neural networks.\nIn 2012, everything changed with a breakthrough called AlexNet. It was a large neural network that could look at pictures and recognize objects (like cats, dogs and cars etc.) better than anything before. It was taught how to recognize images very efficiently and which allowed it to see millions of images and learning how to recognize patterns, stitch them together, and make predictions.\nIn 2017, another big leap happened with the invention of Transformers. These systems could understand very long sequences (like language) at once, making them great at tasks like writing, translating, and summarizing text. This is the technology behind tools like ChatGPT, which can chat, write stories, or even help with coding."
  },
  {
    "objectID": "docs/intro_to_deep_learning/intro_to_fundamental_ML.html#what-makes-ai-good-at-its-job",
    "href": "docs/intro_to_deep_learning/intro_to_fundamental_ML.html#what-makes-ai-good-at-its-job",
    "title": "A visual introduction to fundamental modern machine Learning",
    "section": "What makes AI good at its job?",
    "text": "What makes AI good at its job?\n\n\nCode\n# Generate 20 random x values\nx = np.linspace(0, 10, 20)\n\n# Compute y values with some noise\ny = x**2 + np.random.normal(0, 5, x.shape)\n\n# Plot the points using seaborn with slightly bigger points\nplt.figure(figsize=(8, 5))\nplt.scatter(x=x, y=y, s=100, alpha=0.6, label='Data')\nplt.title('Social media followers vs Engagement')\nplt.xlabel('X (Followers in thousands)')\nplt.ylabel('Y (Engagement in thousands)')\nplt.show()\n\n\nThis is a plot of social media followers vs engagement. Lets say you want to predict engagement based on followers.\n\n\nCode\ndegree = 2\n# Fit a polynomial of given degree to the data using numpy\ncoeffs = np.polyfit(x, y, degree)\n\n# Compute predicted y values\ny_pred_2 = np.polyval(coeffs, x)\n\n# Plot the original data and the fitted polynomial\nplt.figure(figsize=(8, 5))\nplt.scatter(x=x, y=y, s=100, alpha=0.6, label='Data')\nplt.plot(x, y_pred_2, color='red', label=f'Degree {degree} Fit')\nplt.title('Social media followers vs Engagement')\nplt.xlabel('X (Followers in thousands)')\nplt.ylabel('Y (Engagement in thousands)')\nplt.show()\n\n\nThis red line “fit” to the data\n\n\nCode\ndegree = 20\n\n# Fit a polynomial of given degree to the data using numpy\ncoeffs = np.polyfit(x, y, degree)\n# Compute predicted y values\ny_pred_15 = np.polyval(coeffs, x)\n\n# Plot the original data and the fitted polynomial\nplt.figure(figsize=(8, 5))\nplt.scatter(x=x, y=y, s=100, alpha=0.6, label='Data')\nplt.plot(x, y_pred_15, color='orange', label=f'Degree {degree} Fit')\nplt.title('Social media followers vs Engagement')\nplt.xlabel('X (Followers in thousands)')\nplt.ylabel('Y (Engagement in thousands)')\nplt.show()\n\n\nThis orange line “fit” to the data\n\n\nCode\nplt.figure(figsize=(8, 5))\nplt.scatter(x=x, y=y, s=100, alpha=0.6, label='Data')\nplt.plot(x, y_pred_15, color='orange', label=f'Degree {15} Fit')\nplt.plot(x, y_pred_2, color='red', label=f'Degree {2} Fit')\nplt.title('Social media followers vs Engagement')\nplt.xlabel('X (Followers in thousands)')\nplt.ylabel('Y (Engagement in thousands)')\nplt.show()\n\n\nWhat is better?\nThe red line seems better, but the orange line has lower error since it goes directly through ALL points.\nSay you put a point between some of the blue dots? Is the orange line more likely to predict a better position for the points or the red?\n\nNo! The red line generalizes to points that don’t exist in the plot much better.\n\nWhat about if you add points to the left and right of the existing points?\n\nThe red line does better again for unseen points.\n\n\nTo prevent your model from memorizing your data, you intentionally split your data into 2. The training data and testing data. The test data should not influence training in any way.\nCredit - CPSC 330 \nKey Takeaways\n\nYour model needs to be able to generalize.\nYour model should not blindly memorize the data it is given. Eg. force its way through every point.\nThe test data should not influence training in any way.\n\nThis is the fundamental rule of learning.\nExample:\nSay your vision model to drive self-driving cars is trained with lots of videos from Los Angeles (Now you drive to Vancouver and realize that your system works a lot worse. One reason for this is that the model is too used to the LA data.\nIn reality, self-driving cars are trained on data from all types of cities, suburbs, and rural areas, as well as in all driving conditions and weather. But when highly unexpected things happen (like sandstorms or forest fires that make the sky orange), these systems perform worse.\n\nTLDR; Your predictor should generalize and find the underlying pattern not memorize that data it has seen like a puppet."
  },
  {
    "objectID": "docs/intro_to_deep_learning/intro_to_fundamental_ML.html#neural-networks",
    "href": "docs/intro_to_deep_learning/intro_to_fundamental_ML.html#neural-networks",
    "title": "A visual introduction to fundamental modern machine Learning",
    "section": "Neural Networks",
    "text": "Neural Networks\n\nExample 1\n\n\nCode\nx = np.linspace(1, 50, 100)\n# Generate y values for -log(x) with noise\ny_neg_log = np.log(x) + np.random.normal(scale=0.1, size=x.shape)\n\n# Create the scatter plot\nplt.figure(figsize=(8, 5))\nplt.scatter(x, y_neg_log, label='Data', color='red', s=75, alpha=0.6)\nplt.title('Years since 1960 vs Population of a city (in millions)')\nplt.xlabel('X (Years since 1960)')\nplt.ylabel('Y (Population in millions)')\nplt.tight_layout()\nplt.show()\n\n\nWe cannot draw a line through this data.\n\n\nCode\n# Original data\nx = np.linspace(1, 50, 100)\ny_neg_log = np.log(x) + np.random.normal(scale=0.1, size=x.shape)\n\n# Apply a square root transformation to x\nx_transformed = np.sqrt(x)\n\n# Create the scatter plot\nplt.figure(figsize=(8, 5))\nplt.scatter(x_transformed, y_neg_log, label='Stretched Data', color='blue', s=75, alpha=0.6)\nplt.title('Years since 1960 vs Population of a city (in millions)')\nplt.xlabel('$\\sqrt{X}$ (Years since 1960)')\nplt.ylabel('Y (Population in millions)')\nplt.tight_layout()\nplt.show()\n\n\nHere we compressed the X axis and made the data easier to draw a line through.\n\n\nCode\nfrom sklearn.linear_model import Ridge\n\n# Original data\nx = np.linspace(1, 50, 100)\ny_neg_log = np.log(x) + np.random.normal(scale=0.1, size=x.shape)\n\n# Apply a square root transformation to x\nx_transformed = np.sqrt(x).reshape(-1, 1)\n\n# Fit Ridge regression\nridge = Ridge(alpha=1.0)\nridge.fit(x_transformed, y_neg_log)\ny_pred = ridge.predict(x_transformed)\n\n# Create the scatter plot + ridge line\nplt.figure(figsize=(8, 5))\nplt.scatter(x_transformed, y_neg_log, label='Stretched Data', color='blue', s=75, alpha=0.6)\nplt.plot(x_transformed, y_pred, color='red', linewidth=2, label='Ridge Regression Line')\nplt.title('Years since 1960 vs Population of a city (in millions)')\nplt.xlabel('X (Years since 1960)')\nplt.ylabel('Y (Population in millions) COMPRESSED')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\nThis is what neural networks do. They make it easier to draw a line through or draw a line separating the data. We will use another example to make it clearer.\nNeural networks facilitate the “Make the data easier to work with” step. They do so by learning the transformation needed to do so by looking at the data.\n\n\nExample 2 and what do models learn?\nAnother example where we classify data.\nSay we have data that has coordinates (longitude and latitude), and the data are people. You want to classify the people based on “living in urban areas” and “living in rural areas”. This data can look like the following:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nrng = np.random.default_rng(19_750)\nr_city, r_rural = 2.0, 5.0\n\n# Generate \"city\" as a dense filled disk (class 1)\nn_city = 1000\ntheta_c = 2 * np.pi * rng.random(n_city)\nr_c = r_city * np.sqrt(rng.random(n_city))  # uniform over area\nx_c = r_c * np.cos(theta_c)\ny_c = r_c * np.sin(theta_c)\n\n# Generate \"rural\" as a sparse thin ring (class 0)\nn_rural = 250\ntheta_r = 2 * np.pi * rng.random(n_rural)\nr_r = r_rural + rng.normal(0, 0.35, n_rural)  # thin ring\nx_r = r_r * np.cos(theta_r)\ny_r = r_r * np.sin(theta_r)\n\n# Stack data\nX = np.vstack([np.c_[x_r, y_r], np.c_[x_c, y_c]])\ny = np.concatenate([np.zeros(n_rural, dtype=int), np.ones(n_city, dtype=int)])\n\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n# Just plot the data (train and test, color by class)\nplt.figure(figsize=(6, 5))\nplt.scatter(Xtr[ytr == 1, 0], Xtr[ytr == 1, 1], s=15, c='gold', edgecolor='k', label='City (train)')\nplt.scatter(Xtr[ytr == 0, 0], Xtr[ytr == 0, 1], s=15, c='purple', edgecolor='k', label='Rural (train)')\nplt.scatter(Xte[yte == 1, 0], Xte[yte == 1, 1], s=25, c='gold', edgecolor='k', marker='^', label='City (test)')\nplt.scatter(Xte[yte == 0, 0], Xte[yte == 0, 1], s=25, c='purple', edgecolor='k', marker='^', label='Rural (test)')\nplt.legend(loc='best', fontsize=9)\nplt.title(\"Urban vs Rural Areas Population\")\nplt.gca().set_aspect('equal')\nplt.tight_layout()\nplt.show()\n\n\nYou might want to do this classifying for resource allocation. For example, where should I put more fire stations?\nThough here, it is just a simple example/demonstration. You cannot draw a line to seperate the 2 polulations.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\n\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n\n# Small neural net (non-linear)\nmlp = MLPClassifier(hidden_layer_sizes=(8, 8), activation='relu', solver='lbfgs', alpha=1e-4, random_state=42, max_iter=2000)\nmlp.fit(Xtr, ytr)\n\nacc_mlp = accuracy_score(yte, mlp.predict(Xte))\n\n# Decision boundary visualization for MLP only\nx_min, x_max = X[:, 0].min() - 1.0, X[:, 0].max() + 1.0\ny_min, y_max = X[:, 1].min() - 1.0, X[:, 1].max() + 1.0\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 400), np.linspace(y_min, y_max, 400))\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\nZ_mlp = mlp.predict_proba(grid)[:, 1].reshape(xx.shape)\n\nfig, ax = plt.subplots(figsize=(6, 5))\ncs = ax.contourf(xx, yy, Z_mlp, levels=[0.0, 0.5, 1.0], alpha=0.25, cmap='coolwarm')\nax.scatter(Xtr[ytr == 1, 0], Xtr[ytr == 1, 1], s=15, c='gold', edgecolor='k', label='City (train)')\nax.scatter(Xtr[ytr == 0, 0], Xtr[ytr == 0, 1], s=15, c='purple', edgecolor='k', label='Rural (train)')\nax.scatter(Xte[yte == 1, 0], Xte[yte == 1, 1], s=25, c='gold', edgecolor='k', marker='^', label='City (test)')\nax.scatter(Xte[yte == 0, 0], Xte[yte == 0, 1], s=25, c='purple', edgecolor='k', marker='^', label='Rural (test)')\nax.set_title(f\"Neural net (MLP) — acc={acc_mlp:.3f}\")\nax.set_aspect('equal')\nax.legend(loc='upper right', fontsize=8)\nplt.title(\"Urban vs Rural Areas Population\\nDecision Boundry using a Neural Network\")\nplt.tight_layout()\nplt.show()\n\n\nWhat might have the neural network done to classify things correctly?\nThis is a very difficult problem. We only have glimses into what neural networks learn and how they internally represent things.\nBut in this case, we could venture a guess and suggest that the model may be doing something like this:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nrng = np.random.default_rng(19_750)\nr_city, r_rural = 2.0, 5.0\n\n# Generate \"city\" as a dense filled disk (class 1)\nn_city = 1000\ntheta_c = 2 * np.pi * rng.random(n_city)\nr_c = r_city * np.sqrt(rng.random(n_city))\nx_c = r_c * np.cos(theta_c)\ny_c = r_c * np.sin(theta_c)\n\n# Generate \"rural\" as a sparse thin ring (class 0)\nn_rural = 250\ntheta_r = 2 * np.pi * rng.random(n_rural)\nr_r = r_rural + rng.normal(0, 0.35, n_rural)\nx_r = r_r * np.cos(theta_r)\ny_r = r_r * np.sin(theta_r)\n\n# Vertical separation for classes\nz_c = np.full_like(x_c, 1.5)      # City at height 1.5\nz_r = np.full_like(x_r, -1.5)     # Rural at height -1.5\n\n# Stack data\nX_city = np.c_[x_c, y_c, z_c]\nX_rural = np.c_[x_r, y_r, z_r]\nX = np.vstack([X_rural, X_city])\ny = np.concatenate([np.zeros(n_rural, dtype=int), np.ones(n_city, dtype=int)])\n\nXtr, Xte, ytr, yte = train_test_split(X, y, test_size=0.3, stratify=y, random_state=1975)\n\n# 3D plot with vertical separation\nfig = plt.figure(figsize=(6, 5))\nax = fig.add_subplot(111, projection='3d')\n\n# Train points\nax.scatter(Xtr[ytr == 1, 0], Xtr[ytr == 1, 1], Xtr[ytr == 1, 2], s=15, c='gold', edgecolor='k', label='City (train)')\nax.scatter(Xtr[ytr == 0, 0], Xtr[ytr == 0, 1], Xtr[ytr == 0, 2], s=15, c='purple', edgecolor='k', label='Rural (train)')\n\n# Test points\nax.scatter(Xte[yte == 1, 0], Xte[yte == 1, 1], Xte[yte == 1, 2], s=25, c='gold', edgecolor='k', marker='^', label='City (test)')\nax.scatter(Xte[yte == 0, 0], Xte[yte == 0, 1], Xte[yte == 0, 2], s=25, c='purple', edgecolor='k', marker='^', label='Rural (test)')\n\n# Draw a separating plane between classes (e.g., z=0)\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\nxx, yy = np.meshgrid(np.linspace(*xlim, 20), np.linspace(*ylim, 20))\nzz = np.zeros_like(xx)  # Plane at z=0\n\nax.plot_surface(xx, yy, zz, alpha=0.5, color='gray', edgecolor='none')\n\nax.set_title(\"Urban vs Rural Areas Population (3D with vertical class separation)\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\")\nax.set_zlabel(\"Class (vertical separation)\")\nax.view_init(elev=25, azim=30)\nax.legend(loc='best', fontsize=9)\nplt.tight_layout()\nplt.show()\n\n\nThis could be a good guess (approximately) of what a neural network does to separate between rural and urban populations.\nSo what drives the model to do this? How does it decide to do this?\n\n\nHow does the model learn?\nOne thing to note is that we don’t tell the model to do something like this; it decides on its own. This is why we don’t understand how AI works in any practical model. We simply specify the problem.\nHere, the specification can be “minimize the mistakes in classifying between the 2 populations”. Then you pick the correct parameters (using calculus), and the model has learned to do its job.\n\n\nDeep Learning\nDeep learning is simply when you make these geometric decisions in steps rather than all at once this has made models much more practical and powerful\nWhat is the point of this?\n\nModel any written language that has ever existed. Translate between any languages. ChatGPT simply classifies the next word and is able to reason, help, deceive, and “think”.\nPredict the weather.\nModel any type of music or images, or everything altogether to create videos.\nDrive cars, move robots, and do surgery (poorly for now).\nPredict the stock market (to some extent). Detect fraud every time you swipe your credit card.\nSpot cancer, strokes, and blood clots. Discover new drugs and select genes."
  },
  {
    "objectID": "docs/intro_to_deep_learning/intro_to_fundamental_ML.html#risks-and-misuse.",
    "href": "docs/intro_to_deep_learning/intro_to_fundamental_ML.html#risks-and-misuse.",
    "title": "A visual introduction to fundamental modern machine Learning",
    "section": "Risks and misuse.",
    "text": "Risks and misuse.\nBut with all these capabilities come risks. These can be very broadly characterized into:\n\nRisks of misuse.\nInherent risks of the model.\n\nSome inherent risk can emerge from models being trained on biased (human) data.\n\nInherent\n\nCredit - CPSC 340, L35\n\nMisuse\n\nCredit - The Independent\n\nInherent\n\nCredit - CPSC 340, L35\n\nInherent\n\nCredit - CPSC 340, L35\n\nMisuse\n\nCredit - CPSC 340, L35\n\nMisuse & Inherent\n\nCredit - BBC\n\nMisuse\n\nCredit - MIT Technology Review\n\nMore:\n\nAutomated hacking and scams.\nGiving AI control of a banking/credit database.\nCopyright regulation. AI books, music, and movies that look like others.\nUsing AI to persuade groups of people and communities online.\nAnd so much more!\n\nPeople will misuse AI, and computer scientists are working on trying to make that harder. But turning models “unsafe” and “harmful” is still very easy to do. For free within ~3 hours for a smaller open version of ChatGPT.\nFor the inherent risks of AI, we need to invest more time and energy into understanding what the models are learning rather than purely focusing on the results or what the models can do.\nWe also need people from different domains to understand, use and play a role in making AI (eg. curating diverse data). In the meantime, we must be careful not to give AI control over systems where its unexpected performance can hurt people (eg, deciding if someone can go to jail)"
  },
  {
    "objectID": "docs/intro_to_cnns/intro_to_cnn.html",
    "href": "docs/intro_to_cnns/intro_to_cnn.html",
    "title": "Mostly Harmless Convolutional Neural Networks (CNNs)",
    "section": "",
    "text": "Before you begin: Install the dependencies that you don’t have by running the code cell below.\n\n\nCode\n# !pip install opencv-python\n# !pip install numpy\n# !pip install matplotlib\n# !pip install pandas\n# !pip install scikit-learn\n# !pip install seaborn\n# !pip install datasets\n# !pip install torch\n# !pip install tqdm\n\n\n\n\nWhat are Neural Networks?\nWhat is the first thing that comes to your mind when you hear the word “neural network”? If you are thinking about the human brain and neurons, you are not wrong. In fact, the term “neural network” is inspired by the way how human brain and nervous systems work, where neurons are connected to each other and communicate with each other to process information.\nIn the context of machine learning, a neural network, or more precisely, an artificial neural network (ANN) is defined as “a program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions”.\nThe definition seems way too formal and scientific, but we can easily translate it into daily language. Think of taking a closed-book multiple choice exam (Oops, gross). Your brain calls on a team of “experts”, one for course facts, another for what you remember about the professor’s hints in class, another for gut instincts, etc. When you read a question, each expert gives you a confidence score. You weight each score by how much you trust that expert, then add them up. The answer with the highest total “trust \\(\\times\\) confidence” wins. After the exam, you see which answers were wrong and adjust those trust weights (trust the right experts more, the wrong ones less), and prepare for the next exam based on this experience. This exactly how a neural network makes decisions and learns via its feedback loop. Neural networks are following a similar thought and learning process as you and me, and this is why they are flexible and powerful, being able to handle complex, abstract tasks and evolve on their own, like an intelligent creature.\nWhile the core idea is not complex, you may want to master some bluffing terms to translate the professional discussions. In the example above, the “experts” you consulted in your mind are called neurons; the key clues you noticed when reading question are called features; your understanding of exam question is called the input layer; your thought process rounds are called hidden layers; your chosen answer is reflected as the output layer; the mind map that connects all the “experts” and input features is architecture; and each exam attempt with the review of feedback is called a training epoch. See, they are really not that deep! You now can also talk about it as an expert.\n\n\nAn Intuitive Understanding of Convolutional Neural Networks (CNNs)\nNow that we understood what is an artificial neural network, let’s dive into the real topic here: What’s unique about convolutional neural networks (CNNs) and why they are revolutionary to computer vision and image processing?\nLet’s start by discussing the unique point of CNNs. Imagine you are reading a bird guide and trying to learn the characteristics of a night heron and a grey heron so that you can easily distinguish between the two in the field, what would you do? I believe you would naturally try to observe the birds piece by piece: first comparing the features of the juveniles and adults, then noting how they look both in flight and on land. Gradually, your brain forms a complete comparison: the night heron has a shorter beak, a shorter neck, striking red eyes, and dark blue plumage as an adult; while the grey heron has a longer beak, a longer neck, yellow eyes, and wears grey color plumage.\n\n\n\n\nNight heron in a bird guide\n\n\n\n\n\nGrey heron in a bird guide\n\n\n\nA convolutional neural network would read things in the same way, as it doesn’t look at things in a big picture directly (which is usually costly and slow), but would see an image as multiple small patches to study the unique features and construct a detailed field guide of its own. The way how a CNN sees things this way is through convolution: it has a convolutional layer on top of the input layer to learn features piece by piece in its architecture, such that it can process information from an image in a cleverer way. Moreover, CNNs work quite well even when training images are not as tidy and organized as those in a field guide, which makes it efficient in solving real-life problems.\nLet’s recall some basic concepts of convolution and see how they are applied in the CNNs, typically within the convolutional layer. Here, the inputs are images, and they are interpreted by a computer as grids of numbers. The kernels (also called filters) are still the “brushes” you apply on the input image to extract certain features, but in a CNN, there are usually multiple distinct kernels applied at the same time to extract and map different features. After different features are extracted, they will be pooled together with another kernel and produce a summarized output to be passed into the fully-connected layer for classification or other tasks.\nWhile the principles behind the architectures are complicated, many python libraries now offer easy ways to implement these architectures. In a word, with a labeled image dataset, you can also train a CNN classifier yourself. Let’s try out an example together.\n\n\n(Optional) Build Our Own CNN Classifier: An Example Using CIFAR-10 Dataset\nClassifying is central in the application of CNNs, so let’s try building a classifier using CNN and see how it works with an example. Let’s say, we want to train a model (the “expert”) that identify and distinguish between some daily objects, such as cars, planes, cat, dogs, etc. We first need to find a dataset that contains images of these objects with labels. This is usually hard as we wouldn’t always have clean, labelled datasets of a specific topic. But luckily, we have many datasets for daily objects.\nThe dataset we are using here is CIFAR-10, it is a widely used practice dataset for beginners to image processing that consists of 60000 32 \\(\\times\\) 32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. Let’s first load the dataset and see what it’s like.\n\nStep 1: Data Preprocessing\n\n\nCode\n# import CIFAR-10 dataset from HuggingFace\nfrom datasets import load_dataset\n\ndataset_train = load_dataset(\n    'cifar10',\n    split='train'# training dataset\n)\n\ndataset_train\n\n\n\n\nCode\n# check how many labels/number of classes\nnum_classes = len(set(dataset_train['label']))\nnum_classes\n\n\nWe can also display one of the images to see what it’s like.\n\n\nCode\n# let's view the image (it's very small)\nsample = dataset_train[0]['img']\n\nplt.imshow(sample)\nplt.axis('off')\nplt.show()\n\n\nCan you see what the image is about? Can you imagine how computers understands it?\nAs most CNNs can only accept images of a fixed size, we will reshape all images to 32 \\(\\times\\) 32 pixels using torchvision.transforms; a pipeline built for image preprocessing. You can think of a pipeline as a series of small programs that together handles a specific task in a sequential order, which in here is resizing the images in the training set.\n\n\nCode\nimport torchvision.transforms as transforms\nfrom tqdm.auto import tqdm\n\n# image size\nimg_size = 32\n\n# preprocess variable, to be used ahead\npreprocess = transforms.Compose([\n    transforms.Resize((img_size,img_size)),\n    transforms.ToTensor()\n])\n\ninputs_train = []\n\nfor record in tqdm(dataset_train):\n    image = record['img']\n    label = record['label']\n\n    # convert from grayscale to RGB\n    if image.mode == 'L':\n        image = image.convert(\"RGB\")\n        \n    # prepocessing\n    input_tensor = preprocess(image)\n    \n    # append to batch list\n    inputs_train.append([input_tensor, label]) \n\n\nOther than normalizing the general size of the images, we should also normalize the pixel values in the dataset.\n\n\nCode\nmean = [0.4670, 0.4735, 0.4662]\nstd = [0.2496, 0.2489, 0.2521]\n\npreprocess = transforms.Compose([\n    transforms.Normalize(mean=mean, std=std)\n])\n\nfor i in tqdm(range(len(inputs_train))):\n    # prepocessing\n    input_tensor = preprocess(inputs_train[i][0])\n    # replace with normalized tensor\n    inputs_train[i][0] = input_tensor\n\n\nHere, we load and process the training set that we are using to validate the model quality.\n\n\nCode\n# Loading the dataset\ndataset_val = load_dataset(\n    'cifar10',\n    split='test'  # test set (used as validation set)\n)\n\n# Integrate the preprocessing steps\npreprocess = transforms.Compose([\n    transforms.Resize((img_size,img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ninputs_val = []\ni = 0\nfor record in tqdm(dataset_val):\n    image = record['img']\n    label = record['label']\n\n    # convert from grayscale to RBG\n    if image.mode == 'L':\n        image = image.convert(\"RGB\")\n        \n    # prepocessing\n    input_tensor = preprocess(image)\n    inputs_val.append((input_tensor, label)) # append to batch list\n\n\nWe noticed that the testing and training data are gigantic in size, which would lag our trainings. To avoid the long training time and huge training cost, we often need to split our data into multiple small batches.\nIn CNN training, choosing a batch size of, say, 32 or 64 gives you the best of both worlds: you “study” small, manageable mini-quizzes, get regular feedback to adjust your filter-weights, and keep your compute requirements reasonable, all while learning robustly across the entire image dataset.\n\n\nCode\nimport torch\n\n# Given the amount of data, we set the batch size as 64 to improve the efficiency when running our model\nbatch_size = 64\n\n# We use DataLoader to split both the training and validation dataset into shuffled batches. \n# Shuffle helps prevent model overfitting by ensuring that batches are more representative of the entire dataset.\ndloader_train = torch.utils.data.DataLoader(\n    inputs_train, batch_size=batch_size, shuffle=True\n)\n\ndloader_val = torch.utils.data.DataLoader(\n    inputs_val, batch_size=batch_size, shuffle=False\n)\n\n\n\n\nStep 2: Training the CNN Classifier\nAfter carefully processing both the training and the test data, we finally came to a stage where we can train our own CNN classifier. The first thing we need to do is to decide which architecture we want to use for the model.\nArchitecture determines the way how a CNN integrate and learn from the features it extracted, and thus largely determines the performance of a model. Throughout the years, there have been several hugely successful CNN architectures, which we won’t be able to discuss in detail. Here, I will only demonstrate the architecture of LeNet-5: It reads images in a sequence that starts with a partial and combines the partials into a comprehensive one. Intuitively, the learning process of this architecture can be thought as learning to write a new character: You learn to write each stroke first, and then follow the structure of the character to put those strokes together into a complete character.\n\n\n\nCode\nimport torch.nn as nn\n\n# creating a CNN class\nclass ConvNeuralNet(nn.Module):\n    #  determine what layers and their order in CNN object \n    def __init__(self, num_classes):\n        super(ConvNeuralNet, self).__init__()\n        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n        self.relu1 = nn.ReLU()\n        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n        self.relu2 = nn.ReLU()\n        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1)\n        self.relu3 = nn.ReLU()\n        \n        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n        self.relu4 = nn.ReLU()\n\n        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n        self.relu5 = nn.ReLU()\n        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        self.dropout6 = nn.Dropout(p=0.5)\n        self.fc6 = nn.Linear(1024, 512)\n        self.relu6 = nn.ReLU()\n        self.dropout7 = nn.Dropout(p=0.5)\n        self.fc7 = nn.Linear(512, 256)\n        self.relu7 = nn.ReLU()\n        self.fc8 = nn.Linear(256, num_classes)\n    \n    # progresses data across layers    \n    def forward(self, x):\n        out = self.conv_layer1(x)\n        out = self.relu1(out)\n        out = self.max_pool1(out)\n        \n        out = self.conv_layer2(out)\n        out = self.relu2(out)\n        out = self.max_pool2(out)\n\n        out = self.conv_layer3(out)\n        out = self.relu3(out)\n\n        out = self.conv_layer4(out)\n        out = self.relu4(out)\n\n        out = self.conv_layer5(out)\n        out = self.relu5(out)\n        out = self.max_pool5(out)\n        \n        out = out.reshape(out.size(0), -1)\n        \n        out = self.dropout6(out)\n        out = self.fc6(out)\n        out = self.relu6(out)\n\n        out = self.dropout7(out)\n        out = self.fc7(out)\n        out = self.relu7(out)\n\n        out = self.fc8(out)  # final logits\n        return out\n\n\nAfter designing the network architecture, we initialize it. And if we have access to hardware acceleration (through CUDA or MPS), we move the model to that device to speed up the training.\n\n\nCode\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# set the model to device\nmodel = ConvNeuralNet(num_classes).to(device)\n\n\nNext, we will set the loss and optimizer function used during the training. These are the factors that determine how much your network learn from the mistakes and adjust the distribution of weights how it trust the “experts”.\nThe loss function is a metric that measures the classification performance. Here the Cross-Entropy Loss function is one of them that is commonly used in neural networks. The optimizer function drives the model to reflect and adjust its weights after each validation, and its parameter learning rate decides how much the model absorb from the lessons. While it seems that a higher learning rate is beneficial, it is actually not as a high learning rate could lead to severe overshooting. That’s why we set the learning rate lr = 0.01 here to prevent overly progressive learning.\n\n\nCode\n# set loss function\nloss_func = nn.CrossEntropyLoss()\n# set learning rate \nlr = 0.01\n# set optimizer as SGD\noptimizer = torch.optim.SGD(\n    model.parameters(), lr=lr\n) \n\n\nWe will train the model for 25 epochs. To ensure we’re not overfitting to the training set, we pass the validation set through the model for inference only at the end of each epoch. If we see validation set performance suddenly degrade while train set performance improves, we are likely overfitting.\nYou can run the training and fitting loop as follows, but be cautious: This cell will take a long time to run. Alternatively, you can skip 3 cells and load the model we pre-trained directly.\n\n\nCode\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\nnum_epochs = 25\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    for i, (images, labels) in enumerate(dloader_train):  \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = loss_func(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    avg_train_loss = running_loss / len(dloader_train)\n    train_losses.append(avg_train_loss)\n    \n    with torch.no_grad():\n        model.eval()\n        correct = 0\n        total = 0\n        all_val_loss = []\n        for images, labels in dloader_val:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            total += labels.size(0)\n            predicted = torch.argmax(outputs, dim=1)\n            correct += (predicted == labels).sum().item()\n            all_val_loss.append(loss_func(outputs, labels).item())\n            \n        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n        mean_val_acc = 100 * (correct / total)\n        \n        val_losses.append(mean_val_loss)\n        val_accuracies.append(mean_val_acc)\n        \n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {mean_val_loss:.4f}, Val Acc: {mean_val_acc:.2f}%')\n\n\nWe can visualize how training loss, validation loss and validation accuracy evolve over time.\n\n\nCode\nplt.figure(figsize=(12,5))\n\n# Plot Loss\nplt.subplot(1,2,1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\n\n# Plot Accuracy\nplt.subplot(1,2,2)\nplt.plot(val_accuracies, label='Validation Accuracy', color='green')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Validation Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nThe loss curve and validation accuracy curve show that the training loss and validation loss goes down while the validation accuracy of the model goes up as the training epochs increase. If we add more epochs (costly!), the validation accuracy of the model will be higher, but the model will also be more at risk of overfitting. To prevent which, we often need to regularize the model.\nAfter training for 25 epochs, we see our validation accuracy has passed 70%, we can save the model to file and load it again with the following codes:\n\n\nCode\n# save to file\ntorch.save(model, 'data/cnn.pt')\n\n\n\n\nCode\n# load from file and switch to inference mode\nmodel = torch.load('data/cnn.pt', weights_only=False)\nmodel.eval()\n\n\n\n\nStep 3: Inference the Classifier\nNow, we can use the trained classifier to predict the labels of the new input. But here, we are just using the test set for validation (which is not recommended).\n\n\nCode\ninput_tensors = []\n\nfor image in dataset_val['img'][:10]:\n    tensor = preprocess(image)\n    input_tensors.append(tensor.to(device))\n\n# stack into a single tensor\ninput_tensors = torch.stack(input_tensors)\ninput_tensors.shape\n\n\n\n\nCode\n# process through model to get output logits\noutputs = model(input_tensors)\n# calculate predictions\npredicted = torch.argmax(outputs, dim=1)\npredicted\n\n# here are the class names\ndataset_val.features['label'].names\n\n\n\n\nCode\n# Print out the output label and the true label\nfor i in range(10):\n    example = dataset_val[i]           # get the i-th example as a dict\n    image   = example['img']\n    true_id = example['label']\n    pred_id = predicted[i]\n    \n    true_label = dataset_val.features['label'].names[true_id]\n    pred_label = dataset_val.features['label'].names[pred_id]\n    \n    plt.figure(figsize=(4,4))\n    plt.imshow(image)\n    plt.title(f\"True: {true_label}   |   Pred: {pred_label}\")\n    plt.axis('off')\n    plt.show()\n\n\nWe can visualize the feature maps at different layers to see how the CNN see images.\n\n\nCode\n# Visualization of features at different layers\nimport torchvision.transforms as T\n\nto_tensor = T.ToTensor()\n\npil_img, _ = dataset_val[0]['img'], dataset_val[0]['label']\n\ninput_tensor = to_tensor(pil_img).unsqueeze(0).to(device)\n\nactivations = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activations[name] = output.detach().cpu()\n    return hook\n\nfor layer_name in ['conv_layer1','conv_layer2','conv_layer3','conv_layer4','conv_layer5']:\n    getattr(model, layer_name).register_forward_hook(get_activation(layer_name))\n\nmodel.eval()\nwith torch.no_grad():\n    _ = model(input_tensor)\n\n# Plot the feature map\nfor name, fmap in activations.items():\n    num_filters = fmap.shape[1]\n    cols = 6\n    rows = min((num_filters + cols - 1) // cols, 4)\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n    fig.suptitle(f'Feature maps from {name}', fontsize=16)\n    for i in range(rows * cols):\n        r, c = divmod(i, cols)\n        ax = axes[r, c] if rows &gt; 1 else axes[c]\n        if i &lt; num_filters:\n            ax.imshow(fmap[0, i], cmap='viridis')\n            ax.set_title(f'#{i}')\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\nWe can see that the model made mostly correct predictions, despite the image quality was low that even human may have difficulty to correctly classify. This somewhat shows the advantage of CNNs over humans when confronted with complex, blurry images, but CNNs have more applications than that. They power a host of real-world applications, from enabling your smartphone’s camera to automatically recognize faces and apply portrait effects, to guiding autonomous vehicles by detecting pedestrians, road signs, and lane markings in real time. In healthcare, CNNs help radiologists spot tumors in MRI and CT scans, and dermatologists classify skin lesions from photos. They underpin optical character recognition for digitizing handwritten forms, fuel quality-control systems that spot manufacturing defects on assembly lines, and even drive wildlife monitoring by identifying animals in camera-trap images.\nThis technology is also reshaping some humanities and social science research. For example, in archaeology, CNNs are being used to categorize, complete, and translate broken clay tablets and cuneiform texts; in art history, CNNs are being used to study the pigments and materials used in paintings, as well as the expressions and gestures of the figures in them; and in anthropology, CNNs are being used to distinguish between human races and complex kinships. It is for this reason that we are here to introduce it to you! I hope you enjoyed the class and got something different out of it!\n\n\n\nKey takeaways from this part:\n\nArtificial Neural Networks (ANNs) are programs or models that make decisions in a similar manner to the thought process of a human brain.\nConvolutional Neural Networks (CNNs) differ from other neural networks in the convolutional layer that allows them to understand features from image input in a more efficient way.\nArchitectures are central in neural networks as they determine the ways how a model learn from the input features and thereby determine the model performance.\nMachine Learning and CNNs are fun and practical in the field of humanities and social sciences!\n\n\n\nAdditional Resources\n\nMLU-EXPLAIN: Neural Networks: A website with straightforward explanation and interactive visualizations of neural networks (with some math and technical terms), including more professional terminologies and advanced concepts that we won’t cover in this notebook. But if you find this notebook to be too light and really hope to learn more, this is a good place to go!\nCNN Explainer: An interesting interactive tutorial that explains how CNN work in a more visual way (but you may also find the explanation a little too technical). Try it out! You can also upload your own images of interest to see how the neural network processes them and classify them. Do you get the same results as you expected? What can you say about it?\n\n\n\nReferences\n\nPinecone. Embedding Methods for Image Search. https://www.pinecone.io/learn/series/image-search\nIBM. What is a neural network? https://www.ibm.com/think/topics/neural-networks\nIBM. What are convolutional neural networks? https://www.ibm.com/think/topics/convolutional-neural-networks\nConvolutional Neural Network From Scratch. https://medium.com/latinxinai/convolutional-neural-network-from-scratch-6b1c856e1c07"
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html",
    "href": "docs/SOCI-415/soci_415_network_analysis.html",
    "title": "SOCI 415 Network Analysis",
    "section": "",
    "text": "This notebook introduces key concepts in network analysis pertaining to sociology and provides a hands-on tutorial to using the NetworkX Python library."
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html#what-is-network-analysis",
    "href": "docs/SOCI-415/soci_415_network_analysis.html#what-is-network-analysis",
    "title": "SOCI 415 Network Analysis",
    "section": "1. What is Network analysis?",
    "text": "1. What is Network analysis?\nNetwork Analysis is a set of techniques used to study the structure and dynamics of networks. Networks are collections of objects/locations/entities (called nodes) connected by relationships (called edges). Network analysis has applications in many fields, including sociology, biology, economics, computer science, and more.\n\n\n\nRegional networks of ceramic similarity across time in the greater Arizona/New Mexico area of the US (From Mills et al. 2013, Fig. 2)\n\n\nNetwork science in Sociology Network analysis in sociology is the systematic study of social structure through the mapping and measurement of relationships among individuals, groups, or organizations. It is not a discipline on its own, but rather a methodological and theoretical approach within sociology that helps conceptualize, describe, and model society as interconnected sets of actors linked by specific relationships. Network analysis in sociology is a core tool for understanding how relationships and social structures impact individuals and groups,\n\n1.1 Key terms\n\n\n\n\n\nAn example of a graph with nodes and edges.\n\n\n\nNode: A node is a representation of an individual entity or actor in a network. In different contexts, nodes can be people, organizations, cities, or any other unit of analysis.\nEdge: An edge represents the relationship or connection between two nodes. Edges can be directed (having a specific direction from one node to another) or undirected (no direction, implying a mutual relationship).\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nDegree: The degree of a node is the number of edges connected to it. In directed networks, this can be further divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges).\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nThe network above is an example of a Undirected graph, a graph with no direction. This means that if there is a connection between node A and node B, it is bidirectional: A is connected to B, and B is connected to A.\nThe example to the left is a directed graph: the edges between nodes have a specific direction. This means that if there is an edge from node A to node B, it does not imply there is an edge from B to A unless explicitly stated.\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nDensity: Density is a measure that indicates how closely connected the nodes in a network are. Specifically, it refers to the ratio of the number of actual edges in the network to the maximum possible number of edges between nodes.\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nCentrality: Centrality measures the importance, influence, or prominence of nodes (entities) within a network. The centrality of a node tells us how “important” a node is to the aggregate network. There are many different kinds of centrality, but the four most well-known ones are degree, betweenness, closeness, and eigenvector centrality.\n\n\n\n\n1.2 NetworkX\nNetworkX is a Python library that is used for the creation, manipulation, and visualization of complex networks. It provides tools to work with both undirected and directed networks, perform network-related calculations, and visualize the results.\nA library in Python is a collection of code that makes everyday tasks more efficient. In this case working with networks becomes much simpler when using NetworkX.\nIf you want to read the NetworkX documentation you can follow the NetworkX documentation link. This link shows what kind of commands exist within the NetworkX library.\n\n1.2.1 Importing NetworkX\nWe can import NetworkX using the import command. At the same time, we’ll also import the matplotlib.pyplot library, for plotting graphs. Additionally, we’ll import pandas for basic data wrangling, and numpy for math. The as command allows us to use networkx commands without needing to type out networkx each time. Along with some other libraries.\n\nimport matplotlib.pyplot as plt #allows us to call the matplotlib.pyplot library as 'plt'\nimport matplotlib.patches as mpatches #imports mpatches matplotlib subpackage \nimport networkx as nx #allows us to call the networkx library as 'nx'\nimport pandas as pd #allows us to call the pandas library as 'pd'\nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\n\n\n\n1.2.2 Creating simple networks using NetworkX\nWe’ll start by creating a simple graph:\nBelow in the code we choose our nodes and edges between them.\n\nG = nx.Graph() #creates an empty network graph\n\nnodes = (1, 2, 3, 4, 5, 6) #our nodes, labeled 1,2,3,4,5,6.\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\n        #the connections between our nodes are stored in an array, containing pairs of numbers called tuples.\nG.add_edges_from(edges) #the `add_edges_from()` command adds edges to the network\nG.add_nodes_from(nodes) #the `add_nodes_from()` command adds nodes to the network\n\nnx.draw(G, with_labels = True) #renders the graph in the notebook\n        #the `with_labels = True` argument specifies that we want labels on the nodes.\n\nLet’s create a directed graph using nx.DiGraph(). We’ll also set our node positions using a seed: this will ensure that each time the nodes are rendered they hold the same position on the graph. You can set the seed to any number.\n\nG = nx.DiGraph() #creates an empty directed graph object\nnodes = (1, 2, 3, 4, 5, 6) #our nodes\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)] #our tuples stored in an array which represent our nodes\nG.add_edges_from(edges) #connects edges to nodes\nG.add_nodes_from(nodes) #connects edges to nodes\n\nposition = nx.spring_layout(G, seed=100)\n\n#nx.draw plots our network\nnx.draw(G, pos = position, with_labels = True) # `pos` argument assigns a position to each node\n\n\n\n\n1.3 Creating Random Graphs\nInstead of creating a graph with predetermined positions of nodes and edges we can also generate a random graph with a set amount of nodes and edges. Below you can change the amount of nodes and edges by changing n and d which correspond to the number of nodes and the degree (number of edges) that each node has. Creating a random graph could be more helpful for testing or when you want to try something and don’t wish to spend time plotting a real network and determining paths for all edges and nodes.\nThe command we will use is the nx.random_regular_graph command. Which generates a random regular graph.\n\n# Set a seed for reproducibility so that everytime the code runs we get the same random graph\nrandom.seed(42)\n\n# Parameters\nn = 20  # number of nodes\nd = 3   # degree of each node\n\n# Generate the random regular graph\nrr_graph = nx.random_regular_graph(d, n)\n\n# Visualize the graph, you can change the size, color, font and node size. \nplt.figure(figsize=(8, 6)) \nnx.draw(rr_graph, with_labels=True, node_color='lightgreen', node_size=500, font_size=10, font_weight='bold')\nplt.title(\"Random Regular Graph\")\nplt.show()\n\n# Print some basic information about the graph\nprint(f\"Number of nodes: {rr_graph.number_of_nodes()}\")\nprint(f\"Number of edges: {rr_graph.number_of_edges()}\")\nprint(f\"Degree of each node: {d}\")"
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html#degrees-density-and-weights",
    "href": "docs/SOCI-415/soci_415_network_analysis.html#degrees-density-and-weights",
    "title": "SOCI 415 Network Analysis",
    "section": "2. Degrees, Density and Weights",
    "text": "2. Degrees, Density and Weights\n\n2.1 Degrees\nThe degree of a node is the number of edges that are connected to a node.\nWe can see the degree of each node by running dict(G.degree()). This create a dictionary of key-value pairs for our network, where each key is the name of the node and the value is it’s respective degree.\n\ndegrees = dict(G.degree())\n\nIf we want to see the degree of node \\(n\\), we can do so by running print(degrees[n]). For instance:\n\nprint(degrees[1])\n\nLet’s color the nodes of our graph based on their degree. We’ll create a function called get_node_colors which takes in the degree dictionary of each node and returns a color. We’ll then create a for-loop that iterates over each nodes in the list of nodes, gets the color of each node using the get_node_colors function we defined earlier, and appends it to an empty list called color_map.\n\ndegrees = dict(G.degree())\nnodes = list(G.nodes())\n\ndef get_node_colors(degree):\n    if degree in [1, 2]:\n        return 'blue'\n    elif degree in [3, 4]:\n        return 'green'\n    elif degree in [5, 6]:\n        return 'yellow'\n    else:\n        return 'red' \n\ncolor_map = [] #`color_map` is an empty list\n\nfor node in nodes:\n  color = get_node_colors(degrees[node]) # get color of current node using node_colors according to degree of node\n  color_map.append(color) # appends color of each node to color_map for each node in nodes\n\nprint(degrees)\nprint(nodes)\nprint(color_map)\n\nThe \\(n\\)-th entry in color_map corresponds to the \\(n\\)-th node in nodes. For instance, color_map[0] returns the color of the first node (1).\n\ncolor_map[0]\n\nWe can now color the nodes of our graph, using the color map we defined above. The node_color argument takes in an array or list of colors that it uses to color each node.\n\nG = nx.DiGraph() # creates an empty directed graph object\nnodes = (1, 2, 3, 4, 5, 6) \nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nG.add_edges_from(edges) \nG.add_nodes_from(nodes) \n\nposition = nx.spring_layout(G, seed=100)\n\nnx.draw(G, pos = position, node_color=color_map, with_labels=True) \n    # node_color argument colors the nodes based on a given list or array of colors, \n    # with the first color corresponding to the first node, second to the second node, etc.\n\nLet’s also add a legend to our graph, which gives information about the meaning of each color. We’ll do this using the mpatches subpackage we imported earlier.\n\nblue_patch = mpatches.Patch(color='blue', label='1-2 edges') \ngreen_patch = mpatches.Patch(color='green', label='3-4 edges')\nyellow_patch = mpatches.Patch(color='yellow', label='5-6 edges')\nplt.legend(handles=[blue_patch, green_patch, yellow_patch]) #adds legend to the plot\n\nnx.draw(G, pos = position, node_color=color_map, with_labels=True)\n\n\n\n2.2 Density\nDensity refers to the proportion of actual edges in a network compared to the total possible connections. It indicates how interconnected the nodes in a network are, with a higher density suggesting a more connected network. Density is defined from [0,1], meaning a network with a density of 0.95 is very interconncected. Note that self-loops (edges from and to the same node) are counted in the total number of edges but not in the maximum number of edges so graphs can have a density greater than 1.\nWe can calculate the density of our graph:\n\nnx.density(G)\n\n\n\n2.3 Weights\nOften times, you may end up working with weighted graphs: for instance, these weights could correspond to popularity of roads in road networks, or the size of pipes in a sewage network.\nWe’ll standardize our weights to be between 1 and 2 (as otherwise the results are messy). We’ll do this using a for-loop, like we did with the degrees.\n\nG_weights = nx.DiGraph() #creating a new graph object called G_weights\nnodes = [1, 2, 3, 4, 5, 6]\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nweights = [100, 50, 75, 50, 60, 100, 100, 75, 40, 50, 50, 100, 100] #add list of weights\nG_weights.add_edges_from(edges) \nG_weights.add_nodes_from(nodes) \n\nadjusted_weights = []\nfor weight in weights:\n    adjusted_weight = 1+ (max(weights)-weight)/(max(weights)-min(weights)) #standardizes weights to be between 1 and 2\n    adjusted_weights.append(adjusted_weight)\n\nposition = nx.spring_layout(G, seed=100)\n\nprint(adjusted_weights)\nnx.draw(G_weights, pos = position, width = adjusted_weights, with_labels = True) \n    # width argument take in a list or array of numbers corresponding to weights\n\nThis is great, but the results aren’t very clear. Let’s add a color gradient to the edges to represent different weights.\n\nnorm = plt.Normalize(min(weights), max(weights), clip=False) \n    #`plot.normalizes` normalizes the weights such that they are evenly distributed across the gradient spectrum\nedge_colors = plt.cm.Greys(norm(weights)) \n    # norm(weights) normalizes the weights \n    # plot.cm.greys() assigns the weights to color values\n    # edge_colors is a multidimensional array of RGBA color values corresponding to each edge\n\nfig, ax = plt.subplots() #explicitly specifying figure and axes in order to create a color bar\n\nnx.draw(G_weights, pos=position, edge_color=edge_colors, width=adjusted_weights, with_labels=True, ax=ax) \n    #ax = ax argument needed for color bar\n\n# Adding color bar\nsm = plt.cm.ScalarMappable(cmap=\"Greys\", norm=norm) # creates a scalarmappable object which acts \n                                                    # as a bridge between the numerical weight values and color map\nplt.colorbar(sm, ax=ax) #plotting color bar"
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html#adjacency-matrices",
    "href": "docs/SOCI-415/soci_415_network_analysis.html#adjacency-matrices",
    "title": "SOCI 415 Network Analysis",
    "section": "3. Adjacency matrices",
    "text": "3. Adjacency matrices\nAn Adjacency matrix is a method of representing graphs in matrix form. In an adjacency matrix, the rows and columns correspond to the vertices (or nodes) of the graph. The entries of the matrix indicate whether pairs of vertices are adjacent or not in the graph. Normally, a value of 1 is assigned to entries where an edge is present, and 0 is assigned to entries where an edge is not. For a weighed graph, the weight of the edge is represented as a numerical value for entries where an edge is present.\nWe can convert our simple graph to an adjacency matrix:\n\nnx.to_pandas_adjacency(G)\n\nIf we want to use our weighted graph, we can use the following code:\n\n# len(edges) returns the total number of entries in the list of edges.\n# range(len(edges)): This generates a sequence of numbers from 0 to n-1 where n is len(edges), \n    #so the for-loop will run n times with i taking each value in that range, one at a time.\n\nfor i in range(len(edges)):\n    edge = edges[i] # retrieves the edge at position i in the list of edges\n    weight = weights[i] # retrieves the weight at position i in the list of weights\n    G_weights.add_edge(edge[0], edge[1], weight=weight) # adds an edge with a weight to the graph \n    \nnx.to_pandas_adjacency(G_weights, nodelist=nodes, weight='weight') #converts to pandas adjacency matrix with the weights in place\n\nWe can visualize our matrix using the code below. Note that instead of using nx.to_pandas_adjacency we use nx.to_numpy_array: this allows us to store the matrix in the form of an array.\n\nadj_matrix = nx.to_numpy_array(G_weights, nodelist=nodes, weight='weight')\n\n\nplt.figure(figsize=(8, 8)) #displays data as an image on a 2d raster; in our case, a numpy array\n\nplt.imshow(adj_matrix, cmap='gray_r')\n\nfor i in range(adj_matrix.shape[0]): #loops through each row of the matrix\n    for j in range(adj_matrix.shape[1]): #for each row, loops through each column of the matrix\n        plt.text(j, i, int(adj_matrix[i, j]),\n                 ha='center', va='center', color='red', size=30) #prints the value at that position in the matrix on the graph\n\nplt.title('Adjacency Matrix Visualization')\nplt.xlabel('Node Index')\nplt.ylabel('Node Index')"
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html#measures-of-centrality",
    "href": "docs/SOCI-415/soci_415_network_analysis.html#measures-of-centrality",
    "title": "SOCI 415 Network Analysis",
    "section": "4.0 Measures of Centrality",
    "text": "4.0 Measures of Centrality\nCentrality is defined as the set of metrics used to determine the importance or influence of a particular node within a network. It helps to identify which nodes hold strategic significance in terms of connectivity, information flow, or influence over other nodes. Various centrality metrics, such as degree, betweenness, and eigenvector centrality, provide different perspectives on the role each node plays within the network’s overall structure.\n\n4.1 Network Distance and Eccentricity\nBefore talking about centrality, we first need to talk a bit about distance. Distance, also known as Geodesic distance, is defined as the number of edges traversed by the shortest path between two nodes.\n\nThe distance between a node and itself is 0.\nThe distance between a node and a node for which no shortest path exists (such as a node that is disconnected from other nodes) is \\(\\infty\\).\nThe distance between a node and it’s neighbor is 1.\n\nA node’s eccentricity is the maximum distance from said node to all other nodes in the graph. For instance, in the following network, the eccentricity of node \\(A\\) is 2, but the eccentricity of node \\(B\\) is 1.\n\nnodes = (\"A\",\"B\", \"C\")\nedges = [(\"A\",\"B\"), (\"B\", \"C\")]\n\nG_example = nx.Graph()\nG_example.add_edges_from(edges)\nG_example.add_nodes_from(nodes)\n\ncolor_map = [\"salmon\", \"lightblue\", \"salmon\"]\n\n\nred_patch = mpatches.Patch(color='salmon', label='eccentricity = 1') \nblue_patch = mpatches.Patch(color='lightblue', label='eccentricity = 2') \nplt.legend(handles=[blue_patch, red_patch])\n\nnx.draw(G_example, node_color=color_map, with_labels=True)\n\nIf we color the nodes of our random graph by eccentricity, we can see:\n\n#Create random graph\nrandom.seed(415) #Course code\nn = 20 \nd = 3\nrr_graph = nx.random_regular_graph(d, n)\n\n# Compute eccentricity\necc = nx.eccentricity(rr_graph)\nunique_ecc = sorted(set(ecc.values()))\n\n# Choose a color for each unique eccentricity using matplotlib\ncolors = plt.get_cmap('tab10', len(unique_ecc))\necc_to_color = {e: colors(i) for i, e in enumerate(unique_ecc)}\ncolor_map = [ecc_to_color[ecc[node]] for node in rr_graph.nodes()]\n\n# Create a legend\npatches = [mpatches.Patch(color=colors(i), label=f\"eccentricity = {e}\") for i, e in enumerate(unique_ecc)]\n\nplt.figure(figsize=(8, 6))\nnx.draw(rr_graph, node_color=color_map, with_labels=True, node_size=500, font_size=10, font_weight='bold')\nplt.legend(handles=patches, bbox_to_anchor=(1, 1))\nplt.title(\"Random Regular Graph with Eccentricity Coloring\")\nplt.tight_layout()\nplt.show()\n\n# Print summary\nprint(f\"Number of nodes: {rr_graph.number_of_nodes()}\")\nprint(f\"Number of edges: {rr_graph.number_of_edges()}\")\nprint(f\"Eccentricity distribution: {ecc}\")\n\nWe can see that the nodes in the furthest corners have an eccentricity of 6 and more central nodes have a lower eccentricity.\n\n\n4.2 Degree Centrality\nDegree centrality is simple: Recall that the degree of a node is the number of nodes directly connected to it. In degree centrality, the more adjacent nodes, the more important the network is considered to be. Degree centrality is used primarily in social networks, where nodes with higher degrees are commonly major channels of information. A high degree means a node has many direct ties with other nodes, and has better access to resources within the network.\nNote that the NetworkX nx.degree_centrality() function normalizes each node’s degree by dividing by the maximum possible degree in the network. Therefore for graphs without self-loops the degree centrality is always \\(\\leq 1\\). For educational purposes, we un-normalize the degree values, but this is not common practice.\nWe can not use the same random graph technique as each node will have a degree equal to 3, instead we will use the first graph from this notebook.\nWe can calculate the degree centrality of all our nodes in our network:\n\n# Define the graph\nnodes = (1, 2, 3, 4, 5, 6)\nedges = [\n    (1, 2), (2, 3), (3, 1), (1, 5), (3, 5),\n    (4, 5), (4, 6), (6, 1), (6, 3), (6, 4),\n    (4, 3), (5, 5), (3, 5)\n]\nG = nx.Graph()\nG.add_edges_from(edges)\nG.add_nodes_from(nodes)\n\n# Get degree for each node\ndegree_dict = dict(G.degree())\ndegrees = list(degree_dict.values())\nunique_degrees = sorted(set(degrees))\n\n# Assign a unique color per degree\ncolors = plt.get_cmap('viridis', len(unique_degrees))\ndegree_to_color = {deg: colors(i) for i, deg in enumerate(unique_degrees)}\ncolor_map = [degree_to_color[degree_dict[n]] for n in G.nodes()]\n\n# Create legend for each degree\npatches = [mpatches.Patch(color=colors(i), label=f\"degree = {deg}\") for i, deg in enumerate(unique_degrees)]\n\nplt.figure(figsize=(6, 4))\nnx.draw(G, with_labels=True, node_color=color_map, node_size=800, edge_color='gray', font_weight='bold')\nplt.legend(handles=patches, bbox_to_anchor=(1, 1))\nplt.title(\"Degree Centrality Graph\")\nplt.tight_layout()\nplt.show()\n\n\n\n4.3 Closeness Centrality\nCloseness centrality is a measure of how close a node is to all other nodes in the network. It can be computed as the “sum of the geodesic distances of a node to all other nodes in the network”. A node is important if it is close to all other nodes in the network. One flaw of closeness centrality is that while it is a useful indicator of node importance in small networks, it produces little variation in large networks with many edges.\n\n# Define the graph\nnodes = (1, 2, 3, 4, 5, 6)\nedges = [(1, 2), (2, 3), (3, 1), (1, 5), (3, 5),(4, 5), (4, 6), (6, 1), (6, 3), (6, 4),(4, 3), (5, 5), (3, 5)]\nG = nx.Graph()\nG.add_edges_from(edges)\nG.add_nodes_from(nodes)\n\n# Calculate closeness centrality\ncentrality = nx.closeness_centrality(G)\n\n# Normalize centrality values for color mapping (so 0 = min, 1 = max)\ncentralities = np.array(list(centrality.values()))\nnorm_centrality = (centralities - centralities.min()) / (centralities.max() - centralities.min() + 1e-9)\n\n# Map normalized centrality to colormap\ncmap = plt.cm.plasma\ncolor_map = [cmap(val) for val in norm_centrality]\n\nplt.figure(figsize=(6, 4))\nnx.draw(G, with_labels=True, node_color=color_map, node_size=800, edge_color='gray', font_weight='bold')\nplt.title(\"Graph Colored by Closeness Centrality\")\nplt.tight_layout()\nplt.show()\n\n# Print node centrality table\nprint(\"Node\\tCloseness Centrality\")\nfor node in G.nodes():\n    print(f\"{node}\\t{centrality[node]:.4f}\")\n\n\n\n4.4 Betweenness Centrality\nBetweenness Centrality is a measure of the importance of a node based on how well it serves as a bridge between nodes in a network. The mathematical representation of the betweeness centrality of a node is the number of times each node has to pass through that node to reach every other node in a network. Nodes with high betweenness thus serve as “bridges” within a network.\nConsider the graph below:\n\n#Define our network\nG_betweenness_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_betweenness_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_betweenness_example, seed=1000)\n\n#Draw our graph\nnx.draw(G_betweenness_example,pos=pos, with_labels=True, edgecolors=\"black\", node_color=\"bisque\", node_size=800)\n\nNode \\(4\\) serves as a bridge between nodes 5 and 6 to the rest of the nodes in the network. For a path to be drawn between nodes 6 or 5 to nodes 0,1,2,3, the path must go through node 4. Let’s calculate the betweenness centrality of this network, and label nodes by centrality:\n\n#Define our network\nG_betweenness_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_betweenness_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_betweenness_example, seed=1000)\n\n#Find the centrality values for our nodes\ncentrality = nx.betweenness_centrality(G_betweenness_example, normalized=False)\ncentrality_values = np.array(list(centrality.values()))\ncmap=\"BuPu\"\n\n#Put labels on our network\nlabels = {}\nfor node in G_betweenness_example.nodes():\n    labels[node] = centrality_values[node]\n\n\n#Draw our graph using `nx.draw`\nnx.draw(G_betweenness_example,pos=pos, node_color=centrality_values, edgecolors=\"black\", cmap=cmap, node_size=800)\nnx.draw_networkx_labels(G_betweenness_example, pos, labels=labels, font_color=\"orangered\")\n\nWe can see that node 4 does indeed have the highest betweenness centrality. The values of 0 for nodes 0, 1, 2, 3 and 6 indicate that each node can reach every other node without passing through those nodes. The value of 5.0 for node 5 indicates that five nodes must pass through node 5 in order to reach another node.\n\n\n4.5 Eigenvector centrality\nEigenvector centrality is a measure of the influence of a node in a network by considering not just how many connections it has (as we did with degree centrality), but also the importance of those connections: A node with high eigenvector centrality is connected to many nodes that themselves have high centrality, making it more influential in spreading information or resources. Unlike simpler measures like degree centrality, which only counts connections, eigenvector centrality looks at the overall structure of the network. It helps identify key players in a network who might not have the most connections but are well-connected to other important nodes.\n\n# Create a larger network \nG = nx.barabasi_albert_graph(n=20, m=2, seed=42)  # 20 nodes and each connects to 2\n\n# Calculate eigenvector centrality\ncentrality = nx.eigenvector_centrality(G)\n\n# Normalize values for coloring\ncentrality_values = np.array(list(centrality.values()))\nnorm_centrality = (centrality_values - centrality_values.min()) / (centrality_values.max() - centrality_values.min() + 1e-9)\ncmap = plt.cm.plasma\ncolor_map = [cmap(val) for val in norm_centrality]\n\n# Draw the graph\nplt.figure(figsize=(8, 6))\npos = nx.spring_layout(G, seed=42)\nnx.draw(G, pos=pos, with_labels=True, node_color=color_map,\n        node_size=800, font_weight='bold', edge_color='gray')\nplt.title('Large Graph Colored by Eigenvector Centrality')\nplt.tight_layout()\nplt.show()\n\n# Print node eigenvector centrality table\nprint(\"Node\\tEigenvector Centrality\")\nfor node in G.nodes():\n    print(f\"{node}\\t{centrality[node]:.4f}\")"
  },
  {
    "objectID": "docs/SOCI-415/index_SOCI415.html",
    "href": "docs/SOCI-415/index_SOCI415.html",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "Before First Lecture:\n\nPower in Networks: The Medici Only Section’s 1,2, and 7\nKINMATRIX Data Only Page 789 - 793: Intro and Methods\n\nBefore Second Lecture:\n\nThe China Biographical Database User Guide Only Page 1-2\n\n\n\n\nBy the end of this lesson, students will:\n\nBe familiar with Network Analysis Terminology such as: nodes, edges, degree, density, centrality and clustering and will have seen them used in a real research context.\nHave some insight into how to create networks and utilize them to answer research questions\nExplore real life examples with the KINMATRIX and CBDB Dataset for family analysis\nHave multiple discussions about the data, network analysis and limitations of the techniques used.\n\n\n\n\n\nAccess to a Google drive with a .zip file (can be a link on Canvas)\nDevice with internet access (laptop preferred)\nNo previous coding experience required (familiarity with Python is an asset)\nNo previous network analysis or high-level math experience required\n\n\n\n\n\nInstructors should: Test if the .html files from the .zip file render correctly. Once it is tested they render correctly the instructor should read over and be familiar with the context to answer the student’s questions.\nStudents should: Complete the pre-readings for each lecture and come with a laptop to class.\n\n\n\n\nIntroductory Notebook:\n\nIntro for what is network analysis and key terminology\nHow to create simple and random graphs in Python with NetworkX\nDegree, Density and Weights\nAdjacency Matrices\nMeasures of Centrality (Network Distance and Eccentricity, Degree Centrality, Closeness Centrality, Betweenness Centrality)\n\nKINMATRIX Notebook:\n\nIntro to the KINMATRIX Dataset and broad patterns across countries\nDensity by Age and Gender Across Countries\nColored Family Networks and Visualization of Gender\nHealth and Education Across Countries\nFamily size vs Political Allegiance in USA and Poland\nEmployment and Education in Poland\nParental Separation\nGender and Sexuality Across Countries and Regions\n\nThe China Biographical Database Notebook:\n\nIntro\nLouvain Algorithm and Community Clustering\nDegree Centrality and Important Family members (Who they were)\nDid women act more as bridges and who these key women were\nNetworks over time between dynasties\n\n\n\n\nKINMATRIX Discussions:\n\nDiscussion on Density by Age and Gender Across the Countries\nDiscussion on Family Structure\nDiscussion on Family size and Political Allegiance\nDiscussion on Parental Separation Across the Countries\nHands on Section where students draw their own ego-networks of their families (Maybe website we will see)\n\nThe China Biographical Database Discussions:\n\nDiscussion on Louvain and Visualizations\nDiscussion on Centrality\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Network Analysis to Analyze The China Biographical Dataset\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the KINMATRIX Dataset to explore network analysis for SOCI 415\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/SOCI-415/index_SOCI415.html#pre-readings",
    "href": "docs/SOCI-415/index_SOCI415.html#pre-readings",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "Before First Lecture:\n\nPower in Networks: The Medici Only Section’s 1,2, and 7\nKINMATRIX Data Only Page 789 - 793: Intro and Methods\n\nBefore Second Lecture:\n\nThe China Biographical Database User Guide Only Page 1-2"
  },
  {
    "objectID": "docs/SOCI-415/index_SOCI415.html#learning-objectives",
    "href": "docs/SOCI-415/index_SOCI415.html#learning-objectives",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "By the end of this lesson, students will:\n\nBe familiar with Network Analysis Terminology such as: nodes, edges, degree, density, centrality and clustering and will have seen them used in a real research context.\nHave some insight into how to create networks and utilize them to answer research questions\nExplore real life examples with the KINMATRIX and CBDB Dataset for family analysis\nHave multiple discussions about the data, network analysis and limitations of the techniques used."
  },
  {
    "objectID": "docs/SOCI-415/index_SOCI415.html#materials-and-technical-requirements",
    "href": "docs/SOCI-415/index_SOCI415.html#materials-and-technical-requirements",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "Access to a Google drive with a .zip file (can be a link on Canvas)\nDevice with internet access (laptop preferred)\nNo previous coding experience required (familiarity with Python is an asset)\nNo previous network analysis or high-level math experience required"
  },
  {
    "objectID": "docs/SOCI-415/index_SOCI415.html#pre-lesson-requirements",
    "href": "docs/SOCI-415/index_SOCI415.html#pre-lesson-requirements",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "Instructors should: Test if the .html files from the .zip file render correctly. Once it is tested they render correctly the instructor should read over and be familiar with the context to answer the student’s questions.\nStudents should: Complete the pre-readings for each lecture and come with a laptop to class."
  },
  {
    "objectID": "docs/SOCI-415/index_SOCI415.html#brief-lesson-structure",
    "href": "docs/SOCI-415/index_SOCI415.html#brief-lesson-structure",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "Introductory Notebook:\n\nIntro for what is network analysis and key terminology\nHow to create simple and random graphs in Python with NetworkX\nDegree, Density and Weights\nAdjacency Matrices\nMeasures of Centrality (Network Distance and Eccentricity, Degree Centrality, Closeness Centrality, Betweenness Centrality)\n\nKINMATRIX Notebook:\n\nIntro to the KINMATRIX Dataset and broad patterns across countries\nDensity by Age and Gender Across Countries\nColored Family Networks and Visualization of Gender\nHealth and Education Across Countries\nFamily size vs Political Allegiance in USA and Poland\nEmployment and Education in Poland\nParental Separation\nGender and Sexuality Across Countries and Regions\n\nThe China Biographical Database Notebook:\n\nIntro\nLouvain Algorithm and Community Clustering\nDegree Centrality and Important Family members (Who they were)\nDid women act more as bridges and who these key women were\nNetworks over time between dynasties"
  },
  {
    "objectID": "docs/SOCI-415/index_SOCI415.html#discussions",
    "href": "docs/SOCI-415/index_SOCI415.html#discussions",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "KINMATRIX Discussions:\n\nDiscussion on Density by Age and Gender Across the Countries\nDiscussion on Family Structure\nDiscussion on Family size and Political Allegiance\nDiscussion on Parental Separation Across the Countries\nHands on Section where students draw their own ego-networks of their families (Maybe website we will see)\n\nThe China Biographical Database Discussions:\n\nDiscussion on Louvain and Visualizations\nDiscussion on Centrality"
  },
  {
    "objectID": "docs/SOCI-415/index_SOCI415.html#modules",
    "href": "docs/SOCI-415/index_SOCI415.html#modules",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Network Analysis to Analyze The China Biographical Dataset\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing the KINMATRIX Dataset to explore network analysis for SOCI 415\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/SOCI-415/SOCI_415_Lesson_Plan.html",
    "href": "docs/SOCI-415/SOCI_415_Lesson_Plan.html",
    "title": "SOCI 415 Network Analysis - Lesson Plan",
    "section": "",
    "text": "Before First Lecture:\n\nPower in Networks: The Medici Only Section’s 1,2, and 7\nKINMATRIX Data Only Page 789 - 793: Intro and Methods\n\nBefore Second Lecture:\n\nThe China Biographical Database User Guide Only Page 1-2\n\n\n\n\nBy the end of this lesson, students will:\n\nBe familiar with Network Analysis Terminology such as: nodes, edges, degree, density, centrality and clustering and will have seen them used in a real research context.\nHave some insight into how to create networks and utilize them to answer research questions\nExplore real life examples with the KINMATRIX and CBDB Dataset for family analysis\nHave multiple discussions about the data, network analysis and limitations of the techniques used.\n\n\n\n\n\nAccess to a Google drive with a .zip file (can be a link on Canvas)\nDevice with internet access (laptop preferred)\nNo previous coding experience required (familiarity with Python is an asset)\nNo previous network analysis or high-level math experience required\n\n\n\n\n\nInstructors should: Test if the .html files from the .zip file render correctly. Once it is tested they render correctly the instructor should read over and be familiar with the context to answer the student’s questions.\nStudents should: Complete the pre-readings for each lecture and come with a laptop to class.\n\n\n\n\nIntroductory Notebook:\n\nIntro for what is network analysis and key terminology\nHow to create simple and random graphs in Python with NetworkX\nDegree, Density and Weights\nAdjacency Matrices\nMeasures of Centrality (Network Distance and Eccentricity, Degree Centrality, Closeness Centrality, Betweenness Centrality)\n\nKINMATRIX Notebook:\n\nIntro to the KINMATRIX Dataset and broad patterns across countries\nDensity by Age and Gender Across Countries\nColored Family Networks and Visualization of Gender\nHealth and Education Across Countries\nFamily size vs Political Allegiance in USA and Poland\nEmployment and Education in Poland\nParental Separation\nGender and Sexuality Across Countries and Regions\n\nThe China Biographical Database Notebook:\n\nIntro\nLouvain Algorithm and Community Clustering\nDegree Centrality and Important Family members (Who they were)\nDid women act more as bridges and who these key women were\nNetworks over time between dynasties\n\n\n\n\nKINMATRIX Discussions:\n\nDiscussion on Density by Age and Gender Across the Countries\nDiscussion on Family Structure\nDiscussion on Family size and Political Allegiance\nDiscussion on Parental Separation Across the Countries\nHands on Section where students draw their own ego-networks of their families (Maybe website we will see)\n\nThe China Biographical Database Discussions:\n\nDiscussion on Louvain and Visualizations\nDiscussion on Centrality"
  },
  {
    "objectID": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#pre-readings",
    "href": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#pre-readings",
    "title": "SOCI 415 Network Analysis - Lesson Plan",
    "section": "",
    "text": "Before First Lecture:\n\nPower in Networks: The Medici Only Section’s 1,2, and 7\nKINMATRIX Data Only Page 789 - 793: Intro and Methods\n\nBefore Second Lecture:\n\nThe China Biographical Database User Guide Only Page 1-2"
  },
  {
    "objectID": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#learning-objectives",
    "href": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#learning-objectives",
    "title": "SOCI 415 Network Analysis - Lesson Plan",
    "section": "",
    "text": "By the end of this lesson, students will:\n\nBe familiar with Network Analysis Terminology such as: nodes, edges, degree, density, centrality and clustering and will have seen them used in a real research context.\nHave some insight into how to create networks and utilize them to answer research questions\nExplore real life examples with the KINMATRIX and CBDB Dataset for family analysis\nHave multiple discussions about the data, network analysis and limitations of the techniques used."
  },
  {
    "objectID": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#materials-and-technical-requirements",
    "href": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#materials-and-technical-requirements",
    "title": "SOCI 415 Network Analysis - Lesson Plan",
    "section": "",
    "text": "Access to a Google drive with a .zip file (can be a link on Canvas)\nDevice with internet access (laptop preferred)\nNo previous coding experience required (familiarity with Python is an asset)\nNo previous network analysis or high-level math experience required"
  },
  {
    "objectID": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#pre-lesson-requirements",
    "href": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#pre-lesson-requirements",
    "title": "SOCI 415 Network Analysis - Lesson Plan",
    "section": "",
    "text": "Instructors should: Test if the .html files from the .zip file render correctly. Once it is tested they render correctly the instructor should read over and be familiar with the context to answer the student’s questions.\nStudents should: Complete the pre-readings for each lecture and come with a laptop to class."
  },
  {
    "objectID": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#brief-lesson-structure",
    "href": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#brief-lesson-structure",
    "title": "SOCI 415 Network Analysis - Lesson Plan",
    "section": "",
    "text": "Introductory Notebook:\n\nIntro for what is network analysis and key terminology\nHow to create simple and random graphs in Python with NetworkX\nDegree, Density and Weights\nAdjacency Matrices\nMeasures of Centrality (Network Distance and Eccentricity, Degree Centrality, Closeness Centrality, Betweenness Centrality)\n\nKINMATRIX Notebook:\n\nIntro to the KINMATRIX Dataset and broad patterns across countries\nDensity by Age and Gender Across Countries\nColored Family Networks and Visualization of Gender\nHealth and Education Across Countries\nFamily size vs Political Allegiance in USA and Poland\nEmployment and Education in Poland\nParental Separation\nGender and Sexuality Across Countries and Regions\n\nThe China Biographical Database Notebook:\n\nIntro\nLouvain Algorithm and Community Clustering\nDegree Centrality and Important Family members (Who they were)\nDid women act more as bridges and who these key women were\nNetworks over time between dynasties"
  },
  {
    "objectID": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#discussions",
    "href": "docs/SOCI-415/SOCI_415_Lesson_Plan.html#discussions",
    "title": "SOCI 415 Network Analysis - Lesson Plan",
    "section": "",
    "text": "KINMATRIX Discussions:\n\nDiscussion on Density by Age and Gender Across the Countries\nDiscussion on Family Structure\nDiscussion on Family size and Political Allegiance\nDiscussion on Parental Separation Across the Countries\nHands on Section where students draw their own ego-networks of their families (Maybe website we will see)\n\nThe China Biographical Database Discussions:\n\nDiscussion on Louvain and Visualizations\nDiscussion on Centrality"
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html",
    "href": "docs/SOCI-280/soci_280_bert.html",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "",
    "text": "Disinformation is defined as being deliberately false information, created with the intention to mislead it’s reader. Disinformation has been weaponized since the early middle ages: for example, in the 19th century, New Ywork-based newspaper The Sun published a series of articles, about the discovery of life on the Moon, with the purpose of increasing sales of The Sun. The papers claimed that, using a massive telescope, an english astronomer had discovered vegetation, bipedal beavers, and human-like aliens, dubbed “man-bats”, that were four feet tall, had wings, and could fly (Zielinski, 2015). Whether-or-not the great Moon Hoax lead to The Sun becoming a successfull paper remains uncertain; some accounts claim that the series of papers brought The Sun to international fame, however it’s likely that rumors of The Sun’s hoax increasing the paper’s circulation were exaggerated.\n\n\n&lt;img \n  src=\"media/disinformation_example_1.png\" \n  alt=\"An example of disinformation spread on twitter urnign voters to vote via twitter\" \n  style=\"width: 100%; height: auto; display: block;\"&gt;\n\n\n&lt;img \n  src=\"media/muller_report_1.png\" \n  alt=\"Middle example\" \n  style=\"width: 100%; height: auto; display: block;\"&gt;\n\n\nThis is an example of a (mostly) harmless disinformation campaign; However, disinformation campaigns can, and have, been used instead to sway public opinion on critical matters. For example, during the Cold War, the KGB orchestrated a widespread disinformation campaign, alledging that HIV/AIDs was a bioeapon engineedred by the United States, in an effort to stoke global mistrust of public health authorities and foster anti-americanism (Selvage & Nehring, 2019). A particularly relevant example is political elections: State-sponsored Russian actors have mounted disinformation campaigns in ever single US federal election since 2016, at the latest. In 2016, for instance, the Russian state-sponsored Internet Research Agency (IRA) ran hundreds of facebook and Twitter groups that amplified divisive content, and organized astroturf rallies in key US states, most notably Pennsylvania (Menn and Dave, 2018). The extent to which these coordinated campains influenced the 2016 United-States election remains unclear: initial findings by the DOJ suggested that Russia coordinated a sweeping, large scale multi-million dollar online campaign aimed to praise Donald Trump and disparage Hillary Clinton (Muller, 2019). However, multiple studies have found that even under generous assumptions about presuasion rates, the vote-shifts caused by the IRA’s disinformation campaigns were too small to sway the election’s outcome (Allcot & Gentzkow, 2017, Eady et al., 2023).\nWith the rise of digital platforms and generative AI, the scale, speed, and sophistication of disinformation have grown exponentially. From elections and pandemics to social justice movements and international conflicts, false or misleading content is being spread online to manipulate emotions and polarize public opinion. The challenge today is not just the volume of disinformation, but how convincing and targeted it has become. Former U.S. Director of National Intelligence Avril Haines describes how state-sponsored campaigns, like Russia’s Kremlin, now operate using “a vast multimedia influence apparatus,” including bots, cyber-actors, fake news websites, and social media trolls. Large language models (LLMs) can now generate human-like tweets, comments, and articles at scale. Combined with deepfakes, doppelgänger sites, and AI-generated personas, these tools allow bad actors to craft propaganda that appears authentic, emotionally resonant, and difficult to detect.\nIn this notebook, we’ll use machine learning — specifically, pretrained large language models — to study the language of disinformation in a real dataset of English and Russian-language tweets. These tweets include both propagandist and non-propagandist content."
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#disinformation-in-the-information-age",
    "href": "docs/SOCI-280/soci_280_bert.html#disinformation-in-the-information-age",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "",
    "text": "Disinformation is defined as being deliberately false information, created with the intention to mislead it’s reader. Disinformation has been weaponized since the early middle ages: for example, in the 19th century, New Ywork-based newspaper The Sun published a series of articles, about the discovery of life on the Moon, with the purpose of increasing sales of The Sun. The papers claimed that, using a massive telescope, an english astronomer had discovered vegetation, bipedal beavers, and human-like aliens, dubbed “man-bats”, that were four feet tall, had wings, and could fly (Zielinski, 2015). Whether-or-not the great Moon Hoax lead to The Sun becoming a successfull paper remains uncertain; some accounts claim that the series of papers brought The Sun to international fame, however it’s likely that rumors of The Sun’s hoax increasing the paper’s circulation were exaggerated.\n\n\n&lt;img \n  src=\"media/disinformation_example_1.png\" \n  alt=\"An example of disinformation spread on twitter urnign voters to vote via twitter\" \n  style=\"width: 100%; height: auto; display: block;\"&gt;\n\n\n&lt;img \n  src=\"media/muller_report_1.png\" \n  alt=\"Middle example\" \n  style=\"width: 100%; height: auto; display: block;\"&gt;\n\n\nThis is an example of a (mostly) harmless disinformation campaign; However, disinformation campaigns can, and have, been used instead to sway public opinion on critical matters. For example, during the Cold War, the KGB orchestrated a widespread disinformation campaign, alledging that HIV/AIDs was a bioeapon engineedred by the United States, in an effort to stoke global mistrust of public health authorities and foster anti-americanism (Selvage & Nehring, 2019). A particularly relevant example is political elections: State-sponsored Russian actors have mounted disinformation campaigns in ever single US federal election since 2016, at the latest. In 2016, for instance, the Russian state-sponsored Internet Research Agency (IRA) ran hundreds of facebook and Twitter groups that amplified divisive content, and organized astroturf rallies in key US states, most notably Pennsylvania (Menn and Dave, 2018). The extent to which these coordinated campains influenced the 2016 United-States election remains unclear: initial findings by the DOJ suggested that Russia coordinated a sweeping, large scale multi-million dollar online campaign aimed to praise Donald Trump and disparage Hillary Clinton (Muller, 2019). However, multiple studies have found that even under generous assumptions about presuasion rates, the vote-shifts caused by the IRA’s disinformation campaigns were too small to sway the election’s outcome (Allcot & Gentzkow, 2017, Eady et al., 2023).\nWith the rise of digital platforms and generative AI, the scale, speed, and sophistication of disinformation have grown exponentially. From elections and pandemics to social justice movements and international conflicts, false or misleading content is being spread online to manipulate emotions and polarize public opinion. The challenge today is not just the volume of disinformation, but how convincing and targeted it has become. Former U.S. Director of National Intelligence Avril Haines describes how state-sponsored campaigns, like Russia’s Kremlin, now operate using “a vast multimedia influence apparatus,” including bots, cyber-actors, fake news websites, and social media trolls. Large language models (LLMs) can now generate human-like tweets, comments, and articles at scale. Combined with deepfakes, doppelgänger sites, and AI-generated personas, these tools allow bad actors to craft propaganda that appears authentic, emotionally resonant, and difficult to detect.\nIn this notebook, we’ll use machine learning — specifically, pretrained large language models — to study the language of disinformation in a real dataset of English and Russian-language tweets. These tweets include both propagandist and non-propagandist content."
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#learning-outcomes",
    "href": "docs/SOCI-280/soci_280_bert.html#learning-outcomes",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this module, you will be able to better understand disinformation and propaganda techniques using the following computational tools:\n\nSentiment analysis to detect emotional tone (positive, negative, neutral)\nToxicity analysis to identify harmful or aggressive language (e.g., insults, threats)\nStatistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian)\n\nYou’ll learn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than normal political tweets?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\nThrough this analysis, we’ll explore various dimmensions of AI applications, critically examining how it can be used to better understand and detect the patterns of disinformation when working with large amounts of social data."
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#key-terms-and-concepts",
    "href": "docs/SOCI-280/soci_280_bert.html#key-terms-and-concepts",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Key Terms and Concepts",
    "text": "Key Terms and Concepts\nDisinformation: Disinformation is generally understood as the intentional spreading of false, misleading, or incorrect information, in order to influence the public or create an outcome, usually political.\nPropaganda: Propaganda is similar to disinformation in it’s intent to spread a cause or doctrine, but can differ in how systematically or overtly it is speread. The two concepts are both forms of misinformation, with propaganda generally being employed to promote a cause.\nLarge Language Model (LLM): A Large Language Model is a langauge model trained on a very large set of text data, accessing the features of the text by converting units of text into quanitative representations that can be used for various tasks, such as chatbotting, or in the case of this notebook, Natural Language Processing.\nNatural Language Processing (NLP): Natural Langauge Processing encompasses a wide variety of techniques and practices wtih the goal of enabling computers to understand, interpret, and produce human language. The key NLP techniques in this notebook are sentiment analysis, a technique that analayzes the relative positivity or negativity of langauge, and toxitcity analyis which analyzes the relative aggressiveness of language.\nBERT: to enable these analyses we will be using two BERT models. BERT is an open-source framework for machine learning, whcih is used for NLP. BERT is well-suited to understanding langauge, rather than generating it, which is what this notebook is concerned with. The specific BERT models we are using are a multi-lingual model, which can analyze tweets in different languages, and a model trained for analyzing tweet sentiments.\nFine Tuning: The multi-lingual model is fine-tuned, or trained specifically on the Russian Disinformation tweet dataset. This is done by inputting a training subset of the data, where the tweets are labeled as Disinformation, into the BERT model to create a model familiar with our data, and well-suited to producing analyses."
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#dataset",
    "href": "docs/SOCI-280/soci_280_bert.html#dataset",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Dataset",
    "text": "Dataset\nOur data for this notebook comes from four sources:\n\n1) The analysis data:\nThe data used from our analysis is a combination of four datasets: 1) The Russian troll tweets dataset, AKA the Internet Research Agency (IRA) tweets dataset is a large, verified collection of social media posts created by the Internet Research agency, the Russian state-sponsored troll farm we talked about earlier. The data was collected and provided by FiveThirtyEight (RIP) an America political news website that focused on providing detailed statistical analyses of the political climate in the United States. Compared to other russian troll datasets, IRA dataset is unparalleled in terms of accuracy as because the data labels were directly provided by Twitter’s internal security teams, which identified the IRA troll accounts and turned over to the United States Congress as evidence for the Senate Select Committee on Intelligence’s investigation into foreign election interference. Hence, every single tweet in the dataset is a known, verified russian disinformation account. 2) The sentiment140 dataset is a massive english language datasets of tweets, created by Standford with the purpose of training sentiment analysis detection models. This dataset serves as our english control group. 3) Likewise, theRusentimental dataset is a dataset of modern-day Russian tweets, collected by Russian NLP researchers for sentiment analysis purposes. This serves as our Russian control group. 4) Lastly, we added tweets taken from a collection of known russian disinformation accounts that were active during the start of the Russia-Ukraine War. Researchers selected these tweets based on the accounts’ “association with state-backed media and a history of spreading disinformation and misinformation”.\n\n\n2) The model training data:\nFor our analysis, we trained a tweet propaganda detection model off of the paper “Labeled Datasets for Research on Information Operations”, which provides the same verified collection of disinformation accounts from the IRA, as well as a “control” dataset of legitimate, topically-related accounts, which allowed for a direct, comparative analysis between malicious and organic online posts, within the same conversational political context. To avoid including training data in our in-class analysis, the IRA dataset was randomly split in two, with one half being used for model training and the other half being used for the in-class demo."
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#hashtag-analysis",
    "href": "docs/SOCI-280/soci_280_bert.html#hashtag-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "0.1 Hashtag analysis",
    "text": "0.1 Hashtag analysis\nWhat hashtags are being used in tweets from propagandist vs. non-propagandist accounts? For the next part of our data analysis, we’ll find the freqency of the 10 most common hashtags used by the two account types. As you explore the tables below, notice the content and language of the hashtags.\nOur earlier language analysis suggests that most disinformation in this dataset is likely in English. By examining the hashtags used in these tweets, we can better understand the specific words, topics, and narratives that disinformation accounts are trying to amplify.\nTo begin, we’ll extract hashtags from our textual data by finding phrases beginning with the character #, and ignoring hashtags shorter than 10 letters for the sake of making things interesting.\n\n\nCode\ntweets['content'] = tweets['content'].astype(str) # because some of the tweets aren't strings, apparently\ndef extract_hashtags(text):\n    return re.findall(r'#(\\w+)', text.lower())\n\n\ntweets['hashtags'] = tweets['content'].apply(extract_hashtags)\ntweets['hashtags'] = tweets['hashtags'].apply(lambda hashtag_list: [tag for tag in hashtag_list if len(tag) &gt; 10])\n\nprint(tweets[['is_propaganda', 'content', 'hashtags']].head())\n\n\nNow we can visualize the frequency of the 10 most common hashtags longer than 10 characters used by propagandist accounts over time.\n\n🔎 Engage Critically\n❓ Key Questions\n\nWith reference to the timeline of tweets, and the hashtags below, describe some of the main targets of Russian disinformation.\nGiven what you know about disinformation, what are the intentions of these accounts, and what outcomes are they attempting to create?\nWhat do the hashtags not tell us about the disinformation accounts? Where might our ability to conclude the intentions and outcomes of these accounts be limited by the data we have examined?\nWhat additional data could we collect to better understand this type of disinformation.\n\n\n\n\nCode\npropaganda_tweets = tweets[tweets['is_propaganda'] == 1]\npropaganda_hashtags = propaganda_tweets.explode('hashtags')\ntop_20_propaganda = propaganda_hashtags['hashtags'].value_counts().head(10)\n\ntop_prop_list = top_20_propaganda.index.tolist()\ntop_prop_df = propaganda_hashtags[propaganda_hashtags['hashtags'].isin(top_prop_list)]\ntop_prop_df['date'] = pd.to_datetime(top_prop_df['date'], errors='coerce')\ntop_prop_df.dropna(subset=['date'], inplace=True)\n\nprop_weekly = top_prop_df.groupby('hashtags').resample('W', on='date').size().reset_index(name='count')\n\nfig, ax = plt.subplots(1, 1, figsize=(18, 10))\nfig.suptitle('Weekly spread of Top 10 propaganda hashtags over time', fontsize=20)\n\nsns.lineplot(ax=ax, data=prop_weekly, x='date', y='count', hue='hashtags', palette='viridis')\nax.set_title('Propaganda Hashtags (IRA only)', fontsize=16)\nax.set_xlabel('Date (by week)')\nax.set_ylabel('Weekly Count')\nax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\nax.grid(True, which='both', linestyle='--', linewidth=0.5)\n\nplt.tight_layout(rect=[0, 0, 0.9, 0.96])\nplt.show()\n\n\nTo explore how these hashtags trend over time, you can use the interactive tool below. Enter one or more hashtags (separated by commas) to visualize their weekly frequency in the dataset. This can help reveal how certain narratives gain momentum or fade in relevance.\n\n\nCode\nall_hashtags_df = tweets.explode('hashtags')\nall_hashtags_df['date'] = pd.to_datetime(all_hashtags_df['date'], errors='coerce')\nall_hashtags_df.dropna(subset=['date', 'hashtags'], inplace=True)\nall_hashtags_df['hashtags'] = all_hashtags_df['hashtags'].str.lower()\n\n\n\n\nCode\nhashtag_input = widgets.Text(value='news,russia,syria',placeholder='Enter hashtags, separated by commas',description='Hashtags:',layout={'width': '50%'})\n\nplot_output = widgets.Output()\n\ndef update_plot(change):\n    hashtags_to_plot = [tag.strip().lower() for tag in change['new'].split(',') if tag.strip()]\n    with plot_output:\n        clear_output(wait=True)\n        if not hashtags_to_plot:\n            print(\"enter at least one hashtag\")\n            return\n        filtered_data = all_hashtags_df[all_hashtags_df['hashtags'].isin(hashtags_to_plot)]\n\n        if filtered_data.empty:\n            print(\"No data found for the specified hashtags\")\n            return\n        weekly_counts = filtered_data.groupby('hashtags').resample('W', on='date').size().reset_index(name='count')\n        fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n        sns.lineplot(data=weekly_counts, x='date', y='count', hue='hashtags', ax=ax)\n        \n        ax.set_title('Weekly Frequency of Selected Hashtags')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Weekly Mentions')\n        ax.grid(True, linestyle='--', linewidth=0.5)\n        plt.tight_layout()\n        plt.show()\n\nhashtag_input.observe(update_plot, names='value')\n\nprint(\"Enter a comma-separated list of hashtags to see their trends over time.\")\ndisplay(widgets.VBox([hashtag_input, plot_output]))\nupdate_plot({'new': hashtag_input.value})\n\n\n\n🛑 Stop and Reflect\nNow that we have gone through the dataset and examined a variety of its features, take a few minutes and disucss the following questions with a partner or small group.\n\nWhat are the conclusions we have come to regarding disinformation? How are they influenced or limited by the dataset?\nSo far we have only looked at statistical aspects of the data without using machine learning techniques. Make some predictions on how the machine learning techniques we will use next might change our understanding of online disinformation. How might machine learning, specifically NLP be used to enrich our understanding of disinformation?\nHas your understanding of online disinformation changed after looking at this data? Write down a few questions you have about online disinformation, the dataset, or the computational methods we have been using."
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#can-we-tune-models-to-detect-online-disinformation-campaigns-classifying-current-tweets-with-a-model-finetuned-on-the-russian_disinformation_tweets-dataset",
    "href": "docs/SOCI-280/soci_280_bert.html#can-we-tune-models-to-detect-online-disinformation-campaigns-classifying-current-tweets-with-a-model-finetuned-on-the-russian_disinformation_tweets-dataset",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "1. Can we tune models to detect online disinformation campaigns? Classifying current tweets with a model finetuned on the russian_disinformation_tweets dataset",
    "text": "1. Can we tune models to detect online disinformation campaigns? Classifying current tweets with a model finetuned on the russian_disinformation_tweets dataset\n\nExploring Our Model\nNow that we have examined our data and looked at some of the key features in the Russian Disinformation Dataset, we can start thinking about ways to use machine learning to answer questions, classify features, and make predictions about our dataset. To do any of these tasks we first require a way to interpret the text data and assign numeric qualities to our tweets.\nThe model we are using to do this is a multilingual model which maps sentences and paragraphs into multi-dimensional vector space. In other words, it takes the sentences and paragraphs of our tweets and assigns them a position associated with their meaning. This is done based on the context of the token (the unit of text, like a word or sentence). The model we are using is capable of interpreting multiple languages and is fine-tuned, or specifically trained, on the data we are examining. The code below is going to call upon a pre-built classifier which uses this fine-tuned model to predict whether a tweet is likely Russian propaganda. The two sample tweets are:\n\n“#qanon #trump Hunter Biden is a Ukranian Shill”\n“What great weather we have today”\n\nThe model is going to take these text inputs, represent them in vector space, and then report whether their respective values are similar to those of disinformation tweets.\n\n\nClassification and It’s Discontents\nBefore we explore the possibilities of our model to classify, we should first consider some of the main concerns and limitations regarding classification. Classification is an essential element of how machine learning operates. At its core it is the method of finding features that are central to a class and assigning units to that class based on those features. As you may already see, this “in-or-out” framework necessarily flattens some of the richness of human life, in order to effectively create these incredibly useful classes.\n\nExample:\nYou might say a cat and a dog are really easy to classify. Most people know what a dog looks like and that it looks different than a cat. But if all I tell you is that there are two animals that commonly live with humans, that have a tail and paws, and make noise you might have a hard time classifying them, because they share common features.\nIt is important to think deeply about how we are classifying, especially as many datasets are labeled by people, who carry their own understandings of what belongs to each class.\nAny class or classifier will be informed by the balance of abstraction to specificity, and we should always keep this in mind when we are classifying. It is important to be specific enough to ascertain the qualities we are interested in, but not so specific we end up with thousands of classes.\n\nNow that we have explored some of the trickiness of classification as a concept, we can look at how machine learning can help us work through some of these challenges. By using data that is labeled as disinformation our model can be trained to associated certain numerical feautres with disinformation, and when we give it text data that is similar to what it knows to be disinformation, it will classify it as such.\nFor this analysis, we trained a model on a dataset very similar to ours, with the purpose of detecting propaganda tweets. Recall, however, that this model was trained soley on tweets around and before the 2016 presidential election- meaning it has never seen any tweets posted after this point. Given this information, what kinds of tweets do you think the model will struggle with the most? What kinds of propaganda tweets will it excel at detecting? Try using the model below, and see if you can get it to label a string of text as disinformation.\n\n\nCode\nclassifier = pipeline(\"text-classification\",model=\"IreneBerezin/soci-280-model\")\n\nprint(classifier(\"crooked hillary is trying to rig the election! #MAGA!\")) #put your text in here\n\n\nLet’s now apply the model to a random sample of the dataset we’ve been studying. How well do you think the model will perform?\n\n\nCode\ntweets_sample = tweets.sample(n=2000, random_state=60)\ntweets_inference = tweets_sample[[\"content\", \"is_propaganda\"]].dropna()\ntweets_inference['is_propaganda'] = 1 - tweets_inference['is_propaganda'] # for whatever reasons the labels are switched in the training data\n\nclassifier = pipeline(\"text-classification\",model=\"IreneBerezin/soci-280-model\",device=-1)\npredictions = classifier(tweets_inference['content'].tolist(),batch_size=32,truncation=True)\npredicted_labels = [int(p['label'].split('_')[1]) for p in predictions]\ntrue_labels = tweets_inference['is_propaganda'].tolist()\n\n\n\n\nCode\ndisplay_labels = ['Disinformation', 'Normal Tweet']\n\ncm = confusion_matrix(y_true=true_labels,y_pred=predicted_labels,labels=[0, 1])\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.set_title(\"Confusion Matrix\", fontsize=16)\ndisp.plot(ax=ax, cmap='Greys') \nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nPretty terribly, apparently! Above is what’s called a confusion matrix: it shows the number of labels that were correctly predicted, and incorrectly predicted. We see that, out of a random sample of 2000 tweets:\n\n797 tweets were marked as disinformation, and were, in fact, disinformation\n504 tweets were labelled as normal tweets while they were, in fact, disinformation\n282 tweets were labelled disinformation when they were in fact normal\n417 tweets were correctly labelled as normal tweets\n\nSo, better than blindly guessing (which would put us on average as 500 tweets in each category) but still very bad.\nThis is an example of overfitting: instead of actually learning what constitutes a propagandist tweet, the model simply memorized the specific writing styles of the troll accounts present in the training set. Hence, when it was shown new tweets from different troll accounts in the test set, it failed, as it never learned the general patterns that define an account as part of a coordinated disinformation campaign. The model did the ML equivalent of memorizing in great detail all the solutions to questions in a math textbook instead of actually learning how to solve them."
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#what-is-sentiment-analysis",
    "href": "docs/SOCI-280/soci_280_bert.html#what-is-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "1. What is Sentiment Analysis?",
    "text": "1. What is Sentiment Analysis?\n\n“Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text” (Hussein, 2018).\n\nAs this definition alludes, sentiment analysis is a part of natural language processing (NLP), a field at the intersection of human language and computation. Because humans are complex, emotional beings, the language we use is often shaped by our affective (emotional) dispositions. Sentiment analysis, sometimes referred to as “opinion mining”, is one way researchers can methodologically understand the emotional intentions, typically positive, negative, or neutral sentiments, that lie in textual datasets.\n\n🔎 Engage Critically\nAt the heart of sentiment analysis is the assumption that language reveals interior, affective states, and that these states can be codified and generalized to broader populations. AI scholar Kate Crawford, in her book Atlas of AI, explores how many assumptions found in contemporary sentiment research (i.e., that there are 7 universal emotions) are largely unsubstantiated notions that emerged from mid-20th century research funded by the US Department of Defense. Rather than maintaining that emotions can be universally categorized, her work invites researchers to think about how emotional expression is highly contextualized by social and cultural factors and the distinct subject positions of content makers.\n\n❓ Consider the research question for your sentiment analysis. How might the text you are working with be shaped by the distinct groups that have generated it?\n\n\n❓ Are there steps you can take to educate yourself around the unique language uses of your dataset (for example, directly speaking with someone from that group or learning from a qualified expert on the subject)?\n\nIf you’re interested, you can learn more about data justice in community research in a guide created by UBC’s Office for Regional and International Community Engagement.\n\nThe rise of web 2.0 has produced prolific volumes of user-generated content (UGC) on the internet, particularly as people engage in a variety of social platforms and forums to share opinions, ideas and express themselves. Maybe you are interested in understanding how people feel about a particular political candidate by examining tweets around election time, or you wonder what people think about a particular bus route on reddit. UGC is often unstructured data, meaning that it isn’t organized in a recognizable way.\nStructured data for opinions about a political candidate might look like this:\n\n\n\n\n\n\n\n\nPro\nCon\nNeutral\n\n\n\n\nSupports climate action policies\nNo plan for lowering the cost of living\nUBC Graduate\n\n\nExpand mental health services\n\n\n\n\n\nWhile unstructured data might look like this:\n\nlove that she’s trying to increase mental health services + actually cares abt the climate 👏 but what’s up w rent n grocieries?? i dont wanna go broke out here 😭 a ubc alum too like i thought she’d understand\n\nIn the structured data example above, the reviewer defines which parts of the feedback are positive, negative or neutral. In the unstructured example on the other hand, there are many typos and a given sentence might include a positive and a negative review as well as more nuanced contextual information (i.e. mentioning being a UBC alum when discussing cost of living). While messy, this contextual information often carries valuable insights that can be very useful for researchers.\nThe task of sentiment analysis is to make sense of these kinds of nuanced textual data - often for the purpose of understanding people, predicting human behaviour, or even in some cases, manipulating human behaviour.\nDisinformation campaigns often aim to sway public opinion by influencing the emotional tone of online conversations. Sentiment analysis allows us to detect and understand these patterns by identifying whether large volumes of text express positive, negative, or neutral sentiment.\nOur model is pretrained, meaning it has already learnt from millions of labelled examples how to distinguish different sentiments. Specifically, because the model we’ll be using was trained on English tweets, it’s tuned to the language and syntax common on Twitter/X, and is limited to analyzing English-language text.\nLanguage is complex and always changing.\nIn the English language, for example, the word “present” has multiple meanings which could have positive, negative or neutral connotations. Further, a contemporary sentiment lexicon might code the word “miss” as being associated with negative or sad emotional experiences such as longing; if such a lexicon were applied to a 19th century novel which uses the word “miss” to describe single women, then, it might incorrectly associate negative sentiment where it shouldn’t be. While sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\nNow, we’re ready to get back to our analysis. Below, we’ll load in our model and tokenizer and start playing around with identifying the sentiment of different phrases.\n\n\nCode\nsentiment = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n\nprint(sentiment(\"I hate everyone and everything\"))\nprint(sentiment(\"Life is great!\"))\nprint(sentiment(\"Hello world\"))\n\n\nLet’s breakdown this output. There are two parts to what the model returns:\n\nLabel → a classification labelling the text as either having positive, negative, or neutral sentiment\nScore → the model’s confidence in it’s classification\n\n\n🔎 Engage Critically\nTry using the interactive tool below to explore how a machine learning model detects sentiment in short texts like tweets. The model classifies each input as positive, neutral, or negative, and assigns a probability score to each label. Type a sentence (like a tweet or short message) into the box below and click “Analyze” to see how the model interprets its emotional tone.\n\n\n\nCode\ntext_input = widgets.Text(\n    value=\"hello world!\",\n    placeholder=\"Type a sentence here\",\n    description=\"Input:\",\n    layout=widgets.Layout(width=\"70%\")\n)\nanalyze_btn = widgets.Button(description=\"Analyze\", button_style=\"primary\")\noutput_area = widgets.Output()\n\ndef on_analyze_clicked(b):\n    with output_area:\n        clear_output(wait=True)\n        scores = sentiment(text_input.value)\n        labels = [item[\"label\"] for item in scores]\n        probs  = [item[\"score\"] for item in scores]\n        fig, ax = plt.subplots(figsize=(6,4))\n        bars = ax.bar(labels, probs)\n        ax.set_ylim(0, 1)\n        ax.set_ylabel(\"Probability\")\n        ax.set_title(\"Sentiment Probability Distribution\")\n        for bar, prob in zip(bars, probs):\n            height = bar.get_height()\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,  \n                height + 0.02,                    \n                f\"{prob:.2f}\",                   \n                ha=\"center\", va=\"bottom\",\n                color=\"red\", fontsize=12)\n        plt.show()\n\nanalyze_btn.on_click(on_analyze_clicked)\n\ndisplay(widgets.VBox([text_input, analyze_btn, output_area]))\n\n\n\nBatch Sentiment Analysis\nNow, let’s start running sentiment analysis on our dataset. The general steps to run our analysis include:\n\nLoading a pretrained model and tokenizer\nWe load a RoBERTa model that has been fine-tuned for sentiment analysis on tweets, along with its corresponding tokenizer.\nCreating sentiment analysis pipeline\nWe set up a Hugging Face pipeline that handles finer steps in our sentiment analysis, such as tokenization (breaking up text into smaller units, called tokens), batching (processing multiple texts at once for efficiency), and prediction (predicting the overall sentiment).\nRunning batch sentiment analysis on the dataset\nTo efficiently analyze large numbers of tweets, we split the dataset into batches of 1,000 tweets and process them one batch at a time. To store the predictions, we extract the predicted sentiment labels and save them in a column named sentiment.\nPreviewing the results\n\n\n\nCode\ntweets_small = tweets.groupby('source').sample(n=1000, random_state=42)\ntweets_small = tweets_small.sample(frac=1, random_state=42).reset_index(drop=True) #you might want to change this value\ntweets_small[\"sentiment\"] = \"\" \n\n\n\n\nCode\nbatch_size = 1000\nn = 0\nfor start in range(0, len(tweets_small), batch_size):\n    end = start + batch_size\n    batch_texts = tweets_small[\"content\"].iloc[start:end].tolist()\n    results = sentiment(batch_texts)           \n    labels  = [res[\"label\"] for res in results]\n    tweets_small.loc[start:end-1, \"sentiment\"] = labels \n    n = n+1\n    print(f'batch {n} done')\n\n\n\n\nCode\nprint(tweets_small[[\"content\", \"sentiment\"]].head())\n\n\nWe can see the first 5 tweets and their predicted sentiment above.\nNow that we know how to run sentiment analysis to identify the overarching sentiment of a tweet, we are now in good position to ask and investigate whether emotionally charged language is more common in propaganda. Let’s explore this by forming a hypothesis and testing it statistically.\n\n🔎 Engage Critically\nHypothesis: Propagandist tweets (is_propaganda == 1) are more emotionally charged — that is, they are more likely to be classified as Positive or Negative (non-neutral) compared to non-propagandist tweets (is_propaganda == 0).\nWe will test whether the difference in sentiment category frequencies between the two groups is statistically significant.\n\nFirst, let’s examine the sentiment distribution for each group:\n\n\nCode\ndist = pd.crosstab(tweets_small['sentiment'], tweets_small['is_propaganda'], normalize='columns')\ndist.columns = ['Non-propagandist', 'Propagandist']\nprint(dist * 100)\n\n\nReading the table, we can see that the majority of non-propagandist tweets are either negative (~43%) or neutral (~44%), while the majority of propagandist tweets (~63%) express neutral sentiment.\n\n\nCode\nprint(tweets_small['sentiment'])\n\n\n\n\nCode\n# We define 'charged' sentiment as Positive or Negative\ntweets_small['charged'] = tweets_small['sentiment'].isin(['positive', 'negative']).astype(int)\n\n# Constructing a  contingency table: rows = propagandist/non propagandist group, columnss = charged vs neutral\ncontingency = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged'])\nprint(\"Contingency table:\\n\", contingency)\n\n# Chi-squared test\nchi2, p, dof, expected = chi2_contingency(contingency)\nprint(f\"p-value = {p:.3e}\")\n\n\n\n🔎 Engage Critically\nTry interpreting the output. What are our results telling us? Based on the p-value, what can we conclude about our hypothesis?"
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#multilingual-sentiment-analysis",
    "href": "docs/SOCI-280/soci_280_bert.html#multilingual-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "3. Multilingual Sentiment Analysis",
    "text": "3. Multilingual Sentiment Analysis\nOur dataset of tweets isn’t entirely in English — many of the tweets are written in Russian. Could this be skewing our results? How is our model actually handling Russian-language tweets compared to English ones?\n\n🔎 Engage Critically\nRecall our discussion on sentiment lexicons in Section 1:\n\nWhile sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\n\n❓ How might the use of a monolingual sentiment model introduce bias into our results? Are non-English tweets being misclassified as neutral, negative, or positive when they shouldn’t be?\n\nWith this in mind, let’s explore below. We’ll use the Unicode values of Cyrillic characters to identify Russian-language tweets, and run sentiment analysis seperately on Russian and and English tweets.\n\n\nCode\ntweets_lang = tweets_small.copy()\ntweets_small['language'] = tweets_small['language'].str.lower().str[:2]\n\nen_tweets = tweets_lang[tweets_lang['language'] == 'en']\nru_tweets  = tweets_lang[tweets_lang['language'] == 'ru']\neng_dist = pd.crosstab(en_tweets['sentiment'], en_tweets['is_propaganda'], normalize='columns') * 100\nru_dist  = pd.crosstab(ru_tweets['sentiment'],  ru_tweets['is_propaganda'],  normalize='columns') * 100\n\n# Renaming columns for clarity\ncol_map = {0: 'Non-propagandist', 1: 'Propagandist'}\n\neng_dist = eng_dist.rename(columns=lambda c: col_map.get(c, str(c)))\nru_dist  = ru_dist.rename(columns=lambda c: col_map.get(c, str(c)))\n\nprint(\"English Tweets Sentiment (%):\\n\", eng_dist, \"\\n\")\nprint(\"Russian Tweets Sentiment (%):\\n\", ru_dist)\n\n\n\n🔎 Engage Critically\nTake a moment to interpret the results before continuing. What do they tell us about the performance of our model on Russian-language tweets? Why do you think that is?\n\nFrom the table above, we can see that our model is performing very poorly on Russian-language tweets, as nearly all of the Russian tweets are being marked as neutral regardless of if they are propagandist or not. This means that the pretrained model we were using before is not an appropriate choice based on the characteristics of our data, namely that a significant portion of the tweets are written in Russian, a language the model was not trained to make reliable predictions on.\nLet’s try re-running our sentiment analysis using a different model. This time, we’ll use a model trained on 198 million tweets that were not filtered by language. As a result, the training data reflects the most commonly used languages on the platform at the time of collection, with Russian conveniently ranking as the 11th most frequent.\nWe’ll follow the same steps for batch sentiment analysis that we did in Section 2:\n\n\nCode\nsentiment_multi = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\")\n\nbatch_size = 1000\nfor start in range(0, len(tweets_small), batch_size):\n    end = start + batch_size\n    batch_texts = tweets_small[\"content\"].iloc[start:end].tolist()\n    results = sentiment_multi(batch_texts)           \n    labels  = [res[\"label\"] for res in results]\n    tweets_small.loc[start:end-1, \"sentiment\"] = labels \n\nprint(tweets_small[[\"content\", \"sentiment\"]].head())\n\n\nTo make the results easier to visualize, let’s create a table that shows the percentage distribution of sentiment labels (positive, neutral, negative) within propagandist and non-propagandist tweets.\n\n\nCode\nmulti_dist = pd.crosstab(tweets_small['sentiment'], tweets['is_propaganda'], normalize='columns') * 100\nmulti_dist.columns = ['Non-propagandist', 'Propagandist']\nprint(\"Multilingual Model Sentiment (%):\\n\", multi_dist)\n\n\nThe sentiment distribution between propagandist and non-propagandist tweets is quite similar when using the multilingual model. Both groups are predominantly neutral (around 50%), with roughly equal proportions of negative and positive sentiment.\nNow, let’s run a statistical test to see if there’s a meaningful difference in sentiment between propagandist and non-propagandist tweets. Specifically, we want to know:\n\nAre propagandist tweets more likely to be emotionally charged (positive or negative) than neutral, compared to non-propagandist tweets?\n\nTo answer this, we’ll use a chi-squared test, which helps us check whether the differences we see in the data are likely due to chance or if they’re statistically significant.\n\n\nCode\ntweets_small['charged_multi'] = tweets_small['sentiment'].isin(['positive','negative']).astype(int)\ncontingency_multi = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged_multi'])\nchi2_multi, p_multi, *_ = chi2_contingency(contingency_multi)\nprint(f\"Chi-squared p-value with multilingual model: {p_multi:.3e}\")\n\n\n\n🔎 Engage Critically\nTake a moment to interpret the results before continuing. What does our p-value tell us about our inital research question above?\n\nOur p-value (0.00003727) is much smaller than the common significance level of 0.05, indicating that the difference in how emotionally charged tweets are distributed between propagandist and non-propagandist groups is very unlikely to be due to random chance.\nThis means there is strong evidence that propagandist tweets are more likely to be emotionally charged compared to non-propagandist tweets, according to the multilingual model’s sentiment analysis."
  },
  {
    "objectID": "docs/SOCI-280/soci_280_bert.html#introduction-to-toxicity-analysis",
    "href": "docs/SOCI-280/soci_280_bert.html#introduction-to-toxicity-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "4. Introduction to Toxicity Analysis",
    "text": "4. Introduction to Toxicity Analysis\nWarning: this section contains examples of potentially offensive or profane text\nToxicity analysis is another type of classification task that uses machine learning to detect whether a piece of text contains toxic speech. Jigsaw, a Google subsidary and leader in technological solutions for threats to civil society, uses the following definition for “toxic speech” proposed by Dixon et al. (2018):\n\n”[R]ude, disrespectful, or unreasonable language that is likely to make someone leave a discussion”\n\n\n🔎 Engage Critically\nThis definition is widely considered by the NLP community to be ill-defined and vague. Why do you think? What issues could potentially arise from this definition, and how could they impact (for example) a comment flagging tool that gives warnings to social media users whose comments meet this definition of toxic speech?\n\nA core issue defined by Berezin, Farahbakhsh, and Crespi (2023) is that the definition “gives no quantitative measure of the toxicity and operates with highly subjective cultural terms”, yet still remains widely used by researchers and developers in the field. We’ll explore some of the ways this definition is influencing toxicity analysis briefly below.\n\n4.1 Positive profanity\n\n\n\nFuck dude, nurses are the shit (Mauboussin, 2022)\n\n\nConsider the Reddit post above. Is the comment an example of toxic speech? Probably not, right?\nNow imagine you are Perspective API, Google’s AI toxicity moderation tool, with your scope of “toxic speech” limited solely to the definition of ”rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion”. Because of your architecture, you are limited in the way you can understand a message in context. You process the comment and immediately detect two profanities that meet your requirement for being rude language, and assign it a subsequent toxicity score:\n\n\n\nFuck dude, nurses are the shit with Toxicity Score (98.62%) (Mauboussin, 2022)\n\n\nThis is where, in the NLP community, there has been a growing discussion to ensure toxicity analysis tools, especially detectors used in online discussion and social media platforms, are more robust than simply being ‘profanity detectors’. They must be able to interpret a word in context.\n\n\n4.2 In-group language\nConsider in-group words used by distinct communities. Many of these words, once used as derogatory slurs against a group of people (such as Black or LGBTQ+ folk), have now largely been reclaimed and are prevalent in the lexicons of individuals identifying within these communities, no longer considered offensive when used by the in-group. However, if human annotators label textual data that ML models then are trained on, biases can permeate the models and lead to the classification of non-toxic, in-group language as harmful or offensive. Notably, African-American Vernacular English (AAVE) has been found to be flagged as toxic due to linguistic bias. XX frames how the challenge impacts toxicity detectors well:\n\n🔎 Engage Critically\nHow do you think this challenge can impact toxicity detectors? Resende et al. (2024) underscore this tension, noting that:\n\n…[S]uch a fine line between causal speaking and offensive discourse is problematic from a human and computational perspective. In that case, these interpretations are confounding to automatic content moderation tools. In other words, toxicity/sentiment analysis tools are usually developed using manual rules or supervised ML techniques that employ human-labeled data to extract patterns. The disparate treatment embodied by machine learning models usually replicates discrimination patterns historically practiced by humans when interacting with processes in the real world. Due to biases in this process, a lack of context leads both rule-based and machine learning-based models to a concerning scenario where minorities do not receive equal treatment.  Resende et al., 2024, p. 2\n\n\nResende et al. (2024) also conducted a comparison analysis of toxicity models, including Google’s Perspective API and Detoxify (the model we’ll be using for our own analysis soon).\n\n\n\nComparing the Toxicity Scoring Models (Resende et al., 2024)\n\n\nThis bias shown in this model’s performance can come from many factors in its structure, from data provenance and annotation to model architecture and processing, to a combination of many.\n\n🔎 Engage Critically\nIf you could, what questions would you want to ask the people who build these models?\n\n\n\nToxicity analysis using Detoxify\nThe model we’ll be using is called Detoxify (you can read more about it here). It was trained on large datasets of online comments across seven languages, including English and Russian. Detoxify provides an overall toxicity score for each text and can also detect five specific subtypes of toxicity: identity_attack, insult, obscene, sexual_explicit, and threat.\nIn the context of our dataset, propagandist tweets often aim to provoke strong emotions, spread hate, or stir conflict. Running toxicity analysis can help us investigate questions like:\n\nAre propagandist tweets more toxic than non-propagandist ones?\nWhat types of toxic language are most common?\nAre there patterns in how toxicity is used to influence or manipulate public discourse?\n\nToxicity analysis gives us another lens to understand how language and emotion are used in disinformation campaigns. Let’s begin by importing the necessary libraries and tools:\n\n\nCode\nfrom detoxify import Detoxify\n\nmodel = Detoxify('original', device='cpu')\n\n\nBefore we throw this model at our dataset, let’s take a look at what ‘toxic’ really means.\nWe’ll be repeating the same hypothesis test that we performed using our sentiment analysis models, this time trying to answer:\n\nAre tweets deemed toxic more likely to originate from propagandists relative to non-propagandist tweets?\n\nHere, we’ll define a tweet toxic if it meets or exceeds a toxicity theshold of 0.5.\n\n\nCode\ntexts = tweets_small['content'].tolist()\nresults = model.predict(texts)\ntoxicity_df = pd.DataFrame(results)\ntweets_small = tweets_small.join(toxicity_df)\n\n\n\n\nCode\nthreshold = 0.5\ntweets_small['charged'] = (tweets_small['toxicity'] &gt; threshold).astype(int)\n\ncontingency = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged'])\nprint(\"Contingency table:\\n\", contingency)\n\nchi2, p, dof, expected = chi2_contingency(contingency)\nprint(f\"Chi-squared statistic: {chi2:.2f}\")\nprint(f\"p-value: {p:.3e}\")\n\nprint(\"\\nSample of results:\")\nprint(tweets_small[['content', 'is_propaganda', 'toxicity', 'charged']].head())\n\n\nFollowing the logic from the sentiment analysis results, what do these results tell us about our hypothesis? How do you think the results would change if we used a different threshold to define toxicity?"
  },
  {
    "objectID": "docs/ECON-227/econ_227_lesson_plan.html",
    "href": "docs/ECON-227/econ_227_lesson_plan.html",
    "title": "Lesson Plan : Large Language Models and Stock Market Predictions",
    "section": "",
    "text": "Duration : 60 minutes\nCourse : ECON 227\n\n\n\nThis lesson explores how large language models (LLMs) work by linking their probability-based predictions to real-world applications. We start with the basics of next-word prediction, then extend the idea to forecasting stock prices using news sentiment. Along the way, we explain how AI models can be powerful but also imperfect predictors.\nThe goal is to understand how these tools fit into broader questions about data and economics, not to make you a machine learning expert.\n\n\n\n\nBy the end of this lesson, students will:\n\nUnderstand how large language models (LLMs) generate text through next-word prediction and probability distributions.\nDescribe the role of temperature in shaping outputs.\nConnect these concepts to other forms of prediction, such as time-series forecasting.\nApply sentiment analysis to real-world news headlines and interpret the results.\nCombine sentiment with stock price data to test whether public mood helps forecast financial markets.\n\n\n\n\n\nThe lesson will be administered through a Jupyter Notebook llm_distributions.ipynb, hosted on the prAxIs UBC website. Students will need access to a device connected to the internet, preferably a laptop. No previous coding experience is required, though some familiarity with Python is an asset. If students do not have access to a capable device, it is acceptable to pair up into groups of two or three.\n\n\n\n\n\nYou should have read the short introduction on probability and prediction (5–10 minutes).\nThe instructor has loaded and tested the notebook before class.\nBring your laptop or be ready to pair up.\n\n\n\n\n\n\nPre-discussion & mini-lecture (10 min)\n\n\nWarm-up example: “The cat sat on the ___.”\nDiscuss how prediction works in language and how it might connect to forecasting in other contexts.\nQuick question : Can you think of where else we use probability in everyday life?\n\n\nSection 1 : Next-word prediction (5 min)\n\n\nStudents run toy examples showing probability distributions across a small vocabulary.\nQuick question : Why does context matter?\nInstructor explains chain rule and conditional probability.\n\n\nSection 2 : Sampling and temperature (10 min)\n\n\nStudents experiment with temperature values (0.5, 1.0, 1.5).\nCompare “sharper” vs. “flatter” distributions.\nQuick question : When might you want more creativity versus more focus in an AI system?\n\n\nSection 3 : From words to stocks (5 min)\n\n\nTransition from language to financial data: show how LLM ideas generalize.\nInstructor ties this to econometric forecasting models.\n\n\nSection 4 : Sentiment analysis of news headlines (10 min)\n\n\nApply RoBERTa classifier to Starbucks news.\nStudents classify a few headlines/sentences themselves, then compare with the model’s output.\nStudents try using the classifier, inputting different sentences.\nQuick question : Did the model get it right ?\n\n\nSection 5 : Sentiment + stock prices (5 min)\n\n\nExplore combined dataset of sentiment and daily stock returns.\nInstructor introduces SARIMAX and forecast visualization.\nStudents interpret forecast plots: does sentiment lead stock prices, or not?\n\n\nSection 6 : AI on AI (5 min)\n\n\nSentiment analysis for top 10 AI companies.\nQuick question : Is the news optimistic or pessimistic about AI?\n\n\nWrap-up and reflection (5–10 minutes)\n\n\nKey takeaways: probability, sentiment, prediction, evaluation.\n\n\n\n\n\nDiscussion Post (2-3 sentences)\n\nHow does changing temperature alter model predictions?\nWhy might news sentiment affect stock prices? Does it cause changes, or just reflect them?\nWhat are the benefits and risks of using LLMs in economic or social research?\nIf you built a forecasting model like this, what else would you include to improve it?"
  },
  {
    "objectID": "docs/ECON-227/econ_227_lesson_plan.html#what-is-this-lesson-about",
    "href": "docs/ECON-227/econ_227_lesson_plan.html#what-is-this-lesson-about",
    "title": "Lesson Plan : Large Language Models and Stock Market Predictions",
    "section": "",
    "text": "This lesson explores how large language models (LLMs) work by linking their probability-based predictions to real-world applications. We start with the basics of next-word prediction, then extend the idea to forecasting stock prices using news sentiment. Along the way, we explain how AI models can be powerful but also imperfect predictors.\nThe goal is to understand how these tools fit into broader questions about data and economics, not to make you a machine learning expert."
  },
  {
    "objectID": "docs/ECON-227/econ_227_lesson_plan.html#learning-objectives",
    "href": "docs/ECON-227/econ_227_lesson_plan.html#learning-objectives",
    "title": "Lesson Plan : Large Language Models and Stock Market Predictions",
    "section": "",
    "text": "By the end of this lesson, students will:\n\nUnderstand how large language models (LLMs) generate text through next-word prediction and probability distributions.\nDescribe the role of temperature in shaping outputs.\nConnect these concepts to other forms of prediction, such as time-series forecasting.\nApply sentiment analysis to real-world news headlines and interpret the results.\nCombine sentiment with stock price data to test whether public mood helps forecast financial markets."
  },
  {
    "objectID": "docs/ECON-227/econ_227_lesson_plan.html#materials-and-technical-requirements",
    "href": "docs/ECON-227/econ_227_lesson_plan.html#materials-and-technical-requirements",
    "title": "Lesson Plan : Large Language Models and Stock Market Predictions",
    "section": "",
    "text": "The lesson will be administered through a Jupyter Notebook llm_distributions.ipynb, hosted on the prAxIs UBC website. Students will need access to a device connected to the internet, preferably a laptop. No previous coding experience is required, though some familiarity with Python is an asset. If students do not have access to a capable device, it is acceptable to pair up into groups of two or three."
  },
  {
    "objectID": "docs/ECON-227/econ_227_lesson_plan.html#pre-lesson-checklist",
    "href": "docs/ECON-227/econ_227_lesson_plan.html#pre-lesson-checklist",
    "title": "Lesson Plan : Large Language Models and Stock Market Predictions",
    "section": "",
    "text": "You should have read the short introduction on probability and prediction (5–10 minutes).\nThe instructor has loaded and tested the notebook before class.\nBring your laptop or be ready to pair up."
  },
  {
    "objectID": "docs/ECON-227/econ_227_lesson_plan.html#agenda",
    "href": "docs/ECON-227/econ_227_lesson_plan.html#agenda",
    "title": "Lesson Plan : Large Language Models and Stock Market Predictions",
    "section": "",
    "text": "Pre-discussion & mini-lecture (10 min)\n\n\nWarm-up example: “The cat sat on the ___.”\nDiscuss how prediction works in language and how it might connect to forecasting in other contexts.\nQuick question : Can you think of where else we use probability in everyday life?\n\n\nSection 1 : Next-word prediction (5 min)\n\n\nStudents run toy examples showing probability distributions across a small vocabulary.\nQuick question : Why does context matter?\nInstructor explains chain rule and conditional probability.\n\n\nSection 2 : Sampling and temperature (10 min)\n\n\nStudents experiment with temperature values (0.5, 1.0, 1.5).\nCompare “sharper” vs. “flatter” distributions.\nQuick question : When might you want more creativity versus more focus in an AI system?\n\n\nSection 3 : From words to stocks (5 min)\n\n\nTransition from language to financial data: show how LLM ideas generalize.\nInstructor ties this to econometric forecasting models.\n\n\nSection 4 : Sentiment analysis of news headlines (10 min)\n\n\nApply RoBERTa classifier to Starbucks news.\nStudents classify a few headlines/sentences themselves, then compare with the model’s output.\nStudents try using the classifier, inputting different sentences.\nQuick question : Did the model get it right ?\n\n\nSection 5 : Sentiment + stock prices (5 min)\n\n\nExplore combined dataset of sentiment and daily stock returns.\nInstructor introduces SARIMAX and forecast visualization.\nStudents interpret forecast plots: does sentiment lead stock prices, or not?\n\n\nSection 6 : AI on AI (5 min)\n\n\nSentiment analysis for top 10 AI companies.\nQuick question : Is the news optimistic or pessimistic about AI?\n\n\nWrap-up and reflection (5–10 minutes)\n\n\nKey takeaways: probability, sentiment, prediction, evaluation."
  },
  {
    "objectID": "docs/ECON-227/econ_227_lesson_plan.html#activity-materials",
    "href": "docs/ECON-227/econ_227_lesson_plan.html#activity-materials",
    "title": "Lesson Plan : Large Language Models and Stock Market Predictions",
    "section": "",
    "text": "Discussion Post (2-3 sentences)\n\nHow does changing temperature alter model predictions?\nWhy might news sentiment affect stock prices? Does it cause changes, or just reflect them?\nWhat are the benefits and risks of using LLMs in economic or social research?\nIf you built a forecasting model like this, what else would you include to improve it?"
  },
  {
    "objectID": "docs/AMNE-376/amne376_image_embedding.html",
    "href": "docs/AMNE-376/amne376_image_embedding.html",
    "title": "A Study of Richter’s Kouroi Through Image Embedding",
    "section": "",
    "text": "Learning Outcomes\nIn the notebook you will\n\nFamiliarize yourself with concepts such as computer vision, convolution, convolutional neural network (CNN) and image embeddings.\nUnderstand how computers “see” and distinguish between different images by identifying unique visual elements and quantifying their similarity.\nExplore photographs of Kouroi from Richter’s book Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942) and create image embeddings for these photographs using pre-trained models.\nLearn how to cluster and classify Kouroi based solely on photographs, and critically analyze the advantages and limitations of these techniques and their potential applications.\n\n\n\nPrerequisites\nBefore you start, make sure you\n\nHave at least 200 MB storage available on your device.\nDownload the 2 folders from SharePoint by hovering around the three points next to the folder name, then click download (You will only have access if you are an enrolled UBC student with an activated student email. If you are a UBC student but don’t have a student email, please follow this link to activate it). They will be saved locally as .zip files.\nFind the downloaded zipped files in your device and upload them to Jupyter in the same directory as this notebook.\nHave the required libraries installed, if not, uncomment the lines below (i.e. remove the #) and run the cell to install them:\n\n\n\nCode\n# !pip install matplotlib numpy pandas opencv-python scikit-learn torch torchvision transformers datasets grad-cam tensorflow keras\n\n\nImportant: Please run the following cells before continuing, as it sets up the libraries and image folders required for this notebook.\n\n\nCode\n# Standard library\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\n\n# Imaging / computer vision\nfrom PIL import Image\nimport cv2\nimport numpy as np\nfrom torchvision import transforms\n\n# Visualization\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\nimport plotly.express as px\nimport plotly.io as pio\n\n# Data handling\nimport pandas as pd\n\n# SciPy / numerical utilities\nfrom scipy.ndimage import convolve\n\n# scikit-learn\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\n# TensorFlow / Keras (VGG16)\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing import image as keras_image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\n# PyTorch / torchvision / transformers\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torchvision.models import convnext_base, ConvNeXt_Base_Weights\nfrom transformers import AutoImageProcessor, ConvNextV2Model\n\n# Misc\nfrom tqdm.notebook import tqdm\n\n# Local helpers\nfrom data.zip_extractor import ZipExtractor\n\n\n\n\nCode\n# Don't change this cell\n# Define the source path and the destination\nsources = [\"example_images.zip\", \"richter_kouroi_complete_front_only.zip\"]\ndest = \"../data/\"\nfor src in sources:\n    extractor = ZipExtractor(src, dest)\n    extractor.extract_all()\n\n\n\n\n1. Introduction: How Do Computers “See” Visual Art?\nIn this notebook, we explore a dataset of photographs collected from Gisela Richter’s Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942).\nGisela Richter’s 1942 book was one of the first systematic efforts to catalog and classify kouroi based on their stylistic evolution. Her work combined archaeological evidence with visual comparison, laying the foundation for how we study ancient sculpture today.\nIn this project, we aim to apply computer vision techniques to digitally analyze and group images from this dataset. Just as Richter used her trained eye to identify patterns and typologies, we’ll explore how machines can “see” these sculptures through their eyes (image embeddings, clustering, and convolutional neural networks).\nHave you ever wondered how images are stored in computers, how computers see them and distinguish the difference between them?\nMany of you probably know that digital images are stored based on pixels as a grid of figures, but when we are doing image searches using a search engine or uploading them to a Generative AI model, how exactly do computers interpret, distinguish, and process them? Here is a brief introduction that introduces you to some of the basic forms and methods.\n\n1.1 Digital Representations of Images\nHave you ever heard of the RGB primary colour model? For those who are unfamiliar with the concept, the model uses numbers in a range 0 ~ 255 to represent the colour intensity of red, green and blue and add up the three colour channels to generate any colour that’s visible to human. In a colorful digital image, each pixel is characterized by its colour stored in the form (R, G, B), so knowing the distribution of colour intensity gives you a lot of information about the image.\nHowever, for monochrome images, there is only one colour channel, the grayscale. We can still use the distribution of grayscale intensities to represent the image. Since all of our images (cropped from scanned pdf books) are printed in monochrome, we can represent them using a grayscale colour histogram.\nLet’s start with a three-view of the New York Kouros, here we read in the images and present them together.\n\n\nCode\n# Define the folder path where the images are stored\nimage_path = '../data/example_images' \n\nfig, axes = plt.subplots(1, 3, figsize=(8, 5))\n\n# List of specific image filenames\nimage_names = {'page188_img01_photo12.jpg': \"Left\", 'page188_img01_photo13.jpg': \"Front\", 'page189_img01_photo3.jpg': \"Back\"}\n\n# Display the images side by side\naxes = axes.flatten()\nfor i, img_name in enumerate(image_names):\n    img_path = f\"{image_path}/{img_name}\"\n    image = Image.open(img_path)\n    axes[i].imshow(image)\n    axes[i].set_title(image_names[img_name])\n    axes[i].axis('off')\n\nplt.suptitle(\"Selected Images from Richter's Kouroi Dataset\")\nplt.tight_layout()\nplt.show()\n\n\nEach of these images, when loaded into the computer, becomes a 2D array of numbers representing intensity values. We then plot the colour histogram for each image representing the distribution of grayscale intensity.\n\nDiscussion: What do you notice by looking at the three histograms?\n\n\n\nCode\n# Generate and plot greyscale histograms for the selected images\nfig, axes = plt.subplots(1, 3, figsize=(7, 4))\n\nfor i, img_name in enumerate(image_names):\n    img_path = f\"{image_path}/{img_name}\"\n    image = Image.open(img_path)\n    histogram = image.histogram()\n\n    axes[i].plot(histogram, color='black')\n    axes[i].set_title(f'{image_names[img_name]}')\n    axes[i].set_xlim([0, 255])\n    axes[i].set_xlabel(\"Intensity\")\n    if i == 0:\n        axes[i].set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\nThey look very similar! This result is not surprising given that the three images were taken at the same time with the same equipment of the same Kouros. The above example shows us that comparing the similarity of colour distributions is one way that computers understand the similarity of images.\nHowever, one can quickly realize the drawbacks of this approach. First, it relies on the correct representation of colour, so two identical images with color differences may not be recognized as similar. Second, since it focuses only on colour, it ignores the fundamental information for object recognition such as spatial, shape and texture in the image. Last but not least, there may exist two completely different images with exactly the same color distribution. Therefore, we need better methods to consider the similarity between images.\nBag of Visual Words (BoVW) is a more practical method for recognizing similarity. The rationale behind this is very complicated, but to put it simply, it treats a “feature” in an image as a “word” (a set of numbers containing information about the feature) and calculates how often each word appears in the image. Here, we created a visual vocabulary containing 20 “words” using three-view photos of the New York Kouros, and visualized what a visual word represents on the left-view image. Here, we pick the visual word with ID 6:\n\n\nCode\n# Define the number of clusters for KMeans\nn_clusters   = 20\nword_to_show = 6\nmax_patches  = 30\n\n# Initialize ORB detector\norb = cv2.ORB_create(nfeatures=500)\nall_descriptors = []      # for stacking\nimage_data      = []      # (img_name, kps, descs)\n\n# Detect and describe all images\nfor img_name in image_names:\n    img_path = os.path.join(image_path, img_name)\n    img      = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    keypoints, descriptors = orb.detectAndCompute(img, None)\n\n    if descriptors is None:\n        descriptors = np.zeros((0, orb.descriptorSize()), dtype=np.uint8)\n\n    all_descriptors.append(descriptors)\n    image_data.append((img_name, keypoints, descriptors))\n\n# Build the visual vocabulary\nall_descriptors_stacked = np.vstack(all_descriptors)\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nkmeans.fit(all_descriptors_stacked)\n\n# Compute BoVW histograms\nhistograms = []\nfor img_name, _, descriptors in image_data:\n    if descriptors.shape[0] &gt; 0:\n        words = kmeans.predict(descriptors)\n        hist, _ = np.histogram(words, bins=np.arange(n_clusters + 1))\n    else:\n        hist = np.zeros(n_clusters, dtype=int)\n    histograms.append((img_name, hist))\n\n# Find the locations matching visual word ID = 6\nlocations = []\nfor img_idx, (_, keypoints, descriptors) in enumerate(image_data):\n    if descriptors.shape[0] == 0:\n        continue\n    assignments = kmeans.predict(descriptors)\n    for kp, w in zip(keypoints, assignments):\n        if w == word_to_show:\n            x, y = map(int, kp.pt)\n            locations.append((img_idx, x, y))\n            if len(locations) &gt;= max_patches:\n                break\n    if len(locations) &gt;= max_patches:\n        break\n\n# Group by image and visualize\nimgs = defaultdict(list)\nfor idx, x, y in locations:\n    imgs[idx].append((x, y))\n\nif imgs:\n    img_idx, pts = next(iter(imgs.items()))\n    fname = image_data[img_idx][0]\n    img   = cv2.imread(os.path.join(image_path, fname), cv2.IMREAD_GRAYSCALE)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    for x, y in pts:\n        cv2.circle(img_rgb, (x, y), radius=25, color=(0,255,0), thickness=2)\n\n    plt.figure(figsize=(6,6))\n    plt.imshow(img_rgb)\n    plt.title(f\"Word {word_to_show} Keypoints on the Left-View Photo\")\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\nWe note that the word “6” could represent the beads in the beadwork worn by the Kouros.\n\nDiscussion: Based on your knowledge of the various Kouros, do you think this visual word can be the key to differentiating between different Kouros, or even different sculptural subjects?\n\n\n\n1.2 Measurement of Similarity\nAs you may have realized, the visual word frequency distributions of different images are not exactly the same, so how can we determine if these images are similar? More importantly, what can we use as a criterion to categorize different images based on visual words? Here, we will use something called cosine similarity to make a measurement. \nYou can think of the visual word frequency histogram for each image as an arrow in space, and cosine similarity is a measure of how much those arrows are pointing in the same direction. The criterion is very intuitive: the closer the cosine similarity of two images is to 1, the more similar the two images are; the closer the cosine similarity is to 0, the less similar the two images are.\nHere, we perform pairwise cosine similarity measurements on left-view, front-view, and back-view photographs of New York Kouros, and show the results.\n\n\nCode\nhist_list = []\nfor img_name, _, descriptors in image_data:\n    if descriptors.shape[0] &gt; 0:\n        words = kmeans.predict(descriptors)\n        hist, _ = np.histogram(words, bins=np.arange(n_clusters + 1))\n    else:\n        hist = np.zeros(n_clusters, dtype=int)\n    hist = hist.astype(float)\n    if hist.sum() &gt; 0:\n        hist /= hist.sum()\n    hist_list.append(hist)\n    \nhistograms = np.array(hist_list)\n\nsim_matrix = cosine_similarity(histograms)\n\nimage_keys = list(image_names.keys())\nimage_labels = list(image_names.values())\n\n# Display the similarity matrix\nfig, ax = plt.subplots(figsize=(8, 5))\ncax = ax.imshow(sim_matrix, interpolation='nearest', cmap='viridis')\nax.set_title('BoVW Cosine Similarity between Images')\nax.set_xticks(np.arange(len(image_labels)))\nax.set_yticks(np.arange(len(image_labels)))\nax.set_xticklabels(image_labels, rotation=45, ha='right')\nax.set_yticklabels(image_labels)\nfig.colorbar(cax, ax=ax, label='Cosine Similarity')\nplt.tight_layout()\nplt.show()\n\nprint(\"Pairwise Cosine Similarity Matrix:\")\nfor i in range(len(image_labels)):\n    for j in range(i + 1, len(image_labels)):\n        print(f\"{image_labels[i]} vs {image_labels[j]}: {sim_matrix[i, j]:.3f}\")\n\n\nAs you can see, the pairwise cosine similarities are all very high, even though the BoVW histograms look very different! This is good evidence that they are photographs of the same object, and computers can understand this by setting the appropriate threshold.\nHowever, would this also work for photos of different objects? Let’s find out by calculating the cosine similarity between existing images and another Kouros currently exhibited in the Piraeus Archaeological Museum.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_image_path = '../data/richter_kouroi_complete_front_only/page312_img01_photo4.jpg'\nnew_image_label = 'A Kouros from Athens' # Suppose this is a new artifact we just discovered\n\nimg_new = cv2.imread(new_image_path, cv2.IMREAD_GRAYSCALE)\norb = cv2.ORB_create(nfeatures=500)\nkp_new, desc_new = orb.detectAndCompute(img_new, None)\n\nif desc_new is not None and len(desc_new) &gt; 0:\n    words_new = kmeans.predict(desc_new)\n    hist_new, _ = np.histogram(words_new, bins=np.arange(kmeans.n_clusters + 1))\nelse:\n    hist_new = np.zeros(kmeans.n_clusters, dtype=int)\n    \nhist_new = hist_new.astype(float)\nif hist_new.sum() &gt; 0:\n    hist_new /= hist_new.sum()\n\nsims = cosine_similarity(histograms, hist_new.reshape(1, -1)).flatten() \n\n# Print the cosine similarity of the new image with existing images\nprint(f\"\\nCosine Similarity of '{new_image_label}' with existing images:\")\nfor i, label in enumerate(image_labels):\n    print(f\"{label} vs {new_image_label}: {sims[i]:.3f}\")\n\n# Show the new image\nplt.figure(figsize=(5, 5))\nplt.imshow(img_new, cmap='gray')\nplt.title(f\"{new_image_label}\")\nplt.axis('off')\nplt.show()\n\n\nBy looking at the results, we see that it has a lower but still relatively high cosine similarity to the previous images, albeit with different textures and poses. Although computers do not understand what “Kouroi” are simply by collecting visual words, they can still see the similarity! To support this view, let’s look at an example that is also a standing figure, but from a different culture (China, Sanxingdui). If our conjecture is correct, its cosine similarity to the previous images will decrease significantly.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_image_path2 = '../data/example_images/sanxingdui.jpeg'\nnew_image_label2 = 'A Bronze Figure from Sanxingdui' # Suppose this is a new artifact we just discovered\n\nimg_new2 = cv2.imread(new_image_path2, cv2.IMREAD_GRAYSCALE)\norb = cv2.ORB_create(nfeatures=500)\nkp_new, desc_new = orb.detectAndCompute(img_new2, None)\n\nif desc_new is not None and len(desc_new) &gt; 0:\n    words_new = kmeans.predict(desc_new)\n    hist_new, _ = np.histogram(words_new, bins=np.arange(kmeans.n_clusters + 1))\nelse:\n    hist_new = np.zeros(kmeans.n_clusters, dtype=int)\n    \nhist_new = hist_new.astype(float)\nif hist_new.sum() &gt; 0:\n    hist_new /= hist_new.sum()\n\nsims = cosine_similarity(histograms, hist_new.reshape(1, -1)).flatten()  \n\n# Print the cosine similarity of the new image with existing images\nprint(f\"\\nCosine Similarity of '{new_image_label2}' with existing images:\")\nfor i, label in enumerate(image_labels):\n    print(f\"{label} vs {new_image_label2}: {sims[i]:.3f}\")\n\n# Show the new image\nplt.figure(figsize=(5, 5))\nplt.imshow(img_new2, cmap='gray')\nplt.title(f\"{new_image_label2}\")\nplt.axis('off')\nplt.show()\n\n\nThe results were exactly as we expected. The cosine similarity measured for different artistic styles, different poses and different angles of the Sanxingdui sculpture is significantly lower.\nThis provides us with a hint on how to build an automatic image-based classifier for art and artifacts of different genres, cultures, and textures. Although BoVW also has some obvious limitations (lack of spatial relationships, lack of ability to detect specific objects in complex images), the examples above demonstrate the fundamentals of computer vision, and with the help of more advanced techniques we can do much more in analyzing artwork based on digitized images.\n\n\n\n2. Convolutions on Images\n\n2.1 What are convolutions?\nBefore diving into applying a convolutional neural network, let’s first make an intuitive introduction to the concept convolution.\nImagine sliding a tiny image over an image as a filter to make the actual image appear the same as the filter. Convolution is the mathematical operation to achieve such an effect.\nBelow is one of such filters, or its professional term, a kernel, how do you think it will filter an image to make the image look like it?\n\n\nCode\n# Kernel\nkernel = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\n# Map: 1 -&gt; 1.0 (white), 0 -&gt; 0.0 (black), -1 -&gt; 1.0 (white)\ndisplay_kernel = np.where(kernel == 0, 0, 1)\n\nfig, ax = plt.subplots()\ncax = ax.matshow(display_kernel, cmap='gray', vmin=0, vmax=1)\nplt.colorbar(cax)\n\n# Annotate the kernel values\n\nax.set_title('Example of a Kernel')\nplt.show()\n\n\nThis is how an actual image Convolved with the filter:\n\n\nCode\n# Load the image as grayscale\nimg_path = \"../data/example_images/page300_img01_photo8.jpg\"\nimage = Image.open(img_path).convert('L')\nimg_array = np.array(image)\n\n# Define the horizontal edge detection kernel\nkernel = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\n# Convolve the image with the kernel\nconvolved = convolve(img_array, kernel, mode='reflect')\n\n# Display the original and convolved images\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nax[0].imshow(img_array, cmap='gray')\nax[0].set_title(\"Original Image\")\nax[0].axis('off')\nax[1].imshow(convolved, cmap='gray')\nax[1].set_title(\"Convolved with Kernel\")\nax[1].axis('off')\nplt.show()\n\n\nThe above is just one example of a convolutional kernel that extracts horizontal edges in an image. In fact, there are many different kernels with different effects. For example, here is a filter that blurs all images:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\ndef gaussian_kernel(size=21, sigma=5):\n    ax = np.linspace(-(size-1)//2, (size-1)//2, size)\n    xx, yy = np.meshgrid(ax, ax)\n    kernel = np.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n    return kernel / np.sum(kernel)\n\nkernel = gaussian_kernel(21, 5)\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"21x21 Gaussian Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Heavily Blurred\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nBelow is a kernel that preserves the input image as it is; it is also known as the identity kernel:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Identity kernel (3x3)\nkernel = np.zeros((3, 3))\nkernel[1, 1] = 1\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray', vmin=0, vmax=1); ax[1].set_title(\"Identity Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nThere is also a kernel that sharpens the images, known as the sharpening filter:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Sharpen kernel\nkernel = np.array([[ 0, -1,  0],\n                   [-1,  5, -1],\n                   [ 0, -1,  0]])\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"Sharpen Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nOther than sharpening, there is even a filter to emboss the image:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Emboss kernel\nkernel = np.array([[-2, -1, 0],\n                   [-1,  1, 1],\n                   [ 0,  1, 2]])\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"Emboss Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nOther than only detecting vertical or horizontal edges, a filter named after Laplace was discovered to detect all edges:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\nlaplacian = np.array([[0,-1,0],[-1,8,-1],[0,-1,0]])\nedge = convolve(img, laplacian)\n\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off'\n)\nax[1].imshow(laplacian, cmap='gray'); ax[1].set_title(\"Laplacian Kernel\"); ax[1].axis('off')\nax[2].imshow(edge, cmap='gray'); ax[2].set_title(\"Edges\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\n\n\n2.2 What can Machine Learning do?\nOver the years, people have discovered these tiny images or “kernels” or “filters”. In machine learning, we discover or learn these potentially useful filters directly from the data, rather than through mathematical derivation. In a word, machine learning can “learn” the filters from data what would be useful for downstream tasks like classifying images or identifying things in an image.\n\n\nCode\n# 1. Load a pretrained conv model (VGG16 without top)\nmodel = VGG16(weights='imagenet', include_top=False)\n\n# 2. Choose three conv layers: early, middle, late\nearly_layer = model.get_layer('block1_conv2')\nmid_layer   = model.get_layer('block3_conv3')\nlate_layer  = model.get_layer('block5_conv3')\n\n# 3. Extract one kernel from each layer\n# Each kernel has shape (k, k, in_channels, out_channels)\nkernel_early = early_layer.get_weights()[0][:, :, 0, 0]\nkernel_mid   = mid_layer.get_weights()[0][:, :, 0, 0]\nkernel_late  = late_layer.get_weights()[0][:, :, 0, 0]\n\n# 4. Load and preprocess an image\ndef load_and_gray(path, target_size=(224,224)):\n    img = keras_image.load_img(path, target_size=target_size)\n    img_arr = keras_image.img_to_array(img)\n    # convert to grayscale\n    gray = cv2.cvtColor(img_arr.astype('uint8'), cv2.COLOR_RGB2GRAY)\n    # normalize\n    gray = gray.astype('float32') / 255.0\n    return gray\n\ngray = load_and_gray(img_path)\n\n# 5. Convolve the image with each kernel\ndef apply_filter(img, kernel):\n    # Flip kernel for convolution\n    k = kernel.shape[0]\n    # OpenCV uses correlation; flip kernel to perform convolution\n    flipped = np.flipud(np.fliplr(kernel))\n    filtered = cv2.filter2D(img, -1, flipped)\n    return filtered\n\nout_early = apply_filter(gray, kernel_early)\nout_mid   = apply_filter(gray, kernel_mid)\nout_late  = apply_filter(gray, kernel_late)\n\n# 6. Visualize\nplt.figure(figsize=(8, 8))\n\nplt.subplot(2, 2, 1)\nplt.title('Original Gray')\nplt.imshow(gray, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 2)\nplt.title('Early Layer Kernel')\nplt.imshow(out_early, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 3)\nplt.title('Mid Layer Kernel')\nplt.imshow(out_mid, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 4)\nplt.title('Late Layer Kernel')\nplt.imshow(out_late, cmap='gray')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()  # In Colab this will display inline\n\n\nDifferent parts of the model highlight different things. The important thing to note is that no one wrote the filters themselves. The network learned that the features highlighted by these filters are useful.We simply wrote the learning algorithm, then the model learned from data by itself.\nTypically, a model used for image classification can (on its own) learn filters to highlight things the model needs, such as edges and lines, and more importantly, in addition to these simple features, the model can learn filters to detect heads, eyes, ears, and other abstract concepts, and this is exactly how convolution makes it possible to detect, characterize, and categorize complex objects in complex images. In order to achieve these amazing features, we usually need to employ models such as Convolutional Neural Networks.\n\n\n\n3. Data Exploration\n\n3.1 Exploring the Metadata\nFor the rest of the notebook, we will use a small selection of photographs from Richter’s Kouroi (1942), which contain frontal shots of Kouroi with a full torso and recognizable facial features. We have also prepared a labeled metadata that shows information about which group and era these Kouroi belong to and what materials they are made of. We can begin by looking at some basic information from the metadata:\n\n\nCode\n# Read in the metadata CSV file\n# Note that we are only going to investigate a subset of the full dataset\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')\n\ndf = df.drop(columns = 'page')\n\nprint(df.head())\n\n\n\n\nCode\nprint(\"Information of the dataset:\")\nprint(f\"Number of images: {df.shape[0]}\")\nprint(f\"Number of distinct eras: {df['era'].nunique()}\")\nprint(f\"Number of distinct materials: {df['material'].nunique()}\")\n\n\nWe can also see the distribution of each label by plotting histograms:\n\n\nCode\ndef bar_plot(df, column1, column2):\n    # Calculate counts of each value in the specified columns\n    label_counts1 = df[column1].value_counts()\n    label_counts2 = df[column2].value_counts()\n\n    # Create a figure with a fixed size\n    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n    \n    # Plot the bar chart\n    label_counts1.plot(kind='bar', ax=axes[0], color='steelblue')\n    axes[0].set_title(f'Distribution of {column1.capitalize()}')\n    axes[0].set_xlabel(column1.capitalize())\n    axes[0].set_ylabel('Count')\n    axes[0].tick_params(axis='x', rotation=90)\n    label_counts2.plot(kind='bar', ax=axes[1], color='darkorange')\n    axes[1].set_title(f'Distribution of {column2.capitalize()}')\n    axes[1].set_xlabel(column2.capitalize())\n    axes[1].set_ylabel('Count')\n    axes[1].tick_params(axis='x', rotation=90)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot the distribution of labels in the dataset\nbar_plot(df, 'era', 'material')\n\n\n\n\n3.2 Exploring the Images\nTo get a direct idea of the general characteristics of this subset of photographs, we read the photographs from the image directory and show the first 4 images in this dataset.\n\n\nCode\n# Read in the images as a list \ndata_dir = Path(\"../data/richter_kouroi_complete_front_only\")\nimage_paths = sorted(data_dir.glob(\"*.jpg\"))\n\nimages = []\nfor p in image_paths:\n    img = Image.open(p).convert(\"RGB\")   # ensure 3‑channel\n    img_arr = np.array(img)\n    images.append(img_arr)\n    \nfig, axes = plt.subplots(1, 4, figsize=(6, 4))\nfor ax, img in zip(axes, images[:4]):\n    ax.imshow(img)\n    ax.axis(\"off\")\nplt.suptitle(\"First 4 images in the dataset\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n4. Image Embedding Using ConvNeXt V2\n\n4.1 CNN and Models See Images\nWhat we’ll do next is we will bring in ConvNeXt V2, it is a CNN model trained on millions of images based on ImageNet.\nWe’re sure everyone has used some Large Language Models (LLMs) and gotten a feel for how these models mimic the way humans think. This imitation of the human way of thinking comes from Artificial Neural Networks (ANN). Just as LLMs process and generate natural language, CNN models process visual images, examining the features in those images through convolutional layers and generating their own digital representations of the images.\nHere, we will create a visualization that shows the early, middle, and late feature layers of the 4 images after convolution, what commonality and difference did you notice from the extracted features in these layers?\n\n\nCode\nmodel = VGG16(weights='imagenet', include_top=False)\n\n# Grab kernels from the first and last conv layers\nearly_layer = model.get_layer('block1_conv2')\nmid_layer = model.get_layer('block3_conv3')\nlate_layer  = model.get_layer('block5_conv3')\n\n# choose the (0,0) filter for each\nkernel_early = early_layer.get_weights()[0][:, :, 0, 0]\nkernel_mid  = mid_layer.get_weights()[0][:, :, 0, 0]\nkernel_late  = late_layer.get_weights()[0][:, :, 0, 0]\n\n# Convolution helper (flip kernel for true conv)\ndef apply_filter(img, kernel):\n    flipped = np.flipud(np.fliplr(kernel))\n    return cv2.filter2D(img, -1, flipped)\n\nfig, axes = plt.subplots(3, 4, figsize=(8, 8))\n\nfor col, img in enumerate(images[:4]):\n    gray = (\n        cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2GRAY)\n          .astype('float32') / 255.0\n    )\n    out_early = apply_filter(gray, kernel_early)\n    out_early = (out_early - out_early.min()) / (out_early.max() - out_early.min())\n\n    out_mid = apply_filter(gray, kernel_mid)\n    out_mid = (out_mid - out_mid.min()) / (out_mid.max() - out_mid.min())\n    \n    out_late = apply_filter(gray, kernel_late)\n    out_late = (out_late - out_late.min()) / (out_late.max() - out_late.min())\n\n    # plot\n    axes[0, col].imshow(out_early, cmap='gray')\n    axes[0, col].set_title(f'Early Layer #{col+1}')\n    axes[0, col].axis('off')\n\n    axes[1, col].imshow(out_mid, cmap='gray')\n    axes[1, col].set_title(f'Mid Layer #{col+1}')\n    axes[1, col].axis('off')\n\n    axes[2, col].imshow(out_late, cmap='gray')\n    axes[2, col].set_title(f'Late Layer #{col+1}')\n    axes[2, col].axis('off')\n\nfig.suptitle('First, Middle vs. Last Conv Layer Responses', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n4.2 Creating Image Embeddings\nImage embedding is a process where images are transformed into numerical representations, specifically, lists of numbers that carry informations about the images. While this sounds somewhat similar to the idea of visual words, they are not the same. Think of BoVW as counting how many times specific words appear in a book without caring about grammar or sentence structure– this can identify simple patterns, but cannot summarize the big picture of the book. Image embeddings, on the other hand, are like reading the entire book and summarizing its meaning in a well crafted passage, they capture the bigger picture, context, and nuance.\nWe can build a vocabulary of visual words quite easily, but creating image embeddings usually require using deep neural networks pretrained on millions of images. These networks process the entire image and learn hierarchical, abstract features that are more semantically meaningful.\nHere, we will load the pre-trained ConvNeXt V2 model, pass the image folder to generate embeddings, and save the embeddings in a grid of numbers.\n\n\nCode\n# Read in the pre-trained ConvNeXtV2 model\n# Load the pre-trained ConvNeXtV2 model and image processor\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-base-22k-224\") \n\n# Move the model to the appropriate device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = ConvNextV2Model.from_pretrained(\"facebook/convnextv2-base-22k-224\")\n\n# Move the model to the appropriate device (GPU or CPU)\n_ = model.to(device)\n\n# Define the image directory for later use\nimage_directory = \"../data/richter_kouroi_complete_front_only\"\n\n\n\n\nCode\nbatch_size = 16\nembeddings = []\nvalid_filenames = []\n\nfilenames = df['filename'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i : i + batch_size]\n    images = []\n    for filename in batch_filenames:\n        path = os.path.join(image_directory, filename)\n        try:\n            img = Image.open(path).convert(\"RGB\")\n            images.append(img)\n        except FileNotFoundError:\n            print(f\"Missing: {path}\")\n        except Exception as e:\n            print(f\"Error with {filename}: {e}\")\n\n    if not images:\n        continue\n\n    # Prepare inputs\n    inputs = processor(images=images, return_tensors=\"pt\")\n    pixel_values = inputs[\"pixel_values\"].to(device)\n\n    # Forward through feature extractor\n    with torch.no_grad():\n        outputs = model(pixel_values)   \n    # Global average pool\n    if isinstance(outputs, torch.Tensor):\n        hidden = outputs           \n    else:\n        hidden = outputs.last_hidden_state \n\n    batch_emb = hidden.mean(dim=(2, 3)).cpu().numpy()\n\n    embeddings.extend(batch_emb)   \n    \nembeddings = np.stack(embeddings, axis=0) \n\nnp.save('../data/embeddings/convnextv2_image_embeddings.npy', embeddings)\n\n\n\n\nCode\n# Load embeddings from the saved file\nembeddings = np.load('../data/embeddings/convnextv2_image_embeddings.npy')\n\nprint(embeddings)\n\nprint(f\"The embedding has {embeddings.shape[0]} rows and {embeddings.shape[1]} numbers each row.\")\n\n\nWe printed the embedded results above to see what they look like, and we also displayed the shape of the grid. As you can see, it contains 62 rows representing the 62 images in the dataset, and each row has 1024 numbers representing all the information extracted from each image. From now on, we will use this embedded data instead of the original image data.\n\n\n\n5. Analysis of Image Embeddings\n1024 is way too many numbers for us to examine and understand with our brains. So, we use something techniques called dimensionality reduction to squish the data down to just 2 or 3 dimensions, making it easy to visualize in 2D.\nPrincipal Component Analysis (PCA) is one of such techniques, it is essentially finding the two or three major axes through our huge data along with which the data has the most variations. By PCA we decompose our data with 1024 dimensions (number of cells in each row) to 2 dimensions and represent each image as a point on our scatterplots. We then colour the data with “era” and “material” respectively.\nHere, we will use plotly to create interactive visualizations, feel free to play with it and discuss patterns that you notice.\n\n\nCode\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n# load metadata\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')   \n\n# PCA to 2 components\npca = PCA(n_components=2)\npc2 = pca.fit_transform(embeddings)\n\n# build DataFrame\npc_df = pd.DataFrame(pc2, columns=['PC1','PC2'])\npc_df['filename'] = df['filename'].values\npc_df['era'] = df['era'].values\n\n# interactive scatter\nfig = px.scatter(\n        pc_df,\n        x='PC1',\n        y='PC2',\n        color='era',\n        hover_data=['filename'], \n        title='Interactive PCA of Image Embeddings Colored by Era',\n        width=700, height=500\n    )\n\nfig.show()\n\n\n\n\nCode\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\npc_df['material'] = df['material'].values\n\nfig = px.scatter(\n        pc_df,\n        x='PC1',\n        y='PC2',\n        color='material',\n        hover_data=['filename'], \n        title='Interactive PCA of Image Embeddings Colored by Material',\n        width=700, height=500\n    )\n\nfig.show()\n\n\n\nDiscussion: Did you see any clear patterns of distributions by looking at the visualizations above? How can you interpret the results? What primary “features” do you think the PCA embedding captured in the first two dimensions?\n\n\n\n6. Classification of Kouroi\nArchaeological classification has always been an important issue in archaeology and artifact research. This problem is especially challenging when faced with a large amount of artifact data, or when faced with new artifacts with insufficient information. With the development of machine learning and image recognition technology, the use of computer technology to assist classification has become a trend in the new era of information archaeology. In this section, we would like to provide an example of classifying Kouroi by visual element for your reference.\n\n6.1 Traditional Approach\nIn addition to observing how the labels are clustered based on the embeddings, we can train classifiers to categorize objects into appropriate labels based on the image embeddings directly. A traditional approach is through a technique called logistic regression.\n\n\nCode\nX = embeddings\ny1 = pc_df['era'].tolist()\ny2 = pc_df['material'].tolist()\n\ny1 = np.array(y1)\ny2 = np.array(y2)\n\n\nNote that here we use image embeddings and labels as training data and test data respectively, this is because we want to evaluate the effectiveness of the classifier when dealing with unseen data. After training the classifier for eras, we use it to predict the labels of the test data and compare the results with the real labels. The report is printed below:\n\n\nCode\n# Perform the classification of eras\n# Split the data into training and testing sets\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y1, test_size=0.25, random_state=42, stratify=y1)\n\n# Create a logistic regression model\nclf1 = LogisticRegression(max_iter=1000, random_state=42)\n\nclf1.fit(X_train1, y_train1)\n\ny_pred1 = clf1.predict(X_test1)\n\n# Calculate classification metrics\nclassification_report1 = metrics.classification_report(y_test1, y_pred1, zero_division=0)\n\n# Print the classification report \nprint(\"Classification Report for Eras:\")\nprint(classification_report1)\n\n\nThe key metrics we especially care about here is the accuracy of our classifier, it is defined by\n\\[\n\\text{Accuracy} = \\frac{\\text{True Predictions}}{\\text{True Predictions} + \\text{Flase Predictions}}\n\\]\nIt reflects the proportion of true predictions out of all predictions made using the classifier. However, as shown above in the report, the accuracy of predicting era based on image embedding is not satisfactory. Still, we can visualize the decision boundary of this classifier on our 2D PCA of image embeddings to see what went wrong:\n\n\nCode\n# Prepare your 2D data + labels\nX_pca = pc_df[['PC1','PC2']].values\ny_era = pc_df['era'].values\n\n# Encode eras as integers\nle = LabelEncoder()\ny_enc = le.fit_transform(y_era)\n\n# Train the logistic on the encoded labels\nclf_2d = LogisticRegression(max_iter=1000, random_state=42)\nclf_2d.fit(X_pca, y_enc)\n\n# Build a mesh grid over the plotting area\nx_min, x_max = X_pca[:,0].min() - 1, X_pca[:,0].max() + 1\ny_min, y_max = X_pca[:,1].min() - 1, X_pca[:,1].max() + 1\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\n\n# Predict integer labels on the mesh\nZ = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(7,7))\n\n# Number of classes\nn_classes = len(le.classes_)\n\n# Pick a sequential colormap name:\nbase_cmap_red = plt.cm.Reds\nbase_cmap_blue = plt.cm.Blues\n\n# Sample 7 colors evenly from the light part of the cmap for regions\n# and the dark part for points.\nregion_colors = base_cmap_red(np.linspace(0.1, 0.6, n_classes))\npoint_colors  = base_cmap_blue(np.linspace(0.1, 1.0, n_classes))\n\ncmap_light = ListedColormap(region_colors)\ncmap_bold  = ListedColormap(point_colors)\n\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n\n# scatter original points, mapping back to string labels in the legend\nfor class_int, era_label in enumerate(le.classes_):\n    mask = (y_enc == class_int)\n    plt.scatter(\n        X_pca[mask,0], X_pca[mask,1],\n        color=cmap_bold(class_int),\n        label=era_label,\n        edgecolor='k', s=40\n    )\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Logistic Decision Boundary on 2D PCA Embedding\\n(colored by era)')\nplt.legend(title='Era')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.grid(True)\nplt.show()\n\n\n\nDiscussion: What can you say about this decision boundary?\n\nSimilarly, we can use the same approach to classify material of Kouroi. We first perform a train-test split, then train the classifier, use the trained classifier to predict the labels of the test set, and print out the classification report for quality evaluation.\n\n\nCode\n# Perform the classification of materials\n# Split the data into training and testing sets\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2, test_size=0.25, random_state=42, stratify=y2)\n\n# Create a logistic regression model\nclf2 = LogisticRegression(max_iter=1000, random_state=42)\n\nclf2.fit(X_train2, y_train2)\n\ny_pred2 = clf2.predict(X_test2)\n\n# Calculate classification metrics\nclassification_report2 = metrics.classification_report(y_test2, y_pred2, zero_division=0)\n\n# Print the classification report\nprint(\"Classification Report for Materials:\")\nprint(classification_report2)\n\n\nAs you can see, the accuracy is much higher now, but does that mean the classifier is good? You may have noticed that bronze and marble are classified almost perfectly, but other materials are not. This means that the classifier, while having a high accuracy, may have low precision or recall, as defined below:\n\nPrecision: The ratio of the number of true positives to the number of positive predictions. Precision tells us how often the model predicts correctly.\nRecall: The ratio of the number of true positives to the number of actual positives. Recall answers the question, “What percentage of positive results did we correctly predict?”\n\nWe can also visualize the decision boundary on the 2D PCA of materials\n\n\nCode\n# Prepare 2D data and labels\nX_pca = pc_df[['PC1','PC2']].values\ny_era = pc_df['material'].values\n\n# Encode eras as integers\nle = LabelEncoder()\ny_enc = le.fit_transform(y_era)\n\n# Train the logistic on the encoded labels\nclf_2d = LogisticRegression(max_iter=1000, random_state=42)\nclf_2d.fit(X_pca, y_enc)\n\n# Build a mesh grid over the plotting area\nx_min, x_max = X_pca[:,0].min() - 1, X_pca[:,0].max() + 1\ny_min, y_max = X_pca[:,1].min() - 1, X_pca[:,1].max() + 1\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\n\n# Predict integer labels on the mesh\nZ = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(7,7))\n\n# light colors for regions\ncmap_light = ListedColormap(['#FFCCCC','#CCFFCC','#CCCCFF','#FFE5CC','#E5CCFF'][:len(le.classes_)])\n# bold colors for points\ncmap_bold  = ListedColormap(['#FF0000','#00AA00','#0000FF','#FF8000','#8000FF'][:len(le.classes_)])\n\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n\n# Scatter original points, mapping back to string labels in the legend\nfor class_int, era_label in enumerate(le.classes_):\n    mask = (y_enc == class_int)\n    plt.scatter(\n        X_pca[mask,0], X_pca[mask,1],\n        color=cmap_bold(class_int),\n        label=era_label,\n        edgecolor='k', s=40\n    )\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Logistic Decision Boundary on 2D PCA Embedding\\n(colored by material)')\nplt.legend(title='Era')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.grid(True)\nplt.show()\n\n\n\nDiscussion: Now, going back to the classification reports shown above, do you think the classifier trained on era is a good chronological classifier? What about materials?\n\n\n\n6.2 CNN Classification of Materials\nThe last classifier we’re going to visit today is a neural network classifier, using a Multi-layer Perceptron (MLP) network architecture, which means we’re going to add a classification layer to the ConvNeXt V2 model to classify the material. All the model does here is act as a backbone to observe and extract features of interest in the input image. The MLP process, on the other hand, can be visualized as a number of experts examining different features on an image, then discussing them with each other, and finally voting to reach a final conclusion.\nWe begin by creating the data loader and load the Kouroi data directly from the folder:\n\n\nCode\n# Map the materials to integers\nMAT2IDX = {\n    'Marble': 0,\n    'Bronze': 1,\n    'Other': 2\n}\n\n# Create a custom dataset class for the Kouroi dataset\nclass KouroiDataset(Dataset):\n    def __init__(self, df, img_dir, processor, mat2idx):\n        self.df = df\n        self.img_dir = image_directory\n        self.processor = processor\n        self.mat2idx = MAT2IDX\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(os.path.join(self.img_dir, row.filename)).convert(\"RGB\")\n\n        inputs = self.processor(images=img, return_tensors=\"pt\")\n\n        for k,v in inputs.items():\n            inputs[k] = v.squeeze(0)\n        label = self.mat2idx[row.material]\n        return inputs, label\n\n\n\n\nCode\n# Tain-test split\ntrain_df, test_df = train_test_split(\n    df,\n    test_size=0.25,                 # 20% held out for testing\n    stratify=df[\"material\"],       # preserve class proportions\n    random_state=42\n)\n\n# Initialize the dataset and dataloader\ntrain_ds = KouroiDataset(\n    df=train_df,\n    img_dir=image_directory,\n    processor=processor,\n    mat2idx=MAT2IDX\n)\ntest_ds  = KouroiDataset(\n    df=test_df,\n    img_dir=image_directory,\n    processor=processor,\n    mat2idx=MAT2IDX\n)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)\n\n\nAs mentioned above, here we freeze the model so that training does not change the way it understands the input image, but we are going to add a new classification layer on top of the network so that it can now use the additional knowledge about Kouroi for classification.\n\n\nCode\n# Build the model with convnextv2 as the backbone and a linear layer for classification\nclass Classifier(nn.Module):\n    def __init__(self, backbone_name, num_classes):\n        super().__init__()\n        # Load the ConvNeXtV2 backbone correctly and freeze it\n        self.backbone = ConvNextV2Model.from_pretrained(\n            backbone_name,\n            output_hidden_states=False,\n            output_attentions=False\n        )\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n        embed_dim = self.backbone.config.hidden_sizes[-1]\n\n        # Build a simple 2-layer MLP head\n        self.head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(embed_dim // 2, num_classes)\n        )\n\n    def forward(self, pixel_values):\n        # Forward through the frozen backbone\n        outputs = self.backbone(pixel_values=pixel_values)\n        x = outputs.pooler_output\n\n        # Classification head\n        logits = self.head(x)\n        return logits\n\n# Instantiate and move to device\nmodel = Classifier(\n    backbone_name=\"facebook/convnextv2-base-22k-224\",\n    num_classes=len(MAT2IDX)\n).to(device)\n\n\nHere, after setting up the new model, we will define the training loop and train the model. Please note that this process may take some time, especially when running on devices without a GPU. This also suggests that the high computational power requirement is a drawback when using CNNs for classification.\n\n\nCode\n# Set up the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.head.parameters(), lr=1e-4, weight_decay=0.01)\nepochs = 20  \n\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n        inputs, labels = batch\n        # move to device\n        inputs = {k:v.to(device) for k,v in inputs.items()}\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        loss   = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * labels.size(0)\n\n    avg_loss = total_loss / len(train_ds)\n    print(f\" Epoch {epoch} avg loss: {avg_loss:.4f}\")\n\ntorch.save(model.state_dict(), \"../data/models/mlp_model.pth\")\n\n\nAfter training, we set the model to evaluation mode and assessed the classification quality by printing the confusion matrix and classification report. It is clear that the accuracy does improve with the MLP architecture. However, we still lacked samples of materials other than bronze and marble, which undoubtedly harmed the quality of our training.\n\n\nCode\n# load the trained model\nmodel.load_state_dict(torch.load(\"../data/models/mlp_model.pth\"))\nmodel.to(device)  # move to the right device \n\n# Run one pass over your data in eval mode\nmodel.eval()\n\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        if inputs is None: continue\n        inputs = {k:v.to(device) for k,v in inputs.items()}\n        logits = model(**inputs)\n        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\nreport = classification_report(all_labels, all_preds, target_names=list(MAT2IDX.keys()), zero_division=0)\nprint(\"Classification Report for Materials:\")\nprint(report)\n\n\n\nDiscussion: What difference did you notice in the result? Does this mean that MLP is not applicable to classifying materials?\n\nWhile the MLP classifier via CNN has advantages in dealing with more complex data structures (especially high-dimension nonlinear data), we note that it has two serious drawbacks: 1. it is more susceptible to the randomness of the training-testing split, with greater model variability; and 2. it is more susceptible to overfitting, whereas logistic regression is more susceptible to underfitting (for more detailed information on these concepts discussion can be found in Appendix B). This tells us that no one model is perfect for all situations. We must remain cautious in our choice of models.\n\n\n6.3 Example: Predicting the Material of Unseen Kouroi Photos\nNow, let’s imagine a scenario where we find a new Kouros, but we are not sure what it is made of, and we want to use our trained classifier to classify it based on its image features.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_artifact_path = '../data/example_images/NAMA_3938_Aristodikos_Kouros.jpeg'\nnew_artifact_label = 'A Kouros from Athens' # Suppose this is a new artifact we just discovered\n\nart_new = cv2.imread(new_artifact_path, cv2.IMREAD_GRAYSCALE)\n\n# Show the new image\nplt.figure(figsize=(6, 6))\nplt.imshow(art_new, cmap='gray')\nplt.title(f\"{new_artifact_label}\")\nplt.axis('off')\nplt.show()\n\n\n\nDiscussion: Looking at this photo, how would you classify the era and material of this Kouros?\n\nWe pass it into our trained logistic regression classifier and see how would its era be classified:\n\n\nCode\n# reload the processor and the model\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-base-22k-224\") \n\nmodel = ConvNextV2Model.from_pretrained(\"facebook/convnextv2-base-22k-224\")\n\n# Open as new img\nimg_new = Image.open(new_artifact_path).convert(\"RGB\")\n\ninputs = processor(images=img_new, return_tensors=\"pt\")\npixel_values = inputs[\"pixel_values\"].to(device)\n\n# Forward through feature extractor\nwith torch.no_grad():\n    outputs = model(pixel_values) \n\n# Global average pool\nif isinstance(outputs, torch.Tensor):\n    hidden = outputs           \nelse:\n    hidden = outputs.last_hidden_state \n\nemb_new = hidden.mean(dim=(2, 3)).cpu().numpy()\n\n\npred_era = clf1.predict(emb_new)[0]\n\nprint(\"Predicted Era:\", pred_era)\n\n\nWe then pass it into our trained CNN classifier and see how its material would be classified:\n\n\nCode\n# reload the model\nmodel = Classifier(\n    backbone_name=\"facebook/convnextv2-base-22k-224\",\n    num_classes=len(MAT2IDX)\n).to(device)\nmodel.load_state_dict(torch.load(\"../data/models/mlp_model.pth\"))\nmodel.to(device) \n\n# Resize the model to an appropriate size\npreprocess = transforms.Compose([\n    transforms.Resize(256),                \n    transforms.CenterCrop(224),            \n    transforms.ToTensor(),\n    transforms.Normalize(                   \n        mean=[0.485, 0.456, 0.406],\n        std= [0.229, 0.224, 0.225]\n    ),\n])\n\nidx2mat = {idx: mat for mat, idx in MAT2IDX.items()}\n\ndef predict_image(image_path, model, device):\n    # Load\n    img = Image.open(image_path).convert(\"RGB\")\n    # Preprocess\n    x = preprocess(img)\n    x = x.unsqueeze(0).to(device)\n    # Inference\n    model.eval()\n    with torch.no_grad():\n        logits = model(**{\"pixel_values\": x}      \n                       if isinstance(x, torch.Tensor) else x)\n        pred_idx = logits.argmax(dim=1).item()\n\n    return idx2mat[pred_idx]\n\nmodel.to(device)\n\npredicted_material = predict_image(new_artifact_path, model, device)\nprint(\"Predicted Material:\", predicted_material)\n\n\n\nDiscussion: Do you think the predictions made above are correct? What is your evidence?\n\n\n\n6.4 Additional Note on Fine-tuning\nWhat we didn’t really include here are the more advanced applications of fine-tuning. Fine-tuning refers to the process of taking a pre-trained model (like ConvNeXt V2) and continuing its training on your specific dataset, allowing the model to adapt its learned features to better suit your task. Why we did not cover fine-tuning because it requires more computational resources, careful hyperparameter tuning, and a larger dataset to avoid overfitting. Additionally, fine-tuning can be time-consuming and is often not practical in an introductory or resource-limited setting.\nHowever, fine-tuning has the potential to significantly improve classification quality, especially for irregular data. By allowing the model to update its internal representations based on the unique characteristics of your images, it can learn more relevant features for distinguishing between subtle differences in style, era, or material. For research or production applications with sufficient data and compute, fine-tuning is a powerful next step to achieve higher accuracy and more robust results.\n\n\n\n7. Conclusion\nThrough this notebook, you’ve taken a journey with the example of Richter’s Kouroi from the basics of how computers “see” images to advanced techniques for analyzing and classifying images. You’ve explored how simple pixel values can be transformed into powerful representations using convolution, image embeddings, and neural networks. Along the way, you learned to visualize, cluster, and classify artworks…… these are all skills that are at the heart of modern computer vision.\nRemember, the tools and concepts you’ve practiced here are not just limited to art history or archaeology: they are widely used in fields ranging from medicine to astronomy, and beyond. As you continue your studies, keep experimenting, stay curious, and don’t be afraid to explore new datasets or try more advanced models. The intersection of technology and the humanities is full of exciting possibilities, and your creativity is the key to unlocking them.\nCongratulations on completing this exploration, and we hope you feel inspired to keep learning and discovering the field of Machine Learning!\n\n\nKey Takeaways\n\nThere are multiple different ways for computers to represent and understand content in images, including intensity histograms, BoVW distribution and image embeddings.\nConvolution is a common technique to process and extract different features in input images.\nConvolutional Neural Networks (CNN) are trained models that mimic the way people view images and understand them through convolutional layers.\nImage embeddings produced by a pretrained CNN map each kouros image into a high‑dimensional feature space. We can then apply dimensionality‑reduction techniques such as Principal Component Analysis to visualize clusters among different archaic sculptures.\nFor formal classification of kouroi, such as distinguishing groups/eras or identifying materials, both logistic regression and neural-network classification provide robust methods to assign style labels based on extracted image features, but we should always be aware of issues such as underfitting and overfitting.\nDifferent models may yield different results for image embedding, and sometimes the features recognized from an image are not exactly what we want. We should always be cautious about our results.\n\n\n\nGlossary\n\nComputer Vision: Computer Vision is a field of artificial intelligence that enables computers to “see” and interpret images and videos, mimicking human vision.\nConvolution: In the context of Computer Vision, Convolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\nConvolutional Neural Network (CNN): Convolutional Neural Network is a type of feedforward neural network that learns features via filter (or kernel) optimization. It is distinguished from other neural networks by its superior performance with image, speech or audio signal inputs.\nImage Embedding: Image Embedding is a process where images are transformed into numerical representations, called vectors, that capture the semantic meaning of the image.\nPrincipal Component Analysis (PCA): Principal Component Analysis is a statistical technique that simplifies complex data sets by reducing the number of variables while retaining key information. It does so by finding the major axes where the data sets vary the most.\nLogistic Regression: Logistic Regression is a statistical model used for binary or multiclass classification tasks. It estimates the probability that an input belongs to a particular class by applying the logistic (sigmoid) function to a weighted sum of the input features, making it well-suited for problems where outputs are discrete categories.\nMultilayer Perceptron (MLP): A Multilayer Perceptron is a class of feedforward artificial neural network composed of an input layer, one or more hidden layers of nonlinear activation units, and an output layer. It learns complex patterns by adjusting the weights of connections through backpropagation and is versatile for both classification and regression tasks.\nUnderfitting: Underfitting in machine learning occurs when a model is too simple to capture the underlying patterns in the training data.\nOverfitting: Overfitting in machine learning occurs when a model learns the training data too well, including its noise and random fluctuations, leading to poor performance on new, unseen data.\n\n\n\nAppendix A: Image Data Collection and Preprocessing from .pdf Files\nThis part provides a brief overview of how the data was collected and preprocessed for the analysis, typically how we cropped the images and prepared the metadata.\nWe used the following python script to convert a scanned pdf of Richter (1942) to image files in .jpg format.\nimport fitz  \n\n# Change the filename here if you want to reuse the script for your own project\ndoc = fitz.open(\"kouroiarchaicgre0000rich_1.pdf\") \n\nimport os\nout_dir = \"extracted_images\"\nos.makedirs(out_dir, exist_ok=True)\n\n# Iterate pages\nfor page_index in range(len(doc)):\n    page = doc[page_index]\n    image_list = page.get_images(full=True)  # get all images on this page\n\n    # Skip pages without images\n    if not image_list:\n        continue\n\n    # Extract each image\n    for img_index, img_info in enumerate(image_list, start=1):\n        xref = img_info[0]                   \n        base_image = doc.extract_image(xref)  \n        image_bytes = base_image[\"image\"]     \n        image_ext   = base_image[\"ext\"]      \n\n        # Write to file\n        out_path = os.path.join(\n            out_dir,\n            f\"page{page_index+1:03d}_img{img_index:02d}.{image_ext}\"\n        )\n        with open(out_path, \"wb\") as f:\n            f.write(image_bytes)\n\nprint(f\"Saved all images to {out_dir}\")\nThe following script cropped the photos by applying convolution.\nimport cv2\nimport glob\nimport os\n\n# Folder containing your page images\ninput_folder = \"extracted_images\"\noutput_folder = \"cropped_photos\"\nos.makedirs(output_folder, exist_ok=True)\n\ndef extract_photos_from_page(image_path, min_area=5000):\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # Blur and threshold to get binary image\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY_INV)\n    \n    # Dilate to merge photo regions\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n    dilated = cv2.dilate(thresh, kernel, iterations=2)\n    \n    # Find contours\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    crops = []\n    for cnt in contours:\n        x, y, w, h = cv2.boundingRect(cnt)\n        area = w * h\n        # Filter by area to remove small artifacts\n        if area &gt; min_area:\n            crop = img[y:y+h, x:x+w]\n            crops.append((crop, (x, y, w, h)))\n    return crops\n\n# Process all pages\nfor img_path in glob.glob(os.path.join(input_folder, \"*.*\")):\n    base = os.path.splitext(os.path.basename(img_path))[0]\n    photos = extract_photos_from_page(img_path)\n    for idx, (crop, (x, y, w, h)) in enumerate(photos, start=1):\n        out_file = os.path.join(output_folder, f\"{base}_photo{idx}.jpg\")\n        cv2.imwrite(out_file, crop)\n\nprint(f\"Saved all images at {output_folder}\")\nThe following script employed tesseract OCR engine to detect pure text images and filter out photos of Kouros. You may want to visit the GitHub repository https://github.com/tesseract-ocr/tesseract to see how to install and setup appropriately.\nimport os\nimport shutil\nfrom PIL import Image\nimport pytesseract\n# Ensure you have Tesseract installed and pytesseract configured correctly.\n# On Windows, you might need:\n# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n\n# Folders\ninput_folder = \"cropped_photos\"\ntext_folder = \"text_crops\"\nphoto_folder = \"filtered_photos\"\nos.makedirs(text_folder, exist_ok=True)\nos.makedirs(photo_folder, exist_ok=True)\n\n# Threshold for text length to consider as \"text-only\"\n# You can also adjust this threshold based on your specific needs.\nTEXT_CHAR_THRESHOLD = 2 # Be careful with this threshold, do remember to check the results manually\n\n\nfor filename in os.listdir(input_folder):\n    path = os.path.join(input_folder, filename)\n    img = Image.open(path)\n\n    # Perform OCR to extract text\n    extracted_text = pytesseract.image_to_string(img)\n\n    # Classify based on length of extracted text\n    if len(extracted_text.strip()) &gt;= TEXT_CHAR_THRESHOLD:\n        dest = os.path.join(text_folder, filename)\n    else:\n        dest = os.path.join(photo_folder, filename)\n\n    shutil.move(path, dest)\n    print(f\"Moved {filename} -&gt; {os.path.basename(dest)}\")\n\nprint(\"Filtering complete\")\nThis script creates a .csv file for mannual labelling.\nimport os, re\nimport pandas as pd\n\n# Scan your filtered_photos folder\nrecords = []\n\n# Updated regex to match \"page&lt;number&gt;_img&lt;number&gt;_photo&lt;number&gt;.&lt;ext&gt;\"\npattern = re.compile(r\"page(\\d+)_img\\d+_photo(\\d+)\\.(?:png|jpe?g)\", re.IGNORECASE)\n\nfor fn in os.listdir(\"richter_kouroi_head_front_only\"):\n    m = pattern.match(fn)\n    if not m:\n        continue\n    page = int(m.group(1))\n    photo_idx = int(m.group(2))\n    records.append({\n        \"filename\": fn,\n        \"page\": page,\n        \"group\": \"\",    # blank for manual entry\n        \"era\": \"\",  # blank for manual entry\n        \"material\": \"\"  # blank for manual entry\n    })\n\n# Build DataFrame\ndf = pd.DataFrame(records)\n\ndf.sort_values([\"page\", \"filename\"], inplace=True)\n\n# Save out to CSV for manual labeling\ndf.to_csv(\"label_template.csv\", index=False)\nYou can try out the scripts with your interested pdf files yourself by running them in a python environment.\n\n\nAppendix B: The Risk of Underfitting and Overfitting\nIn the context of machine learning, two common pitfalls are underfitting and overfitting.\nUnderfitting occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test sets. This is often seen when using models like logistic regression on complex, non-linear datasets, as shown in the left panel above, where the decision boundary fails to separate the classes effectively. On the other hand, overfitting happens when a model is excessively complex, such as a deep neural network with many layers, and learns not only the true patterns but also the noise in the training data. This leads to excellent performance on the training set but poor generalization to new, unseen data, as illustrated in the right panel where the decision boundary is overly intricate.\nLet’s examine the two problems with a simulated two-class data:\n\n\nCode\n# Generate a simulater two-class data\nX, y = make_moons(n_samples=500, noise=0.40, random_state=0)\n\n# Visualize the simulated data\nplt.figure(figsize=(6, 4))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolor='k')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Simulated Two-Class Data')\nplt.show()\n\n\nWe can clearly see that the data is very non-linear, which gives us a hint that the right model should be able to account for non-linear relationships. However, for demonstration purposes, I will be using logistic regression to generate an underfitting classifier; and while MLP is suitable for use here, I will let it generate an overfitting classifier by significantly oversizing the neurons and iterations.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=1\n)\nunder = LogisticRegression()\n\nover  = MLPClassifier(hidden_layer_sizes=(200,200,200),\n                      max_iter=2000,\n                      random_state=1)\n\n# Train both models\nunder.fit(X_train, y_train)\nover.fit(X_train, y_train)\n\n\nThen, we get the decision boundaries of the two classifiers and visualize them with the test data, what do you find about their accuracy?\n\n\nCode\n# Build grid\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Get decision boundary\nZu = under.predict_proba(grid)[:,1].reshape(xx.shape)\nZo = over.predict_proba(grid)[:,1].reshape(xx.shape)\n\n# Plot side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n\nfor ax, Z, title in [\n    (ax1, Zu, 'Extreme Underfit'),\n    (ax2, Zo, 'Extreme Overfit')\n]:\n    ax.contourf(xx, yy, Z&gt;0.5, alpha=0.3)\n    # draw precise decision boundary P=0.5\n    ax.contour(   xx, yy, Z, levels=[0.5], colors='k', linewidths=1.5)\n\n    ax.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor='k')\n    ax.set_title(title)\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n\nfig.suptitle('Underfitting vs. Overfitting in Classification', fontsize=16)\nplt.tight_layout(rect=[0,0.03,1,0.95])\nplt.show()\n\n\nWe can tell easily from the above visualization that both scenarios are harmful: underfitting prevents the model from making meaningful predictions, while overfitting results in unreliable predictions on real-world data. The example above also highlights the necessity to validate classifier quality using test data.\nWhile underfitting and overfitting can seem scary, there are many tools that have been developed to address this issue. We can adjust the complexity of the model, use regularization techniques, collect more data, or employ cross-validation to find a balance that generalizes well to new examples. These are left for you to explore on your own.\n\n\nReferences\n\nRichter, G. M. A. (1970). Kouroi: Archaic Greek youths: A study of the development of the Kouros type in Greek sculpture. Phaidon. Accessed through Internet Archive https://archive.org/details/kouroiarchaicgre0000rich.\nPinecone. Embedding Methods for Image Search. Accessed through Pinecone https://www.pinecone.io/learn/series/image-search/.\nIBM. What are convolutional neural networks? https://www.ibm.com/think/topics/convolutional-neural-networks\nHugging Face. Image Classification. https://huggingface.co/docs/transformers/tasks/image_classification\nColeman, C., Lyon, S., & Perla, J. (2020). Introduction to Economic Modeling and Data Science. QuantEcon. Retrieved from https://datascience.quantecon.org/\nWoo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I. S., & Xie, S. (2024). ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders. arXiv preprint arXiv:2301.00808v1."
  },
  {
    "objectID": "docs/ECON-227/llm_distributions.html",
    "href": "docs/ECON-227/llm_distributions.html",
    "title": "ECON 227 - How Do Large Language Models Predict?",
    "section": "",
    "text": "This notebook explores how large language models (LLMs) work by linking their probability-based predictions to real-world applications. We start with the basics of next-word prediction, then extend the idea to forecasting stock prices using news sentiment.\n\nPrerequisite\nBefore you start, make sure you have the required libraries installed, if not, uncomment the lines below (i.e. remove the #) and run the cell to install them:\n\n# # Core packages\n# !pip install \\\n#     yfinance \\\n#     finvizfinance \\\n#     transformers \\\n#     pandas \\\n#     numpy \\\n#     statsmodels \\\n#     holidays \\\n#     plotly \\\n#     ipywidgets \\\n#     scikit-learn\n# !pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu --quiet\n\nImportant: Run this cell to load the libraries we need for running this notebook.\n\n# load libraries we need to run this notebook\nimport os\nimport glob\nimport warnings\nfrom datetime import datetime, timedelta\nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport yfinance as yf\nfrom finvizfinance.quote import finvizfinance\nfrom transformers import pipeline\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport holidays\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom forecast_plot import create_plot\n\n\n\n1 Introduction: Distribution of LLM Predictions\nLarge language models like ChatGPT do something that seems very simple: Next word prediction.\nWhat does that mean? It means that given a sequence of words, the model predicts the next word in the sequence. For example, if the input is “The cat sat on the”, the model might predict “mat” as the next word.\n\nWe saw an example of predicting just one word. These models predict only one word at a time, but they can do this for very long sequences of words.\nFor example: “bob went to the store” to buy some milk.\n\nHere, let’s say we will output a total of \\(T\\) words to form a sentence, \\(w_1, w_2, \\ldots, w_{t-1}, w_t\\) are the words in the sentence and the subscript \\(t\\) means the word is at the position \\(t\\).\nWhat the model does is learning the probability distribution of the next word given the previous words. It is to create a list of non-negative numbers, one for each possible next word, that add up to 1.\nGiven the previous words \\(w_1, w_2, \\ldots, w_{t-1}\\), the probability of predicting the next word \\(w_{t}\\) is called conditional probability: it essentially measures for all words in the vocabulary, “the probability of the next word being this word, based on what was already said”.\nMathematically, this is defined as below:\n\\[\nP(w_t \\mid w_1, w_2, \\ldots, w_{t-1}) = \\frac{P(w_1, w_2, \\ldots, w_{t-1}, w_t)}{P(w_1, w_2, \\ldots, w_{t-1})}\n\\]\nLLMs can approximate this probability by learning from training datasets. Then, when they make predictions, they will select the word at position \\(t\\) as the word with the maximum \\(P(w_t \\mid w_1, w_2, \\ldots, w_{t-1})\\).\nWith chain rule, the model can also use the conditionals for every position to calculate the probability of a whole sentence:\n\\[\nP(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^T P(w_t\\mid w_1, \\ldots w_{t-1})\n\\]\nThese models give a probability distribution over the entire vocabulary (all the words the model was trained on). We can then pick the word with the highest probability as the next word or we can sample from this distribution to get more varied (creative) outputs.\nLet’s look at an example of how this works in practice:\n\nwarnings.filterwarnings(\"ignore\")\n\n# Example vocabulary\nvocab = ['buy', 'some', 'milk', 'along', 'the', 'way']\n\n# Probabilities at each step (toy example)\nprobs_step1 = [0.8, 0.05, 0.05, 0.03, 0.04, 0.03]  # 'buy' high\nprobs_step2 = [0.05, 0.7, 0.1, 0.05, 0.05, 0.05]   # 'some' high\nprobs_step3 = [0.05, 0.05, 0.75, 0.05, 0.05, 0.05] # 'milk' high\n\nprob_distributions = [probs_step1, probs_step2, probs_step3]\nstep_labels = ['Step 1: Predict \"buy\"', 'Step 2: Predict \"some\"', 'Step 3: Predict \"milk\"']\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 8), sharey=True)\n\nfor i, ax in enumerate(axes):\n    sns.barplot(x=vocab, y=prob_distributions[i], palette='muted', ax=ax)\n    ax.set_title(step_labels[i])\n    ax.set_ylim(0, 1)\n    ax.set_ylabel('Probability' if i == 0 else '')\n    ax.set_xlabel('Vocabulary')\n    # Highlight the max prob bar in gold\n    max_idx = prob_distributions[i].index(max(prob_distributions[i]))\n    ax.bar(max_idx, prob_distributions[i][max_idx], color='gold')\n\n    for patch, token, prob in zip(ax.patches, vocab, prob_distributions[i]):\n        height = patch.get_height()\n        ax.annotate(\n            f\"{prob:.2f}\",                            \n            xy=(patch.get_x() + patch.get_width() / 2, height),  \n            xytext=(0, 3),       \n            textcoords=\"offset points\",\n            ha='center', va='bottom',\n            fontsize=9\n        )\n\nplt.tight_layout()\nplt.show()\n\n\n\n2 Sampling and Temperature\nThe example above shows us the process of generating \\(T=3\\): at each step, the model calculates the conditional probability of the next word and then selects the word with the highest probability to insert into the sentence. The final output we obtain is “buy some milk”.\nTo get more creative responses you change the distribution at the output where you pick the next word. Very simply this involves making the distribution sharper or flatter. If you make the distribution sharper, you are more likely to pick the word with the highest probability. If you make it flatter, you are more likely to pick a word that is not the most probable one.\nThis is called temperature. A higher temperature makes the distribution flatter, while a lower temperature makes it sharper. You would want to use a temperature of more than 1 \\((1.2-1.5)\\) for creative responses, and a temperature of less than 1 \\((0.1 - 0.5)\\) for more focused responses. For a balanced response, you can use a temperature of \\(0.7-1\\). Another set of parameters are called top-p and top-k sampling.\n\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\n\n# Example vocabulary\nvocab = [\n    'apple', 'banana', 'cherry', 'date', 'elderberry',\n    'fig', 'grape', 'honeydew', 'kiwi', 'lemon',\n    'mango', 'nectarine', 'orange', 'papaya', 'quince',\n    'raspberry', 'strawberry', 'tangerine', 'ugli', 'watermelon'\n]\n\n# Normalize to sum to 1\nvocab_size = len(vocab)\nbase_probs = np.random.rand(vocab_size)\nbase_probs /= base_probs.sum()  \n\ndef apply_temperature(probs, temp):\n    logits = np.log(probs + 1e-20)\n    scaled_logits = logits / temp\n    exp_logits = np.exp(scaled_logits)\n    return exp_logits / exp_logits.sum()\n\ntemperatures = [0.5, 1.0, 1.5]\ndistributions = [apply_temperature(base_probs, t) for t in temperatures]\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 8), sharey=True)\nfor i, (ax, dist, temp) in enumerate(zip(axes, distributions, temperatures)):\n    sns.barplot(x=vocab, y=dist, palette='muted', ax=ax)\n    ax.set_title(f'Temperature = {temp}')\n    ax.set_ylim(0, 0.25)\n    ax.set_ylabel('Probability' if i == 0 else '')\n    ax.set_xlabel('Vocabulary')\n    ax.tick_params(axis='x', rotation=90)\n\n    # Highlight the max-prob bar in gold\n    max_idx = dist.argmax()\n    ax.patches[max_idx].set_color('gold')\n\n    # Annotate each bar with its probability\n    for patch, prob in zip(ax.patches, dist):\n        height = patch.get_height()\n        ax.annotate(\n            f\"{prob:.2f}\",\n            xy=(patch.get_x() + patch.get_width() / 2, height),\n            xytext=(0, 3),            # 3 points vertical offset\n            textcoords=\"offset points\",\n            ha='center', va='bottom',\n            fontsize=9\n        )\n\nplt.tight_layout()\nplt.show()\n\nIn the example above, we see how the probability distribution changes with different temperatures. A high temperature (1.5) results in a flatter distribution, meaning the model is more likely to sample from less probable tokens, while a low temperature (0.5) results in a sharper distribution, favoring the most probable tokens.\nNote that while this is with words and language, the same idea applies to any sequential data, like stock prices, weather data, etc. The model looks at what happened before and tries to guess what comes next.\nIf you are interested in understanding the inner workings of these models, take a look at the interactive visualization at The Illustrated Transformer . It provides an excellent, hands-on way to explore the core ideas behind modern language models.\nSo, just like it guesses the next word in a sentence, it can guess the next day’s temperature or the next movement in a stock price, based on the pattern it sees in the earlier numbers.\n\nn = 10\nm = 9\n\n# Generate actual data: random walk + small trend\nactual_data = np.cumsum(np.random.normal(0, 1, n)) + 50\n\n# Predictor approximates entire data closely with small noise everywhere\npredicted = actual_data + np.random.normal(0, 0.2, n)\n\n# Posterior uncertainty: low and roughly constant over entire period\nposterior_std = np.full(n, 0.3)\n\nupper = predicted + posterior_std\nlower = predicted - posterior_std\n\nplt.figure(figsize=(6,6))\nplt.plot(range(n), actual_data, label=\"Actual Data\", color='blue')\nplt.plot(range(n), predicted, label=\"Predicted\", color='orange')\nplt.axvline(x=m-1, color='black', linestyle='--', label=\"Observed / Future Split\")\n\nplt.xlabel(\"Time\")\nplt.ylabel(\"Value\")\nplt.title(\"Stock price over time.\")\nplt.legend()\nplt.show()\n\nTypically, when building a model to predict stock prices, you would use more information than just the past prices. For example, you might include things like public sentiment (how people feel about the stock), news headlines, or other features that could influence the price.\nIn the examples below, that’s exactly what we’re going to try! We’ll see how adding these extra features can help the model make better predictions about what happens next.\n\n\n3 Predicting Stock Prices from News Headlines with AI\nJust like LLMs predict the next word based on the context of prior words:\n\\[\nP(w_t \\mid w_1, w_2, \\ldots, w_{t-1})\n\\]\nWe can use similar models to predict the next value in a time series, like stock prices or percentage changes in returns.\nWhile prediction of next word in language models is inherently univariate that the model predicts the next word based solely on the sequence of previous words, predicting daily stock returns is often a multivariate problem as more exogenous factors must be taken into account. Here, we don’t just use past stock prices (returns) as context, but also incorporate additional features such as public sentiment from news headlines.\nIn other words, instead of predicting the next token from a single stream (words), we predict the next value in a time series using multiple sources of information: historical price data and external signals like news sentiment. This richer, multivariate context allows the model to capture more complex relationships and potentially make more accurate forecasts.\nThe comparison of word prediction and stock price prediction is given as follows:\n\n\n\n\n\n\n\nWord Prediction\nStock Price Prediction\n\n\n\n\nPrevious words\nPast daily returns + aggregated news sentiment\n\n\nNext word prediction\nFuture return prediction\n\n\nAttention to important words\nFeature weights on returns and sentiment\n\n\nTemperature to control randomness\nConfidence or prediction intervals in forecasts\n\n\nWord probability distribution\nForecasted return distribution\n\n\n\nIn this case study, we use a language model to analyze real-time news headlines alongside historical stock prices in order to forecast short-term changes in stock value.\nYou will:\n\nCollect news headlines about real companies (like Amazon or Starbucks)\nUse a pre-trained AI model to classify the sentiment (positive or negative) of these news headlines\nCombine that with stock prices\nUse a forecasting model to predict future price changes\nVisualize your results interactively\n\n\n\nPreview the News Data\nHere we use the finvizfinance packages to retrieve real-time news headlines for companies like Starbucks (SBUX).\n\n# The code below will give us a snapshot of the 100 most recent news headlines for a particular stock in the last 30 days.  \n\ndef get_news_data(ticker):\n    stock = finvizfinance(ticker)\n    news_df = stock.ticker_news()\n    news_df = pd.DataFrame(news_df)\n\n    # Drop NaN and clean whitespace\n    news_df = news_df.dropna(subset=[\"Title\"])\n    news_df = news_df[news_df[\"Title\"].str.strip() != \"\"]\n    news_df['Title'] = news_df['Title'].str.lower()\n    news_df['Date'] = pd.to_datetime(news_df['Date'])\n    news_df['DateOnly'] = news_df['Date'].dt.date\n    news_df[\"Ticker\"] = ticker.upper()\n    # Remove the 'Date' column\n    news_df = news_df.drop(columns=['Date'])\n\n    return news_df.reset_index(drop=True)\n\n# For the sake of reproducibility (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n\n# Uncomment the line below get a more recent snapshot of the data! \n# SBUX_news_df = get_news_data(\"SBUX\")\n# SBUX_news_df.to_csv(\"datasets/SBUX_news.csv\", index=False)\n\nLet’s look at what the cleaned news data looks like. We’ll start with Starbucks (SBUX).\nEach row is a headline, what website it was from, the date it was published and a ticker indicating what stock it is for.\n\nSBUX_news_df = pd.read_csv(\"datasets/SBUX_news.csv\")\nSBUX_news_df.head()\n\nLet’s take a closer look at the news titles. What would you say about their sentiment?\n\ntitles = SBUX_news_df['Title'].tolist()\nprint(\"News on Starbucks:\\n\")\nfor i, title in enumerate(titles[:5], start=1):\n    print(f\"{i}. {title}\")\n\n\n\n4 Classifying Headline Sentiment with LLM\nEarlier, we explored how large language models (LLMs) predict the next word by learning the probability distribution of possible outcomes based on context.\nNow, we apply a similar idea to entire sentences: in this case, financial news headlines. Instead of predicting the next word, the model assigns a probability to each sentiment category (e.g. POSITIVE, NEGATIVE, or NEUTRAL).\n\nHow it works:\n\nA pre-trained model reads the headline.\nIt assigns probabilities to the sentiment labels.\nWe keep only positive or negative headlines, since those are more likely to affect stock prices.\n\nThis is like asking:\n\nGiven the words in this sentence, what is the most likely emotion behind it?\n\nWe will rely on some AI tools to figure out.\nBERT (short for Bidirectional Encoder Representations from Transformers) is a pretrained language model that learns a word’s meaning by looking at the words before and after it, so it understands context and tone well. And Hugging Face is an open-source platform and Python library that hosts many pretrained models (like BERT) and provides easy tools.\nIn this notebook, we are going to use Hugging Face’s pipeline() function to load a RoBERTa model: an optimized version of the BERT model trained to understand the tone of text. Although it is specifically trained on tweets and social media text. It’s well-suited to handling short, informal writing like news headlines.\nThis builds directly on our earlier discussion of LLMs predicting probability distributions, but here, the prediction is over sentiment classes rather than words.\nWe now apply a pre-trained large language model to each headline.\nIt returns:\n\nPOSITIVE: news that sounds good (e.g., “profits surge”)\nNEGATIVE: news that sounds bad (e.g., “lawsuit filed”)\n\n\nCan you think of a positive and negative news headline?\n\nNote that we will skip NEUTRAL news to focus on strong market signals.\n\nwarnings.filterwarnings(\"ignore\")\n# Here we are just specifying the classifier (AI model) that decides on a sentiment \n\nclassifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", device=-1)\n\n# define a function to classify sentiment of each text \ndef classify_sentiment(text):\n    return classifier(text)[0][\"label\"].upper()\n\n# define classify_sentiment to entire dataframe\ndef apply_sentiment(news_df):\n    news_df[\"Sentiment\"] = news_df[\"Title\"].apply(classify_sentiment)\n    return news_df # [news_df[\"Sentiment\"] != \"NEUTRAL\"] # remove neutral headlines\n\ndef process_sentiment(news_df):\n    grouped = news_df.groupby([\"DateOnly\", \"Sentiment\"]).size().unstack(fill_value=0)\n    grouped = grouped.reindex(columns=[\"POSITIVE\", \"NEGATIVE\"], fill_value=0)\n    # Calculate a rolling 7-day total of positive headlines\n    grouped[\"7day_avg_positive\"] = grouped[\"POSITIVE\"].rolling(window=7, min_periods=1).sum()\n    # Calculate a rolling 7-day total of negative headlines\n    grouped[\"7day_avg_negative\"] = grouped[\"NEGATIVE\"].rolling(window=7, min_periods=1).sum()\n    # Calculate the percentage of positive headlines each day (out of total positive + negative)\n    grouped[\"7day_pct_positive\"] = grouped[\"POSITIVE\"] / (grouped[\"POSITIVE\"] + grouped[\"NEGATIVE\"])\n\n    return grouped.reset_index()\n\nLet’s use our model to see wether the sentence “I hate bananas” is negative or positive\n\nTry changing the words inside classify sentiment to see wether its is classified positive or negative\n\n\nclassify_sentiment(\"I hate bananas\")        # You can change the words inside the function to test anything you want! \n\nNow let’s apply this to our entire SBUX_news_df and see how each news headline is classified.\n\nnews_df = apply_sentiment(SBUX_news_df)       # Classify sentiment of each Starbucks news headline\nnews_df.head()\n\n\nprint(\"News Titles and Sentiments\\n\")\n\nfor i, row in news_df.iterrows():\n    if i &lt; 5:\n        print(f\"Title: {row['Title']}\\nSentiment: {row['Sentiment']}\\n\")\n\nLet’s summarize the sentiment results by date. For each day we count the number of positive and negative headlines, then calculate 7-day moving averages and the daily percentage of positive news. This gives us a quick overview of news sentiment trends over time.\n\nsentiment_df = process_sentiment(news_df)       # Summarize daily sentiment statistics\nsentiment_df.head()                             # Display the first 5 rows of the dataframe\n\n\n\nWhat this table shows?\nThis table is the output of a function that summarizes news sentiment over time.\nEach row corresponds to a specific date and gives us a snapshot of how positive or negative the news headlines were for that day and the surrounding week.\nMarkets move not just based on today’s headlines, but on short-term trends in public sentiment.\nThis table lets us track how optimism or pessimism is building up over time, which we can later use to help predict stock price movements.\n\n\nWhy is this useful?\n\nWe know that in the last 100 news stories about NVIDIA 24 have been positive and 5 have been negative.\nIf the 7day_pct_positive is rising, the overall tone of news is getting more optimistic.\nIf it’s dropping, it could mean public or investor concern is growing.\nWe can later plot this and compare it against stock price to see if sentiment influences market behavior.\n\n\n\n\n5 Getting Stock Price Data\nWe are using the yfinance package to get real stock price data directly from Yahoo Finance. The function below helps us download historical stock prices and compute the daily percentage change in the stock’s closing price. To make sure our results are replicable, this data has been saved as a .csv file “datasets/SBUX_price.csv”\nThis allows us to analyze how stock prices change over time.\n\n# The code below will give us a snapshot of stock prices for the duration in we have news headlines for.  \n\ndef get_stock_data(ticker, start, end):\n    stock = yf.download(ticker, start=start, end=end)\n\n    # Flatten columns if multi-indexed (e.g., multiple tickers)\n    if isinstance(stock.columns, pd.MultiIndex):\n        stock.columns = ['_'.join(col).strip() for col in stock.columns]  # \"Close_SBUX\", etc.\n        close_col = f\"Close_{ticker}\"\n    else:\n        close_col = \"Close\"\n\n    stock[\"Pct_Change\"] = stock[close_col].pct_change() * 100\n    stock.reset_index(inplace=True)\n    stock[\"DateOnly\"] = pd.to_datetime(stock[\"Date\"])\n    return stock[[\"DateOnly\", \"Pct_Change\"]]\n\n# merges sentiment and stock data by date, and lags sentiment by one day to align with price changes.\ndef combine_data(sent_df, stock_df):\n    sent_df = sent_df.reset_index(drop=True)\n    stock_df = stock_df.reset_index(drop=True)\n\n    sent_df[\"DateOnly\"] = pd.to_datetime(sent_df[\"DateOnly\"])\n    stock_df[\"DateOnly\"] = pd.to_datetime(stock_df[\"DateOnly\"])\n\n    return (\n        pd.merge(sent_df, stock_df, on=\"DateOnly\", how=\"inner\")\n          .assign(lagged_sentiment=lambda df: df[\"7day_pct_positive\"].shift(1))\n    )\n\n\n# For the sake of reproducibility (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n\n# Uncomment the line below get a more recent snapshot of the data ! \n\n# SBUX_news_df[\"DateOnly\"] = pd.to_datetime(SBUX_news_df[\"DateOnly\"])\n# start_date = SBUX_news_df[\"DateOnly\"].min() - pd.Timedelta(days=1) \n# end_date = SBUX_news_df[\"DateOnly\"].max() + pd.Timedelta(days=1) \n# stock_df = get_stock_data(\"SBUX\", start_date, end_date)\n# stock_df.to_csv(\"datasets/SBUX_price.csv\", index=False)\n\nSBUX_price_df = pd.read_csv(\"datasets/SBUX_price.csv\")\nSBUX_price_df.head()\n\nNow we bring together the sentiment summary data and the stock price changes. By merging these two datasets, we can analyze how changes in news sentiment might be related to changes in Starbucks’ stock price. This combined dataset will help us answer the question: “Does positive news sentiment lead to an increase in stock price?”\n\ncombined_df = combine_data(sentiment_df, SBUX_price_df)\ncombined_df\n\n\nForecasting Future Stock Changes with Sentiment\nIn this notebook, we will try to predict future stock price changes using the SARIMAX model, a powerful forecasting model that allows us to include external information, in our case, public sentiment. For those who are interested in learning more about the SARIMAX model and its implementation in Python, see this comprehensive guide on GeeksforGeeks.\nBelow, we define two key functions for our forecasting workflow:\n\nget_future_dates(): Returns the next business days for which we want to make predictions.\nfit_and_forecast(): Uses both historical stock prices and recent news sentiment to predict how Starbucks’ (SBUX) stock price might change over the next few days. This function fits a SARIMAX model, which incorporates both past price data and the influence of news sentiment, and then generates forecasts along with confidence intervals.\n\nNext, we will use a pre-defined create_plot() function to generate an interactive line chart that visualizes both stock price percentage changes and sentiment trends over time. This allows us to explore the relationship between market sentiment and stock performance in a clear, interactive way.\nBy combining these functions, we can see how shifts in news sentiment may impact SBUX’s future stock movements. The forecasting approach essentially answers the question: “Given recent sentiment, what does the model predict for this stock’s price in the coming days?”\n\n# business days we should forecast for \ndef get_future_dates(start_date, num_days):\n    if not isinstance(start_date, pd.Timestamp):\n        start_date = pd.to_datetime(start_date)\n\n    us_holidays = holidays.US()\n    future_dates = []\n    current_date = start_date + pd.Timedelta(days=1)\n\n    while len(future_dates) &lt; num_days:\n        if current_date.weekday() &lt; 5 and current_date.date() not in us_holidays:\n            future_dates.append(current_date)\n        current_date += pd.Timedelta(days=1)\n\n    return future_dates\n\n# prediction model\ndef fit_and_forecast(combined_df, forecast_steps=3):\n    combined_df = combined_df.dropna(subset=['Pct_Change', 'lagged_sentiment'])\n\n    endog = combined_df['Pct_Change']\n    exog = combined_df['lagged_sentiment']\n\n    model = SARIMAX(endog, exog=exog, order=(1, 1, 1))\n    fit = model.fit(disp=False)\n\n    future_dates = get_future_dates(combined_df.index[-1], forecast_steps)\n    future_exog = np.tile(combined_df['lagged_sentiment'].iloc[-1], forecast_steps).reshape(-1, 1)\n\n    forecast = fit.get_forecast(steps=forecast_steps, exog=future_exog)\n    return forecast.predicted_mean, forecast.conf_int(), future_dates\n\n\n\nPlot: Sentiment vs Stock % Change Forecast\nThis chart shows how news sentiment about a company relates to its stock price changes over time, and how we can use this relationship to make simple predictions.\n\nThe blue line shows the standardized 7-day average of positive sentiment extracted from financial news headlines. A higher value means news sentiment was more positive.\nThe green line shows the actual daily percentage change in the company’s stock price.\nThe red line shows our simple forecast of future stock movement based on past sentiment trends. The shaded red area represents uncertainty around the forecast (a 95% confidence interval).\n\nWe want to see whether the emotions in the news (blue) can help us predict price changes (green and red). If they move together, it suggests that public mood might influence investor behavior.\n\nThis plot helps us visualize correlations and test basic forecasting using real-world data like stock prices and media sentiment.\n\n\ncombined_df['DateOnly'] = pd.to_datetime(combined_df['DateOnly'])  # convert to datetime\ncombined_df.set_index('DateOnly', inplace=True)  # use as index\ncombined_df.sort_index(inplace=True)  # ensure time order\n\nforecast_mean, forecast_ci, forecast_index = fit_and_forecast(combined_df)\ncreate_plot(combined_df, forecast_mean, forecast_ci, forecast_index)\n\n\nDisclaimer: This is a simplified model. In reality, stock prices are influenced by many factors, such as interest rates, earnings reports, geopolitical events, and investor speculation. This chart only considers one variable: news sentiment. It should not be used for actual trading decisions.\n\n\nThink about what you would include in a model other than news to help us predict how a stock price might change ?\n\n\n\n\n6 How Do AIs Feel About AI?\nIn this section of the notebook, we explore how our AI Sentiment Analysis model feels about AI-related stocks. That’s a mouthful!\nThe goal is to see if public sentiment (as captured by the headlines) is generally optimistic or pessimistic toward leading AI companies — as interpreted by another AI (we are using BERT here!).\nWe’ll start by selecting the top 10 AI stocks in 2025 as suggested by financial news magazine, Forbes - ACN (Accenture) - ADBE (Adobe) - AMD (Advanced Micro Devices) - APP (Applovin) - AVGO (Broadcom) - CRM (Salesforce) - MRVL (Marvell Technology) - MU (Micron Technology) - NVDA (NVIDIA) - QCOM (Qualcomm)\nFor each company, we will: 2. Classify the sentiment using our RoBERTa-based model 1. Pull recent news headlines 3. Analyze the 7-day rolling trends in public sentiment 4. Compare results across companies\nLet’s find out if the machines love themselves ?\n\n# This code collects recent news headlines for each company in our AI stock list. It uses our  `get_news_data()` which we defined above. \n\nai_tickers = [\"ACN\", \"ADBE\", \"AMD\", \"APP\", \"AVGO\", \"CRM\", \"MRVL\", \"MU\", \"NVDA\", \"QCOM\"] # Top 10 AI stocks \n\nall_news = []\n\ndef fetch_all_news(ticker_list):\n    all_news = []\n    for ticker in ticker_list:\n        try:\n            news = get_news_data(ticker)\n            all_news.append(news)\n        except Exception as e:\n            print(f\"Failed to get news for {ticker}: {e}\")\n    if all_news:\n        return pd.concat(all_news, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n# For the sake of reproducability (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n\n# Uncomment the line below get a more recent snapshot of the data !\n\n# Usage :\n# AI_combined_news_df = fetch_all_news(ai_tickers)\n# #AI_combined_news_df = pd.concat(all_news, ignore_index=True)\n# AI_combined_news_df.to_csv(\"datasets/AI_news_snapshot.csv\", index=False)\n# AI_sentiment_news_df = apply_sentiment(AI_combined_news_df)         \n# AI_sentiment_news_df.to_csv(\"datasets/AI_news_sentiment.csv\", index=False)\n\n\nAI_sentiment_news_df = pd.read_csv(\"datasets/AI_news_sentiment.csv\")\nAI_sentiment_news_df.head()\n\nThisAI_process_sentiment does for a bunch of companies what we did for SBUX. It automatically collect all their news headlines and put them in one place for analysis.\n\ndef AI_process_sentiment(news_df):\n    filtered = news_df[news_df[\"Sentiment\"].isin([\"POSITIVE\", \"NEGATIVE\"])].copy()\n\n    # Group by DateOnly, Ticker, Sentiment → count headlines\n    grouped = (\n        filtered\n        .groupby([\"DateOnly\", \"Ticker\", \"Sentiment\"])\n        .size()\n        .unstack(fill_value=0)\n        .reset_index()\n    )\n\n    # Ensure both sentiment columns exist\n    if \"POSITIVE\" not in grouped.columns:\n        grouped[\"POSITIVE\"] = 0\n    if \"NEGATIVE\" not in grouped.columns:\n        grouped[\"NEGATIVE\"] = 0\n\n    # Sort for rolling computation\n    grouped = grouped.sort_values([\"Ticker\", \"DateOnly\"])\n\n    # 7-day rolling sums by ticker\n    grouped[\"7day_avg_positive\"] = (\n        grouped.groupby(\"Ticker\")[\"POSITIVE\"]\n        .transform(lambda x: x.rolling(window=7, min_periods=1).sum())\n    )\n    grouped[\"7day_avg_negative\"] = (\n        grouped.groupby(\"Ticker\")[\"NEGATIVE\"]\n        .transform(lambda x: x.rolling(window=7, min_periods=1).sum())\n    )\n\n    # Compute % positive\n    grouped[\"7day_pct_positive\"] = grouped[\"7day_avg_positive\"] / (\n        grouped[\"7day_avg_positive\"] + grouped[\"7day_avg_negative\"]\n    )\n\n    return grouped[[\"DateOnly\", \"Ticker\", \"7day_pct_positive\"]]\n\n\nAI_sentiment_df = AI_process_sentiment(AI_sentiment_news_df)\nAI_sentiment_df.head()\n\n\nLike we did with the SBUX stock above let’s get stock prices for all of our AI stocks now.\n\n# Match date range to your sentiment dataset\nstart_date = pd.to_datetime(AI_sentiment_df[\"DateOnly\"]).min() - pd.Timedelta(days=1)\nend_date = pd.to_datetime(AI_sentiment_df[\"DateOnly\"]).max()\n\ndef fetch_and_save_stock_data(ticker_list, start_date, end_date, save_dir=\"data/ai_prices\"):\n    \"\"\"\n    Fetches stock price data for each ticker in the list and saves it as a CSV file.\n    Prints a success or error message for each ticker.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    for ticker in ticker_list:\n        try:\n            stock_df = get_stock_data(ticker, start_date, end_date)\n            stock_df[\"Ticker\"] = ticker\n            file_path = f\"{save_dir}/{ticker}_price.csv\"\n            stock_df.to_csv(file_path, index=False)\n        except Exception as e:\n            print(f\"Failed to get stock data for {ticker}: {e}\")\n\n\n# For the sake of reproducability (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n\n# Uncomment the line below get a more recent snapshot of the data !\n\n# Usage :\n# fetch_and_save_stock_data(ai_tickers, \"start_date\", \"end_date\")\n# price_files = glob.glob(\"data/ai_prices/*.csv\")\n# price_dfs = [pd.read_csv(f) for f in price_files]\n# AI_combined_price_df = pd.concat(price_dfs, ignore_index=True)\n# AI_combined_price_df.to_csv(\"datasets/AI_top10_price_snapshot.csv\", index=False)\n\nAI_stock_prices = pd.read_csv(\"datasets/AI_top10_price_snapshot.csv\")\nAI_stock_prices.head()\n\nAs we did before let’s merge everything into one dataframe to analyse it!\n\n# Ensure both DateOnly columns are datetime\nAI_sentiment_df['DateOnly'] = pd.to_datetime(AI_sentiment_df['DateOnly'])\nAI_stock_prices['DateOnly'] = pd.to_datetime(AI_stock_prices['DateOnly'])\n\n# Merge on DateOnly and Ticker\nfinal_df = pd.merge(\n    AI_stock_prices,\n    AI_sentiment_df,\n    on=['DateOnly', 'Ticker'],\n    how='inner'                  \n)\n\nfinal_df.head()\n\nSimilar to our approach with the SBUX stock price, we can fit a SARIMAX model to this combined dataset and forecast the percentage change in AI stock prices for the next 7 business days, using both historical price trends and recent news sentiment as inputs.\n\ndef fit_and_forecast(final_df, forecast_steps=7):\n    from statsmodels.tsa.statespace.sarimax import SARIMAX\n    import numpy as np\n    import pandas as pd\n\n    # Drop missing values\n    final_df = final_df.dropna(subset=['Pct_Change', 'lagged_sentiment'])\n\n    # Define endogenous and exogenous variables\n    endog = final_df['Pct_Change']\n    exog = final_df['lagged_sentiment']\n\n    # Fit SARIMAX\n    model = SARIMAX(endog, exog=exog, order=(1, 1, 1), enforce_stationarity=False, enforce_invertibility=False)\n    fit = model.fit(disp=False)\n\n    # Future exog (use last known lagged sentiment)\n    last_sentiment = exog.iloc[-1]\n    future_exog = np.full(shape=(forecast_steps,), fill_value=last_sentiment)\n\n    # Forecast\n    forecast = fit.get_forecast(steps=forecast_steps, exog=future_exog)\n    forecast_mean = forecast.predicted_mean\n    forecast_ci = forecast.conf_int()\n\n    # Create future dates\n    last_date = final_df.index[-1]\n    forecast_index = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_steps, freq='B')\n\n    return forecast_mean, forecast_ci, forecast_index\n\n\nfinal_df = final_df.copy()\n\n# Sort and index\nfinal_df['DateOnly'] = pd.to_datetime(final_df['DateOnly'])\nfinal_df.set_index('DateOnly', inplace=True)\nfinal_df.sort_index(inplace=True)\n\n# Create lagged and standardized sentiment\nfinal_df['sentiment_std'] = (\n    final_df['7day_pct_positive'] - final_df['7day_pct_positive'].mean()\n) / final_df['7day_pct_positive'].std()\n\nfinal_df['lagged_sentiment'] = final_df['sentiment_std'].shift(1)\nfinal_df.dropna(subset=['lagged_sentiment', 'Pct_Change'], inplace=True)\n\n# Now call\nforecast_mean, forecast_ci, forecast_index = fit_and_forecast(final_df)\ncreate_plot(final_df, forecast_mean, forecast_ci, forecast_index)\n\n\n\nHow well does the model predict?\nWe can fetch the actual data to see how well our model predicts the AI stocks.\n\n# Create function to generate the compare window\ndef get_compare_window(compare_start, compare_target_end):\n\n    # Define today to avoid fetching beyond range\n    today = pd.Timestamp(datetime.utcnow().date())  # use UTC date\n\n    compare_end = min(today, compare_target_end)\n    return compare_start, compare_end\n\n# Create function to fetch the actual percentage change\ndef fetch_actual_pct_changes(ticker_list, start_date, end_date, buffer_days=7):\n    # This function fetches the pct change of the given list of tickers and return as a DataFrame\n    rows = []\n    # fetch one ticker at a time to avoid group_by complexity\n    start_fetch = (start_date - pd.Timedelta(days=buffer_days)).strftime('%Y-%m-%d')\n    end_fetch = (end_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n\n    for t in ticker_list:\n        try:\n            df = yf.download(t, start=start_fetch, end=end_fetch, progress=False, interval='1d', auto_adjust=False)\n            if df.empty:\n                # no data for ticker in window\n                continue\n            df.index = pd.to_datetime(df.index).normalize()\n            # compute percent change on Close\n            if 'Adj Close' in df.columns:\n                price_col = 'Adj Close'\n            else:\n                price_col = 'Close'\n            df['pct_change'] = df[price_col].pct_change() * 100.0\n            # select only rows in [start_date, end_date]\n            sel = df.loc[(df.index &gt;= start_date) & (df.index &lt;= end_date)]\n            # Fix: Properly select the pct_change column\n            sel = sel[['pct_change']].dropna()\n            for idx, r in sel.iterrows():\n                rows.append({'DateOnly': idx, 'Ticker': t, 'Actual_Pct_Change': r['pct_change']})\n        except Exception as e:\n            print(f\"yfinance fetch failed for {t}: {e}\")\n\n    actuals_df = pd.DataFrame(rows)\n    if not actuals_df.empty:\n        actuals_df['DateOnly'] = pd.to_datetime(actuals_df['DateOnly']).dt.normalize()\n    return actuals_df\n\n# Function to compare predictions with actuals\ndef compare_predictions(pred_mean, pred_index, actuals_df, ticker=None):\n\n    pred_df = pd.DataFrame({\n        'DateOnly': pd.to_datetime(pred_index).normalize(),\n        'Predicted_Pct_Change': np.asarray(pred_mean).astype(float)\n    })\n    if ticker is not None:\n        actuals_df = actuals_df[actuals_df['Ticker'] == ticker].copy()\n    # If actuals contain multiple tickers and ticker=None, will compare using all actual rows\n    merged = pd.merge(pred_df, actuals_df, on='DateOnly', how='inner')\n\n    if merged.empty:\n        print(\"No overlapping observed days to compare (maybe market closed or today &lt; start).\")\n        return merged, {}\n\n    # If actuals has multiple tickers for same DateOnly (if ticker=None), aggregated handling:\n    if 'Ticker' in merged.columns and ticker is None:\n        # average actuals across tickers for that day\n        aggregated = merged.groupby('DateOnly').agg({\n            'Predicted_Pct_Change': 'first',  \n            'Actual_Pct_Change': 'mean'\n        }).reset_index()\n        mdf = aggregated\n    else:\n        mdf = merged[['DateOnly', 'Predicted_Pct_Change', 'Actual_Pct_Change']].copy()\n\n    # metrics\n    mae = mean_absolute_error(mdf['Actual_Pct_Change'], mdf['Predicted_Pct_Change'])\n    rmse = sqrt(mean_squared_error(mdf['Actual_Pct_Change'], mdf['Predicted_Pct_Change']))\n    metrics = {'count': len(mdf), 'MAE': mae, 'RMSE': rmse}\n\n    return mdf, metrics\n\nThe evaluation metrics we use are the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), which measure how far off our predictions are from the actual stock price changes.\nThe MAE is the average of the absolute differences between predicted and actual values, while the RMSE is the square root of the average of squared differences. Usually, lower values of these metrics indicate better model performance.\nThe MAE is calculated as follows: \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\] And the RMSE is calculated as follows: \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]\nHowever, we must note that these metrics are not unitless, meaning they depend on the scale of the data. In our case, since we are predicting percentage changes in stock prices, the MAE and RMSE will be in percentage points.\n\n# Fit the previous example for comparison\ncompare_start = forecast_index[0]\ncompare_target_end = forecast_index[-1]\n\ncompare_start, compare_end = get_compare_window(compare_start, compare_target_end)\n\n# Create the actual comparison DataFrame\nactuals_df = fetch_actual_pct_changes(ai_tickers, compare_start, compare_end, buffer_days=7)\n\n# Calculate the metrics\npred_mean_values = forecast_mean.values  # array-like\nmdf, metrics = compare_predictions(pred_mean_values, forecast_index, actuals_df)\n\nprint(\"Comparison metrics:\")\nprint(f\"Count: {metrics['count']}\\nMAE: {metrics['MAE']:.2f}\\nRMSE: {metrics['RMSE']:.2f}\\n\")\n\nif not mdf.empty:\n    print(mdf.head())\n\nNow, with the actual stock price data, let’s visualize it in the plot alongside our predictions to see if our confidence intervals capture the actual movements well.\n\n# Visualize the comparison and evaluate the confidence intervals\ncreate_plot(final_df, forecast_mean, forecast_ci, forecast_index, actuals_df)\n\n\n\nDo you think the model’s predictions are accurate enough for practical use?\n\nWhat strategies could you use to:\n\nReduce the width of the confidence intervals?\n\nImprove the accuracy of the predictions?\n\nIncrease the model’s robustness to outliers?\n\nShare your ideas with your classmates!\n\n\n\n\n\nConclusion\nThis notebook provides an overview of how large language models (LLMs) make predictions, typically with an example showing the distribution of a LLM’s prediction and an example of stock price forecasting based on historical data and news sentiment. It demonstrates how LLMs work and how they can be applied to real-world problems like stock price prediction. The use of LLM and sentiment analysis allows us to incorporate additional qualitative information into our predictions, potentially allowing for better inference and forecasting of economic variables like stock prices.\nHowever, it is important to note that the model’s predictions are not perfect and should be used with caution. The notebook also highlights the importance of evaluating the model’s performance using metrics like MAE and RMSE, and encourages users to think critically about how to improve the model’s accuracy and robustness.\n\n\nKey Takeaways\n\nLarge language models (LLMs) predict the next word in a sequence by learning the probability distribution of possible outcomes based on context.\nLLMs can also be used to predict future values in a time series, such as stock prices, by incorporating additional features like public sentiment.\nThe SARIMAX model is a powerful forecasting model that allows us to include external information, such as news sentiment, in our predictions.\nWe must evaluate the model’s performance using metrics like MAE and RMSE to ensure its predictions are reliable.\n\n\n\nGlossary\n\nLarge Language Model (LLM): A type of AI model that predicts the next word in a sequence based on the context of previous words.\nNext Word Prediction: The task of predicting the next word in a sequence given the previous words.\nSentiment Analysis: The process of determining the emotional tone behind a series of words, used to understand the sentiment expressed in text.\nSARIMAX Model: A statistical model used for forecasting time series data that can incorporate external variables.\nMean Absolute Error (MAE): A measure of prediction accuracy that calculates the average absolute difference between predicted and actual values.\nRoot Mean Squared Error (RMSE): A measure of prediction accuracy that calculates the square root of the average of squared differences between predicted and actual values.\n\n\n\nReferences\n\nKirsch, N. (2024, April 29). 10 Best AI Stocks Of August 2024. Forbes Advisor. https://www.forbes.com/advisor/investing/best-ai-stocks/\nChen, J. (2023, October 6). Efficient Market Hypothesis (EMH): Forms and criticisms. Investopedia. https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp\nYahoo Finance. (n.d.). Yahoo Finance — Stocks, financial news, quotes, and market data. Retrieved August 1, 2025, from https://ca.finance.yahoo.com/\nAroussi, R. (n.d.). yfinance [Python package]. GitHub. Retrieved August 1, 2025, from https://github.com/ranaroussi/yfinance\nLi, T. (2025). finvizfinance (Version 1.1.1) [Python package]. PyPI. Retrieved August 1, 2025, from https://pypi.org/project/finvizfinance/\nGeeksforGeeks. (2023, September 27). Complete guide to SARIMAX in Python. https://www.geeksforgeeks.org/python/complete-guide-to-sarimax-in-python/\nSprenger, T. O., Tumasjan, A., Sandner, P. G., & Welpe, I. M. (2014, February 5). Twitter sentiment and stock market movements: The predictive power of social media. VoxEU. https://cepr.org/voxeu/columns/twitter-sentiment-and-stock-market-movements-predictive-power-social-media"
  },
  {
    "objectID": "docs/SOCI-280/soci_280_lesson_plan.html",
    "href": "docs/SOCI-280/soci_280_lesson_plan.html",
    "title": "Lesson Plan: Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "",
    "text": "Duration: 110 minutes  Course: SOCI 280\n\nLearning Objectives\n\nBy the end of this lesson, students will:\n\nUnderstand and complete sentiment analysis to detect emotional tone (positive, negative, neutral), and understand and complete toxicity analysis to identify harmful or aggressive language (e.g., insults, threats)\nIntroduce concepts in statistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian)\nLearn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than normal political tweets?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\n\nThrough this analysis, we’ll explore various dimensions of AI applications, critically examining how it can better understand and detect the patterns of disinformation when working with large amounts of social data.\n\n\nMaterials and Technical Requirements:\n\nThe lesson will be administered through a Jupyter Notebook, hosted on the prAxIs UBC website. Students will need access to a device connected to the internet, preferably a laptop. No previous coding experience is required, though some familiarity with Python is an asset. If students do not have access to a capable device, it is acceptable to pair up into groups of two or three. Group work is encouraged throuhgout the lesson and students will be asked to compare their findings with other students.\n\n\nPre-lesson Checklist:\n\n\nStudents have previously completed the reading up to Section 0 (roughly 5-10 minutes).\n\nThe instructor has recently loaded the Notebook and can access and project it if needed.\nStudents were reminded to bring a device to class.\n\n\n\nAgenda\n\n\nPre-discussion and brief lecture based on Notebook reading (5-10 minutes)  Briefly discuss misinformation, disinformation, and propaganda, emphasizing the role of digital platforms in enabling their spread. Briefly explain the ways researchers can discern disinformation online and consider how new technology based on personal data might be more influential.  Example: Researchers secretly experimented on Reddit users with AI generated comments\nLoad the Notebook (5 minutes)  Have students open the Jupyter Notebook, pairing any students who are unable to access the materials with a student who can. Once all students are able to access the materials, instruct them to complete Section 0.\nSection 0 (20 Minutes)  Instruct the students to begin working through the code and activities in Notebook Section 0. Once students get to the Screen Time activity pause for a brief discussion to compare results.\nSection 1 (15 Minutes)  Before instructing students to complete Notebook Section 1, have a brief (2-3 minute) explanation of classification, connecting course materials, and previous lessons. Students will complete Notebook Section 1 at their own pace, taking roughly 15 minutes to complete the lesson.\nSections 3-4 (20 Minutes)  After pausing for a moment to discuss the findings of Section 1 in a large group, students will complete the rest of the Notebook.\nTakeaways and Activity (5-10 Minutes)  The last section of this lesson is time-dependent, and reserved for questions about the Notebook methods, a brief lecture outlining the key takeaways from the Notebook, and for students to get started on any potential participation activities such as discussion posts, worksheets, etc. (samples below).\n\n\n\nActivity Materials\n\n\nDiscussion Post\nRespond to the following questions in 100-250 words each:\n\nHow can sentiment analysis be useful in answering research questions? Can you think of any tasks it would be well suited to?\nDo you think the methods in the notebook were useful in understanding disinformation campaigns? What might be some of the limitations to these approaches?\nDiscuss the role AI plays in disinformation, both the detection and analysis of it, and the production of it.\n\n\n\nShort Response:\nOn a social media platform of your choice, try to identify a post you believe to be disinformation. You can search for specific topics that you think will likely produce disinformation or wait for something to come across in your feed. Link to the content here: ____________________________________________________________________\nThen, respond to the following questions:\n\nWhy do you think this is disinformation? What data/information are you using or pulling out from the content and its features to come to this conclusion? Explain why you think this is disinformation in 150-300 words.\nThink back to the classifier in the notebook. What data was it using to classify text as disinformation? Is that process similar or different from how you classified your post as disinformation? Do you think you are more likely to be correct in identifying disinformation? If so, why? Respond in 200-350 words."
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html",
    "href": "docs/SOCI-415/cbdb_dataset.html",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "",
    "text": "SOCI 415 Network Analysis Intro Notebook\nKinmatrix Dataset Notebook\n\nThe China Biographical Database is a freely accessible relational database with biographical information about approximately 641,568 individuals as of August 2024, currently mainly from the 7th through 19th centuries. It was developed as a collaborative project between scholars at Harvard University, Academia Sinica, and Peking University. The data is useful for statistical, social network, and spatial analysis as well as serving as a biographical reference for Chinese History.\nThis dataset has many variables and is far more complex than the KINMATRIX Dataset we have used before, the dataset is far larger and contains more traditional networks as opposed to KINMATRIX’s ego networks. If you wish to read more about the structure of the dataset you can follow the link Structure of CBDB."
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html#network-level-analysis-clusters-and-clustering-coefficients",
    "href": "docs/SOCI-415/cbdb_dataset.html#network-level-analysis-clusters-and-clustering-coefficients",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "2.1 Network-level analysis: Clusters and clustering coefficients",
    "text": "2.1 Network-level analysis: Clusters and clustering coefficients\nA cluster (also known as a community) is a set of nodes in a graph that are densely connected to each other but sparsely connected to nodes in other clusters. For example, in a social network, a cluster might represent a group of people who frequently interact with each other but have fewer interactions with people outside the group. Community detection is the process of finding such communities within nodes.\nBefore diving into community detection, we first need to understand modularity. Modulaity is a numerical measure for the community structure of a graph: it compares the density of edges within the communities of a network to the density of edges between communities. A positive modularity value suggests a strong community structure, while values closer to zero or negative indicate that the divisions are no better than random.\nThe Louvain algorithm is a community detection method in networks that aims to optimize modularity. By optimizing modularity, the Louvain algorithm effectively uncovers natural divisions in the network where connections are dense within clusters and sparse between them, thus identifying meaningful community structures.\nFirst, each node is assigned to its own community, and nodes are then iteratively moved to neighboring communities if it increases the modularity. In the second phase, the algorithm creates a new network where each community from the first phase is treated as a single node, and the process is repeated. This hierarchical approach continues until no further modularity improvements can be made, resulting in a final set of communities that maximize modularity.\nLet’s first try running the Louvain Algorithm on a random graph to demonstrate how it works before running it on our real data.\n\n#Set the seed so it is reproducable\nrandom.seed(1)\n\nn = 20  # number of nodes\nd = 3   # degree of each node\n\n# Generate the random regular graph\nrr_graph = nx.random_regular_graph(d, n)\n\npartition = community_louvain.best_partition(rr_graph)\npos = nx.spring_layout(rr_graph, seed=42)\nnum_communities = max(partition.values()) + 1\ncmap = cm.get_cmap('viridis', num_communities)\nnx.draw_networkx_nodes(\n    rr_graph, pos, node_size=40, cmap=cmap, node_color=list(partition.values())\n)\nnx.draw_networkx_edges(rr_graph, pos, alpha=0.5)\nplt.show()\n\nWe can see by the node coloring that by optimizing modularity the Louvain Algorithm has found smaller subgroups within our random network. Now we can try it on our real data."
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html#louvain-run-on-real-data",
    "href": "docs/SOCI-415/cbdb_dataset.html#louvain-run-on-real-data",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "2.2 Louvain Run on Real Data",
    "text": "2.2 Louvain Run on Real Data\nAs with before we will set a random seed so that the analysis is reporducable. We will also print out average community size along with the size of the largest and smallest communities. We will also print the top 10 largest communities and their sizes.\nWe will also be working with the largest connected component for this analysis.\n\n#Set a seed for reproducability of our results\nnp.random.seed(1)\n\n# Load kinship data into DataFrame\nconn = sqlite3.connect(db_path)\ndf = pd.read_sql_query(\"SELECT c_personid, c_kin_id, c_kin_code FROM KIN_DATA\", conn)\nconn.close()\n\n# Build the Graph\nG = nx.Graph()\nfor _, row in df.iterrows():\n    person = row['c_personid']\n    kin = row['c_kin_id']\n    kin_type = row['c_kin_code']\n    G.add_edge(person, kin, kinship=kin_type)\n\n# Work with the largest connected component\nlargest_cc = max(nx.connected_components(G), key=len)\nG_sub = G.subgraph(largest_cc).copy()\nprint(f\"Largest connected component nodes: {G_sub.number_of_nodes()}\")\n\n# Run Louvain Community Detection\nprint(\"Running Louvain algorithm...\")\npartition = community_louvain.best_partition(G_sub)\n\n# Community Analysis Output\nnum_communities = len(set(partition.values()))\nprint(f\"In our data Louvain has detected {num_communities} communities.\")\n\n# Count community sizes\ncommunity_sizes = Counter(partition.values())\nprint(f\"\\nMetrics about Community size:\")\nprint(f\"Average community size: {np.mean(list(community_sizes.values())):.1f}\")\nprint(f\"Largest community: {max(community_sizes.values())} people\")\nprint(f\"Smallest community: {min(community_sizes.values())} people\")\n\n#Show top 10 largest communities\nprint(f\"\\nLargest Communities:\")\nfor i, (comm_id, size) in enumerate(community_sizes.most_common(10)):\n    print(f\"Community {comm_id}: {size:,} people\")\n\n# Calculate modularity\nmodularity = community_louvain.modularity(partition, G_sub)\nprint(f\"\\nModularity Score: {modularity:.4f}\")\nprint(\"(Higher modularity indicates stronger community structure)\")\n\n#Distributon Histogram of Community Size\nfig, ax = plt.subplots(figsize=(15, 12))\nfig.suptitle('CBDB Kinship Network Community Analysis', fontsize=16, fontweight='bold')\n\n# Community size histogram\nax.hist(list(community_sizes.values()), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nax.set_xlabel('Community Size (number of people)')\nax.set_ylabel('Number of Communities')\nax.set_title('Distribution of Community Sizes')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nFrom the visualization we can see most communities are around 200 - 400 nodes in size.\nLets look at two of these communities in more detail, we will look at one average sized one and one large one.\nThe two we will use are:\n\nCommunity 20: 703 nodes with 940 edges\nCommunity 125: 308 nodes with 392 edges\n\n\nnp.random.seed(1)\n\ndef visualize_cbdb_community(G_sub, partition, community_id, max_nodes=1000):\n    community_nodes = [str(node) for node, comm_id in partition.items() if comm_id == community_id]\n    \n    print(f\"Community {community_id}: {len(community_nodes)} people\")\n    \n    # Create subgraph with string node IDs\n    subgraph = G_sub.subgraph([int(node) for node in community_nodes])\n    print(f\"Showing {len(community_nodes)} people, {subgraph.number_of_edges()} relationships\")\n    \n    net = Network(height=\"700px\", width=\"100%\", bgcolor=\"#ffffff\", notebook=True)\n    \n    for node in community_nodes:\n        degree = subgraph.degree[int(node)] \n        size = max(15, min(35, 15 + degree))\n        net.add_node(str(node), \n                     label=str(node), \n                     size=size, \n                     color=\"#3498db\",\n                     title=f\"Person {node}\\nConnections: {degree}\")\n    \n    for u, v, data in subgraph.edges(data=True):\n        net.add_edge(str(u), str(v), color=\"#cccccc\", title=f\"Kinship: {data.get('kinship','family')}\")\n    \n    filename = f\"community_{community_id}.html\"\n    net.show(filename)\n    print(f\"Interactive network: {filename}\")\n\n#Run the function\nvisualize_cbdb_community(G_sub, partition, 20)\nvisualize_cbdb_community(G_sub, partition, 125)\n\nWe will again use a PyVis visualization, just like with the KINMATRIX Visualizations we can zoom and pan around and hover on the nodes. This time you can also drag the nodes and the surrounding nodes will move like bacteria under a microscope."
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html#macro-stats-and-dynasties",
    "href": "docs/SOCI-415/cbdb_dataset.html#macro-stats-and-dynasties",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "2.3 Macro Stats and Dynasties",
    "text": "2.3 Macro Stats and Dynasties\nTo get a better understanding of our communities we will look at some summary statistics. Let’s briefly define what these macro statistics will be:\n\nDiameter: The longest shortest path between any two nodes in the network or component. This is a way to measure how “wide” our network is.\nAverage Path Lenght: The average number of steps along the shortest paths for all possible pairs of nodes in the graph\nAverage Clustering: The likelihood that any two neighbors of a node are also connected to each other. Higher values mean people in the community tend to form “groups”.\n\nWe will also look at degree centrality, density and number of nodes.\n\nnp.random.seed(1)\n\ndef macro_stats(G_sub, partition, community_id):\n    nodes = [n for n, c in partition.items() if c == community_id]\n    subg = G_sub.subgraph(nodes)\n    stats = {}\n\n    if nx.is_connected(subg):\n        stats['diameter'] = nx.diameter(subg)\n        stats['avg_path_length'] = nx.average_shortest_path_length(subg)\n    else:\n        # statistics of largest connected component only\n        lcc = subg.subgraph(max(nx.connected_components(subg), key=len))\n        stats['diameter'] = nx.diameter(lcc)\n        stats['avg_path_length'] = nx.average_shortest_path_length(lcc)\n\n    degrees = dict(subg.degree())\n    max_degree = max(degrees.values())\n    n = len(degrees)\n    if n &gt; 1:\n        degree_centralization = sum(max_degree - d for d in degrees.values()) / ((n-1)*(n-2))\n    else:\n        degree_centralization = 0\n    stats['degree_centralization'] = degree_centralization\n    stats['avg_clustering'] = nx.average_clustering(subg)\n    stats['density'] = nx.density(subg)\n    stats['n_nodes'] = n\n    return stats\n\n#Print:\nstats_20 = macro_stats(G_sub, partition, 20)\nstats_125 = macro_stats(G_sub, partition, 125)\nprint(\"Community 50 macro stats:\", stats_20)\nprint(\"Community 125 macro stats:\", stats_125)\n\nWe now have more quantifiable metrics for our visualizations. Let’s contunue our analysis with finding which dynasties these communities are primarily from.\n\nnp.random.seed(1)\n\n# Community 125\npersonids_125 = [n for n, c in partition.items() if c == 125]\nconn = sqlite3.connect(db_path)\nids_tuple_125 = tuple(personids_125)\nquery_125 = f'''\n    SELECT c_personid, c_dy FROM BIOG_MAIN\n    WHERE c_personid IN {ids_tuple_125}\n'''\ndf_dyn_125 = pd.read_sql_query(query_125, conn)\n\n# Community 20\npersonids_20 = [n for n, c in partition.items() if c == 20]\nids_tuple_20 = tuple(personids_20)\nquery_20 = f'''\n    SELECT c_personid, c_dy FROM BIOG_MAIN\n    WHERE c_personid IN {ids_tuple_20}\n'''\ndf_dyn_20 = pd.read_sql_query(query_20, conn)\nconn.close()\n\n# Show the most common dynasty codes\nprint(\"Community 125 dominant dynasties:\")\nprint(df_dyn_125['c_dy'].value_counts().head(3))\n\nprint(\"Community 20 dominant dynasties:\")\nprint(df_dyn_20['c_dy'].value_counts().head(3))\n\nWe can see that Community 125 is primarily from Dynasty 6, and Community 50 is primarily from Dynasty 15, but this does not tell us much we need to translate 6 and 15 into real dynasty names. These values represent names of real dynasties, but without the key this output does not mean anything.\nLets now map these values to real dynasty names.\n\nnp.random.seed(1)\n\nconn = sqlite3.connect(db_path)\ndynasty_mapping = pd.read_sql_query('SELECT c_dy, c_dynasty FROM DYNASTIES', conn)\nconn.close()\n\ndef get_dynasty_breakdown(df_dyn, dynasty_mapping):\n    df_merged = df_dyn.merge(dynasty_mapping, on='c_dy', how='left')\n    return df_merged['c_dynasty'].value_counts()  \n\nprint(\"Community 125 top 3 dynasties:\")\nprint(get_dynasty_breakdown(df_dyn_125, dynasty_mapping).head(3))\n\nprint(\"Community 20 top 3 dynasties:\")\nprint(get_dynasty_breakdown(df_dyn_20, dynasty_mapping).head(3))\n\nNow we know that Community 125 is primarily made up of individuals from the Tang Dynasty and Community 50 is primarily made up of individuals from the Song Dynasty."
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html#coloring-nodes",
    "href": "docs/SOCI-415/cbdb_dataset.html#coloring-nodes",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "2.4 Coloring Nodes",
    "text": "2.4 Coloring Nodes\nLike with the KINMATRIX Dataset we will color our Pyvis visualizations by a variable of interest. In this case we will color Community 125 by gender where male is purple and women are yellow. There are no unknown gender variables in either of these communities so we can just have two colors in our color map.\n\nnp.random.seed(1)\n\n# Get all node and person id's\nall_personids = list(set([n for n in partition.keys()]))\n\nconn = sqlite3.connect(db_path)\nquery = f'''\n    SELECT c_personid, c_female FROM BIOG_MAIN\n    WHERE c_personid IN ({','.join(str(pid) for pid in all_personids)})\n'''\ndf_gender = pd.read_sql_query(query, conn)\nconn.close()\n\n# Function to map female/male from number 1/0\ndef sex_label(val):\n    return 'female' if val == 1 else ('male' if val == 0 else 'unknown')\n\ngender_dict = {row['c_personid']: sex_label(row['c_female']) for _, row in df_gender.iterrows()}\n\ndef visualize_cbdb_community_gender(G_sub, partition, community_id, gender_dict, max_nodes=1000):\n    community_nodes = [str(node) for node, comm_id in partition.items() if comm_id == community_id]\n    print(f\"Community {community_id}: {len(community_nodes)} people\")\n    if len(community_nodes) &gt; max_nodes:\n        print(f\"Sampling {max_nodes} nodes for performance...\")\n        subgraph_full = G_sub.subgraph([int(node) for node in community_nodes])\n        degrees = dict(subgraph_full.degree())\n        sorted_nodes = sorted(community_nodes, key=lambda x: degrees.get(int(x), 0), reverse=True)\n        community_nodes = sorted_nodes[:max_nodes//2] + random.sample(sorted_nodes[max_nodes//2:], max_nodes//2)\n\n    subgraph = G_sub.subgraph([int(node) for node in community_nodes])\n    print(f\"Showing {len(community_nodes)} people, {subgraph.number_of_edges()} relationships\")\n    net = Network(height=\"700px\", width=\"100%\", bgcolor=\"#ffffff\", notebook=True)\n\n    color_map = {'male': '#800080',   # purple\n             'female': '#FFFF00'}  # yellow \n    for node in community_nodes:\n        gender = gender_dict.get(int(node), 'unknown')\n        degree = subgraph.degree[int(node)]\n        size = max(15, min(35, 15 + degree))\n        net.add_node(str(node), \n                     label=str(node),\n                     size=size,\n                     color=color_map.get(gender, '#bdbdbd'),\n                     title=f\"Person {node}\\nConnections: {degree}\\nGender: {gender}\")\n\n    for u, v, data in subgraph.edges(data=True):\n        net.add_edge(str(u), str(v), color=\"#cccccc\", title=f\"Kinship: {data.get('kinship','family')}\")\n    filename = f\"community_{community_id}_gender.html\"\n    net.show(filename)\n    print(f\"Interactive network: {filename}\")\n\nvisualize_cbdb_community_gender(G_sub, partition, 125, gender_dict)\n\nWe can see that most of the nodes are male especially the most central nodes, we will come back to this later, but think about why that is? Continuing on with the notion of centrality lets color the nodes in Community 20 with a color gradient where the nodes with the lowest degree centrality will be blue and the node with the highest will be red. This will serve as an intuitive method to visualize degree centrality.\n\nnp.random.seed(1)\n\ndef visualize_cbdb_community_degree(G_sub, partition, community_id, max_nodes=1000):\n    community_nodes = [str(node) for node, comm_id in partition.items() if comm_id == community_id]\n    print(f\"Community {community_id}: {len(community_nodes)} people\")\n\n    subgraph = G_sub.subgraph([int(node) for node in community_nodes])\n    print(f\"Showing {len(community_nodes)} people, {subgraph.number_of_edges()} relationships\")\n    net = Network(height=\"700px\", width=\"100%\", bgcolor=\"#ffffff\", notebook=True)\n\n    degrees = dict(subgraph.degree())\n    deg_values = [degrees[int(node)] for node in community_nodes]\n    min_deg, max_deg = min(deg_values), max(deg_values)\n    norm = plt.Normalize(min_deg, max_deg)\n    cmap = plt.get_cmap('coolwarm') \n\n    for node in community_nodes:\n        degree = degrees[int(node)]\n        size = max(15, min(35, 15 + degree))\n        rgb_vals = cmap(norm(degree))[:3]\n        hex_color = '#%02x%02x%02x' % tuple(int(x*255) for x in rgb_vals)\n        net.add_node(str(node),\n                     label=str(node),\n                     size=size,\n                     color=hex_color,\n                     title=f\"Person {node}\\nConnections: {degree}\")\n\n    for u, v, data in subgraph.edges(data=True):\n        net.add_edge(str(u), str(v), color=\"#cccccc\", title=f\"Kinship: {data.get('kinship','family')}\")\n    filename = f\"community_{community_id}_degree.html\"\n    net.show(filename)\n    print(f\"Interactive network: {filename}\")\n    print(f\"Color gradient: low degree (blue) to high degree (red).\")\n\nvisualize_cbdb_community_degree(G_sub, partition, 20)"
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html#visualization-discussion",
    "href": "docs/SOCI-415/cbdb_dataset.html#visualization-discussion",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "2.5 Visualization Discussion",
    "text": "2.5 Visualization Discussion\nIn small groups of 3-4 look at the visualizations from this dataset and compare them to our pyVis visualizations from the KINMATRIX Dataset. Try to be specific and use the terminology introduced in the introductory notebook. How are they similar and how are they different?"
  },
  {
    "objectID": "docs/SOCI-415/kinmatrix.html",
    "href": "docs/SOCI-415/kinmatrix.html",
    "title": "SOCI 415 Network Analysis - KINMATRIX Dataset",
    "section": "",
    "text": "SOCI 415 Network Anlysis Notebook\n\n\nThe KINMATRIX dataset represents families as ego-centric networks of younger adults aged 25 to 35, collecting extensive data about both nuclear and extended kin across ten countries. The data include over 12,000 anchor respondents and more than 252,000 anchor-kin dyads, encompassing a wide range of relatives such as parents, siblings, grandparents, aunts, uncles, cousins, and complex kin (e.g., step- and half-relatives).\nAnchor respondents refer to the ones directly sampled from. These anchors filled out the survey about their families and kin meaning that they will always be at the center of these family networks.\nThe countries in the dataset are the United Kingdom, Germany, Poland, Italy, Sweeden, Denmark, Finland, Norway, the Netherlands and the USA.\n\n\nWe will begin by looking at broader patterns across different countries. This will serve as an introduction into the dataset and will be a foundation for more focused analysis later on. First we import all the necessary Python libraries for data manipulation, network analysis, and visualization. This will allow us to get results for each country. The output of this cell will be a list of countries in the data.\nImport Libraries and the Data:\n\n#Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.patches as mpatches \nimport networkx as nx \nimport pandas as pd \nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\nimport plotly.graph_objects as go\nfrom pyvis.network import Network\nimport re\nimport matplotlib.cm as cm\n\nfile_path = r'ZA8825_v1-0-0.dta' #path\ndf = pd.read_stata(file_path)\n\n#Print country codes\nprint(df['anc_cou'].unique())\n\n#Make a list of countries\ncountry_dfs = {}\nfor country in df['anc_cou'].unique():\n    country_dfs[country] = df[df['anc_cou'] == country]\n\n\n\nHere, we systematically construct a kinship network for each country in the dataset using the NetworkX Python package. For each country, we filter the data, create a new graph, and add nodes representing anchor individuals and their kin. Then we draw edges between the nodes. Finally, we print out the number of nodes and edges for each country’s network, providing a quick overview of network size and complexity across the dataset.\n\n#Set up a dictionary of countries\ncountry_map = {\n    'UK': '1. UK',\n    'Germany': '2. Germany',\n    'Poland': '3. Poland',\n    'Italy': '4. Italy',\n    'Sweden': '5. Sweden',\n    'Denmark': '6. Denmark',\n    'Finland': '7. Finland',\n    'Norway': '8. Norway',\n    'Netherlands': '9. Netherlands',\n    'USA': '10. USA'\n}\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store the graph and anchor nodes\n    anchor_nodes = list(filtered_df['anc_id'].unique())\n    graphs[country] = {\n        'graph': G_filtered,\n        'anchor_nodes': anchor_nodes\n    }\n    \n    print(f\"{country}:\")\n    print(f\"  Number of nodes: {G_filtered.number_of_nodes()}\")\n    print(f\"  Number of edges: {G_filtered.number_of_edges()}\")\n    print()\n\nWe can see that the Kinmatrix dataset has a wide range of networks ranging from smaller countries like Norway and Denmark (~3000 Nodes) to expansive networks like the United States with 118,702 nodes. This is a good overview, but it does not really give any information on the nature of the networks, their size, family relations and other interesting statistics.\n\n\n\nThis block computes and displays key network statistics for each country’s kinship network. We calculate the mean degree (average number of connections per node), and the gender distribution among anchor nodes.\n\nfor country, data in graphs.items():\n    G = data['graph']\n    anchor_nodes = data['anchor_nodes']\n    \n    # Mean degree for anchor nodes only\n    anchor_degrees = [G.degree(n) for n in anchor_nodes]\n    mean_anchor_degree = np.mean(anchor_degrees) if anchor_degrees else 0\n    \n    # Gender distribution among anchor nodes\n    female_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '2. Female')\n    male_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '1. Male')\n    other_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '3. Other gender or no gender')\n    \n    print(f\"{country}:\")\n    print(f\"  Mean degree (anchor nodes): {mean_anchor_degree:.2f}\")\n    print(f\"  Female anchor nodes: {female_count}\")\n    print(f\"  Male anchor nodes: {male_count}\")\n    print(f\"  Other/No gender anchor nodes: {other_count}\")\n    print()\n\nThe results are interesting we can see that the smallest family networks are in Germany (16.33 nodes per anchor) and the largest are in the United States (22.72 nodes per anchor). We can also see that there are more women than men in most countries espcially in the US (2,992 women compared to 1,956 men).\n\n\n\nHere we have enough information to do a visualization of our findings. We will use the Pyvis package. This package will make an interactive visualization which you can zoom in and pan around. This will be the most simple visualization we will make, and it will just be on the shape and size of the network. The code loads the NetworkX graph into Pyvis, cleans any missing attribute values, and provides interactive controls for exploring the network.\n\n# Retrieve the Norway Network\nG_Norway = graphs['Norway']['graph']\nanchor_nodes_norway = graphs['Norway']['anchor_nodes']\n\n# Clean None attributes\nfor n, attrs in G_Norway.nodes(data=True):\n    for k, v in attrs.items():\n        if v is None:\n            G_Norway.nodes[n][k] = \"NA\"\n\nfor u, v, attrs in G_Norway.edges(data=True):\n    for k, val in attrs.items():\n        if val is None:\n            G_Norway.edges[u, v][k] = \"NA\"\n\n# Create a new Pyvis Network\nnet = Network(height='800px', width='100%', notebook=True)\n\n# Create simple anchor mapping\nanchor_to_label = {}\nfor i, anchor in enumerate(anchor_nodes_norway, 1):\n    anchor_to_label[anchor] = f\"Anchor-{i}\"\n\n# Add nodes manually \nfor node in G_Norway.nodes():\n    if node in anchor_nodes_norway:\n        # Anchor nodes with labels\n        net.add_node(node, label=anchor_to_label[node], color='#4ECDC4', size=15)\n    else:\n        # Kin nodes\n        net.add_node(node, color='#4ECDC4', size=8)\n\n# Add edges manually\nfor u, v in G_Norway.edges():\n    net.add_edge(u, v)\n\nfor node in net.nodes:\n    if node['id'] not in anchor_nodes_norway:\n        node['label'] = '' \n\n# Minimal settings\nnet.show('norway_updated_network.html')\n\nprint(\"All blue nodes version saved as 'norway_all_blue_nodes.html'\")\nprint(f\"Total nodes: {G_Norway.number_of_nodes()}\")\nprint(f\"Total edges: {G_Norway.number_of_edges()}\")\nprint(f\"Anchor nodes labeled: {len(anchor_nodes_norway)}\")\n\n\n\n\n\nThis code block constructs kinship network graphs for each country in the KINMATRIX dataset and then analyzes how the number of nodes in the family networks vary by age and gender within each country. This analysis helps reveal patterns and differences in family network structure across demographic groups and between countries, providing insight into how kinship connectivity varies by age and gender in different countries.\n\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n\n    graphs[country] = G_filtered\n\ndef analyze_family_degree_by_demographics(G_filtered):\n    degree_by_age = {}\n    degree_by_gender = {}\n\n    # Identify anchor nodes by prerequisite attribute\n    anchor_nodes = [n for n, d in G_filtered.nodes(data=True)\n                    if ('age' in d and 'gender' in d)]\n\n    for anchor_id in anchor_nodes:\n        # Degree of the anchor node (number of kin ties)\n        deg = G_filtered.degree(anchor_id)\n        age = G_filtered.nodes[anchor_id].get('age')\n        gender = G_filtered.nodes[anchor_id].get('gender')\n\n        if age is not None:\n            age_group = age // 10 * 10  \n            degree_by_age.setdefault(age_group, []).append(deg)\n        if gender is not None:\n            degree_by_gender.setdefault(gender, []).append(deg)\n\n    print(\"Mean Degree by Age Group:\")\n    for age_group, degrees in sorted(degree_by_age.items()):\n        print(f\"  Age {age_group}s: Mean Degree = {np.mean(degrees):.2f}, Count = {len(degrees)}\")\n\n    print(\"\\nMean Degree by Gender:\")\n    for gender, degrees in degree_by_gender.items():\n        print(f\"  Gender {gender}: Mean Degree = {np.mean(degrees):.2f}, Count = {len(degrees)}\")\n\n#Show results\nfor country in country_map:\n    if country in graphs:\n        print(f\"\\n--- {country} ---\")\n        analyze_family_degree_by_demographics(graphs[country])\n    else:\n        print(f\"No graph available for {country}\")\n\n\n\nInteractive part: Have a look at our findings from sections 1 and 2, based on frameworks taught in class explain a reason for them.\nSome example findings are listed below, you can try to answer these 3, or look for your own and try to link it to a concept from class.\n\nAge does not seem to be a factor in mean degree. In six out of ten countries, anchors in their 20’s have a higher degree than anchors in their 30’s. Why is this, we would expect older anchors to have larger families?\nWomen tend to have a higher mean degree than men in all countries (Poland: Male = 17.97, Female = 20.25). Why is this?\nThe USA has the largest mean degree out of all countries in the dataset by both age and gender. Is it purely because of the large sample size in the US or are there other factors? We might expect the US to have a lower mean degree especially among anchors aged 20-30 as their parents or grandparents are likely to be immigrants disconnected from their families in their original countries."
  },
  {
    "objectID": "docs/SOCI-415/kinmatrix.html#kinmatrix-data-description",
    "href": "docs/SOCI-415/kinmatrix.html#kinmatrix-data-description",
    "title": "SOCI 415 Network Analysis - KINMATRIX Dataset",
    "section": "",
    "text": "The KINMATRIX dataset represents families as ego-centric networks of younger adults aged 25 to 35, collecting extensive data about both nuclear and extended kin across ten countries. The data include over 12,000 anchor respondents and more than 252,000 anchor-kin dyads, encompassing a wide range of relatives such as parents, siblings, grandparents, aunts, uncles, cousins, and complex kin (e.g., step- and half-relatives).\nAnchor respondents refer to the ones directly sampled from. These anchors filled out the survey about their families and kin meaning that they will always be at the center of these family networks.\nThe countries in the dataset are the United Kingdom, Germany, Poland, Italy, Sweeden, Denmark, Finland, Norway, the Netherlands and the USA.\n\n\nWe will begin by looking at broader patterns across different countries. This will serve as an introduction into the dataset and will be a foundation for more focused analysis later on. First we import all the necessary Python libraries for data manipulation, network analysis, and visualization. This will allow us to get results for each country. The output of this cell will be a list of countries in the data.\nImport Libraries and the Data:\n\n#Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.patches as mpatches \nimport networkx as nx \nimport pandas as pd \nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\nimport plotly.graph_objects as go\nfrom pyvis.network import Network\nimport re\nimport matplotlib.cm as cm\n\nfile_path = r'ZA8825_v1-0-0.dta' #path\ndf = pd.read_stata(file_path)\n\n#Print country codes\nprint(df['anc_cou'].unique())\n\n#Make a list of countries\ncountry_dfs = {}\nfor country in df['anc_cou'].unique():\n    country_dfs[country] = df[df['anc_cou'] == country]\n\n\n\nHere, we systematically construct a kinship network for each country in the dataset using the NetworkX Python package. For each country, we filter the data, create a new graph, and add nodes representing anchor individuals and their kin. Then we draw edges between the nodes. Finally, we print out the number of nodes and edges for each country’s network, providing a quick overview of network size and complexity across the dataset.\n\n#Set up a dictionary of countries\ncountry_map = {\n    'UK': '1. UK',\n    'Germany': '2. Germany',\n    'Poland': '3. Poland',\n    'Italy': '4. Italy',\n    'Sweden': '5. Sweden',\n    'Denmark': '6. Denmark',\n    'Finland': '7. Finland',\n    'Norway': '8. Norway',\n    'Netherlands': '9. Netherlands',\n    'USA': '10. USA'\n}\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store the graph and anchor nodes\n    anchor_nodes = list(filtered_df['anc_id'].unique())\n    graphs[country] = {\n        'graph': G_filtered,\n        'anchor_nodes': anchor_nodes\n    }\n    \n    print(f\"{country}:\")\n    print(f\"  Number of nodes: {G_filtered.number_of_nodes()}\")\n    print(f\"  Number of edges: {G_filtered.number_of_edges()}\")\n    print()\n\nWe can see that the Kinmatrix dataset has a wide range of networks ranging from smaller countries like Norway and Denmark (~3000 Nodes) to expansive networks like the United States with 118,702 nodes. This is a good overview, but it does not really give any information on the nature of the networks, their size, family relations and other interesting statistics.\n\n\n\nThis block computes and displays key network statistics for each country’s kinship network. We calculate the mean degree (average number of connections per node), and the gender distribution among anchor nodes.\n\nfor country, data in graphs.items():\n    G = data['graph']\n    anchor_nodes = data['anchor_nodes']\n    \n    # Mean degree for anchor nodes only\n    anchor_degrees = [G.degree(n) for n in anchor_nodes]\n    mean_anchor_degree = np.mean(anchor_degrees) if anchor_degrees else 0\n    \n    # Gender distribution among anchor nodes\n    female_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '2. Female')\n    male_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '1. Male')\n    other_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '3. Other gender or no gender')\n    \n    print(f\"{country}:\")\n    print(f\"  Mean degree (anchor nodes): {mean_anchor_degree:.2f}\")\n    print(f\"  Female anchor nodes: {female_count}\")\n    print(f\"  Male anchor nodes: {male_count}\")\n    print(f\"  Other/No gender anchor nodes: {other_count}\")\n    print()\n\nThe results are interesting we can see that the smallest family networks are in Germany (16.33 nodes per anchor) and the largest are in the United States (22.72 nodes per anchor). We can also see that there are more women than men in most countries espcially in the US (2,992 women compared to 1,956 men).\n\n\n\nHere we have enough information to do a visualization of our findings. We will use the Pyvis package. This package will make an interactive visualization which you can zoom in and pan around. This will be the most simple visualization we will make, and it will just be on the shape and size of the network. The code loads the NetworkX graph into Pyvis, cleans any missing attribute values, and provides interactive controls for exploring the network.\n\n# Retrieve the Norway Network\nG_Norway = graphs['Norway']['graph']\nanchor_nodes_norway = graphs['Norway']['anchor_nodes']\n\n# Clean None attributes\nfor n, attrs in G_Norway.nodes(data=True):\n    for k, v in attrs.items():\n        if v is None:\n            G_Norway.nodes[n][k] = \"NA\"\n\nfor u, v, attrs in G_Norway.edges(data=True):\n    for k, val in attrs.items():\n        if val is None:\n            G_Norway.edges[u, v][k] = \"NA\"\n\n# Create a new Pyvis Network\nnet = Network(height='800px', width='100%', notebook=True)\n\n# Create simple anchor mapping\nanchor_to_label = {}\nfor i, anchor in enumerate(anchor_nodes_norway, 1):\n    anchor_to_label[anchor] = f\"Anchor-{i}\"\n\n# Add nodes manually \nfor node in G_Norway.nodes():\n    if node in anchor_nodes_norway:\n        # Anchor nodes with labels\n        net.add_node(node, label=anchor_to_label[node], color='#4ECDC4', size=15)\n    else:\n        # Kin nodes\n        net.add_node(node, color='#4ECDC4', size=8)\n\n# Add edges manually\nfor u, v in G_Norway.edges():\n    net.add_edge(u, v)\n\nfor node in net.nodes:\n    if node['id'] not in anchor_nodes_norway:\n        node['label'] = '' \n\n# Minimal settings\nnet.show('norway_updated_network.html')\n\nprint(\"All blue nodes version saved as 'norway_all_blue_nodes.html'\")\nprint(f\"Total nodes: {G_Norway.number_of_nodes()}\")\nprint(f\"Total edges: {G_Norway.number_of_edges()}\")\nprint(f\"Anchor nodes labeled: {len(anchor_nodes_norway)}\")\n\n\n\n\n\nThis code block constructs kinship network graphs for each country in the KINMATRIX dataset and then analyzes how the number of nodes in the family networks vary by age and gender within each country. This analysis helps reveal patterns and differences in family network structure across demographic groups and between countries, providing insight into how kinship connectivity varies by age and gender in different countries.\n\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n\n    graphs[country] = G_filtered\n\ndef analyze_family_degree_by_demographics(G_filtered):\n    degree_by_age = {}\n    degree_by_gender = {}\n\n    # Identify anchor nodes by prerequisite attribute\n    anchor_nodes = [n for n, d in G_filtered.nodes(data=True)\n                    if ('age' in d and 'gender' in d)]\n\n    for anchor_id in anchor_nodes:\n        # Degree of the anchor node (number of kin ties)\n        deg = G_filtered.degree(anchor_id)\n        age = G_filtered.nodes[anchor_id].get('age')\n        gender = G_filtered.nodes[anchor_id].get('gender')\n\n        if age is not None:\n            age_group = age // 10 * 10  \n            degree_by_age.setdefault(age_group, []).append(deg)\n        if gender is not None:\n            degree_by_gender.setdefault(gender, []).append(deg)\n\n    print(\"Mean Degree by Age Group:\")\n    for age_group, degrees in sorted(degree_by_age.items()):\n        print(f\"  Age {age_group}s: Mean Degree = {np.mean(degrees):.2f}, Count = {len(degrees)}\")\n\n    print(\"\\nMean Degree by Gender:\")\n    for gender, degrees in degree_by_gender.items():\n        print(f\"  Gender {gender}: Mean Degree = {np.mean(degrees):.2f}, Count = {len(degrees)}\")\n\n#Show results\nfor country in country_map:\n    if country in graphs:\n        print(f\"\\n--- {country} ---\")\n        analyze_family_degree_by_demographics(graphs[country])\n    else:\n        print(f\"No graph available for {country}\")\n\n\n\nInteractive part: Have a look at our findings from sections 1 and 2, based on frameworks taught in class explain a reason for them.\nSome example findings are listed below, you can try to answer these 3, or look for your own and try to link it to a concept from class.\n\nAge does not seem to be a factor in mean degree. In six out of ten countries, anchors in their 20’s have a higher degree than anchors in their 30’s. Why is this, we would expect older anchors to have larger families?\nWomen tend to have a higher mean degree than men in all countries (Poland: Male = 17.97, Female = 20.25). Why is this?\nThe USA has the largest mean degree out of all countries in the dataset by both age and gender. Is it purely because of the large sample size in the US or are there other factors? We might expect the US to have a lower mean degree especially among anchors aged 20-30 as their parents or grandparents are likely to be immigrants disconnected from their families in their original countries."
  },
  {
    "objectID": "docs/SOCI-415/kinmatrix.html#discussion-1",
    "href": "docs/SOCI-415/kinmatrix.html#discussion-1",
    "title": "SOCI 415 Network Analysis - KINMATRIX Dataset",
    "section": "8.1 Discussion",
    "text": "8.1 Discussion\nThe findings in this section are surprising. Please discuss these findings in your group. Here are some possible topics for discussion:\n\nCross-national variation: Poland and Italy, which are often seen as more traditional and Christian countries show the highest parental separation rates in this dataset.\nPoland has no difference in parental seperation based on political regions, but the United States does. Why is that?\nHow do we interpret the finding that within the US, ‘religious’ families (by self-report) have higher parental separation than non-religious ones? Does anyone have any ideas as to why?"
  },
  {
    "objectID": "docs/hist_workshop/text_embeddings_workshop.html",
    "href": "docs/hist_workshop/text_embeddings_workshop.html",
    "title": "Embedding Driven Text Analysis of Crease’s Stance Towards Chinese Immigrants",
    "section": "",
    "text": "Library Loading\n\n\nCode\n# This cell loads the necessary libraries for executing the notebook.\nimport pandas as pd\nimport numpy as np\nimport re\nimport umap\nimport textwrap\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Patch\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom nltk import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nimport spacy\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.spatial.distance import cosine\nfrom transformers import AutoTokenizer, AutoModel, pipeline\nimport torch\nimport warnings\nfrom collections import defaultdict, Counter\nfrom typing import Dict, Any, Union\n\n\n\n\nIntroduction\n\nOverview of Historical Background\nThe 1884 Chinese Regulation Act in British Columbia is widely regarded as one of the most notorious discriminatory provincial laws targeting Chinese immigration, it was challenged and ultimately declared unconstitutional in the 1885 case of R v. Wing Chong by the judge Henry Pering Pellew Crease. The Justice Crease found the legislation to be unconstitutional on economic grounds; infringing on federal authority over immigration, trade, commerce, treaty-making, and taxation.\nThe central figure in the ruling, Henry Pering Pellew Crease, came from a wealthy English military family, and possessed a prestigious law background.\n\nHis social identity was above-all English, and this was made clear in his politics.\nHe viewed Canada not as a new society to be built, but as an extension of the british empire.\nHe displayed mistrust towards Canadians, referring to them as “North American Chinamen”, afraid that they would “rule the country and job its offices” (Tina Loo).\n\nIn previous years, students expressed interest Crease’s opinion on the 1884 Chinese regulation act, given that the regulation act was strongly condemned and ultimately struck down by Crease. However, this seems at odds with Crease’s position on Chinese immigrants.\nThis raises an interesting question: Did Judge Crease strike down the act because of genuine anti-racism concerns, or because he saw the Chinese immigrant labor force as a valuable asset for growing the Canadian economy?\n\n\nObjective\n\nWe aim to explore this question by analyzing the language used by Justice Crease in his legal opinions and writings related to Chinese immigrants through Natural Language Processing (NLP) approaches. By examining the text, we hope to uncover insights into his stance.\nThe workshop is also to demonstrate how historians can use computational tools to help them answer such a research question, by showing each step in the research process.\nIn the end, we will be able to transform the text documents into something more intuitive and visually appealing, such as a 2D UMAP projection of the legal text embeddings by sentences. This can possibly help historians to better interpret the relationship between different texts.\n\n\n\n\nThe 2D UMAP projection of legal text embeddings by sentences\n\n\n\n\nThe Problem: Legal Text Analysis\nLegal text analysis is itself a complex task, as legal documents are often lengthy, dense, formal, and filled with specialized terminology. They are also often written in neutral or passive voice, making it difficult to discern the author’s personal opinions or biases, it poses unique challenges for historians and legal scholars alike, which also challenged the usual methods of natural language processing (NLP).\nMining insights from such texts requires sophisticated techniques to extract meaningful information and identify patterns. We need the technique to be able to: * Understand legal vocabulary: Legal texts often contain specialized terminology and complex sentence structures, the technique should be able to handle legal jargon and formal language. * Identify contextual semantics: Legal texts often involve nuanced meanings and interpretations, so the technique should be able to capture the context and semantics of legal language. * Handle ambiguity: Legal texts can be ambiguous, with multiple interpretations possible, the technique should be able to handle ambiguity and provide insights into different interpretations. * Extract relevant topics: Legal texts often cover multiple topics and issues, the technique should be able to extract relevant topics and themes from the text. * Analyze sentiment: Legal texts can convey different sentiments, such as positive, negative, or neutral, the technique should be able to analyze sentiment and provide insights into the author’s tone and attitude.\n\n\nResearch Approach\nIn this workshop, we will explore how to address these challenges using a comparison approach, that is, while we focus on the text of Justice Crease, we will compare it with other legal texts from the same period to gain a better understanding of the language used in legal documents at that time.\nThe first subject we will use for comparison is the 1884 Chinese Regulation Act, which was the law that Crease struck down. The second subject we will use for comparison is Justice Matthew Baillie Begbie, who testified alongside Crease in the 1884 Royal Commission on Chinese Immigration.\n\nUnlike Crease, historical accounts describe Begbie as protective of marginalized peoples, particularly Indigenous communities and Chinese immigrants.\nSimilar to what Crease did to the Chinese Regulation Act, Begbie struck down discriminatory municipal by-laws in Victoria that targeted Chinese-owned businesses in the 1888 case of R v. Victoria.\n\nWe use machine learning techniques, specifically text embeddings, to do the following:\n\nCompile a corpus of legal cases and commission reports authored by contemporary judges concerning Chinese immigrants.\nApply Optical Character Recognition (OCR) to the reports in order to convert them to a machine-readable format.\nExamine keywords in the texts, to compare the positions of different justices and regulations.\nUse machine learning to assess the relative emphasis on economic versus social justice concerns.\nUse sentiment analysis to evaluate the tone of the documents, focusing on whether they reflect positive, negative, or neutral sentiments, and compare the sentiments of writings by different authors to identify patterns.\nUse zero-shot classification to evaluate whether the documents reflect pro-discrimination, neutral, or anti-discrimination positions.\n\nThis approach demonstrates different techniques historians can use to identify patterns in documents for analysis.\n\n\nData Collection and Preprocessing\nWe plan to use 10 digitalized texts, they are:\n\nLegal Documents that address Chinese immigration in BC during the period:\n\nR v. Wing Chong\nWong Hoy Woon v. Duncan\nR v. Mee Wah, R v. Victoria\nChinese Regulation Act, 1884\n\nReports authored by Crease and Begbie for the Royal Commission that show the judges’ personal perspectives.\nThe remaining documents enrich our corpus for analysis and supplement our study.\n\nA big issue with working with historical texts is the format they’re stored in: usually scans of varying quality from physical books, articles, etc. However, these are not machine-readable file formats (e.g., text files), so our first step will be using Optical Character Recognition (OCR) to convert the scanned images into machine-readable text. We chose this approach because:\n\nIt is a common technique for digitizing printed texts that is already widely used in legal case archives such as the CanLii database, and\nThere are many OCR tools available that vary in cost, effectiveness, and ease of use.\n\nBelow is a brief overview of early and modern OCR techniques:\n\nEarly OCR (Pattern Matching):\n\nCompared each character image to a library of fonts and shapes.\nWorked well for clean, printed text.\nStruggled with handwriting, unusual fonts, or degraded scans.\n\nModern OCR (Intelligent Recognition):\n\nUses AI to “read” text more like a human.\nAnalyzes shapes, context, and layout.\nHandles messy, handwritten, or complex documents much better.\n\n\nAfter testing several tools, we found that modern, AI-based OCR methods produced the most accurate results for our historical documents.\n\n\nData Overview\nAfter OCR, we obtained a .csv file containing the text and metadata of the documents. Note that we removed the direct quotes of the 1884 Chinese Regulation Act in Crease’s ruling, as they don’t reflect his own language. The structure of the data is as follows: | Column Name | Description | | —————————– | ——————————————————– | | filename | Name of the file containing the document text. | | author | Author of the document (e.g., “Crease”, “Begbie”). | | type | Document type (e.g., “case”, “report”). | | text | Full text of the document, which may include OCR errors. | | act_quote_sentences_removed | Number of quoted sentences removed from the full text. |\nHere, we read the .csv file into a pandas DataFrame and display.\n\n\nCode\n# Load the dataset\ndf = pd.read_csv(\"data/metadata_cleaned.csv\")\n\ndf\n\n\nWe are also interested in the length of each document, as it can provide insights into the depth and complexity of the text. Therefore, we create a summary below quantifying the number of characters in each document.\n\n\nCode\n# Summary the distribution of document lengths\n# Create a DataFrame to store the document lengths\ndoc_lengths = []\n\n# Measure lengths of each document by number of characters\nfor row in df.iterrows():\n    text_length = len(row[1]['text'])\n    doc_lengths.append({'Document': row[1]['filename'], 'Length': text_length})\n\n# Convert to DataFrame and display\ndoc_lengths_df = pd.DataFrame(doc_lengths)\ndoc_lengths_df\n\n\n\n\n\nHow Computers Interpret Text?\nWhile computers can process text swiftly, they do not “understand” it in the human sense. Instead, they build mathematical models of language from statistical patterns and structural regularities. These models produce symbolic and continuous representations of words and passages that allow downstream algorithms to detect topics, relationships, and affective signals. However, these representations remain proxies for meaning rather than literal comprehension.\nThis process typically involves a sequence of steps:\n\nTokenization: Breaking text into analyzable units (words, subwords, or sentences).\nPreprocessing: Cleaning and normalizing text (lowercasing, removing OCR noise, handling archaic spelling).\nVectorization: Converting tokens or texts into numerical vectors.\n\nSimple count-based approaches (TF-IDF) capture term importance across documents.\nModern contextual methods (BERT, Legal‑BERT) produce dense embeddings that capture usage-dependent semantics.\n\nModeling and Comparison: Applying algorithms to those vectors. Examples include cosine similarity for semantic closeness, UMAP for visualization, and zero‑shot classification.\nAggregation and Interpretation: Aggregating sentence- or snippet-level outputs to produce document- or author-level summaries (mean stance vectors, topic distributions), followed by careful human interpretation.\n\nWhy this is helpful for social science and humanities research:\n\nScalability: Enables analysis of large corpora beyond human reading capacity.\nPattern Discovery: Uncovers latent structures and relationships not easily seen by humans.\nQuantification: Provides numerical measures of abstract concepts (e.g., sentiment, stance).\nReproducibility: Offers systematic, repeatable methods for text analysis.\n\n\nCount Approach: TF-IDF\nThe Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure that evaluates the importance of a word in a document relative to a collection of documents (corpus). It is one of the earliest and most widely used methods for text analysis. It is essentially a count-based approach that quantifies the importance of words in a document based on their frequency and distribution across multiple documents. TF-IDF works by calculating two components: 1. Term Frequency (TF): Measures how frequently a term appears in a document. 2. Inverse Document Frequency (IDF): Measures how important a term is across the entire corpus, by considering how many documents contain the term.\nFor our purpose, we can use TF-IDF to identify the most important words in each document, which can help us understand the key themes and topics discussed in the text. More details on what we are going to do:\n\nRegroup the text data into 5 groups:\n\nAll writings\nCrease’s writings\nBegbie’s writings\nChinese Regulation Act\nOther documents\n\nFor each group, we will:\n\nCreate a TF-IDF vectorizer to convert the text into numerical vectors.\nRemove common filler words (“the”, “and”, etc.).\nCalculate the TF-IDF scores for each word in the documents.\nIdentify the most important words based on their TF-IDF scores.\n\nThe most frequent remaining words can reveal the main topics of each case.\n\n\n\nCode\n# Define the function to preprocess text in a DataFrame column\ndef preprocess_text(text_string):\n    \"\"\"\n    Cleans and preprocesses text by:\n    1. Converting to lowercase\n    2. Removing punctuation and numbers\n    3. Tokenizing\n    4. Removing English stop words \n    5. Removing words with 4 or fewer characters\n    \"\"\"\n    # Start with the standard English stop words\n    stop_words = set(stopwords.words('english'))\n    \n    # Add custom domain-specific stop words if needed\n    custom_additions = {'would', 'may', 'act', 'mr', 'sir', 'also', 'upon', 'shall'}\n    stop_words.update(custom_additions)\n    \n    # Lowercase and remove non-alphabetic characters\n    processed_text = text_string.lower()\n    processed_text = re.sub(r'[^a-z\\s]', '', processed_text)\n    \n    # Tokenize\n    tokens = processed_text.split()\n    \n    # Filter out stop words AND short words in a single step\n    filtered_tokens = [\n        word for word in tokens \n        if word not in stop_words and len(word) &gt; 4\n    ]\n    # Re-join the words into a single string\n    return \" \".join(filtered_tokens)\n\n\n\n\nCode\n# Apply the function to create the 'processed_text' column\ndf['processed_text'] = df['text'].apply(preprocess_text)\n\n# Display the first few rows of the processed text\ndf['processed_text'].head(5)\n\n\n\n\nCode\n# Perform TF-IDF vectorization on the processed text\n\n# Regrouping the DataFrame for better representation\ndf['group'] = 'Other'\ndf.loc[df['author'] == 'Crease', 'group'] = 'Crease'\ndf.loc[df['author'] == 'Begbie', 'group'] = 'Begbie'\ndf.loc[df['author'] == 'Others', 'group'] = 'Regulation Act'\n\n# Load the vectorizer and transform the processed text\n# This calculates IDF based on word rarity across ALL individual texts.\nvectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 3))\ntfidf_matrix = vectorizer.fit_transform(df['processed_text'])\n\n# Create a new DataFrame with the TF-IDF scores\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n\n# Add the 'group' column to this TF-IDF DataFrame for aggregation\ntfidf_df['group'] = df['group'].values\n\n# Group by author and calculate the MEAN TF-IDF score for each word\nmean_tfidf_by_group = tfidf_df.groupby('group').mean()\n\n# Calculate TF-IDF for the combined corpus (\"All\") using the same vectorizer\nprocessed_all = \" \".join(df['processed_text'])\nall_vec = vectorizer.transform([processed_all]).toarray().ravel()\nall_series = pd.Series(all_vec, index=feature_names, name='All')\n\n# Add the \"All\" row to the grouped TF-IDF DataFrame\nmean_tfidf_by_group = pd.concat([all_series.to_frame().T, mean_tfidf_by_group], axis=0)\n\n# Collect top words and arrange them into a side-by-side DataFrame\nlist_of_author_dfs = []\nfor group_name in ['All', 'Crease', 'Begbie', 'Regulation Act', 'Other']:\n    if group_name not in mean_tfidf_by_group.index:\n        # If a group is missing, append an empty frame to keep column alignment\n        empty_df = pd.DataFrame({group_name: [], f'{group_name}_score': []})\n        list_of_author_dfs.append(empty_df)\n        continue\n\n    # Get the top 10 terms and scores for the current author/group\n    top_words = mean_tfidf_by_group.loc[group_name].sort_values(ascending=False).head(10)\n\n    # Convert the Series to a DataFrame\n    top_words_df = top_words.reset_index()\n    top_words_df.columns = [group_name, f'{group_name}_score']\n\n    list_of_author_dfs.append(top_words_df)\n\n# Concatenate the list of DataFrames horizontally\nfinal_wide_df = pd.concat(list_of_author_dfs, axis=1)\n\n# Display the final combined DataFrame (includes \"All\")\nfinal_wide_df\n\n\nUndoubtedly, the TF-IDF practice on our corpus has identified some interesting patterns, such as:\n\nThe emphasis on “Chinese” in all groups.\nThe emphasis on “labor” in Crease’s writings.\nThe emphasis on “license” in Begbie’s writings.\nAnd the emphasis on “dollars” in the Chinese Regulation Act.\nOther texts put “Canada” on the top of the list, and “legislation” right after “Chinese”.\n\nHowever, this approach has limitations, as it does not capture the semantic meaning of words or their relationships to each other. For example, it cannot distinguish between “Chinese” as a noun and “Chinese” as an adjective, or between “labor” as a noun and “labor” as a verb. It also does not consider the context in which words are used, which can lead to misinterpretation of their meaning.\n\n\nEmbedding Approach\nWith the advancement of machine learning, text embeddings emerged as a more powerful technique for text analysis. It represents words or phrases as dense vectors in a high-dimensional space, capturing semantic relationships between them. This allows for more nuanced understanding of text, enabling tasks like similarity measurement, clustering, and classification.\nThere are several popular text embedding models, including: - Word2Vec: A neural network-based model that learns word embeddings by predicting context words given a target word (or vice versa). - GloVe: A global vector representation model that learns word embeddings by factorizing the word co-occurrence matrix. - FastText: An extension of Word2Vec that represents words as bags of character n-grams, allowing it to handle out-of-vocabulary words and capture subword information. - BERT: A transformer-based model that generates contextualized embeddings by considering the entire sentence context, allowing it to capture word meanings based on their surrounding words.\nIn this workshop, we will use a BERT-based model to generate text embeddings for our corpus. nlpaueb/legal-bert-base-uncased is a BERT model pre-trained on English legal texts, including legislation, law cases, and contracts. It is designed to capture the legal language and semantics, making it suitable for our analysis.\nHowever, we must note that the model is not perfect and may still have limitations in understanding the nuances of legal language, especially in historical texts.\n\n\n\nWord Embeddings\n\nCreating Word Embeddings\nWhile the model itself has the ability to generate word embeddings that capture the semantic meaning of words, we still need to design our own strategy to extract these meanings from our corpus.\n\nLoad LEGAL-BERT model and tokenizer.\nTokenize sentences into smaller subword units using a tokenizer.\nProcess each tokenized sentence through the model to extract hidden layer representations.\nCombine subword embeddings to form a single vector for each word by averaging the embeddings of its subword components.\nAggregate embeddings for repeated words across sentences by averaging their vectors.\nReturn a dictionary mapping each word to its mean embedding, capturing its semantic meaning in the context of the text.\n\nIn this way, we are not only able to generate word embeddings with contextual meanings over the whole corpus, but also be able to aggregate our corpus into different groups, and generate contextualized word embeddings for each group.\n\n\nCode\n# We will use the Legal-BERT model for this task\ntokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\nmodel = AutoModel.from_pretrained('nlpaueb/legal-bert-base-uncased').eval() # set the model to evaluation mode\n\n# Define a function to embed words using the tokenizer and model\ndef embed_words(sentences, tokenizer=tokenizer, model=model, target_words=None,\n                device=None, max_length=512):\n    \"\"\"\n    Returns a dictionary {word: mean_embedding}.\n    Only the mean embedding (float32 numpy array) per word is kept.\n    \"\"\"\n    if device is None:\n        try:\n            device = next(model.parameters()).device\n        except Exception:\n            device = torch.device(\"cpu\")\n    device = torch.device(device)\n    model.to(device).eval()\n\n    target_set = None if target_words is None else set(target_words)\n\n    sums = {}   # word -&gt; torch.Tensor sum of embeddings\n    counts = {} # word -&gt; occurrence count\n\n    with torch.no_grad():\n        for sent in sentences:\n            enc = tokenizer(\n                sent,\n                return_tensors=\"pt\",\n                truncation=True,\n                max_length=max_length\n            )\n            enc = {k: v.to(device) for k, v in enc.items()}\n            outputs = model(**enc)\n            hidden = outputs.last_hidden_state.squeeze(0)  # (seq_len, hidden)\n            tokens = tokenizer.convert_ids_to_tokens(enc[\"input_ids\"][0])\n\n            i = 0\n            while i &lt; len(tokens):\n                tok = tokens[i]\n                if tok in (\"[CLS]\", \"[SEP]\", \"[PAD]\"):\n                    i += 1\n                    continue\n\n                # Gather wordpieces\n                j = i + 1\n                piece_embs = [hidden[i]]\n                word = tok[2:] if tok.startswith(\"##\") else tok\n                while j &lt; len(tokens) and tokens[j].startswith(\"##\"):\n                    piece_embs.append(hidden[j])\n                    word += tokens[j][2:]\n                    j += 1\n\n                if target_set is not None and word not in target_set:\n                    i = j\n                    continue\n\n                word_emb = torch.stack(piece_embs, dim=0).mean(dim=0)\n                if word in sums:\n                    sums[word] += word_emb\n                    counts[word] += 1\n                else:\n                    sums[word] = word_emb.clone()\n                    counts[word] = 1\n                i = j\n\n    return {w: (sums[w] / counts[w]).cpu().numpy() for w in sums}\n\n\n\n\nCode\n# Define a function to clean and preprocess text\ndef clean_text(text):\n    \n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    \n    return text.strip()\n\n\n\n\nCode\nwarnings.filterwarnings(\"ignore\")\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# Group texts to form a single text per group\ngrouped_texts = df.groupby('group')['text'].apply(lambda x: ' '.join(x)).reset_index()\n\n# Add a row for the combined text of all groups\ngrouped_texts = pd.concat(\n    [grouped_texts, pd.DataFrame([{'group': 'All', 'text': ' '.join(df['text'])}])],\n    ignore_index=True\n)\n\n# Create new columns for word and sentence tokens\ngrouped_texts['word_tokens'] = grouped_texts['text'].apply(lambda x: word_tokenize(clean_text(x)))\n# Sentence tokenization using spaCy\ngrouped_texts['sentence_tokens'] = grouped_texts['text'].apply(lambda x: [sent.text for sent in nlp(x).sents])\n\n# Apply clean_text to the sentence tokens\ngrouped_texts['sentence_tokens'] = grouped_texts['sentence_tokens'].apply(\n    lambda x: [clean_text(sent) for sent in x]\n)\n\n\n\n\nCode\n# Embed the words in each group\ngrouped_texts['word_embeddings'] = grouped_texts['sentence_tokens'].apply(\n    lambda x: embed_words(x)\n    )\n\n# Compute the number of unique words in each group\ngrouped_texts['num_unique_words'] = grouped_texts['word_tokens'].apply(lambda x: len(set(x)))\n\ngrouped_texts.head()\n\n\nWe created word embeddings of all tokens in each group, respectively. The word embeddings are stored in a dictionary format, where each key is a word and the value is its corresponding embedding vector.\nIt is clear that the word embeddings of the same word in different groups are different, which reflects the contextualized meaning of the word in each group.\n\nFor example, the word “Chinese” has a different embedding in Crease’s writings compared to Begbie’s writings, indicating that the two authors used the word in different contexts and with different connotations.\nHowever, since they were embedded using the same model, the word embeddings of the same word in different groups are still similar, which reflects the shared meaning of the word across different contexts.\nThe dimensionality of all word embeddings is 768, which is the size of the hidden layer in the LEGAL-BERT model we used.\n\n\n\nCode\n# Display the word embedding of Chinese for the whole corpus\nchinese_embedding = grouped_texts[grouped_texts['group'] == 'All']['word_embeddings'].values[0].get('chinese')\n\n# Display first 20 dimensions for brevity\nprint(f\"First 20 Dimensions of Word Embedding for 'Chinese' in the Full Corpus:\\n {chinese_embedding[:20]}\\n\")\nprint(f\"Total Dimensions of Word Embedding for 'Chinese': {len(chinese_embedding)}\\n\")\n\n\n\n\nCode\n# Display the word embedding of Chinese in Crease's text\ncrease_embeddings = grouped_texts[grouped_texts['group'] == 'Crease']['word_embeddings'].values[0]\n# Display first 20 dimensions for brevity\nprint(f\"First 20 Dimensions of Word Embeddings for 'Chinese' in Crease's Text:\\n{crease_embeddings.get('chinese')[:20]}\\n\") \nprint(f\"Total Dimensions of Word Embeddings for 'Chinese' in Crease's Text: {len(crease_embeddings.get('chinese'))}\\n\")\n\n\n\n\nCode\nbegbie_embeddings = grouped_texts[grouped_texts['group'] == 'Begbie']['word_embeddings'].values[0]\n# Display first 20 dimensions for brevity\nprint(f\"First 20 Dimensions of Word Embeddings for 'Chinese' in Begbie's Text:\\n{begbie_embeddings.get('chinese')[:20]}\\n\")\nprint(f\"Total Dimensions of Word Embeddings for 'Chinese' in Begbie's Text: {len(begbie_embeddings.get('chinese'))}\\n\")\n\n\n\n\nMeasurement of Similarity\nAnother important aspect of word embeddings is the ability to measure the similarity between words based on their embeddings. This can be done using cosine similarity, which calculates the cosine of the angle between two vectors in the embedding space. The cosine similarity ranges from 0 to 1, where:\n\n0 indicates no similarity (orthogonal vectors)\n1 indicates perfect similarity (identical vectors)\nThe closer the cosine similarity is to 1, the more similar the words are in meaning.\n\nThis allows us to identify related words and concepts based on their embeddings, enabling us to explore the semantic relationships between words in our corpus. And more importantly, it doesn’t only allows us to measure the similarity between words, but also allows us to measure the similarity between sentences, paragraphs, and even entire documents, as long as they are represented as vectors in the same embedding space.\nThe math behind cosine similarity is as follows: \\[\n\\text{cosine similarity}(a, b) = \\frac{a \\cdot b}{||a|| \\cdot ||b||}\n\\] Where \\(a\\) and \\(b\\) are the embedding vectors of the two words, and \\(||a||\\) and \\(||b||\\) are their Pythagorean norms (lengths).\nFocusing on the word “Chinese”, we can calculate its cosine similarity with other words in the same group to identify related terms. This can help us understand how the word is used in different contexts and how it relates to other concepts. Here, we will list out the top 10 most similar words to “Chinese” in each group, along with their cosine similarity scores.\nNote: All words are put into lowercase.\n\n\nCode\n# Compute top-10 most similar words to target for EVERY group (including \"All\")\ntarget = \"chinese\"\ntop_n = 10\nall_results = []\n# Iterate through each group and compute similarities\nfor _, grp_row in grouped_texts.iterrows():\n    group = grp_row['group']\n    emb_dict = grp_row['word_embeddings']\n    if target not in emb_dict:\n        continue\n    target_vec = emb_dict[target]\n    sims = []\n    for w, vec in emb_dict.items():\n        if w == target:\n            continue\n        try:\n            sim = 1 - cosine(target_vec, vec)\n        except Exception:\n            continue\n        sims.append((w, sim))\n    sims_sorted = sorted(sims, key=lambda x: x[1], reverse=True)[:top_n]\n    for rank, (w, sim) in enumerate(sims_sorted, 1):\n        all_results.append({'group': group, 'rank': rank, 'word': w, 'similarity': sim})  # Use :4f for better readability\n\nsimilar_words_df = pd.DataFrame(all_results)\n\n# Display the first few rows of the DataFrame with similar words\nsims_wide = similar_words_df.pivot(index='rank', columns='group', values='similarity')\nwords_wide = similar_words_df.pivot(index='rank', columns='group', values='word')\n\n# Combine with a tidy multi-level column index: \nwide_combined = pd.concat({'word': words_wide, 'similarity': sims_wide}, axis=1)\nwide_combined = (\n    wide_combined.swaplevel(0,1, axis=1)\n                 .sort_index(axis=1, level=0)\n)\n\nwide_combined  # Display\n\n\n\n\n\nEmbedding Driven Text Analysis\n\nCreating Keyword-Focused Stance Embeddings\nIn comparison to generating word embeddings, modeling stance of each text is more challenging, as it requires us to capture the author’s position on a specific issue or topic. Oftentimes, the stance is not explicitly stated in the text, but rather implied through the language used.\nThere is not a universal optimum for stance modeling, as it depends on the specific context and the author’s perspective. However, we can use a combination of techniques to create focused embeddings that capture the stance of each text. The strategy we used is as follows:\n\nTokenize the text into smaller units and identify the positions of specific keywords or phrases that are relevant to the stance being analyzed.\nFor each occurrence of the keywords, extract a surrounding “window” of text to capture the context in which the keywords are used.\nRepresent the text in the window as numerical vectors using a pre-trained language model, which encodes the meaning of the words and their relationships.\nCombine the vectors within each window using a pooling method (e.g., averaging or selecting the maximum value) to create a single representation for the context around the keyword.\nIf multiple occurrences of the keywords are found, average their representations to create a unified vector that captures the overall stance in the text.\nIf no keywords are found, use a fallback representation based on the overall text.\n\nThis approach thus allows us to create focused embeddings that capture the stance of each text focusing on specific keywords or phrases. The sentence is used as the basic unit of analysis here, but larger chunks of text can also be used if needed.\nIn the end, we will store the lists of embeddings in a dictionary format, where each key is the author and the value is a list of embeddings for each text authored by that author.\n\n\nCode\ndef embed_text(\n    text,\n    focus_token=None,\n    window=10,\n    pooling=\"mean\",  # \"mean\" (default), \"max\", or \"min\"\n    tokenizer=tokenizer,\n    model=model):\n\n    # Get stopwords for filtering\n    stop_words = set(stopwords.words('english'))\n    \n    # Run the model once\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    hidden = outputs.last_hidden_state.squeeze(0) \n\n    if focus_token is None:\n        return hidden[0].cpu().numpy()\n    \n    # Normalize to list\n    keywords = (\n        [focus_token] if isinstance(focus_token, str)\n        else focus_token\n    )\n\n    # Pre-tokenize each keyword to its subtoken ids\n    kw_token_ids = {\n        kw: tokenizer.convert_tokens_to_ids(tokenizer.tokenize(kw))\n        for kw in keywords\n    }\n\n    input_ids = inputs[\"input_ids\"].squeeze(0).tolist()\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    spans = []  # list of (start, end) index pairs\n\n    # find every match of every keyword\n    for kw, sub_ids in kw_token_ids.items():\n        L = len(sub_ids)\n        for i in range(len(input_ids) - L + 1):\n            if input_ids[i:i+L] == sub_ids:\n                spans.append((i, i+L))\n\n    if not spans:\n        # fallback on CLS vector\n        return hidden[0].cpu().numpy()\n\n    # For each span, grab the window around it\n    vecs = []\n    for (start, end) in spans:\n        lo = max(1, start - window)\n        hi = min(hidden.size(0), end + window)\n        \n        # Filter out stopwords from the window\n        non_stop_indices = [i for i in range(lo, hi) \n                           if tokens[i] not in stop_words and not tokens[i].startswith('##')]\n        \n        # If all tokens are stopwords, use the original window\n        if not non_stop_indices:\n            span_vec = hidden[lo:hi]\n        else:\n            span_vec = hidden[non_stop_indices]\n        \n        if pooling == \"mean\":\n            pooled = span_vec.mean(dim=0)\n        elif pooling == \"max\":\n            pooled = span_vec.max(dim=0).values\n        elif pooling == \"min\":\n            pooled = span_vec.min(dim=0).values\n        else:\n            raise ValueError(f\"Unknown pooling method: {pooling}\")\n        \n        vecs.append(pooled.cpu().numpy())\n\n    # Average across all spans\n    return np.mean(np.stack(vecs, axis=0), axis=0)\n\n\n\n\nCode\ncrease_cases = df[(df['author'] == 'Crease') & (df['type'] == 'case')]['text'].tolist()\nbegbie_cases = df[(df['author'] == 'Begbie') & (df['type'] == 'case')]['text'].tolist()\nact_1884 = df[df['type'] == 'act']['text'].tolist()\n\nact_dict = {\n    'Crease': crease_cases,\n    'Begbie': begbie_cases,\n    'Act 1884': act_1884}\n\n\n\n\nCode\nact_snippets = {}\n\nkeywords = [\"Chinese\", \"China\", \"Chinaman\", \"Chinamen\", \n            \"immigrant\", \"immigrants\", \"alien\", \"aliens\", \n            \"immigration\"]\n\nfor auth, texts in act_dict.items():\n    snippets = []\n    for txt in texts:\n        # Sentence tokenize using Spacy\n        sentence = [sent.text for sent in nlp(txt).sents]\n        for sent in sentence:\n            if any(keyword in sent for keyword in keywords):\n                snippets.append(sent)\n    act_snippets[auth] = snippets\n\n\n\n\nCode\n# Investigate the length of the snippets\nn_snippet = {auth: len(snippets) for auth, snippets in act_snippets.items()}\n\nprint(\"Snippet size by author:\")\nfor auth, num in n_snippet.items():\n    print(f\"{auth}: {num}\")\n\n\n\n\nCode\n# Create embeddings\nembeddings_dict = {'Crease': [], 'Begbie': [], 'Act 1884': []}\n\nfor auth, snippets in act_snippets.items():\n    for snip in snippets:\n        v = embed_text(snip, focus_token=keywords, window=15)\n        embeddings_dict[auth].append(v) \n\n\n\n\nMeasuring Stance Similarity\nJust like word embeddings, cosine similarity can also be used to measure the stance similarity between texts. The interpretation of cosine similarity in this context is similar to that of word embeddings, where a higher cosine similarity indicates a stronger alignment in stance between two texts.\nWith sentence being the basic unit of analysis, we can calculate the overall cosine similarity between each pair of authors’ texts in various ways, but here we will focus on two of them: 1. Mean Embeddings: We calculate the mean embedding for each author’s texts and then compute the cosine similarity between these mean embeddings. This gives us a single similarity score for each pair of authors, reflecting their overall stance alignment. 2. Pairwise Embeddings: We calculate the cosine similarity between each pair of texts authored by different authors, then average the scores to get a more comprehensive view of stance alignment across all texts.\nNote that similarity scores are not deterministic, as they depend on the specific texts and the context in which the keywords are used. However, they can provide valuable insights into the stance of each author and how it relates to other authors’ positions. This reinforces the idea that stance is not a fixed attribute, but rather a dynamic and context-dependent aspect of language.\n\n\nCode\n# Compute the pairwise cosine similarity\nmean_crease = np.mean(embeddings_dict[\"Crease\"], axis=0, keepdims=True)\nmean_begbie = np.mean(embeddings_dict[\"Begbie\"], axis=0, keepdims=True)\nmean_act_1884 = np.mean(embeddings_dict[\"Act 1884\"], axis=0, keepdims=True)\n\nsim_crease_begbie = cosine_similarity(mean_crease, mean_begbie)[0, 0]\nsim_crease_act_1884 = cosine_similarity(mean_crease, mean_act_1884)[0, 0]\nsim_begbie_act_1884 = cosine_similarity(mean_begbie, mean_act_1884)[0, 0]\n\nprint(f\"Cosine similarity between mean Crease and mean Begbie: {sim_crease_begbie:.4f}\")\nprint(f\"Cosine similarity between mean Crease and mean Act 1884: {sim_crease_act_1884:.4f}\")\nprint(f\"Cosine similarity between mean Begbie and mean Act 1884: {sim_begbie_act_1884:.4f}\")\n\n\n\n\nCode\n# Extract embeddings for Crease, Begbie and the Act 1884\ncrease_embeddings = embeddings_dict[\"Crease\"]\nbegbie_embeddings = embeddings_dict[\"Begbie\"]\nact_1884_embeddings = embeddings_dict[\"Act 1884\"]\n\n# Define a function to compute mean cosine similarity\ndef mean_cosine_similarity(embeddings1, embeddings2):\n    similarities = [\n        1 - cosine(e1, e2)\n        for e1 in embeddings1\n        for e2 in embeddings2\n    ]\n    return sum(similarities) / len(similarities)\n\n# Extract embeddings\ncrease_emb = embeddings_dict[\"Crease\"]\nbegbie_emb = embeddings_dict[\"Begbie\"]\nact_1884_emb = embeddings_dict[\"Act 1884\"]\n\n# Compute mean similarities\ncrease_begbie_sim = mean_cosine_similarity(crease_emb, begbie_emb)\ncrease_act_sim = mean_cosine_similarity(crease_emb, act_1884_emb)\nbegbie_act_sim = mean_cosine_similarity(begbie_emb, act_1884_emb)\n\n# Output\nprint(f\"Mean cosine similarity between Crease and Begbie embeddings: {crease_begbie_sim:.4f}\")\nprint(f\"Mean cosine similarity between Crease and Act 1884 embeddings: {crease_act_sim:.4f}\")\nprint(f\"Mean cosine similarity between Begbie and Act 1884 embeddings: {begbie_act_sim:.4f}\")\n\n\n\n\nVisualizing Text Embeddings\nWhile the embeddings themselves are high-dimensional vectors (in our case, 768-dimensional), we can visualize them in a lower-dimensional space (e.g., 2D or 3D) using dimensionality reduction techniques such as UMAP (Uniform Manifold Approximation and Projection).\nUMAP is a dimensionality reduction technique that projects high-dimensional embeddings into a 2D space while preserving local structure, making it ideal for visualizing our embeddings.\nUsing Plotly Express, we create an interactive scatter plot where each point represents a text snippet, colored by author, with hover functionality to display the corresponding sentence. This visualization highlights clusters and relationships between snippets, offering insights into semantic similarities across authors.\n\n\nCode\n# Set seed for umap reproducibility\nall_vecs = np.vstack(embeddings_dict[\"Crease\"] + embeddings_dict[\"Begbie\"] + embeddings_dict[\"Act 1884\"])\nlabels  = ([\"Crease\"] * len(embeddings_dict[\"Crease\"])) + ([\"Begbie\"] * len(embeddings_dict[\"Begbie\"])) + (['Act 1884'] * len(embeddings_dict[\"Act 1884\"]))\n\nreducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=42)\nproj = reducer.fit_transform(all_vecs) \n\ndef wrap_text(text, width=60):\n    return '&lt;br&gt;'.join(textwrap.wrap(text, width=width))\n\n\n\n\nCode\numap_df = pd.DataFrame(proj, columns=['UMAP 1', 'UMAP 2'])\numap_df['Author'] = labels\numap_df['Text'] = [snip for auth in act_snippets for snip in act_snippets[auth]]\numap_df['Text'] = umap_df['Text'].apply(lambda t: wrap_text(t, width=60))\n\nfig = px.scatter(umap_df, x='UMAP 1', y='UMAP 2', \n                 color='Author', hover_data=['Text'], \n                 width=800, height=500 )\nfig.update_traces(marker=dict(size=5))\nfig.update_layout(title='UMAP Projection of Stance Embeddings by Author')\nfig.show()\n\n\n\n\nInvestigating Texts\nThe stance embeddings ultimately serve as analytical tools to support our text analysis objectives.\n\nBy calculating the “conceptual mean stance” for each author, we gain a quantitative basis for comparing the positions of different authors.\nHowever, embeddings alone cannot fully capture the nuances of language or the complexity of an author’s stance. To truly understand the perspectives reflected in the texts, it is essential to investigate the sentences that are most similar to the conceptual average position of each author.\n\nHere, we will examine the top 10 sentences with the highest stance similarity to the mean stance of each author.\nThis approach allows us to delve deeper into the texts, uncovering how the language used aligns with the calculated average stance and providing richer insights into the authors’ positions on the issue of Chinese immigrants.\n\n\nCode\n# Print out the 10 most similar embedding sentences to Crease's mean embedding\n\ncrease_similarity_df = pd.DataFrame(columns=['Author', 'Text', 'Similarity Score'])\n\n# Iterate through the embeddings and their corresponding sentences\nfor auth, snippets in act_snippets.items():\n    for snippet, emb in zip(snippets, embeddings_dict[auth]):\n        similarity = cosine_similarity(emb.reshape(1, -1), mean_crease)[0][0]\n        crease_similarity_df.loc[len(crease_similarity_df)] = [auth, snippet, similarity]\n        \n# Sort by similarity score\ncrease_sorted_similarity = crease_similarity_df.sort_values(by='Similarity Score', ascending=False)\n\nprint(\"Top 10 most similar sentences to Crease's mean embedding:\\n\")\nfor _, row in crease_sorted_similarity.head(10).iterrows():\n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    print(f\"Author: {row['Author']}\\nSentence: {wrapped_para}\\nSimilarity Score: {row['Similarity Score']:.4f}\\n\")\n\n\n\n\nCode\n# Print out the 10 most similar embedding sentences to Begbie's mean embedding\n\nbegbie_similarity_df = pd.DataFrame(columns=['Author', 'Text', 'Similarity Score'])\n\n# Iterate through the embeddings and their corresponding sentences\nfor auth, snippets in act_snippets.items():\n    for snippet, emb in zip(snippets, embeddings_dict[auth]):\n        similarity = cosine_similarity(emb.reshape(1, -1), mean_begbie)[0][0]\n        begbie_similarity_df.loc[len(begbie_similarity_df)] = [auth, snippet, similarity]\n        \n# Sort by similarity score\nbegbie_sorted_similarity = begbie_similarity_df.sort_values(by='Similarity Score', ascending=False)\n\nprint(\"Top 10 most similar sentences to Begbie's mean embedding:\\n\")\nfor _, row in begbie_sorted_similarity.head(10).iterrows():\n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    print(f\"Author: {row['Author']}\\nSentence: {wrapped_para}\\nSimilarity Score: {row['Similarity Score']:.4f}\\n\")\n\n\n\n\nCode\n# Print out the 10 most similar embedding sentences to the Regulation Act's mean embedding\n\nregulation_similarity_df = pd.DataFrame(columns=['Author', 'Text', 'Similarity Score'])\n\n# Iterate through the embeddings and their corresponding sentences\nfor auth, snippets in act_snippets.items():\n    for snippet, emb in zip(snippets, embeddings_dict[auth]):\n        similarity = cosine_similarity(emb.reshape(1, -1), mean_act_1884)[0][0]\n        regulation_similarity_df.loc[len(regulation_similarity_df)] = [auth, snippet, similarity]\n        \n# Sort by similarity score\nregulation_sorted_similarity = regulation_similarity_df.sort_values(by='Similarity Score', ascending=False)\n\nprint(\"Top 10 most similar sentences to the Regulation Act's mean embedding:\\n\")\nfor _, row in regulation_sorted_similarity.head(10).iterrows():\n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    print(f\"Author: {row['Author']}\\nSentence: {wrapped_para}\\nSimilarity Score: {row['Similarity Score']:.4f}\\n\")\n\n\n\n\n\nTopic Modeling and Alignment Analysis\n\n\nCode\nfinal_wide_df\n\n\nIn the TF-IDF analysis, we observed that the word “Chinese” was a prominent term across all groups, indicating its centrality to the discussions in all texts. Following the word “Chinese”, we also noticed that “labor”, “white”, “legislation”, and “taxation” were all significant terms reflected in all texts. This make us wonder: How do these topics relate to each other, and how do they align with the stances of different authors?\nIn natural language processing, topic modeling is a technique used to identify and extract topics from a collection of documents. It involves analyzing the words and phrases in the text to identify patterns and themes that can be grouped together into topics. Oftentimes, topic modeling are performed using unsupervised learning algorithms such as Latent Dirichlet Allocation (LDA). However, these methods may not be suitable for our corpus, as they require a large amount of text data and may not capture the nuances of legal language.\nTherefore, we will use a different approach to explore the topics in our corpus, by leveraging the word embeddings we have already generated. The strategy we will use is as follows:\n\nIdentify Key Terms: We will focus on the key terms identified in the TF-IDF analysis, such as “Chinese”, “labor”, “white”, “legislation”, and “taxation”. These terms will serve as anchors for our topic analysis.\nCalculate Cosine Similarity: For each key term, we will calculate its cosine similarity with other words in the same group to identify related terms. This will help us understand how the key terms are used in different contexts and how they relate to other concepts.\nAggregate Related Terms: We will aggregate the related terms for each key term to form a topic. This will allow us to identify the main topics discussed in each group and how they align with the stances of different authors.\nAnalyze Topic Alignment: We will analyze the alignment of the identified topics with the stances of different authors. This will help us understand how the topics reflect the authors’ positions on the issue of Chinese immigrants.\n\n\n\nCode\n# Define our target \"topics\"\ntarget_words = [\"labor\", \"legislation\", \"license\", \"taxation\"]\n\n# Find most similar words to the keywords using the \"All\" group embeddings\nall_emb = grouped_texts.loc[grouped_texts['group'] == 'All', 'word_embeddings'].values\nif len(all_emb) == 0:\n    raise ValueError(\"No 'All' group found in grouped_texts\")\nall_emb = all_emb[0]\n\ntop_n = 10\nresults = {}\n\nfor target in target_words:\n    target_vec = all_emb.get(target)\n    if target_vec is None:\n        # fill with NaN if target missing\n        results[target] = [np.nan] * top_n\n        continue\n\n    sims = []\n    for w, vec in all_emb.items():\n        if w == target:\n            continue\n        try:\n            sim = 1 - cosine(target_vec, vec)\n        except Exception:\n            continue\n        sims.append((w, sim))\n\n    sims_sorted = sorted(sims, key=lambda x: x[1], reverse=True)[:top_n]\n    results[target] = [w for w, _ in sims_sorted]\n\n# Create DataFrame with targets as columns and ranks as rows\nsimilar_words_df = pd.DataFrame(results)\nsimilar_words_df\n\n\n\n\nCode\n# Create anchors for the topics\ndef create_anchor(topic, similar_df=similar_words_df, top_n=10):\n\n    t = topic\n\n    # collect words: topic + top_n similar words \n    similar_words = similar_df[t].astype(str).tolist()[:top_n]\n    words = [t] + [w.lower() for w in similar_words]\n\n    # deduplicate while preserving order\n    seen = set()\n    uniq_words = []\n    for w in words:\n        if w not in seen:\n            seen.add(w)\n            uniq_words.append(w)\n\n    # embed each word and average\n    vecs = []\n    for w in uniq_words:\n        emb = embed_text(w)  \n        vecs.append(emb)\n\n    return np.mean(np.stack(vecs, axis=0), axis=0)\n\n\n\n\nCode\n# Create anchors for the topics\nlabor_anchor = create_anchor(\"labor\")\nlegislation_anchor = create_anchor(\"legislation\")\nlicense_anchor = create_anchor(\"license\")\ntaxation_anchor = create_anchor(\"taxation\")\n\n# Create a DataFrame to hold the anchors\nanchors_df = pd.DataFrame({\n    'Topic': ['labor', 'legislation', 'license', 'taxation'],\n    'Anchor Vector': [labor_anchor, legislation_anchor, license_anchor, taxation_anchor]\n})\n\nanchors_df\n\n\n\n\nCode\n# Calculate the cosine similarity between each anchor and the mean embeddings of Crease, Begbie, and the Act 1884\n# Visualize the results as a box plot\ndef calculate_similarity(anchor, embeddings):\n    return cosine_similarity(anchor.reshape(1, -1), embeddings).flatten()\n\n# Create a DataFrame to hold the similarity scores\nsimilarity_scores = {\n    'Author': [],\n    'Topic': [],\n    'Text': [],\n    'Similarity Score': []\n}\n\nfor topic in anchors_df['Topic']:\n    anchor_vector = anchors_df.loc[anchors_df['Topic'] == topic, 'Anchor Vector'].values[0]\n\n    for author in ['Crease', 'Begbie', 'Act 1884']:\n        emb_list = embeddings_dict.get(author, [])\n        texts = act_snippets.get(author, [])\n\n        if len(emb_list) == 0:\n            continue\n\n        embeddings = np.vstack(emb_list)\n        sim_scores = calculate_similarity(anchor_vector, embeddings)\n\n        for idx, score in enumerate(sim_scores):\n            similarity_scores['Author'].append(author)\n            similarity_scores['Topic'].append(topic)\n            similarity_scores['Text'].append(texts[idx] if idx &lt; len(texts) else \"\")\n            similarity_scores['Similarity Score'].append(float(score))\n\n# Convert to DataFrame\nsimilarity_df = pd.DataFrame(similarity_scores)\n\n\n\n\nCode\n# prepare authors and topics\npreferred_order = ['Crease', 'Begbie', 'Act 1884']\nauthors = [a for a in preferred_order if a in similarity_df['Author'].unique()]\ntopics = list(similarity_df['Topic'].unique())[:4] \n\n# Color Blind friendly palette\nauthor_palette = sns.color_palette(\"colorblind\", n_colors=len(authors))\n\nsns.set(style=\"whitegrid\", context=\"notebook\")\nfig, axes = plt.subplots(2, 2, figsize=(8, 6), sharey=False)\n\nfor i in range(4):\n    ax = axes.flat[i]\n    if i &lt; len(topics):\n        topic = topics[i]\n        df_t = similarity_df[similarity_df['Topic'] == topic]\n\n        # draw boxplot\n        sns.boxplot(\n            data=df_t,\n            x='Author',\n            y='Similarity Score',\n            order=authors,\n            palette=author_palette,\n            width=0.6,\n            fliersize=0,\n            ax=ax,\n            boxprops=dict(linewidth=0.9),\n            medianprops=dict(linewidth=1.1, color='black'),\n            whiskerprops=dict(linewidth=0.9),\n            capprops=dict(linewidth=0.9)\n        )\n\n        # compute per-author means and overlay them\n        means = df_t.groupby('Author')['Similarity Score'].mean().reindex(authors)\n        x_positions = list(range(len(authors)))\n        # plot white diamond with black edge so it stands out on colored boxes\n        ax.scatter(x_positions, means.values, marker='D', s=60,\n                   facecolors='white', edgecolors='black', zorder=10)\n\n        # robust y-limits \n        vals = df_t['Similarity Score'].dropna()\n        if len(vals) == 0:\n            ymin, ymax = -1.0, 1.0\n        else:\n            q1 = vals.quantile(0.25)\n            q3 = vals.quantile(0.75)\n            iqr = q3 - q1\n            if iqr == 0:\n                whisker_low = float(vals.min())\n                whisker_high = float(vals.max())\n            else:\n                whisker_low = float(q1 - 1.5 * iqr)\n                whisker_high = float(q3 + 1.5 * iqr)\n\n            span = max(whisker_high - whisker_low, 1e-6)\n            pad = max(span * 0.08, 0.03)\n            ymin = max(-1.0, whisker_low - pad)\n            ymax = min(1.0, whisker_high + pad)\n            if ymin &gt;= ymax:\n                mid = float(vals.median())\n                ymin = max(-1.0, mid - 0.05)\n                ymax = min(1.0, mid + 0.05)\n\n        ax.set_ylim(ymin, ymax)\n\n        ax.set_title(f\"Topic: {topic}\", fontsize=12, weight='semibold')\n        ax.set_xlabel('')\n        ax.set_ylabel('Cosine Similarity' if i % 2 == 0 else '')\n        ax.axhline(0.0, color='grey', linestyle='--', linewidth=0.8, alpha=0.6)\n        ax.tick_params(axis='x', rotation=0, labelsize=10)\n        ax.tick_params(axis='y', labelsize=9)\n        for spine in ax.spines.values():\n            spine.set_linewidth(0.8)\n        sns.despine(ax=ax, trim=True, left=False, bottom=False)\n    else:\n        ax.set_visible(False)\n\n# Single legend for authors\nlegend_handles = [Patch(facecolor=author_palette[idx], label=authors[idx]) for idx in range(len(authors))]\nfig.legend(handles=legend_handles, title='Author', loc='upper right', frameon=True)\n\nplt.tight_layout(rect=[0, 0, 0.95, 0.96])\nfig.suptitle('Topic Similarity by Author\\n(Mean Labeled by Diamond)', fontsize=14, y=0.99)\nplt.show()\n\n\n\n\nLLM and Zero-Shot Classification\nAnother powerful technique for text analysis is zero-shot classification, which allows us to classify text into predefined categories without requiring labeled training data, with the help of large language models (LLMs). This approach is particularly useful when we have a limited amount of labeled data or when the categories are not well-defined.\nIn addition to classifying text into specific categories, zero-shot classification can also be used to evaluate the stance of a text towards a particular issue or topic by calculating the probability of the text belonging to each category, the calculated probabilities will sum to 1 and they can be interpreted as the model’s confidence in each category. In this workshop, we mainly focus on the second aspect, which allows us to assess the stance of each text.\nWe will use the Hugging Face Transformers library to implement zero-shot classification using a pre-trained model facebook/bart-large-mnli. The model is trained on the Multi-Genre Natural Language Inference (MultiNLI) dataset, which contains pairs of sentences labeled with their relationship (pros, cons, or neutral). This allows the model to learn how to classify text based on its semantic meaning and context, which is particularly useful for our analysis of historical texts.\nThe key steps in our zero-shot classification process are as follows:\n\nDefine the zero-shot pipeline: We create a zero-shot classification pipeline using the pre-trained model and tokenizer from Hugging Face Transformers, and specify a hypothesis template “In this snippet of a historical legal text, the author {}” that will be used to generate hypotheses for classification.\nDefine the candidate labels: We define a set of candidate labels that represent the stance categories we want to classify the text into, which correspond to the basic stance categories we are interested in, such as “pro”, “neutral” or “cons” the equal rights of Chinese immigrants.\nClassify the text: We apply the zero-shot classification pipeline to each text in our corpus, generating a probability distribution over the candidate labels for each text.\nInvestigate the results: We analyze the classification results to identify the stance of each text, focusing on the highest probability label for each text. We also calculate the average probability for each candidate label across all texts to compare the overall stance of different authors and documents.\n\n\n\nCode\n# Create the full snippets dictionary\nact_1884_full = \" \".join(act_1884)\ncrease_cases_full = \" \".join(crease_cases)\nbegbie_cases_full = \" \".join(begbie_cases)\n\nfull_cases = {\"Crease\": crease_cases_full, \"Begbie\": begbie_cases_full, \"Act 1884\": act_1884_full}\n\n\n\n\nCode\n# We create a dictionary to hold the full snippets for each author\nfull_snippets = {}\nfor author, text in full_cases.items():\n    # Tokenize using Spacy\n    sentence = [sent.text for sent in nlp(text).sents]\n    snippets = []\n    for sent in sentence:\n        if len(sent) &gt; 30:  # Filter out short and meaningless sentences created by tokenization\n            snippets.append(sent)\n            \n    full_snippets[author] = snippets\n\n\n\n\nCode\n# Create a DataFrame to display snippet size by author\nsnippet_sizes = [{'Author': auth, 'Snippet Count': len(snippets)} for auth, snippets in full_snippets.items()]\nsnippet_sizes_df = pd.DataFrame(snippet_sizes)\n\n# Display the DataFrame\nprint(snippet_sizes_df)\n\n\nTo ensure that the zero-shot classification results are meaningful, we carefully treat the candidate labels as prompts that guide the model’s understanding of the stance categories. This allows us to leverage the model’s ability to generalize and adapt to new tasks without requiring extensive retraining or fine-tuning. Here, we will use the following candidate labels: - Pro: “… advocates for equal legal treatment of Chinese immigrants compared to white or European settlers, opposing racial discrimination” - Neutral: “… describes or retells the status or treatment of Chinese immigrants without expressing support or opposition to racial inequality, is unrelated to Chinese immigrants, or cannot be classified as either” - Cons: “… justifies or reinforces unequal legal treatment of Chinese immigrants relative to white or European settlers, supporting racially discriminatory policies”\nHowever, we must note that this is also a major limitation of the zero-shot classification approach, as it relies on the quality and relevance of the candidate labels to the text being classified. If the labels are not well-defined or do not accurately reflect the stance categories, the classification results may be misleading or inaccurate. This is particularly important when working with historical texts, where the language and context may differ significantly from modern usage. Therefore, it is essential to carefully select and define the candidate labels to ensure that they accurately reflect the stance categories we are interested in.\n\n\nCode\n# Create pipeline for zero-shot classification with error handling and efficiency improvements\nwarnings.filterwarnings(\"ignore\")\n\nzero_shot = pipeline(\n    \"zero-shot-classification\",\n    model=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\",\n    tokenizer=\"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\",\n    hypothesis_template=\"In this snippet of a historical legal text, the author {}.\",\n    device=0 if torch.cuda.is_available() else -1  # Use GPU if available\n)\n\n\n# Simplified and clearer labels for better classification\nlabels = [\n    \"advocates for equal legal treatment of Chinese immigrants compared to white or European settlers, opposing racial discrimination\",\n    \"describes or retells the status or treatment of Chinese immigrants without expressing support or opposition to racial inequality, is unrelated to Chinese immigrants, or cannot be classified as either\",\n    \"justifies or reinforces unequal legal treatment of Chinese immigrants relative to white or European settlers, supporting racially discriminatory policies\"\n]\n\ndef get_scores(snippet, max_length=512):\n    # Ensure snippet is not empty and is reasonable length\n    if not snippet or len(snippet.strip()) &lt; 10:\n        return {label: 0.33 for label in labels}  # Return neutral scores for empty/short text\n\n    # Truncate if too long to avoid token limit issues\n    if len(snippet) &gt; max_length * 4:  \n        snippet = snippet[:max_length * 4]\n\n    # Run classification\n    out = zero_shot(snippet, candidate_labels=labels, truncation=True, max_length=max_length)\n\n    # Create score dictionary\n    score_dict = dict(zip(out[\"labels\"], out[\"scores\"]))\n\n    # Ensure all labels are present\n    for label in labels:\n        if label not in score_dict:\n            score_dict[label] = 0.0\n\n    return score_dict\n\n\nWe can test the zero-shot classification pipeline on a sample text to see how it works. For example, we can use a paragraph from Sir Chapleau’s report to the Royal Commission on Chinese immigration:\n\nThat assuming Chinese immigrants of the laboring class will persist in retaining their present characteristics of Asiatic life, where these are strikingly peculiar and distinct from western, and that the influx will continue to increase, this immigration should be dealt with by Parliament ; but no legislation should be such as would give a shock to great interests and enterprises established before any probability that Parliament would interfere with that immigration arose. Questions of vested rights might come up, and these ought to be carefully considered before action is taken.\n\n\n\nCode\n# Define the snippet\nchapleau_snippet = \"That assuming Chinese immigrants of the laboring class will persist in retaining their present characteristics of Asiatic life, where these are strikingly peculiar and distinct from western, and that the influx will continue to increase, this immigration should be dealt with by Parliament; but no legislation should be such as would give a shock to great interests and enterprises established before any probability that Parliament would interfere with that immigration arose. Questions of vested rights might come up, and these ought to be carefully considered before action is taken.\"\n\n# Get the scores for the snippet\nchapleau_scores = get_scores(chapleau_snippet)\n\n# Display the scores\nprint(\"Classification Scores for Chapleau's Snippet:\")\nfor label, score in chapleau_scores.items():\n    print(f\"{score:.4f}: {label}\")\n\n\n\nSentence Approach\nAnother limitation of the zero-shot classification is that the number of tokens in the text is limited, which means that we cannot classify the entire text at once if it exceeds the token limit. To avoid exceeding the token limit, we can split the text into smaller chunks, such as sentences or windows of text, and classify each chunk separately.\nThe sentence approach is to classify each sentence in the text separately, which allows us to capture the stance of each sentence and its relationship to the overall text. This approach is particularly useful when the text is long or complex, as it allows us to analyze the stance of each sentence in isolation.\nHowever, it has limitations in understanding the overall stance of the text, as it does not consider the context in which the sentences are used and therefore may capture too much variation in the stance of the text.\n\n\nCode\n# # Run zero-shot classification on the snippets from the Chinese Regulation Act 1884\n# act_scores = {}\n\n# for auth, snippets in full_snippets.items():\n#     scores = []\n#     for snip in snippets:\n#         score = get_scores(snip)\n#         scores.append(score)\n#     act_scores[auth] = scores\n\n# rows = []\n\n# for auth, snippets in full_snippets.items():\n#     for snip, score_dict in zip(snippets, act_scores[auth]):\n#         row = {\n#             \"Author\": auth,\n#             \"Text\": snip,\n#             \"Pro\": score_dict[labels[0]],\n#             \"Neutral\": score_dict[labels[1]],\n#             \"Cons\": score_dict[labels[2]]\n#         }\n#         rows.append(row)\n\n# # Create DataFrame to store the scores\n# df_scores = pd.DataFrame(rows)\n\n# # Save the DataFrame to a CSV file\n# df_scores.to_csv(\"data/zero_shot_sentence_scores.csv\", index=False)\n\n\n\n\nCode\n# Read the saved DataFrame\ndf_scores = pd.read_csv(\"data/zero_shot_sentence_scores.csv\")\n\n# Print out the top 10 sentences with the highest \"Pro\" scores\ntop_pro_sentences = df_scores.nlargest(10, 'Pro')\n\nprint(\"\\nTop 10 sentences with the highest 'Pro' scores:\\n\")\n\nfor _, row in top_pro_sentences.iterrows():\n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    print(f\"Author: {row['Author']}\\nSentence: {wrapped_para}\\nPro Score: {row['Pro']:.4f}\\n\")\n\n\n\n\nCode\n# Print out the top 10 sentences with the highest \"Cons\" scores\ntop_cons_sentences = df_scores.nlargest(10, 'Cons')\n\nprint(\"\\nTop 10 sentences with the highest 'Cons' scores:\\n\")\n\nfor _, row in top_cons_sentences.iterrows():\n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    print(f\"Author: {row['Author']}\\nSentence: {wrapped_para}\\nCons Score: {row['Cons']:.4f}\\n\")\n\n\n\n\nCode\n# Group by author and calculate mean scores\nmean_scores = df_scores.groupby(\"Author\")[[\"Pro\", \"Neutral\", \"Cons\"]].mean()\nmedian_scores = df_scores.groupby(\"Author\")[[\"Pro\", \"Neutral\", \"Cons\"]].median()\n\nprint(\"Mean scores by author:\")\nprint(mean_scores)\n\nprint(\"\\nMedian scores by author:\")\nprint(median_scores)\n\n\n\n\nCode\ndf_scores['Wrapped Text'] = df_scores['Text'].apply(lambda t: wrap_text(t, width = 50))\n\nfig = px.scatter(\n    df_scores,\n    x=\"Pro\",\n    y=\"Cons\",\n    color=\"Author\",\n    hover_data=[\"Wrapped Text\"],\n    title=\"Pros vs Cons Scores by Author\",\n    width=800,\n    height=600\n)\n\nfig.update_traces(marker=dict(size=5))\nfig.show()\n\n\n\n\nWindow Approach\nIn comparison to the sentence approach, the window approach is to classify larger chunks of text that contain multiple sentences with an overlapping context between windows. This allows us to capture the stance of the text in a more holistic way, while still being able to classify each window separately.\nThe limitation of this approach is that it may not capture the nuances of each sentence, as it treats the window containing multiple sentences as a single unit. However, it allows us to capture the overall stance of the text while still being able to classify each window separately.\nHere, we define the function to split the text into overlapping windows of a specified size (maximum number of tokens) with a certain overlap (stride that overlap between consecutive windows). We then apply the zero-shot classification pipeline to each window and average the results to obtain the final classification for the entire text.\n\n\nCode\n# Define a function to chunk text into overlapping windows\ndef chunk_into_windows(text, max_tokens=512, stride=128):\n    \n    # Break into sentences first for cleaner boundaries\n    sents = sent_tokenize(text)\n    windows = []\n    current = \"\"\n    for sent in sents:\n        # Tentative window if we add this sentence\n        cand = current + \" \" + sent if current else sent\n        # Count tokens\n        n_tokens = len(tokenizer.encode(cand, add_special_tokens=False))\n        if n_tokens &lt;= max_tokens:\n            current = cand\n        else:\n            # finalize current window, then start new from overlapping tail\n            windows.append(current)\n            # keep the stride tokens from the end of the current window\n            tail_ids = tokenizer.encode(current, add_special_tokens=False)[-stride:]\n            tail_text = tokenizer.decode(tail_ids)\n            current = tail_text + \" \" + sent\n    if current:\n        windows.append(current)\n    return windows\n\n\n\n\nCode\n# # Run classification per author\n# rows = []\n# for author, text in full_cases.items():\n    \n#     windows = chunk_into_windows(text, max_tokens=256, stride=64)\n    \n#     # classify each window\n#     for win in windows:\n#         out = zero_shot(win, candidate_labels=labels, truncation=True, max_length=256)\n#         # Extract scores and labels\n#         score_dict = dict(zip(out[\"labels\"], out[\"scores\"]))\n#         rows.append({\n#             \"Author\": author,\n#             \"Text\": win,\n#             \"Pro\": score_dict[labels[0]],\n#             \"Neutral\": score_dict[labels[1]],\n#             \"Cons\": score_dict[labels[2]]\n#         })\n\n# all_scores = pd.DataFrame(rows)\n\n# # Save the DataFrame to a CSV file\n# all_scores.to_csv(\"data/zero_shot_windowed_scores.csv\", index=False)\n\n\n\n\nCode\n# Read the saved DataFrame\nall_scores = pd.read_csv(\"data/zero_shot_windowed_scores.csv\")\n\n# Print out the top 5 windows with the highest \"Pro\" scores\ntop_pro_windows = all_scores.nlargest(5, 'Pro')\n\nprint(\"\\nTop 5 windows with the highest 'Pro' scores:\\n\")\nfor _, row in top_pro_windows.iterrows():\n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    print(f\"Author: {row['Author']}\\nWindow: {wrapped_para}\\nPro Score: {row['Pro']:.4f}\\n\")\n\n\n\n\nCode\n# Print out the top 5 windows with the highest \"Cons\" scores\ntop_cons_windows = all_scores.nlargest(5, 'Cons')\n\nprint(\"\\nTop 5 windows with the highest 'Cons' scores:\\n\")\nfor _, row in top_cons_windows.iterrows():\n    wrapped_para = textwrap.fill(row['Text'], width=100)\n    print(f\"Author: {row['Author']}\\nWindow: {wrapped_para}\\nCons Score: {row['Cons']:.4f}\\n\")\n\n\n\n\nCode\n# Calculate the mean scores and median scores for each author\nmean_scores = all_scores.groupby(\"Author\")[[\"Pro\", \"Neutral\", \"Cons\"]].mean()\nmedian_scores = all_scores.groupby(\"Author\")[[\"Pro\", \"Neutral\", \"Cons\"]].median()\n\nprint(\"Mean scores by author:\")\nprint(mean_scores)\n\nprint(\"\\nMedian scores by author:\")\nprint(median_scores)\n\n\n\n\nCode\nall_scores['Text'] = all_scores['Text'].apply(lambda t: wrap_text(t, width = 50))\n\nfig = px.scatter(\n    all_scores,\n    x=\"Pro\",\n    y=\"Cons\",\n    color=\"Author\",\n    hover_data=[\"Text\"],\n    title=\"Pros vs Cons Scores by Author\",\n    width=800,\n    height=600\n)\n\nfig.update_traces(marker=dict(size=5))\nfig.show()\n\n\nWhile both approach successfully identified that the Chinese Regulation Act of 1884 was more discriminatory towards Chinese people, and the relative positions of the three authors matched our expectations, we note that the zero-shot classification results are not deterministic, as they still depend on the specific language used in defining the candidate labels and the context in which the text is used.\nFor instance, the window\n\n…act the legal presumption of innocence until conviction is reversed ; in every case the onus probandi, though in a statute highly penal, is shifted from the informant on to the shoulders of the accused, and he a foreigner not knowing one word of the law, or even the language of the accuser. In other words, every Chinese is guilty until proved innocent—a provision which fills one conversant with subjects with alarm; for if such a law can be tolerated as against Chinese, the precedent is set, and in time of any popular outcry can easily be acted on for putting any other foreigners or even special classes among ourselves, as coloured people, or French, Italians, Americans, or Germans, under equally the same law. That certainly is interfering with aliens. The proposition that it is a Provincial tax for revenue purposes, supposing it to be so intended under the provisions of the Act, is so manifestly calculated to defeat that object, by diminishing the numbers of the members of the persons to be affected by it, that it is difficult to regard it in that light, or in any other light than an indirect mode of getting rid of persons whom it affects out of the country.\n\nis classified as “cons” by the zero-shot classification, which is not accurate, as it essentially reflects Crease’s criticism of the discriminatory nature of the law, and there is no indication that he supports the unequal treatment of Chinese immigrants in this passage.\nThis highlights the limitations of zero-shot classification when dealing with historical legal texts: because the semantics and context of these texts are not obvious, and the rhetorical devices used in them are difficult for models to accurately capture.\nSuch example emphasizes the importance of human interpretation and analysis in understanding the positions in historical texts, and also highlights the need for caution when selecting and defining candidate labels to ensure that these labels accurately reflect the position categories we are interested in. We should still only use machine learning techniques as an auxiliary tool in research.\n\n\n\nPutting It All Together\nNow that we’ve examined topic alignment and quantified stances toward Chinese immigrants using zero‑shot classification, we synthesize the results to evaluate how thematic emphasis correlates with expressed stance. By combining TF‑IDF keywords, embedding‑based topic anchors, and zero‑shot probabilities, we assess whether focus on themes such as “labor”, “taxation”, or “legislation” corresponds with pro‑, neutral, or anti‑discriminatory language across authors and documents.\nThe steps we will take are as follows: 1. Aggregate Topic Alignment Scores: For each sentence, we will aggregate the topic alignment scores to each topic that we previously identified using TF-IDF. 2. Combine with Zero-Shot Scores: We will combine the aggregated topic alignment scores with the zero-shot classification scores to create a paired dataset of topic alignment and stance scores for each sentence. 3. Analyze Correlation: We will analyze the correlation between the topic alignment scores and the zero-shot classification scores to identify patterns and relationships between the topics and the stances expressed in the texts. We will also calculate the correlation coefficients for each author to see how their topic alignments relate to their stances.\n\n\nCode\n# define a function to clean text for matching\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  \n    return text.strip()\n\n# Convert the similarity DataFrame to wide format\nsimilarity_wide = similarity_df.pivot(index=['Author','Text'], columns='Topic', values='Similarity Score').reset_index()\n\n# Clean the text for matching\nsimilarity_wide['Clean Text'] = similarity_wide['Text'].apply(clean_text)\n\nprint(f\"The total size of our corpus is {len(similarity_wide)} sentences.\")\n\n\n\n\nCode\n# Clean the text in df_scores for matching\ndf_scores['Clean Text'] = df_scores['Text'].apply(clean_text)\n\n# Match the cleaned text in similarity_wide with df_scores\nmatched_rows = []\nfor _, row in similarity_wide.iterrows():\n    match = df_scores[df_scores['Clean Text'] == row['Clean Text']]\n    if not match.empty:\n        matched_row = {\n            'Author': row['Author'],\n            'Text': row['Text'],\n            'Pro': match['Pro'].values[0] if 'Pro' in match else None,\n            'Neutral': match['Neutral'].values[0] if 'Neutral' in match else None,\n            'Cons': match['Cons'].values[0] if 'Cons' in match else None,\n            **row.drop(['Author', 'Text', 'Clean Text']).to_dict()\n        }\n        matched_rows.append(matched_row)\n        \n# Create a DataFrame from the matched rows\nmerged_df = pd.DataFrame(matched_rows)\n\nprint(f\"The size of our merged corpus is {len(merged_df)} sentences.\")\n\n\nmerged_df.head()\n\n\n\n\nCode\n# Ensure wrapped text for hover\nmerged_df['wrapped_text'] = merged_df['Text'].apply(lambda t: wrap_text(t, width=50))\n\nauthor_order = ['Crease', 'Begbie', 'Act 1884']\ncolor_map = {'Crease': '#1f77b4', 'Begbie': '#d62728', 'Act 1884': '#2ca02c'}\n\ndef plot_topic_vs_stance(df, topic='labor', authors=None, color_map=None, show_regression=True,\n                         width=800, height=800):\n    df = df.copy()\n\n    # Prepare text wrapping if needed\n    if 'wrapped_text' not in df.columns:\n        df['wrapped_text'] = df['Text'].apply(lambda t: wrap_text(t, width=50))\n\n    # Default authors if not provided\n    if authors is None:\n        authors = df['Author'].unique()\n\n    # Function to create a slightly darker variant of a color for regression lines\n    def darken_color(color, factor=0.7):\n        \n        if isinstance(color, str) and color.startswith('#'):\n            hex_color = color.lstrip('#')\n            rgb = tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n            darkened_rgb = tuple(int(c * factor) for c in rgb)\n            return f\"rgb({darkened_rgb[0]}, {darkened_rgb[1]}, {darkened_rgb[2]})\"\n        return color  # Return as-is if not hex format\n\n    default_colors = [\n        '#2E86C1', '#E74C3C', '#28B463', '#F39C12', '#8E44AD',\n        '#17A2B8', '#FD7E14', '#20C997', \"#87020F\", '#6F42C1'\n    ]\n\n    if color_map is None:\n        color_map = {author: default_colors[i % len(default_colors)]\n                     for i, author in enumerate(authors)}\n\n    # More distinct symbols\n    symbols = ['circle', 'square', 'diamond', 'triangle-up', 'star', 'hexagon']\n    author_symbols = {author: symbols[i % len(symbols)] for i, author in enumerate(authors)}\n\n    # Create subplots with reduced vertical spacing for a tighter layout\n    fig = make_subplots(\n        rows=2, cols=1,\n        shared_xaxes=True,\n        subplot_titles=(f\"Pro Score vs {topic.title()} Similarity\",\n                        f\"Cons Score vs {topic.title()} Similarity\"),\n        horizontal_spacing=0.06,\n        vertical_spacing=0.06\n    )\n\n    # Track which authors have data for legend management\n    authors_with_data = set()\n\n    for author in authors:\n        df_author = df[df['Author'] == author]\n        author_color = color_map.get(author, '#7F8C8D')\n        author_symbol = author_symbols.get(author, 'circle')\n\n        # Pro scores (top subplot)\n        df_pro = df_author.dropna(subset=['Pro', topic])\n        if len(df_pro) &gt; 0:\n            authors_with_data.add(author)\n\n            fig.add_trace(\n                go.Scatter(\n                    x=df_pro[topic],\n                    y=df_pro['Pro'],\n                    mode='markers',\n                    marker=dict(\n                        color=author_color,\n                        size=8,\n                        opacity=0.85,\n                        symbol=author_symbol,\n                        line=dict(color='white', width=1.25)\n                    ),\n                    name=author,\n                    legendgroup=author,\n                    hovertext=df_pro['wrapped_text'],\n                    hovertemplate=(\n                        f\"&lt;b&gt;{author}&lt;/b&gt;&lt;br&gt;\"\n                        f\"{topic.title()} Similarity: %{{x:.3f}}&lt;br&gt;\"\n                        \"Pro Score: %{y:.3f}&lt;br&gt;\"\n                        \"%{hovertext}&lt;br&gt;\"\n                        \"&lt;extra&gt;&lt;/extra&gt;\"\n                    ),\n                ),\n                row=1, col=1\n            )\n\n            # Regression\n            if show_regression and len(df_pro) &gt;= 3:\n                x_vals = df_pro[topic].values\n                y_vals = df_pro['Pro'].values\n                coeffs = np.polyfit(x_vals, y_vals, 1)\n                x_range = np.linspace(x_vals.min(), x_vals.max(), 100)\n                y_pred = np.polyval(coeffs, x_range)\n\n                fig.add_trace(\n                    go.Scatter(\n                        x=x_range,\n                        y=y_pred,\n                        mode='lines',\n                        line=dict(\n                            color=darken_color(author_color),\n                            width=2,\n                            dash='dot'\n                        ),\n                        name=f\"{author} trend\",\n                        legendgroup=author,\n                        showlegend=False,\n                        hoverinfo='skip'\n                    ),\n                    row=1, col=1\n                )\n\n        # Cons scores (bottom subplot)\n        df_cons = df_author.dropna(subset=['Cons', topic])\n        if len(df_cons) &gt; 0:\n            fig.add_trace(\n                go.Scatter(\n                    x=df_cons[topic],\n                    y=df_cons['Cons'],\n                    mode='markers',\n                    marker=dict(\n                        color=author_color,\n                        size=8,\n                        opacity=0.85,\n                        symbol=author_symbol,\n                        line=dict(color='white', width=1.25)\n                    ),\n                    name=author,\n                    legendgroup=author,\n                    showlegend=False, \n                    hovertext=df_cons['wrapped_text'],\n                    hovertemplate=(\n                        f\"&lt;b&gt;{author}&lt;/b&gt;&lt;br&gt;\"\n                        f\"{topic.title()} Similarity: %{{x:.3f}}&lt;br&gt;\"\n                        \"Cons Score: %{y:.3f}&lt;br&gt;\"\n                        \"%{hovertext}&lt;br&gt;\"\n                        \"&lt;extra&gt;&lt;/extra&gt;\"\n                    ),\n                ),\n                row=2, col=1\n            )\n\n            if show_regression and len(df_cons) &gt;= 3:\n                x_vals = df_cons[topic].values\n                y_vals = df_cons['Cons'].values\n                coeffs = np.polyfit(x_vals, y_vals, 1)\n                x_range = np.linspace(x_vals.min(), x_vals.max(), 100)\n                y_pred = np.polyval(coeffs, x_range)\n\n                fig.add_trace(\n                    go.Scatter(\n                        x=x_range,\n                        y=y_pred,\n                        mode='lines',\n                        line=dict(\n                            color=darken_color(author_color),\n                            width=2,\n                            dash='dot'\n                        ),\n                        showlegend=False,\n                        hoverinfo='skip'\n                    ),\n                    row=2, col=1\n                )\n\n    # Update axes styling\n    fig.update_xaxes(\n        title_text=\"\",\n        showgrid=True,\n        gridwidth=1,\n        gridcolor='rgba(128,128,128,0.18)',\n        zeroline=True,\n        zerolinewidth=1,\n        zerolinecolor='rgba(128,128,128,0.25)',\n        row=1, col=1\n    )\n    \n    fig.update_xaxes(\n        title_text=f\"{topic.title()} Topic Similarity\",\n        showgrid=True,\n        gridwidth=1,\n        gridcolor='rgba(128,128,128,0.18)',\n        zeroline=True,\n        zerolinewidth=1,\n        zerolinecolor='rgba(128,128,128,0.25)',\n        row=2, col=1\n    )\n\n    fig.update_yaxes(\n        title_text=\"Pro Score\",\n        showgrid=True,\n        gridwidth=1,\n        gridcolor='rgba(128,128,128,0.18)',\n        zeroline=True,\n        zerolinewidth=1,\n        zerolinecolor='rgba(128,128,128,0.25)',\n        row=1, col=1\n    )\n\n    fig.update_yaxes(\n        title_text=\"Cons Score\",\n        showgrid=True,\n        gridwidth=1,\n        gridcolor='rgba(128,128,128,0.18)',\n        zeroline=True,\n        zerolinewidth=1,\n        zerolinecolor='rgba(128,128,128,0.25)',\n        row=2, col=1\n    )\n\n    # Layout: title moved to top left, legend moved to top right\n    fig.update_layout(\n        width=width,\n        height=height,\n        title=dict(\n            text=f\"Stance Towards Chinese Immigrants vs {topic.title()} Topic Similarity\",\n            x=0.02,  # Move title to top left\n            xanchor='left',\n            font=dict(size=16, color='#2C3E50'),\n            pad=dict(t=12, b=20)\n        ),\n        legend=dict(\n            title=None,\n            orientation='v',  # Vertical orientation for top right\n            y=0.98,  # Position at top\n            yanchor='top',\n            x=0.98,  # Position at right\n            xanchor='right',\n            bgcolor='rgba(255,255,255,0.92)',\n            bordercolor='rgba(52,73,94,0.12)',\n            borderwidth=1,\n            font=dict(size=11),\n            traceorder='normal'\n        ),\n        template=\"plotly_white\",\n        plot_bgcolor='rgba(248,249,250,0.95)',\n        margin=dict(t=90, b=80, l=60, r=60),\n        font=dict(family=\"Arial, sans-serif\", color='#2C3E50')\n    )\n\n    # Subplot title styling (keep compact)\n    if len(fig.layout.annotations) &gt;= 2:\n        fig.layout.annotations[0].update(font=dict(size=13, color='#34495E'))\n        fig.layout.annotations[1].update(font=dict(size=13, color='#34495E'))\n\n    fig.show()\n    return\n\n\n\n\nCode\n# Example usage for all four topics:\nplot_topic_vs_stance(merged_df, topic=\"labor\", color_map=color_map, show_regression=True)\n\n\n\n\nCode\n# Create visualization for legislation\nplot_topic_vs_stance(merged_df, topic=\"legislation\", color_map=color_map, show_regression=True)\n\n\n\n\nCode\n# Create visualization for license\nplot_topic_vs_stance(merged_df, topic=\"license\", color_map=color_map, show_regression=True)\n\n\n\n\nCode\n# Create visualization for taxation\nplot_topic_vs_stance(merged_df, topic=\"taxation\", color_map=color_map, show_regression=True)\n\n\nCorrelation coefficients will be calculated to quantify the strength and direction of the relationship between topic alignment and each stance scores. A positive correlation indicates that as the topic alignment score increases, the stance score also tends to increase (e.g., more pro‑discriminatory language), while a negative correlation indicates the opposite (e.g., more anti‑discriminatory language). The closer the correlation coefficient is to 1 or -1, the stronger the relationship between the topic alignment and stance scores.\nThe math behind correlation coefficients is as follows: \\[\n\\text{corr}(X, Y) = \\frac{\\text{cov}(X, Y)}{\\sigma_X \\cdot \\sigma_Y}\n\\] Where \\(X\\) and \\(Y\\) are the topic alignment scores and stance scores, respectively, \\(\\text{cov}(X, Y)\\) is the covariance between the two variables, and \\(\\sigma_X\\) and \\(\\sigma_Y\\) are their standard deviations.\n\n\nCode\n# Summarize the correlation between topic similarity and stance scores\ncorrelation_results = []\n\n# Overall correlations (across all authors)\nfor topic in ['labor', 'legislation', 'license', 'taxation']:\n    for stance in ['Pro', 'Cons']:\n        corr = merged_df[[topic, stance]].corr().iloc[0, 1]\n        correlation_results.append({\n            'Author': 'All Authors',\n            'Topic': topic,\n            'Stance': stance,\n            'Correlation': corr\n        })\n\n# Correlations by individual author\nfor author in merged_df['Author'].unique():\n    author_data = merged_df[merged_df['Author'] == author]\n    for topic in ['labor', 'legislation', 'license', 'taxation']:\n        for stance in ['Pro', 'Cons']:\n            corr = author_data[[topic, stance]].corr().iloc[0, 1]\n            correlation_results.append({\n                'Author': author,\n                'Topic': topic,\n                'Stance': stance,\n                'Correlation': corr\n            })\n        \ncorrelation_df = pd.DataFrame(correlation_results)\n\n# Pivot the DataFrame to show correlations in a more readable format\ncorrelation_pivot = correlation_df.pivot_table(\n    index=['Author', 'Topic'], \n    columns='Stance', \n    values='Correlation'\n).reset_index()\n\ncorrelation_pivot.columns.name = None  # Remove the name of the columns index\n\n# Convert to wide format for better display\ncorrelation_wide = correlation_pivot.pivot_table(\n    index='Author',\n    columns='Topic',\n    values=['Pro', 'Cons']\n).round(4)\n\nprint(\"\\nCorrelation Coefficients of Topic Alignments and Pro/Cons Scores by Each Author:\")\ncorrelation_wide\n\n\nThese enable us to see how different topics relate to the stances expressed in the texts, and whether certain topics are more associated with pro‑, neutral, or anti‑discriminatory language.\n\n\nConclusion\nIn this workshop, we explored how to effectively digitalize aged text document, as well as how to use various computational techniques to analyze historical legal texts, focusing on the stance of different authors towards Chinese immigrants in British Columbia. We applied a range of methods, including TF-IDF analysis, word embeddings, stance embeddings, zero-shot classification, and topic modeling, to gain insights into the language used in these texts and the positions expressed by different authors.\nThe results of our analysis matched the expectations based on previous historical research, confirming that Crease and Begbie indeed held similar positions on the issue, while the Chinese Regulation Act reflected a more discriminatory stance (with higher “Cons” scores overall). In addition, we found interesting clustering patterns in the UMAP projection of the stance embeddings, which poses further questions for investigation, such as what each UMAP cluster represents and what each axis in the UMAP projection means. We also identified several key topics that were prominent in the texts, and explored how these topics relate to the stances expressed in the texts using zero-shot classification scores and topic alignment scores.\nHowever, we must note that these techniques are not perfect and have limitations, as the pre-trained models may contain biases and may not accurately capture the nuances of historical legal language. We also noted that the topic alignment scores and zero-shot classification scores are not deterministic, as they depend on the specific language used in defining the candidate labels and the context in which the text is used. Therefore, it is essential to interpret the results with caution and consider the limitations of the models used. Machine learning techniques should be used as auxiliary tools in research, and human interpretation and analysis are still crucial for understanding the positions expressed in historical texts.\nIn conclusion, this workshop displayed a new perspective on how we can quantitatively analyze classical historical materials, but it also highlighted the limitations of these techniques and the importance of human interpretation in understanding the positions expressed in historical texts. We hope that this workshop has provided you with valuable insights into how computational techniques can be used to analyze historical legal texts, and how they can complement traditional research methods in the humanities and social sciences.\n\n\nReferences\n\nRegina v. Wing Chong, 1 B.C.R. Pt. II 150 (1885).\nWong Hoy Woon v. Duncan, 3 B.C.R. 318 (1894).\nRegina v. Mee Wah, 3 B.C.R. 403 (1886).\nRegina v. Corporation of Victoria, 1 B.C.R. Pt. II 331 (1888).\nQuong Wing v. The King, 49 S.C.R. 440 (1914).\nLaw Society of British Columbia. (1896). The British Columbia Reports: Being reports of cases determined in the Supreme and County Courts and in Admiralty and on appeal in the Full Court and Divisional Court (Vol. 3). Victoria, BC: The Province Publishing Company.\nCanada. Royal Commission on Chinese Immigration. (1885). Report of the Royal Commission on Chinese Immigration: report and evidence. Ottawa: Printed by order of the Commission.\nThomas, P. (2012, June 12–14). Courts of last resort: The judicialization of Asian Canadian politics 1878 to 1913. Paper presented at the Annual Conference of the Canadian Political Science Association, University of Alberta, Edmonton, Canada. Retrieved from https://cpsa-acsp.ca/papers-2012/Thomas-Paul.pdf\nMcLaren, J. P. S. (1991). The Early British Columbia Supreme Court and the “Chinese Question”: Echoes of the rule of law. Manitoba Law Journal, 20(1), 107–147. Retrieved from https://www.canlii.org/w/canlii/1991CanLIIDocs168.pdf\nChalkidis, I., Fergadiotis, M., Malakasiotis, P., Aletras, N., & Androutsopoulos, I. (2020). LEGAL-BERT: The Muppets straight out of Law School (arXiv:2010.02559). arXiv. https://doi.org/10.48550/arXiv.2010.02559\nLoo, T. (1994). Crease, Sir Henry Pering Pellew. In Dictionary of Canadian Biography (Vol. 13). University of Toronto/Université Laval. Retrieved August 8, 2025, from https://www.biographi.ca/en/bio/crease_henry_pering_pellew_13E.html\nWilliams, D. R. (1990). Begbie, Sir Matthew Baillie. In Dictionary of Canadian Biography (Vol. 12). University of Toronto/Université Laval. Retrieved August 8, 2025, from https://www.biographi.ca/en/bio/begbie_matthew_baillie_12E.html\nAriai, F., Mackenzie, J., & De Martini, G. (2025). Natural Language Processing for the legal domain: A survey of tasks, datasets, models and challenges. arXiv preprint arXiv:2410.21306."
  },
  {
    "objectID": "docs/intro_to_convolutions/intro_to_convolution.html",
    "href": "docs/intro_to_convolutions/intro_to_convolution.html",
    "title": "Mostly Harmless Convolutions",
    "section": "",
    "text": "Before you begin: Install the dependencies that you don’t have by running the code cell below.\n\n\nCode\n# !pip install opencv-python\n# !pip install numpy\n# !pip install matplotlib\n# !pip install pandas\n# !pip install scikit-learn\n\n\n\n\n\nA visualization of convolution operation\n\n\n\nWhy Convolutions?\nMathematically speaking, Convolution is an operation that combines two functions to produce a third function, which has a variety of applications in signal processing, image analysis and more. While this may sound complex, we can minimize the math behind it and explain it in a less harmful and vivid way.\nLet’s begin by imagining a simple scenario: you took a picture of a cute dog, and you want to apply a filter to it so that it looks more vibrant and colorful. Now that you input the image into a computer, how does a computer “see” it? The computer would see the image as a grid of numbers, where the combination of 3 numbers (R, G, B) in a grid represents the color of a pixel, and with all the colored pixels combined, it forms the image. Given the numeric nature of a computer image, we say that the image is digitalized.\nHere, let’s read in the image into Python using OpenCV and display it using matplotlib:\n\n\nCode\n# Load necessary libraries\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Read the image in BGR format\ndoge_bgr = cv2.imread('data/Original_Doge_meme.jpg')\n\ndoge_rgb = cv2.cvtColor(doge_bgr, cv2.COLOR_BGR2RGB)\n\n# Display the image using matplotlib\nplt.figure(figsize=(4, 4))\nplt.imshow(doge_rgb)\nplt.axis('off')  \nplt.title('Input Image')\nplt.show()\n\n\nThe image doesn’t look like numbers, does it? In fact, if we zoom in close enough, we can clearly see that the image is made up of pixels. But we still don’t see the numbers, this is because the numerical information is decoded by the computer and displayed as a colored pixel. However, we can easily convert the image into a numerical representation.\nFor demonstration purpose, I rescale the image to a 10 \\(\\times\\) 10 grid, where within each cell, the tuple represents the RGB values (0-255) of each pixel. Note that compressing images to a smaller size is always easier comparing to enhancing images to a larger size, as compression can be done by going through the image pixel by pixel and averaging the color values in each cell, while enhancement usually requires more complex operations. This gives us a hint on why convolution is important.\n\n\nCode\nimg_rgb = cv2.resize(doge_rgb, (10, 10), interpolation=cv2.INTER_NEAREST)\n\n# Plot the RGB matrix\nfig, ax = plt.subplots(figsize=(6, 6))\n\n\nh, w = img_rgb.shape[:2]\nax.imshow(img_rgb, extent=[0, w, h, 0]) \n\n# Set ticks to show grid at pixel boundaries\nax.set_xticks(np.arange(0, w + 1, 1))\nax.set_yticks(np.arange(0, h + 1, 1))\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.grid(color='black', linewidth=1)\n\nfor i in range(h):\n    for j in range(w):\n        r, g, b = img_rgb[i, j]\n        brightness = int(r) + int(g) + int(b)\n        color = 'white' if brightness &lt; 380 else 'black'\n        ax.text(j + 0.5, i + 0.5, f'({r},{g},{b})',\n                ha=\"center\", va=\"center\", fontsize=6, color=color)\n        \n        \n# Display the Grid\nax.set_title(\"RGB Value Grid of Doge Image Resized to 10x10\")\nplt.tight_layout()\nplt.show()\n\n\nThe image has become a little abstract. Can you still identify the original image out of it?\nWhile the resized image looked significantly different, it still contains the necessary information, and same ideas also transits to larger images. That is, all images can be represented as a grid of numbers, where the 3 numbers in each cell corresponds to the color of a pixel. Computers can’t see colors like we do, the way they see colors is as if they were mixing colors using a palette that only has red, green and blue (RGB), where each color has an “amount” of intensity between 0 and 255. With the 3 values for red, green and blue, computers can create any color we see in the world.\nBack to the dog picture, it is easy to see that resizing the image to a smaller grid loses a lot of details, especially the rich color that makes the image vibrant. If we want to keep the complete color information alternatively, we can plot out the distribution of the RGB values and frequencies in the image using a histogram. While this gives us a good idea of the color distribution, it does not tell us much about the spatial relationships between the pixels.\n\n\nCode\n# Compute and plot the color histogram\ncolors = ('r', 'g', 'b')\nplt.figure(figsize=(6, 4))\n\nfor i, col in enumerate(colors):\n    hist = cv2.calcHist(\n        images=[doge_bgr],       # source image (still in BGR)\n        channels=[i],           # which channel: 0=B, 1=G, 2=R\n        mask=None,              \n        histSize=[256],         \n        ranges=[0, 256]         \n    )\n    plt.plot(hist, color=col)\n    plt.xlim([0, 256])\n    \n# Display the histogram\nplt.title('RGB Histogram')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n\nCan you still identify the original image out of it?\nThe example above shows us what images are like in the eyes of a computer. Computers do not understand images in the same way that humans do, they can only see them as a collection of numbers. It thus make sense that we need to apply some math to these numbers to either change the image or extract some useful information from it, and that’s where convolution comes in.\n\n\nHow Does Convolution Work?\nBefore we dive into the application of it, let’s first understand how convolution operate on an image in a more intuitive way:\nImagine you are copying a painting, you first made a draft of the same size against the original painting and now you want to color it. Other than making your painting a 100% identical copy, you want to add some personal touch to it, so you decide to use a brush with a specific pattern to color the draft. Wherever you apply the brush, it will color the region it touches with a brighter color than the original painting, and your color your draft from left to right, top to bottom, until the whole draft is colored with the brush. You end up with a new painting that is similar to the original one, but with a different style.\nHere, the brush you used is called a kernel in the context of convolution, and the process of applying the brush is what we call convolution operation. We would define the kernel as a small matrix of numbers that represents the pattern of the brush, and the convolution operation as the process of transforming the original image by applying the kernel to it.\nHere is a gif illustrating how our filter (the kernel) will work on the image mathematically. You can see it as the small brush that slides over the image, operating on a small region of the image at a time, and eventually producing a new image that was completely transformed by the filter.\n\n\n\nConvolution Example\n\n\nNow, let’s return to the example of the cute dog picture. What we are going to apply is a brush called sharpening filter, it is a 3 \\(\\times\\) 3 matrix that looks like this:\n\\[\\text{Sharpening Filter} = \\begin{bmatrix}\n0 &-1 & 0 \\\\\n-1 & 5 & -1 \\\\\n0 & -1 & 0\n\\end{bmatrix}\\] Don’t panic as we are not going to do any math here, we will just let computer do the math for us. The only thing you need to know is that this kernel will enhance the edges of the image, making it look sharper and more defined. In fact, if you take a closer look at the kernel, you will see that it has a positive value in the center and negative values around it, and it has 0 values on the corners. This exactly looks like a brush that enhances the center of a region while reducing the intensity of the surrounding pixels, which is exactly what we want to achieve with the filter.\n\n\nCode\n# Define the sharpening filter\nkernel = np.array([\n    [ 0, -1,  0],\n    [-1,  5, -1],\n    [ 0, -1,  0]\n], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nfiltered = cv2.filter2D(doge_rgb, ddepth=-1, kernel=kernel)\n\nfiltered = np.clip(filtered, 0, 255).astype(np.uint8)\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\n# Input\nax1.imshow(doge_rgb)\nax1.set_title(\"Input Image\")\nax1.axis(\"off\")\n\n# Output\nax2.imshow(filtered)\nax2.set_title(\"Filtered Image (Sharper & More Vibrant)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nThe difference is quite obvious now. You can clearly see that the sharpened image (R) has more contrast and the edges are more defined, making it look more vibrant and colorful. This is the power of convolution, it allows us to apply filters to images and transform them in a way that is not possible with simple pixel manipulation. Similarly, we can blur an image quite easily, for which the “brush” we are going to use looks like this\n\\[\\text{Box Blur Filter} = \\begin{bmatrix}\n\\frac{1}{9} &\\frac{1}{9} & \\frac{1}{9} \\\\\n\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9} \\\\\n\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9}\n\\end{bmatrix}\n\\]\nLet’s see what it will make.\n\n\nCode\n# Define the box blur filter\nkernel = np.array([\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9]\n], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nfiltered = cv2.filter2D(doge_rgb, ddepth=-1, kernel=kernel)\n\nfiltered = np.clip(filtered, 0, 255).astype(np.uint8)\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\n# Input\nax1.imshow(doge_rgb)\nax1.set_title(\"Input Image\")\nax1.axis(\"off\")\n\n# Output\nax2.imshow(filtered)\nax2.set_title(\"Filtered Image (Blurred)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nIt worked as we expected, now the dog image becomes more blurry.\nHowever, convolution on image is not limited to filtering, it can also be used to extract features from the image. For example, we can use a kernel to detect edges, lines and texts in images, we can even use specific kernels to detect specific shapes or patterns in images, such as a kernel that detects anything that looks like a dog. As you can imagine, this is a very powerful tool in many applications, in fact, you most likely have already used it in your daily life. For example, when you use a photo editing app to apply a filter to your picture, the app is using convolution. When you use a search engine to search do image searches, the search engine is using convolution to extract features from the images and match them with your search query. Let’s say, the thing you are trying to find is eyes, the gif below shows how a convolution kernel detects “eyes” in an image:\n\nThis is a very simple example, and it is implemented exactly the same way as we did with the sharpening filter. The only difference is that to extract a specific features, we need to use a “brush” designed to detect that feature, which usually requires some knowledge of the feature we want to extract. For example, if we want to detect eyes in the painting, we would need our “brush” to understand what eyes look like and what typical colors they have. This could be way too complicated for a single “brush”, so we often use multiple brushes to detect different features when it comes to the task of feature extraction.\nTo demonstrate how convolution extracts a specific feature from an image, let’s take a look at a different “art tool”. Let’s say this time you don’t want to color the painting differently, but rather you want to sketch a line art based on the original painting. You would use a fineliner pen that detects the edges of a painting and draw a line along them. In the eyes of a computer, these are the tools it is going to use:\n\\[\\text{Horizontal Sobel} = \\begin{bmatrix}\n1 & 0 & -1 \\\\\n2 & 0 & -2 \\\\\n1 & 0 & -1\n\\end{bmatrix}\n\\text{, }\n\\text{Vertical Sobel} = \\begin{bmatrix}\n1 & 2 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -2 & -1\n\\end{bmatrix}\n\\]\nStill, don’t panic, we won’t do any math in this notebook. All you need to know is these two kernels together are called Sobel filter, and what they do is highlighting the edges in the image, making them more visible. If you take a closer look at the kernels, you will see that the first one has positive values on the left and negative values on the right, while the second one has positive values on the top and negative values on the bottom. This pattern intuitively tells us that the first “pen” will scan through the image horizontally and extract the horizontal edges, while the second “pen” will scan through the image vertically and extract the vertical edges.\nLet’s now look at a different example and see what happens when we apply the Sobel filter.\n\n\nCode\n# Generate a greyscale version of the image\nhill_bgr = cv2.imread('data/xiangbishan.jpg')\n\nhill_rgb = cv2.cvtColor(hill_bgr, cv2.COLOR_BGR2RGB)\n\nhill_gray = cv2.cvtColor(hill_bgr, cv2.COLOR_BGR2GRAY)\n\n# Define the Sobel filter kernel\nsb_kernel_h = np.array([\n    [ 1, 0, -1],\n    [ 2, 0, -2],\n    [ 1, 0, -1]\n], dtype=np.float32)\n\nsb_kernel_v = np.array([\n    [ 1, 2, 1],\n    [ 0, 0, 0],\n    [-1, -2, -1]], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nhoriz = cv2.filter2D(hill_gray, -1, sb_kernel_h)\n\nvert = cv2.filter2D(hill_gray, -1, sb_kernel_v)\n\ncombined = cv2.convertScaleAbs(np.sqrt(vert.astype(np.float32)**2 + horiz.astype(np.float32)**2))\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n\n# Input\nax1.imshow(hill_rgb)\nax1.set_title(\"Input Image\")\nax1.axis(\"off\")\n\n# Output\nax2.imshow(combined, cmap = 'gray')\nax2.set_title(\"Filtered Image (Edges Highlighted)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nAs we can see in the example above, the Sobel filter detects the edges in the image and highlights them like a fineliner pen. This is a very useful technique in image processing, as it allows us to extract features from the image that can be used for further analysis or classification. Let’s do more explorations with the example above:\n\n\nCode\n# Compute the magnitude and orientation of the gradient.\n# To put it simpler, they represents the length and direction of the edges at each pixel.\n# For those who are familiar with pythagorean theorem, the magnitude is exactly the length of the\"long edge\" calculated with pythagorean theorem.\nmagnitude = np.sqrt(vert.astype(np.float32)**2 + horiz.astype(np.float32)**2)\norientation = np.arctan2(vert.astype(np.float32), horiz.astype(np.float32))\n\n# Generate an edge binary map\nmag_norm = cv2.normalize(magnitude, None, 0,255, cv2.NORM_MINMAX).astype(np.uint8)\n_, edge_binary = cv2.threshold(mag_norm, 50, 255, cv2.THRESH_BINARY)\n\n# Create the Contour plot\ncontours, _ = cv2.findContours(edge_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncontour_img = hill_rgb.copy()\ncv2.drawContours(contour_img, contours, -1, (255,0,0), 1)\n\n# Create an edge density heatmap\nblock = 32\nh, w = edge_binary.shape\nint_img = cv2.integral(edge_binary)\n\nheatmap = np.zeros_like(edge_binary, dtype=np.float32)\n\nfor y in range(0, h - block + 1):\n    for x in range(0, w - block + 1):\n        y1, x1 = y,       x\n        y2, x2 = y + block, x + block\n        total = (int_img[y2, x2]\n               - int_img[y1, x2]\n               - int_img[y2, x1]\n               + int_img[y1, x1])\n        # center the block’s density back into the heatmap\n        heatmap[y + block//2, x + block//2] = total\n        \nheatmap = cv2.normalize(heatmap, None, 0,255, cv2.NORM_MINMAX).astype(np.uint8)\n\n# Plot out the visualizations\nfig, axs = plt.subplots(2, 2, figsize=(8, 5))\nax1, ax2, ax3, ax4 = axs.flatten()\n\n# Original Image\nax1.imshow(hill_rgb)\nax1.set_title(\"Input Image\")\nax1.axis(\"off\")\n\n# Contours on Original Image\nax2.imshow(contour_img)\nax2.set_title(\"Contours on Original Image\")\nax2.axis(\"off\")\n\n# Edge Density Heatmap\nax3.imshow(heatmap, cmap=\"hot\")\nax3.set_title(\"Edge Density Heatmap\")\nax3.axis(\"off\")\n\n# Edge Orientation Histogram\nangles_deg = np.degrees(orientation[magnitude &gt; 50].flatten())\n\nax4.hist(angles_deg, bins=36, range=(0, 90), color='purple')\nax4.set_title(\"Edge Orientation Histogram\")\nax4.set_xlabel(\"Angle (degrees)\")\nax4.set_ylabel(\"Frequency\")\n\n\nplt.tight_layout()\nplt.show()\n\n\nHere, we have generated three other visualizations based on the extracted edge data.\n\nThe first visualization is a Contour Map on the original image. It circles contours from the image that are distinguishable from others. In many cases, this implies the circled regions have different visual patterns or belong to different objects, which is very useful in tasks such as image recognition.\nThe second visualization is the Edge Density Heatmap that assigns corresponding heat values to regions on the image based on the density distribution of the edges. The lighter the color, the higher the edge density of the region. This helps us to understand which area of the image carries the most information that may be of interest to us.\nThe third visualization is a histogram showing the distribution of edge orientations. edge orientation is the direction of the edge in the graph, expressed as the angle between the edge and the x-axis (horizontal line). Understanding the distribution of edge orientation can help us better recognize the graphical features of objects in an image for tasks such as classification. For example, based on this image featuring a hill and a water surface, we now know that the edges of these two objects typically have either a horizontal or vertical orientation.\n\nThe examples above demonstrated the power of convolution in both image processing and image analysis, and more importantly, convolution is very efficient, as it is easy for computers to understand and process, and can be applied to images of any size without losing any information. This is why convolution has become a fundamental operation in computer vision and image processing.\nBut as you have probably aware, implementing convolution from scratch can be quite tedious, especially when we need to perform more specific tasks such as detecting texts or some specific shapes. To protect the brains of computer scientists, a more advanced, adaptable and efficient way of applying convolution has been developed, which is called Convolutional Neural Networks (CNNs), and we will discuss it in the next notebook, hopefully in a way that also protects your brain.\n\n\nKey takeaways from this Notebook:\n\nComputers see images as grids of numbers, where each grid cell contains the RGB values of a pixel. With the spatial relationships and the color information, computers can understand images without losing information.\nConvolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\nThe kernel that sharpens an image is called a sharpening filter, which enhances the edges of the image and makes it look more vibrant. The kernel that detects edges is called Sobel filter, which highlights the edges in the image and makes them more visible.\nConvolution is a powerful tool that can be used in many applications, such as photo editing, image search and feature extraction. It is efficient and can be applied to images of any size without losing information.\n\n\n\nAdditional Resources\n\n3Blue1Brown: But what is a convolution? A mathematical introductory video to convolution with rich visualized explanations. This may be particularly helpful to those who want to dig deeper in the mathematical perspective and were not panic when they saw the matrices show up in this notebook (I’m really sorry if that made you nervous).\n\n\n\nReferences\n\nWikipedia. Convolution。 Retrieved June 18, 2025, from https://en.wikipedia.org/wiki/Convolution\nNeuromatch Academy. Computational Neuroscience. Tutorial 2: Convolutional Neural Networks. Retrieved June 18, 2025, from https://compneuro.neuromatch.io/tutorials/intro.html\nGeeksForGeeks. Types of Convolution Kernels https://www.geeksforgeeks.org/deep-learning/types-of-convolution-kernels\nOpenCV. Image Processing in OpenCV https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html"
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html",
    "href": "docs/intro_to_transformers/intro_to_transformers.html",
    "title": "Transformers : complething this …",
    "section": "",
    "text": "Author: Krishaant Pathmanathan, PRAXIS UBC Team\nDate: 2025-06\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math, copy, time\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport numpy as np\nimport plotly.graph_objects as go\nimport numpy as np\nimport seaborn as sns"
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#prediction-game-where-will-the-ball-go",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#prediction-game-where-will-the-ball-go",
    "title": "Transformers : complething this …",
    "section": "Prediction Game: Where Will the Ball Go?",
    "text": "Prediction Game: Where Will the Ball Go?\nTransformers are all about making predictions from context. But what does that really mean? Before diving into transformers, lets start with something simple.\nImagine this: You see a bouncing ball mid-air. Where will it go next? How do you know? You don’t need physics equations to guess. You use your intuition. You remember what came before — the ball’s past trajectory — and you predict what’s most likely to happen next. Just like us, models try to guess what comes next based on what they’ve already seen."
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#given-an-image-of-a-ball-can-you-predict-where-it-will-go-next",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#given-an-image-of-a-ball-can-you-predict-where-it-will-go-next",
    "title": "Transformers : complething this …",
    "section": "Given an image of a ball can you predict where it will go next ?",
    "text": "Given an image of a ball can you predict where it will go next ?\n\n \n\nGiven a sequence, can you now tell ? This act of prediction based on prior observations is exactly what transformer models do with text, images, or sequences instead of balls.\nTurns out there are many sequences in the world - words in a sentence, frames in a video, notes in a melody, steps in a recipe. The challenge isn’t just seeing them, it’s predicting what comes next. To understand or predict them, we need a model that doesn’t just look at things in isolation… It has to remember what came before."
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#sequential-prediction-with-rnns",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#sequential-prediction-with-rnns",
    "title": "Transformers : complething this …",
    "section": "Sequential Prediction with RNNs",
    "text": "Sequential Prediction with RNNs\nImagine reading one word at a time, keeping track of what came before… that’s what an RNN does. RNN stands for Recurrent Neural Network. It’s a type of model that learns from sequences — like sentences, music, or even time. It’s an important precursor to a transformer\n\nHow It Works (Step-by-Step)\n\nGive it a sentence — for example:\n\"I love recurrent neural ____\"\nTurn words into numbers (this is called tokenizing)\n&gt; Computers can’t understand words — they only understand numbers!\nSend the numbers into the RNN — one by one\nAt each step, the RNN tries to remember what came before\n&gt; It passes a little memory called a hidden state from word to word\nAt the end, it guesses what comes next!\n&gt; Like filling in the blank at the end of the sentence\n\n\n# Simulated RNN function (takes a look at past to make prediction)\n\ndef my_rnn(word, hidden_state):\n    # Imagine the RNN does something with the word and memory\n    new_hidden_state = [h + 1 for h in hidden_state]  # update memory\n    prediction = \"networks!\" if word == \"neural\" else None\n    return prediction, new_hidden_state\n\n# Initial hidden state\nhidden_state = [0, 0, 0, 0]\n\n# Input sentence (tokenized)\nsentence = [\"I\", \"love\", \"recurrent\", \"neural\"]\n\n# RNN step-by-step loop\nfor word in sentence:\n    prediction, hidden_state = my_rnn(word, hidden_state)\n\n# Final predicted word\nnext_word_prediction = prediction\nprint(\"Next word prediction:\", next_word_prediction)\n\n\n\nWhy RNNs Can Be Tricky ?\n\nThey read one word at a time, so it’s slow\nThey forget things after a while (just like people!)\nThey can’t look at everything at once\n\nRNNs are like someone reading a story out loud, one word at a time. Transformers (like GPT) are like someone looking at the whole page at once.\nRNNs walked, so Transformers could fly."
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#what-is-a-gpt-model",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#what-is-a-gpt-model",
    "title": "Transformers : complething this …",
    "section": "What is a GPT model ?",
    "text": "What is a GPT model ?\nA GPT is a Generative Pre-Trained Transformer. The first two words are self-explanatory: generative means the model generates new text; pre-trained means the model was trained on large amounts of data. What we will focus on is the transformer aspect of the language model, the main proponent of the recent boom in AI."
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#whats-a-transformer",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#whats-a-transformer",
    "title": "Transformers : complething this …",
    "section": "What’s a transformer ?",
    "text": "What’s a transformer ?\nA transformer is a neural network that learns context and thus meaning by tracking relationships in sequential data like the words in this sentence.It is the main component that underlies tools like ChatGPT. It can trained to take in a piece of text, maybe even with some surrounding images or sound accompanying it, then produce a prediction of what comes next, in the form of a probability distribution over all chunks of text that might follow.\n*Note there are many other types of transformers (voice-to-text, text-to-image, etc.)."
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#how-does-a-transformer-work",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#how-does-a-transformer-work",
    "title": "Transformers : complething this …",
    "section": "How does a transformer work ?",
    "text": "How does a transformer work ?\n1) Embeddings A.K.A Turning Words into Numbers:\nBefore the model can understand anything it has to turn words into numbers. Not just any numbers, numbers that capture meaning. For instance the words “table” and “desk” might have numbers that are “simillar” (closer together in a vector space) whereas “table” and “apple” would be less simillar. Also, since the model doesn’t intuitively understand the order of words like you and I we give it positional encodings. It’s like telling the model this is the first, second, … word in the sentence.\n\n# Define a simple vocabulary\nvocab = {\n    'he': 0, 'deposited': 1, 'cash': 2, 'at': 3, 'the': 4, 'bank': 5,\n    'on': 6, 'friday': 7, 'river': 8, 'runs': 9, 'fast': 10\n}\nvocab_size = len(vocab)\nembedding_dim = 5  # keep it small so students can visualize\n\n# Create the embedding layer\nembedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n\n# Try editing this sentence! \nsentence = \"he deposited cash at the bank\"\ntokens = sentence.lower().split()\ntoken_ids = [vocab.get(word, 0) for word in tokens]  # default to 0 if word is missing\n\n# Convert to tensor\ninput_tensor = torch.tensor(token_ids).unsqueeze(0)  # shape: [1, seq_len]\n\n# Get vector representations\nembedded = embedding(input_tensor).squeeze(0).detach()\n\n# Display as a table\nimport pandas as pd\ndf = pd.DataFrame(embedded.numpy(), index=tokens, columns=[f\"dim_{i}\" for i in range(embedding_dim)])\ndisplay(df)\n\n# Optional: Visualize with a bar chart for each word\ndf.plot(kind=\"bar\", figsize=(10, 5), title=\"Word Embeddings\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Embedding Values\")\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nWords that are more simillar like cash and bank, share some bars, whereas words that might represent grammar instead like “he” only has negative bars\n2) Attention : Who’s talking to Whom ?\nImagine a dinner party conversation, You’re trying to follow what someone is saying, but also who they are talking to, and who said what before that. Transformers to the same thing, they look at every word in a sentence and ask “which words should I pay most attention to in order to understand this word ?” This process is called attention, and it helps the model understand context - like knowing the difference between a river bank and a money bank. “He deposited cash at the bank on Friday.” The attention mechanism would allow the model to place more emphasis on the words “deposited”, “cash”, and “Friday”, and less emphasis on function words like “the” — helping the model understand that “bank” refers to a financial institution, not a riverbank.\nThis idea of letting models decide what to pay “attention” to was first outlined in the landmark research paper “Attention is all you need” by Google in 2017\n\n# Simulate a short sentence\ntokens = ['he', 'deposited', 'cash', 'at', 'the', 'bank']\nseq_len = len(tokens)\nembedding_dim = 8  # Choose a small embedding size for demonstration\n\n# === Step 1: Generate positional encodings\nposition = torch.arange(seq_len).unsqueeze(1)  # shape: [seq_len, 1]\ndiv_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n\npe = torch.zeros(seq_len, embedding_dim)\npe[:, 0::2] = torch.sin(position * div_term)  # even indices\npe[:, 1::2] = torch.cos(position * div_term)  # odd indices\n\n# === Step 2: Show in table\ndf = pd.DataFrame(pe.numpy(), index=tokens, columns=[f\"dim_{i}\" for i in range(embedding_dim)])\ndisplay(df)\n\n# === Step 3: Visualize positional encodings\nplt.figure(figsize=(10, 5))\nfor i in range(embedding_dim):\n    plt.plot(pe[:, i], label=f\"dim_{i}\")\nplt.title(\"Positional Encoding Patterns Across Dimensions\")\nplt.xlabel(\"Position in Sentence\")\nplt.ylabel(\"Encoding Value\")\nplt.legend(loc=\"upper right\", bbox_to_anchor=(1.15, 1.0))\nplt.xticks(range(seq_len), tokens)\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\nEach coloured line helps the model tell the difference between words based on where they appear, through their encoding values. These patterns are carefully designed so that each position (1st word, 2nd word, 3rd word…) looks different to the model.\n\nimport torch\nimport torch.nn.functional as F\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Simple token list\ntokens = ['he', 'deposited', 'cash', 'at', 'the', 'bank']\nseq_len = len(tokens)\nembed_dim = 4\n\n# Simulate embeddings (normally from Step 1 + 2)\nx = torch.rand(seq_len, embed_dim)  # [seq_len, embed_dim]\n\n# Simulate attention: Q, K, V are all the same\nQ = x\nK = x\nV = x\n\n# Scaled dot-product attention\nscores = torch.matmul(Q, K.T) / (embed_dim ** 0.5)  # [seq_len, seq_len]\nattention_weights = F.softmax(scores, dim=-1)  # rows = which words each word attends to\n\n# Display as heatmap\ndf = pd.DataFrame(attention_weights.numpy(), index=tokens, columns=tokens)\nplt.figure(figsize=(8, 6))\nsns.heatmap(df, annot=True, cmap=\"Blues\")\nplt.title(\"Self-Attention: Who's Listening to Whom?\")\nplt.ylabel(\"Query Word →\")\nplt.xlabel(\"Key Word →\")\nplt.tight_layout()\nplt.show()\n\nThis maps shows who’s paying attention to whom in this sentence. For example the word “cash” pays most attention to itself (0.24) but also listens to “he” (0.11). “he” looks around but leans toward “cash” and “bank”, possibly tracking who’s doing what. and 0The values here show how much attention is paid to each word, higher numbers = stronger focus. Rows add up to 1 because each word is spreading 100% of its attention across all other words in the sentence. Again this is important because, if the attention scores for “bank” were highest on “cash” or “deposited”, that would help the model guess that “bank” means money bank, not river bank.\n3) Thinking Layers (a.k.a. Feedforward Networks):\nAfter deciding which words to pay “attention” to, the model passes each one through a thinking block (feed forward network) — basically a small decision-making system.It takes in a word’s current understanding and asks it a series of questions to refine its meaning. You can imagine it asking things like: “Is this word a noun? Does it seem important? What kind of word should come next?” Each of these layers updates the word’s internal representation based on what the model has learned from training.\nImagine the word “bank” in this sentence: “He deposited cash at the bank on Friday.”\nThe model already knows — thanks to attention — that “cash” and “deposited” are important. Now the feed-forward layers take over and might internally ask: “Does this word usually appear after the word cash?”, “Is this word often associated with financial transactions?”\nEach question helps the model become more confident in its interpretation:\n“Okay — this is probably a money bank, not a river bank.”\nThe model repeats this back-and-forth process:\nAttention chooses what matters, Feed-forward layers analyze it\nThen it loops again, with each layer getting a more refined picture of the sentence’s meaning\nBy the final layer, the model is ready to make a prediction — like guessing the next word, classifying a sentence, or answering a question. In text generation, this output takes the form of a probability distribution over all the words it knows.\n \n\n# Simulated output from attention (Step 3)\ntokens = ['he', 'deposited', 'cash', 'at', 'the', 'bank']\nattention_output = torch.randn(len(tokens), 8)  # 8-dim vectors per word\n\n# Feed-Forward Network (same for all words)\nffn = nn.Sequential(\n    nn.Linear(8, 16),  # Expand\n    nn.ReLU(),\n    nn.Linear(16, 8)   # Compress back\n)\nffn_output = ffn(attention_output)\n\n# Display before and after transformation\ndf_before = pd.DataFrame(attention_output.detach().numpy(), index=tokens, columns=[f\"before_{i}\" for i in range(8)])\ndf_after = pd.DataFrame(ffn_output.detach().numpy(), index=tokens, columns=[f\"after_{i}\" for i in range(8)])\ndf = pd.concat([df_before, df_after], axis=1)\ndisplay(df)\n\n# Visualize how the vector for one word changes (e.g., \"bank\")\nplt.figure(figsize=(10, 4))\nplt.plot(df.loc['bank'].values[:8], label=\"Before FFN\")\nplt.plot(df.loc['bank'].values[8:], label=\"After FFN\")\nplt.title(\"Word: 'bank' — Vector Before & After Feed-Forward Network\")\nplt.xlabel(\"Vector Dimension\")\nplt.ylabel(\"Value\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nThe orange line shows you that the feed forward network makes the word “bank” the output value for the word bank more smooth, making it more sure of what kind of work it is.\n4) Unembedding matrix:\nOnce the model has chewed through the whole sentence, it takes all that processed information and makes a guess: “Based on everything I’ve seen so far, what word is most likely to come next?”\nIt does this by comparing its internal numbers to a giant list of all the words it knows, and picking the one with the highest score — kind of like autocomplete when texting, but way smarter.\n5) Weights, Weight Matrices, and Fine-tuning:\nWeights: Weights are parameters within a neural network that are learned during the training process. They determine the strength and direction of the connections in the network. Intially, weights are set randomly; during training, the weights are adjusted to minimize the error between the predicted output and the actual output, by minimizing a loss function. This process is known as gradient descent.\nWeight matrices are structured collections of weights arranged in matrix form. They represent the connections between layers in a neural network. The operation of passing inputs through the network involves matrix multiplication: the input vector is multiplied by the weight matrix to produce the output vector for the next layer.\nIn the attention mechanism, each word in the input sequence is transformed into three different vectors: the query vector (used to search for relevant information from other words in the sequence), the key vector (represents the words in the sequence and is used to match with query vectors), and the value vector (holds the actual information of the words in the sequence and is used to generate the output of the attention mechanism), using separate weight matrices \\(^{[14]}\\). For example, if the input is a sequence of words represented as vectors, the queries, keys, and values are computed as:\n\\[Q=W_{Q}(X), K=W_{K}(X), V=W_{V}(X)\\]\nwhere \\(W_{Q}\\)​, \\(W_{K}\\)​, and \\(W_{V}\\)​ are weight matrices \\(^{[14]}\\) \\(^{[15]}\\). These vectors are used to calculate attention scores, which determine how much focus each word should give to every other word in the sequence.\nFine-tuning is the process of updating the key, query and value matrices to reflect new data \\(^{[16]}\\). Because the weight matrices contain both the original, general weights and the new adjustments from the fine-tuning process, fine-tuning allows the model to retain the broad, general knowledge from the pre-training phase while specializing in the a new task, such as sentiment analysis, customer feedback, etc."
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#transformer-applications",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#transformer-applications",
    "title": "Transformers : complething this …",
    "section": "Transformer Applications",
    "text": "Transformer Applications\nTransformers aren’t just for chatbots ! There are plenty of uses for transformers outside of this, espescially in the arts and humanities. For instance : - Archival Text Restoration - filling in gaps in damaged ancient manuscripts - Symbol Classification - Automatically labelling ancient symbols and handwritting (eg. cuneiform tablets) - Culutral Analysis - Using LLMs to trace narrative across historical text"
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#tokenization-this-takes-to-long-to-load-might-be-too-much-else-i-think-including-this-would-be-good",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#tokenization-this-takes-to-long-to-load-might-be-too-much-else-i-think-including-this-would-be-good",
    "title": "Transformers : complething this …",
    "section": "Tokenization (this takes to long to load, might be too much, else I think including this would be good)",
    "text": "Tokenization (this takes to long to load, might be too much, else I think including this would be good)\nJust like text is broken into tokens, images are broken into patches, and audio is split into time steps or spectrogram slices before being passed to transformer models like ViT or Wav2Vec2.\n\nfrom transformers import AutoTokenizer\n\n# Load a BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\n# Tokenize an example text\ntext = 'The ball is round.'\ntokens = tokenizer(text)\ntokens"
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#word-prediction-demo",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#word-prediction-demo",
    "title": "Transformers : complething this …",
    "section": "Word Prediction Demo",
    "text": "Word Prediction Demo\n\nfrom transformers import pipeline\n\ngenerator = pipeline('text-generation')\nprompt = 'The history of the world is'\noutput = generator(prompt, max_length=20)\nprint(output[0]['generated_text'])"
  },
  {
    "objectID": "docs/intro_to_transformers/intro_to_transformers.html#references",
    "href": "docs/intro_to_transformers/intro_to_transformers.html#references",
    "title": "Transformers : complething this …",
    "section": "References",
    "text": "References\n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … & Polosukhin, I. (2017). Attention is All You Need. In Advances in Neural Information Processing Systems. arXiv:1706.03762\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., … & Amodei, D. (2020). Language Models are Few-Shot Learners. arXiv:2005.14165\nVasvammi. Visual Explanation of Transformers. https://vasvammi.github.io/transformer-visual\nMIT Deep Learning: Introduction to Deep Learning (2024) https://introtodeeplearning.com/2024/index.html\n3Blue1Brown: But What is a GPT? https://www.3blue1brown.com/lessons/gpt\nHenry AI Labs: Transformers in a Nutshell (YouTube) https://www.youtube.com/watch?v=zxQyTK8quyY&vl=en\nPeter Bloem: Transformers Explained Visually and Conceptually https://peterbloem.nl/blog/transformers\nJay Alammar: The Illustrated Transformer https://jalammar.github.io/illustrated-transformer/\nCOMET Docs: Fine-tuning LLMs with Ollama (UBC Arts) https://comet.arts.ubc.ca/docs/4_Advanced/advanced_ollama_llm/fine_tuning_llm.html#setting-up\n\n\n# Simplified code demo \n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# === Step 1: Word Embeddings ===\nvocab = {'he': 0, 'deposited': 1, 'cash': 2, 'at': 3, 'the': 4, 'bank': 5, 'on': 6, 'friday': 7}\ntokens = ['he', 'deposited', 'cash', 'at', 'the', 'bank', 'on', 'friday']\ntoken_ids = torch.tensor([vocab[word] for word in tokens]).unsqueeze(0)  # shape: [1, seq_len]\n\nembedding_dim = 8\nembedding = nn.Embedding(len(vocab), embedding_dim)\nembedded_tokens = embedding(token_ids)\n\n# === Step 2: Add Positional Encoding ===\npos = torch.arange(0, embedded_tokens.size(1)).unsqueeze(1).float()\ndiv_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * -(torch.log(torch.tensor(10000.0)) / embedding_dim))\npe = torch.zeros_like(embedded_tokens)\npe[:, :, 0::2] = torch.sin(pos * div_term)\npe[:, :, 1::2] = torch.cos(pos * div_term)\npos_embed = embedded_tokens + pe\n\n# === Step 3: Self-Attention ===\nQ = K = V = pos_embed\ndk = Q.size(-1)\nscores = torch.matmul(Q, K.transpose(-2, -1)) / dk**0.5\nattention_weights = F.softmax(scores, dim=-1)\nattention_output = torch.matmul(attention_weights, V)\n\n# === Step 4: Feed-Forward Network ===\nffn = nn.Sequential(\n    nn.Linear(embedding_dim, 32),\n    nn.ReLU(),\n    nn.Linear(32, embedding_dim)\n)\nffn_output = ffn(attention_output)\n\n# === Step 5: Unembedding and Final Output ===\nunembedding = nn.Linear(embedding_dim, len(vocab))\nlogits = unembedding(ffn_output)\nprobs = F.softmax(logits, dim=-1)\n\n# === Show Top Predictions for Each Token ===\nfor i, token in enumerate(tokens):\n    pred_id = torch.argmax(probs[0, i]).item()\n    pred_word = list(vocab.keys())[pred_id]\n    print(f\"{token} → Top prediction: {pred_word}\")"
  },
  {
    "objectID": "index.html#about-praxis",
    "href": "index.html#about-praxis",
    "title": "Praxis",
    "section": "About prAxIs",
    "text": "About prAxIs\n\nprAxIs is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2025 that seeks to bring Machine Learning and AI into arts classes at UBC.\nBased at UBC’s Vancouver School of Economics and the Centre for Computational Social Science, our team consists of faculty and students working in partnership to develop hands-on learning modules that explore the disciplinary application of AI and machine learning tools in Arts.\n\n\n\nUBC Arts"
  },
  {
    "objectID": "index.html#getting-started-with-praxis",
    "href": "index.html#getting-started-with-praxis",
    "title": "Praxis",
    "section": "Getting Started with prAxIs",
    "text": "Getting Started with prAxIs\n\n\n\n\n For Learners \nThese modules cover different applications of AI or machine learning in the context of a particular discipline or course. Current courses include:\n\nHIST 414\nAMNE 170\nAMNE 376\nSOCI 415\nSOCI 217\nSOCI 280\nECON 227\n\nModules can be accessed from the menus at the top of the page. They come in different formats, but most are written as interactive JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself and complete the exercises on your own. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n For Educators \nThis project aims to develop and pilot a set of discipline-specific educational modules designed to demystify AI for undergraduate students in the Faculty of Arts. Each application is packages as a “module” for a particular discipline. These modules show students that AI tools can be used to better understand their discipline, and why a critical understanding of these tools is necessary to use them well. They provide guidance and lessons plans for implementing these tools in the classroom, and explain what infrastructure and tools are needed.\nIn each module, students undertake a small, discipline-specific project that uses AI tools to address pertinent questions or challenges in their fields of study. Modules will include a theoretical component, designed to complement the application. This includes detailed, but non-technical, explanations of how these tools are constructed, how they function, and how they can be applied in academic research and industry, as well as critical discussion of their development and use.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating prAxIs resources into your instruction, check out our Using prAxIs for Teaching page."
  },
  {
    "objectID": "index.html#citing-praxis",
    "href": "index.html#citing-praxis",
    "title": "Praxis",
    "section": "Citing prAxIs",
    "text": "Citing prAxIs\n\nThis project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\nNelson, L., Graves, J., and other prAxIs Contributors. 2023. ‘The prAxIs Project: Unpacking the Black Box’. https://ubcecon.github.io/praxis-ubc.\n\n\n\nHowever, some notebooks have an additional suggested attribution. Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page."
  },
  {
    "objectID": "index.html#get-involved",
    "href": "index.html#get-involved",
    "title": "Praxis",
    "section": "Get Involved",
    "text": "Get Involved\n\nprAxIs is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with prAxIs!\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe prAxIs Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  },
  {
    "objectID": "pages/index/index_AMNE-376.html",
    "href": "pages/index/index_AMNE-376.html",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "",
    "text": "This section contains material to support UBC’s Greek Art and Architecture (AMNE 376). The visual culture of the ancient Greek world in the second and first millennia BCE, especially from c. 1000 to 30 BCE. Credit will be granted for only one of CLST_V 331, AMNE_V 376 or ARTH_V 331. Equivalency: ARTH_V 331 or CLST_V 331."
  },
  {
    "objectID": "pages/index/index_AMNE-376.html#amne-376-suggested-lesson-plan",
    "href": "pages/index/index_AMNE-376.html#amne-376-suggested-lesson-plan",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "AMNE 376: Suggested Lesson Plan",
    "text": "AMNE 376: Suggested Lesson Plan\n80-minute lecture or 50-minute discussion on computer vision, image embedding, and their applications in Archaeology and Art History.\nAgendas are provided for both lecture and discussion formats.\n\n\nLearning Objectives\nBy the end of this lesson, students will:\n\nBecome familiar with concepts such as computer vision, convolution, convolutional neural network (CNN), and image embeddings.\nUnderstand how computers “see” and distinguish between different images by identifying unique visual elements and quantifying their similarity.\nExplore photographs of Kouroi from Richter’s “Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942)” and create image embeddings for these photographs using pre-trained models.\nLearn how to cluster and classify Kouroi based solely on photographs and critically analyze the advantages and limitations of these techniques and their potential applications in archaeology and art history.\n\n\n\n\nMaterials and Technical Requirements\n\nJupyter notebook (hosted on the prAxIs UBC website)\nDevice with internet access (laptop preferred)\nNo previous coding experience required (familiarity with Python is an asset)\nStudents may pair up (groups of 2 or 3) if device access is limited\nGroup work is encouraged; students will compare findings\n\n\n\n\nPre-lesson Requirements\nInstructor should: - Test run the notebook using Jupyter Open and be familiar with the content. - (Optionally) Instruct students how to run code on Jupyter Open if they are interested in exploring on their own.\nStudents should: - Complete all required readings for “The Archaic World: Temples, Statues, and Colour”. - Browse the text explanations in the static notebook on the prAxIs UBC website. - (If time permits) Browse through the notebooks on convolution and CNN."
  },
  {
    "objectID": "pages/index/index_AMNE-376.html#agenda-for-the-lecture",
    "href": "pages/index/index_AMNE-376.html#agenda-for-the-lecture",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "Agenda for the Lecture",
    "text": "Agenda for the Lecture\nIf this format is chosen, students don’t have to run code themselves, but they may explore after class. Students should have access to image folders and interactive visualization objects.\n\nPre-discussion and brief lecture on Jupyter Notebook, introducing students to the programmatic environment (5–10 min)\nIntroduction section and basics of convolution (10 min)\nDataset exploration, creation of embeddings, PCA visualization and discussion about commonalities among clustered images (20 min)\nIntroduce classification using image embeddings, evaluation of results, group activities (try to classify yourself, strengths and limitations of classifier, etc.) (20 min)\nLead discussion on machine learning applications in archaeology/art history (formalism, relic restoration, the future of archaeology) (15 min)\nConclusions and takeaways (5–10 min)"
  },
  {
    "objectID": "pages/index/index_AMNE-376.html#agenda-for-the-discussion",
    "href": "pages/index/index_AMNE-376.html#agenda-for-the-discussion",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "Agenda for the Discussion",
    "text": "Agenda for the Discussion\nIf the discussion format is chosen, group activities are emphasized and all students should run the notebook code themselves.\n\nBefore discussion: Instructions on how to load/run code on Jupyter Open so students can explore immediately.\nOpen the notebook; execute first two sections and discuss computer vision and convolution (10 min)\nExecute Section 3, 4, 5; discuss observations based on visualized image embeddings, commonalities among Kouros clusters (15 min)\nTry to classify materials/groups of Kouroi based on images, check accuracy (5 min)\nExecute Section 6, discuss classifier quality, metrics, applications, and improvements (15 min)\nClosing discussion (5 min)"
  },
  {
    "objectID": "pages/index/index_AMNE-376.html#activity-materials",
    "href": "pages/index/index_AMNE-376.html#activity-materials",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "Activity Materials",
    "text": "Activity Materials\n\nIn-Class Discussion Questions\n\nDescribe the differences and similarities between how computers and humans identify similarity between two images.\nFind photographs of Kouroi that correspond to scatter plot points; discuss patterns among Kouroi that cluster together. What features do you think the embeddings captured?\nDescribe features that characterize a Kouros: what would you focus on to classify their era or material based only on photos?\nDiscuss what different insights the CNN model may provide, and what might be biased by the model itself.\n\n\n\n\nDiscussion Post Questions\n\nList some AI applications in archaeology/art history research. Do you think any can be replaced by human labor?\nBased on your knowledge of computer vision, do you think CNN models can analyze objects more objectively? Why?\nDiscuss whether you would trust the use of AI in the following for studying cultural relics/artifacts: style analysis, sentiment analysis, cultural relic restoration, age identification. Why?"
  },
  {
    "objectID": "pages/index/index_HIST-414.html",
    "href": "pages/index/index_HIST-414.html",
    "title": "Constitutions in Canadian History: From Pre-Contact to the Charter of Rights (HIST 414)",
    "section": "",
    "text": "This section contains material to support UBC’s Constitutions in Canadian History: From Pre-Contact to the Charter of Rights (HIST 414). This course covers European precedents, Colonial self-government, Canadian Confederation, and issues such as gay rights, abortion, and First Nations land rights.\nCHANGE\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision\nThey can also be used for self-study, with some additional effort\n\nThese materials are a review of basic probability, which contains overlap with AP and IB Statistics from high school.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_SOCI415.html",
    "href": "pages/index/index_SOCI415.html",
    "title": "Theories of Family and Kinship (SOCI 415)",
    "section": "",
    "text": "The modules in this unit are for SOCI 415 - Theories of Family and Kinship. Major theoretical approaches to the study of the family. Each approach is assessed for its strengths and weaknesses on the basis of empirical data. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_SOCI415.html#modules",
    "href": "pages/index/index_SOCI415.html#modules",
    "title": "Theories of Family and Kinship (SOCI 415)",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - CBDB Dataset\n\n\nUsing Network Analysis to Analyze The China Biographical Dataset\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - KINMATRIX Dataset\n\n\nUsing the KINMATRIX Dataset to explore network analysis for SOCI 415\n\n\n\n24 Aug 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/quickstart.html",
    "href": "pages/quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "Want to get started using prAxIs as soon as possible? It’s as easy as 1-2-3!\nAnd select your JupyterHub\nYou’re all ready to go!"
  },
  {
    "objectID": "pages/quickstart.html#using-locally",
    "href": "pages/quickstart.html#using-locally",
    "title": "Quickstart Guide",
    "section": "Using Locally",
    "text": "Using Locally\nWant to run prAxIs on your own computer? Don’t have a reliable internet connection? Want to be a power user or have a favourite IDE? You don’t have to rely on a JupyterHub to use prAxIs.\n\nInstall Jupyter or RStudio locally on your own computer, including R and any other packages necessary.\nSelect “Launch Locally” from the “Launch prAxIs” menu and download the repository files.\nUnzip the files and open them in your local IDE.\n\nYou’re all ready to go!"
  },
  {
    "objectID": "pages/team.html",
    "href": "pages/team.html",
    "title": "prAxIs Team",
    "section": "",
    "text": "The prAxIs project is a true team effort; the learning materials that you see here have taken input from many writers, coders, editors, and reviewers from a variety of disciplines."
  },
  {
    "objectID": "pages/team.html#principal-investigators",
    "href": "pages/team.html#principal-investigators",
    "title": "prAxIs Team",
    "section": "Principal Investigators",
    "text": "Principal Investigators\n\nJonathan Graves\nLaura Nelson"
  },
  {
    "objectID": "pages/team.html#research-assistants",
    "href": "pages/team.html#research-assistants",
    "title": "prAxIs Team",
    "section": "Research Assistants",
    "text": "Research Assistants\n\n Summer 2025 \n\n\nIrene Berezin\n\n\nAlex Ronczewski\n\n\nNathan Zhang\n\n\nJalen Faddick\n\n\nYash Mali\n\n\nAnna Kovtunenko\n\n\nKrishaant Pathmanathan\n\n\nADD"
  },
  {
    "objectID": "pages/team.html#praxis-partners",
    "href": "pages/team.html#praxis-partners",
    "title": "prAxIs Team",
    "section": "prAxIs+ Partners",
    "text": "prAxIs+ Partners\n\nUBC’s Department of Economics: Vancouver School of Economics\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson"
  },
  {
    "objectID": "pages/team.html#special-thanks",
    "href": "pages/team.html#special-thanks",
    "title": "prAxIs Team",
    "section": "Special Thanks",
    "text": "Special Thanks\n\nThe UBC TLEF team, especially Jeff Miller and Jason Myers.\nThe team behind QuantEcon, especially Jesse Perla and Peifan Wu for their advice and support.\nThe UBC CTLT, Arts ISIT, and LT Hub Teams, for their support with Jupyter and GitLab, especially Stephen Michaud, Nausheen Shafiq, and Michael Ha.\nThe UBC DataScience Slack and the Jupyter team, especially Tiffany Timbers, Phil Austin, Firas Moosvi, and the presenters and attendees at JupyterDays 2020\nThe staff at the VSE for facilitating events, payments, and space use, especially Maria Smith and Caroline Gatchalian\nPlus many, many, more!"
  }
]