[
  {
    "objectID": "docs/SOCI-280/soci_280_lesson_plan.html",
    "href": "docs/SOCI-280/soci_280_lesson_plan.html",
    "title": "Lesson Plan: Introduction to Sentiment Analysis",
    "section": "",
    "text": "Duration: 110 minutes  Course: SOCI 280\n\nLearning Objectives\n\nBy the end of this lesson, students will:\n\nUnderstand and complete sentiment analysis to detect emotional tone (positive, negative, neutral), and understand and complete toxicity analysis to identify harmful or aggressive language (e.g., insults, threats)\nIntroduce concepts in statistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian)\nLearn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than normal political tweets?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\n\nThrough this analysis, we’ll explore various dimensions of AI applications, critically examining how it can better understand and detect the patterns of disinformation when working with large amounts of social data.\n\n\nMaterials and Technical Requirements:\n\nThe lesson will be administered through a Jupyter Notebook, hosted on the prAxIs UBC website. Students will need access to a device connected to the internet, preferably a laptop. No previous coding experience is required, though some familiarity with Python is an asset. If students do not have access to a capable device, it is acceptable to pair up into groups of two or three. Group work is encouraged throuhgout the lesson and students will be asked to compare their findings with other students.\n\n\nPre-lesson Checklist:\n\n\nStudents have previously completed the reading up to Section 0 (roughly 5-10 minutes).\n\nThe instructor has recently loaded the Notebook and can access and project it if needed.\nStudents were reminded to bring a device to class.\n\n\n\nAgenda\n\n\nPre-discussion and brief lecture based on Notebook reading (5-10 minutes)  Briefly discuss misinformation, disinformation, and propaganda, emphasizing the role of digital platforms in enabling their spread. Briefly explain the ways researchers can discern disinformation online and consider how new technology based on personal data might be more influential.  Example: Researchers secretly experimented on Reddit users with AI generated comments\nLoad the Notebook (5 minutes)  Have students open the Jupyter Notebook, pairing any students who are unable to access the materials with a student who can. Once all students are able to access the materials, instruct them to complete Section 0.\nSection 0 (20 Minutes)  Instruct the students to begin working through the code and activities in Notebook Section 0. Once students get to the Screen Time activity pause for a brief discussion to compare results.\nSection 1 (15 Minutes)  Before instructing students to complete Notebook Section 1, have a brief (2-3 minute) explanation of classification, connecting course materials, and previous lessons. Students will complete Notebook Section 1 at their own pace, taking roughly 15 minutes to complete the lesson.\nSections 3-4 (20 Minutes)  After pausing for a moment to discuss the findings of Section 1 in a large group, students will complete the rest of the Notebook.\nTakeaways and Activity (5-10 Minutes)  The last section of this lesson is time-dependent, and reserved for questions about the Notebook methods, a brief lecture outlining the key takeaways from the Notebook, and for students to get started on any potential participation activities such as discussion posts, worksheets, etc. (samples below).\n\n\n\nActivity Materials\n\n\nDiscussion Post\nRespond to the following questions in 100-250 words each:\n\nHow can sentiment analysis be useful in answering research questions? Can you think of any tasks it would be well suited to?\nDo you think the methods in the notebook were useful in understanding disinformation campaigns? What might be some of the limitations to these approaches?\nDiscuss the role AI plays in disinformation, both the detection and analysis of it, and the production of it.\n\n\n\nShort Response:\nOn a social media platform of your choice, try to identify a post you believe to be disinformation. You can search for specific topics that you think will likely produce disinformation or wait for something to come across in your feed. Link to the content here: ____________________________________________________________________\nThen, respond to the following questions:\n\nWhy do you think this is disinformation? What data/information are you using or pulling out from the content and its features to come to this conclusion? Explain why you think this is disinformation in 150-300 words.\nThink back to the classifier in the notebook. What data was it using to classify text as disinformation? Is that process similar or different from how you classified your post as disinformation? Do you think you are more likely to be correct in identifying disinformation? If so, why? Respond in 200-350 words."
  },
  {
    "objectID": "docs/AMNE-376/development/kouroi_embeddings.html",
    "href": "docs/AMNE-376/development/kouroi_embeddings.html",
    "title": "Praxis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv('../data/sculpture_dataset_labeled.csv')\n\ndf.head()\n\n\n\n\n\n\n\n\nfilename\npage\ngroup\nera\n\n\n\n\n0\npage184_img01_photo2.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n1\npage184_img01_photo3.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n2\npage184_img01_photo4.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n3\npage184_img01_photo6.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n4\npage184_img01_photo7.jpg\n184\nFORERUNNERS\nBefore 615 BC\n\n\n\n\n\n\n\n\nimport torch\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport timm\nfrom tqdm.notebook import tqdm # Import tqdm for progress bar\n\n\nmodel_name = 'convnextv2_tiny' \nmodel = timm.create_model(model_name, pretrained=True)\nmodel.eval()  # set to eval mode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\n\nbatch_size = 32\nembeddings = []\n\nimage_directory = \"../data/richter_kouroi_filtered_photos\"\n\n# Collect filenames and eras for batching\nfilenames = df['filename'].tolist()\neras = df['era'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i:i + batch_size]\n\n    images = []\n    valid_indices = []\n    for j, filename in enumerate(batch_filenames):\n        image_path = os.path.join(image_directory, filename)\n        try:\n            image = Image.open(image_path).convert('RGB')  # convert to RGB to avoid issues\n            images.append(image)\n            valid_indices.append(i + j)  # Keep track of valid indices for later\n        except FileNotFoundError:\n            print(f\"Image not found at: {image_path}\")\n        except Exception as e:\n            print(f\"Error processing image {filename}: {e}\")\n\n    if len(images) == 0:\n        continue  # Skip empty batches\n\n    # Process the batch of images\n    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    # Mean pooling over patches per image in batch\n    batch_embeddings = last_hidden_states.mean(dim=1).cpu().numpy()\n\n    # Append embeddings maintaining order corresponding to valid images\n    embeddings.extend(batch_embeddings)\n\n\n\n\n\nnp.array(embeddings)\n\nnp.save('../data/embeddings/all_photos_embeddings.npy', embeddings)\nnp.save('../data/embeddings/all_photos_eras.npy', eras)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = np.load('../data/embeddings/all_photos_embeddings.npy')\ny = np.load('../data/embeddings/all_photos_eras.npy')\n\n# X = embeddings, y = your labels\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.5151515151515151\n\n\n\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')\n\nbatch_size = 32\nembeddings = []\n\nimage_directory = \"../data/richter_kouroi_complete_front_only\"\n\n# Collect filenames and eras for batching\nfilenames = df['filename'].tolist()\neras = df['era'].tolist()\nmaterial = df['material'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i:i + batch_size]\n\n    images = []\n    valid_indices = []\n    for j, filename in enumerate(batch_filenames):\n        image_path = os.path.join(image_directory, filename)\n        try:\n            image = Image.open(image_path).convert('RGB')  # convert to RGB to avoid issues\n            images.append(image)\n            valid_indices.append(i + j)  # Keep track of valid indices for later\n        except FileNotFoundError:\n            print(f\"Image not found at: {image_path}\")\n        except Exception as e:\n            print(f\"Error processing image {filename}: {e}\")\n\n    if len(images) == 0:\n        continue  # Skip empty batches\n\n    # Process the batch of images\n    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    # Mean pooling over patches per image in batch\n    batch_embeddings = last_hidden_states.mean(dim=1).cpu().numpy()\n\n    # Append embeddings maintaining order corresponding to valid images\n    embeddings.extend(batch_embeddings)\n\n\n\n\n\nnp.array(embeddings)\n\nnp.save('../data/embeddings/complete_sculptures_embeddings.npy', embeddings)\nnp.save('../data/embeddings/complete_sculptures_eras.npy', eras)\nnp.save('../data/embeddings/complete_sculptures_material.npy', material)\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nX = embeddings\ny1 = eras\ny2 = material\n\n# X = embeddings, y = your labels\nX_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.3076923076923077\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.8461538461538461\n\n\n\ndf = pd.read_csv('../data/head_dataset_labeled.csv')\n\nbatch_size = 32\nembeddings = []\n\nimage_directory = \"../data/richter_kouroi_head_front_only\"\n\n# Collect filenames and eras for batching\nfilenames = df['filename'].tolist()\neras = df['era'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i:i + batch_size]\n\n    images = []\n    valid_indices = []\n    for j, filename in enumerate(batch_filenames):\n        image_path = os.path.join(image_directory, filename)\n        try:\n            image = Image.open(image_path).convert('RGB')  # convert to RGB to avoid issues\n            images.append(image)\n            valid_indices.append(i + j)  # Keep track of valid indices for later\n        except FileNotFoundError:\n            print(f\"Image not found at: {image_path}\")\n        except Exception as e:\n            print(f\"Error processing image {filename}: {e}\")\n\n    if len(images) == 0:\n        continue  # Skip empty batches\n\n    # Process the batch of images\n    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    # Mean pooling over patches per image in batch\n    batch_embeddings = last_hidden_states.mean(dim=1).cpu().numpy()\n\n    # Append embeddings maintaining order corresponding to valid images\n    embeddings.extend(batch_embeddings)\n\n\n\n\n\nnp.array(embeddings)\n\nnp.save('../data/embeddings/head_embeddings.npy', embeddings)\nnp.save('../data/embeddings/head_eras.npy', eras)\n\n\nX = embeddings\ny1 = eras\n\n# X = embeddings, y = your labels\nX_train, X_test, y_train, y_test = train_test_split(X, y1, test_size=0.2, random_state=42)\n\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nAccuracy: 0.5555555555555556"
  },
  {
    "objectID": "docs/AMNE-376/development/AMNE_Statues.html",
    "href": "docs/AMNE-376/development/AMNE_Statues.html",
    "title": "Praxis",
    "section": "",
    "text": "import pandas as pd\n\n\ndf = pd.read_csv(\"../data/head_dataset_labeled.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nfilename\npage\ngroup\nera\n\n\n\n\n0\npage188_img01_photo13.jpg\n188\nSOUNION GROUP\n615 - 590 BC\n\n\n1\npage196_img01_photo5.jpg\n196\nSOUNION GROUP\n615 - 590 BC\n\n\n2\npage200_img01_photo7.jpg\n200\nSOUNION GROUP\n615 - 590 BC\n\n\n3\npage202_img01_photo3.jpg\n202\nSOUNION GROUP\n615 - 590 BC\n\n\n4\npage202_img01_photo4.jpg\n202\nSOUNION GROUP\n615 - 590 BC\n\n\n\n\n\n\n\n\nimport torch\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nfrom transformers import AutoImageProcessor, Dinov2Model\nfrom tqdm.notebook import tqdm # Import tqdm for progress bar\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = Dinov2Model.from_pretrained(\"facebook/dinov2-base\").to(device)\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n\n\n\nbatch_size = 32\nembeddings = []\n\nimage_directory = \"../data/richter_kouroi_head_front_only\"\n\n# Collect filenames and eras for batching\nfilenames = df['filename'].tolist()\neras = df['era'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i:i + batch_size]\n\n    images = []\n    valid_indices = []\n    for j, filename in enumerate(batch_filenames):\n        image_path = os.path.join(image_directory, filename)\n        try:\n            image = Image.open(image_path).convert('RGB')  # convert to RGB to avoid issues\n            images.append(image)\n            valid_indices.append(i + j)  # Keep track of valid indices for later\n        except FileNotFoundError:\n            print(f\"Image not found at: {image_path}\")\n        except Exception as e:\n            print(f\"Error processing image {filename}: {e}\")\n\n    if len(images) == 0:\n        continue  # Skip empty batches\n\n    # Process the batch of images\n    inputs = processor(images=images, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    # Mean pooling over patches per image in batch\n    batch_embeddings = last_hidden_states.mean(dim=1).cpu().numpy()\n\n    # Append embeddings maintaining order corresponding to valid images\n    embeddings.extend(batch_embeddings)\n\nembeddings = np.array(embeddings)\nnp.save('dinov2_image_embeddings.npy', embeddings)\n\n\n\n\n\n\n\n# Proceed with your t-SNE and plotting code...\ntsne = TSNE(n_components=2, init='pca', method='exact')\ntsne_embeddings = tsne.fit_transform(embeddings)\n\nplt.figure(figsize=(12, 10))\nunique_eras = df['era'].unique()\ncolors = plt.cm.get_cmap('tab10', len(unique_eras))\n\nfor i, era in enumerate(unique_eras):\n    indices = df[df['era'] == era].index\n    plt.scatter(tsne_embeddings[indices, 0], tsne_embeddings[indices, 1], color=colors(i), label=era)\n\nplt.title('t-SNE of Image Embeddings Colored by Era')\nplt.xlabel('t-SNE Dimension 1')\nplt.ylabel('t-SNE Dimension 2')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nC:\\Users\\Kaiyan Zhang\\AppData\\Local\\Temp\\ipykernel_16612\\3958630150.py:7: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n  colors = plt.cm.get_cmap('tab10', len(unique_eras))\n\n\n\n\n\n\n!pip install plotly==5.14.0 --quiet\n\n\nimport plotly.express as px\n\n\n# Create a DataFrame for Plotly\nplotly_df = pd.DataFrame({\n    'TSNE_Dim1': tsne_embeddings[:, 0],\n    'TSNE_Dim2': tsne_embeddings[:, 1],\n    'Era': df['era'].tolist(), # Use the 'era' column from your original df\n    'Filename': df['filename'].tolist() # Include the filenames\n})\nfig = px.scatter(\n        plotly_df,\n        x='TSNE_Dim1',\n        y='TSNE_Dim2',\n        color='Era',\n        hover_data=['Filename'], # Show filename when hovering\n        title='Interactive t-SNE of Image Embeddings Colored by Era'\n    )\n\nfig.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning: When grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n  sf: grouped.get_group(s if len(s) &gt; 1 else s[0])\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nfrom sklearn.decomposition import PCA\n\n# Perform PCA instead of t-SNE\npca = PCA(n_components=2)\npca_embeddings = pca.fit_transform(embeddings)\n\n# Plot the PCA result\nplt.figure(figsize=(12, 10))\nunique_eras = df['era'].unique()\ncolors = plt.cm.get_cmap('tab10', len(unique_eras))\n\nfor i, era in enumerate(unique_eras):\n    indices = df[df['era'] == era].index\n    plt.scatter(pca_embeddings[indices, 0], pca_embeddings[indices, 1], color=colors(i), label=era)\n\nplt.title('PCA of Image Embeddings Colored by Era')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nC:\\Users\\Kaiyan Zhang\\AppData\\Local\\Temp\\ipykernel_16612\\119444610.py:10: MatplotlibDeprecationWarning:\n\nThe get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n# e.g. compute 5 principal components\npca = PCA(n_components=6)\npca_embeddings = pca.fit_transform(embeddings)\n# now pca_embeddings.shape == (n_samples, 5)\n\npc_df = pd.DataFrame(\n    pca_embeddings[:, :6],\n    columns=['PC1','PC2','PC3','PC4', 'PC5', 'PC6']\n)\npc_df['era'] = df['era'].values\n\nimport seaborn as sns\nsns.pairplot(pc_df, hue='era', vars=['PC1','PC2','PC3','PC4'])\nplt.show()\n\n\n\n\n\n\n# Create a DataFrame for Plotly\nplotly_df = pd.DataFrame({\n    'TSNE_Dim1': pca_embeddings[:, 0],\n    'TSNE_Dim2': pca_embeddings[:, 1],\n    'Era': df['era'].tolist(), # Use the 'era' column from your original df\n    'Filename': df['filename'].tolist() # Include the filenames\n})\nfig = px.scatter(\n        plotly_df,\n        x='TSNE_Dim1',\n        y='TSNE_Dim2',\n        color='Era',\n        hover_data=['Filename'], # Show filename when hovering\n        title='Interactive t-SNE of Image Embeddings Colored by Era'\n    )\n\nfig.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\plotly\\express\\_core.py:1983: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nmodel = Dinov2Model.from_pretrained(\n    \"facebook/dinov2-base\",\n    output_attentions=True,   # &lt;-- get all the self-attention weights\n).eval().to(device)\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n\nimg = Image.open(\"../data/richter_kouroi_head_front_only/page286_img01_photo3.jpg\").convert(\"RGB\")\ninputs = processor(images=img, return_tensors=\"pt\").to(device)\n\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# outputs.attentions: tuple of length L (num layers), each (batch=1, heads=H, seq_len=T, seq_len=T)\nattns = [attn[0].mean(dim=0).cpu()  # average over heads\n         for attn in outputs.attentions]\n\nT = attns[0].size(-1)  # total tokens = 1 (CLS) + num_patches\nrollout = torch.eye(T)\n\nfor layer_attn in attns:\n    # Add identity (residual) and renormalize rows\n    layer_aug = layer_attn + torch.eye(T)\n    layer_aug = layer_aug / layer_aug.sum(dim=-1, keepdim=True)\n    # Propagate\n    rollout = layer_aug @ rollout\n\n# Extract CLS → patch attentions (skip the CLS→CLS token at rollout[0,0])\npatch_attn = rollout[0, 1:]  # shape: (num_patches,)\n\n# Reshape to 2D grid\ngrid_size = int(np.sqrt(patch_attn.size(0)))\nheatmap = patch_attn.reshape(grid_size, grid_size).numpy()\n\nheatmap_tensor = torch.tensor(heatmap).unsqueeze(0).unsqueeze(0)  # (1,1,G,G)\nheatmap_up = torch.nn.functional.interpolate(\n    heatmap_tensor,\n    size=img.size[::-1],      # (height, width)\n    mode=\"bilinear\",\n    align_corners=False\n)[0,0].numpy()\n\nplt.figure(figsize=(6,6))\nplt.imshow(img, alpha=0.8)\nplt.imshow(heatmap_up, cmap=\"inferno\", alpha=0.5)\nplt.axis(\"off\")\nplt.title(\"DINO-V2 Attention Rollout Heatmap\")\nplt.show()\n\n\n\n\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport pandas as pd\nfrom transformers import AutoImageProcessor, Dinov2Model\nfrom tqdm.auto import tqdm\n\n\n\n# ——————————————————————————————————————\n# 1) Hyperparameters & paths\n# ——————————————————————————————————————\nIMAGE_DIR   = \"../data/richter_kouroi_complete_front_only\"\nCSV_PATH    = \"../data/complete_sculpture_dataset_labeled.csv\"   # with columns ['filename','era']\nBATCH_SIZE  = 32\nLR          = 1e-3\nEPOCHS      = 15\nDEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nMODEL_NAME  = \"facebook/dinov2-base\"\n\n# Map your era strings to integer labels\nERA2IDX = {\n    '615 - 590 BC': 0,\n    '590 - 570 BC': 1,\n    '575 - 550 BC': 2,\n    '555 - 540 BC': 3,\n    '540 - 520 BC': 4,\n    '520 - 485 BC': 5,\n    '485 - 460 BC': 6,\n}\n\nMAT2IDX = {\n    'Marble': 0,\n    'Bronze': 1,\n    'Lead': 2,\n    'Alabaster': 3,\n    'Limestone': 4,\n    'Terracotta': 5,\n}\n\n\n\n# ——————————————————————————————————————\n# 2) Dataset & DataLoader\n# ——————————————————————————————————————\nclass KouroiEraDataset(Dataset):\n    def __init__(self, csv_path, img_dir, processor, mat2idx):\n        self.df = pd.read_csv(csv_path)\n        self.img_dir = img_dir\n        self.processor = processor\n        self.mat2idx = MAT2IDX\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(os.path.join(self.img_dir, row.filename)).convert(\"RGB\")\n        # turn to model inputs\n        inputs = self.processor(images=img, return_tensors=\"pt\")\n        # remove batch dim\n        for k,v in inputs.items():\n            inputs[k] = v.squeeze(0)\n        label = self.mat2idx[row.material]\n        return inputs, label\n\n# Initialize processor + dataset\nprocessor = AutoImageProcessor.from_pretrained(MODEL_NAME)\ndataset = KouroiEraDataset(CSV_PATH, IMAGE_DIR, processor, MAT2IDX)\nloader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\n\n\n# ——————————————————————————————————————\n# 3) Build model: frozen DINO + MLP head\n# ——————————————————————————————————————\nclass EraClassifier(nn.Module):\n    def __init__(self, backbone_name, num_classes):\n        super().__init__()\n        # load DINO‐V2 without gradient updates\n        self.backbone = Dinov2Model.from_pretrained(\n            backbone_name, output_hidden_states=False, output_attentions=False\n        )\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n        embed_dim = self.backbone.config.hidden_size\n        # a simple 2‑layer MLP head\n        self.head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim//2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(embed_dim//2, num_classes)\n        )\n\n    def forward(self, pixel_values):\n        # pixel_values: (B,3,H,W)\n        outputs = self.backbone(pixel_values=pixel_values)\n        # mean‐pool the patch embeddings: (B, num_patches, D) → (B,D)\n        x = outputs.last_hidden_state.mean(dim=1)\n        logits = self.head(x)\n        return logits\n\nmodel = EraClassifier(MODEL_NAME, num_classes=len(MAT2IDX)).to(DEVICE)\n\n\n\n# ——————————————————————————————————————\n# 4) Training loop\n# ——————————————————————————————————————\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.head.parameters(), lr=LR)  \n# note: we only pass head.parameters() so backbone stays frozen\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n        inputs, labels = batch\n        # move to device\n        inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n        labels = labels.to(DEVICE)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        loss   = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * labels.size(0)\n\n    avg_loss = total_loss / len(dataset)\n    print(f\" Epoch {epoch} avg loss: {avg_loss:.4f}\")\n\n\n\n\n Epoch 1 avg loss: 1.5357\n Epoch 2 avg loss: 0.9983\n Epoch 3 avg loss: 0.8323\n Epoch 4 avg loss: 0.5830\n Epoch 5 avg loss: 0.4432\n Epoch 6 avg loss: 0.3940\n Epoch 7 avg loss: 0.3297\n Epoch 8 avg loss: 0.2696\n Epoch 9 avg loss: 0.1759\n Epoch 10 avg loss: 0.1656\n Epoch 11 avg loss: 0.1141\n Epoch 12 avg loss: 0.0824\n Epoch 13 avg loss: 0.0646\n Epoch 14 avg loss: 0.0565\n Epoch 15 avg loss: 0.0493\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# 1) Run one pass over your data in eval mode\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in loader:\n        if inputs is None: continue\n        inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n        logits = model(**inputs)\n        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# 2) Compute & plot\ncm = confusion_matrix(all_labels, all_preds, labels=list(MAT2IDX.values()))\ndisp = ConfusionMatrixDisplay(cm, display_labels=list(MAT2IDX.keys()))\nplt.figure(figsize=(8,8))\ndisp.plot(cmap=\"Blues\", xticks_rotation=45, values_format=\"d\")\nplt.title(\"Material Classification Confusion Matrix\")\nplt.tight_layout()\nplt.show()\n\n&lt;Figure size 800x800 with 0 Axes&gt;\n\n\n\n\n\n\nfeatures, preds, labels = [], [], []\nmodel.eval()\nwith torch.no_grad():\n    for inputs, labs in loader:\n        if inputs is None: continue\n        inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n        out = model.backbone(pixel_values=inputs['pixel_values'])\n        emb = out.last_hidden_state.mean(1).cpu().numpy()\n        features.append(emb)\n        logits = model.head(torch.from_numpy(emb).to(DEVICE))\n        preds.extend(logits.argmax(dim=1).cpu().numpy())\n        labels.extend(labs.numpy())\n\nfeatures = np.vstack(features)  # shape (N, D)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nfeat2d = pca.fit_transform(features)\n\nplt.figure(figsize=(10,8))\nfor idx, material in enumerate(MAT2IDX.keys()):\n    mask = np.array(labels) == idx\n    plt.scatter(feat2d[mask,0], feat2d[mask,1],\n                label=material, alpha=0.6)\nplt.legend(bbox_to_anchor=(1,1))\nplt.title(\"2D PCA of DINO-V2 Features (True Labels)\")\nplt.show()\n\n\n\n\n\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom torch.utils.data import DataLoader\n\n# 1) Prepare DataLoader (reuse your existing dataset & processor)\ndataset = KouroiEraDataset(CSV_PATH, IMAGE_DIR, processor, ERA2IDX)\nloader  = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0)\n\n# 2) Extract embeddings\nmodel.backbone.eval()\nall_feats, all_eras = [], []\nwith torch.no_grad():\n    for batch in loader:\n        if batch is None: \n            continue\n        inputs, labels = batch\n        inputs = {k:v.to(DEVICE) for k,v in inputs.items()}\n        out = model.backbone(pixel_values=inputs['pixel_values'])\n        # (B, num_patches, D) → (B, D)\n        embs = out.last_hidden_state.mean(dim=1).cpu().numpy()\n        all_feats.append(embs)\n        all_eras.extend(labels.numpy())\n\nall_feats = np.vstack(all_feats)   # shape (N_images, D)\nall_eras  = np.array(all_eras)     # shape (N_images,)\n\n# 3) (Optional) PCA to 50 dims for faster, denoised clustering\npca50 = PCA(n_components=50)\nfeats50 = pca50.fit_transform(all_feats)\n\n# 4) Fit K‑Means (8 clusters)\nn_clusters = 8\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nclusters = kmeans.fit_predict(feats50)\n\n# 5) Confusion between cluster IDs and true eras\ncm = confusion_matrix(all_eras, clusters, labels=list(ERA2IDX.values()))\ndisp = ConfusionMatrixDisplay(\n    cm, display_labels=[f\"Cl{c}\" for c in range(n_clusters)]\n)\nplt.figure(figsize=(6,6))\ndisp.plot(cmap=\"Oranges\", xticks_rotation=45)\nplt.title(\"Confusion: True Eras vs. K‑Means Clusters\")\nplt.tight_layout()\nplt.show()\n\n# 6) Visualize clusters in 2D PCA space\npca2 = PCA(n_components=2)\nfeats2 = pca2.fit_transform(all_feats)\n\nplt.figure(figsize=(8,6))\nfor cl in range(n_clusters):\n    mask = clusters == cl\n    plt.scatter(feats2[mask,0], feats2[mask,1],\n                label=f\"Cluster {cl}\", alpha=0.6)\nplt.legend(bbox_to_anchor=(1,1))\nplt.title(\"2D PCA of Embeddings Colored by K‑Means Cluster\")\nplt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\nplt.show()\n\n&lt;Figure size 600x600 with 0 Axes&gt;\n\n\n\n\n\n\n\n\n\n\n# ——————————————————————————————————————\n# 5) (Optional) Save your head\n# ——————————————————————————————————————\ntorch.save(model.head.state_dict(), \"dino2_era_head.pt\")\nprint(\"⭐ Saved MLP head to dino2_era_head.pt\")\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Arc\nfrom matplotlib import rcParams\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# 1) Global white styling\nrcParams['figure.facecolor'] = 'white'\nrcParams['axes.facecolor']   = 'white'\nrcParams['font.family']      = 'serif'\nrcParams['font.serif']       = ['DejaVu Serif']\n\ncolor1, color2, arc_color = '#1f78b4', '#ff7f00', '#33a02c'\n\n# 2) Zero‐inclusive sliders\ndef make_slider(label, init):\n    return widgets.FloatSlider(\n        value=init, min=0.0, max=3.0, step=0.1,\n        description=label, continuous_update=True,\n        layout=widgets.Layout(width='280px')\n    )\n\ns_x1 = make_slider('Vec1 X', 1.2)\ns_y1 = make_slider('Vec1 Y', 2.4)\ns_x2 = make_slider('Vec2 X', 2.0)\ns_y2 = make_slider('Vec2 Y', 1.0)\n\n# 3) Compact controls in HBox/VBox, with white background\ncontrols = widgets.HBox([\n    widgets.VBox([s_x1, s_y1]),\n    widgets.VBox([s_x2, s_y2])\n], layout=widgets.Layout(background_color='white', padding='4px'))\n\n# 4) Output area fully white, no border\nout = widgets.Output(layout=widgets.Layout(background_color='white', border='0px'))\n\n# 5) Plotting function\ndef plot_vectors(x1, y1, x2, y2):\n    with out:\n        clear_output(wait=True)\n        # prepare vectors\n        emb1, emb2 = np.array([x1,y1]), np.array([x2,y2])\n        n1, n2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n\n        # cosine & angle\n        if n1&gt;0 and n2&gt;0:\n            cos_sim = float(np.clip(np.dot(emb1,emb2)/(n1*n2), -1,1))\n            angle_deg = np.degrees(np.arccos(cos_sim))\n        else:\n            cos_sim, angle_deg = 0.0, 0.0\n\n        # angles for arc\n        a1 = np.degrees(np.arctan2(y1,x1)) % 360\n        a2 = np.degrees(np.arctan2(y2,x2)) % 360\n\n        # figure\n        fig, ax = plt.subplots(figsize=(6,6), facecolor='white')\n        # white canvas behind the widget\n        fig.canvas.layout.background_color = 'white'\n        ax.set_facecolor('white')\n\n        # draw vectors\n        ax.quiver(0,0,x1,y1, color=color1, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  label=f'v₁ (|v₁|={n1:.2f})')\n        ax.quiver(0,0,x2,y2, color=color2, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  alpha=0.85, label=f'v₂ (|v₂|={n2:.2f})')\n\n        # draw arc if meaningful\n        if n1&gt;0 and n2&gt;0 and abs(cos_sim)&lt;0.9999:\n            r = 0.5\n            t0, t1 = sorted((a1,a2))\n            arc = Arc((0,0),2*r,2*r,theta1=t0,theta2=t1,color=arc_color,linewidth=2)\n            ax.add_patch(arc)\n            mid = np.radians((t0+t1)/2)\n            ax.text(r*np.cos(mid)+0.02, r*np.sin(mid)+0.02,\n                    f'θ={angle_deg:.1f}°', color=arc_color,\n                    fontsize=13, fontweight='bold')\n        else:\n            ax.text(0.6,0.6, f'θ={angle_deg:.1f}°',\n                    color=arc_color, fontsize=13, fontweight='bold')\n\n        # cosine annotation\n        ax.text(0.05,2.9, f'Cosine = {cos_sim:.2f}', fontsize=14,\n                bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n\n        # remove spines, add grid, equal aspect\n        ax.set_xlim(0,3); ax.set_ylim(0,3)\n        ax.set_aspect('equal','box')\n        ax.grid(True,linestyle='--',alpha=0.4)\n        for sp in ax.spines.values(): sp.set_visible(False)\n        ax.set_xlabel('Dim 1'); ax.set_ylabel('Dim 2')\n        ax.set_title('Visualizing Cosine Similarity', fontsize=14, fontweight='bold')\n        ax.legend(loc='upper right')\n\n        # zero‐margin\n        fig.tight_layout(pad=0)\n        plt.subplots_adjust(left=0,right=1,top=1,bottom=0)\n        plt.show()\n\n# 6) Wire sliders to the plot\nwidgets.interactive_output(\n    plot_vectors,\n    {'x1': s_x1, 'y1': s_y1, 'x2': s_x2, 'y2': s_y2}\n)\n\n# 7) Display everything\ndisplay(controls, out)\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Arc\nfrom matplotlib import rcParams\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# 2) Global styling: ensure white everywhere\nrcParams['figure.facecolor'] = 'white'\nrcParams['axes.facecolor']   = 'white'\nrcParams['font.family']      = 'serif'\nrcParams['font.serif']       = ['DejaVu Serif']\n\ncolor1, color2, arc_color = '#1f78b4', '#ff7f00', '#33a02c'\n\n# 3) Build sliders (allow zero)\ndef make_slider(label, init):\n    return widgets.FloatSlider(\n        value=init, min=0.0, max=3.0, step=0.1,\n        description=label, continuous_update=True,\n        layout=widgets.Layout(width='250px')\n    )\n\ns_x1 = make_slider('Vec1 X', 1.2)\ns_y1 = make_slider('Vec1 Y', 2.4)\ns_x2 = make_slider('Vec2 X', 2.0)\ns_y2 = make_slider('Vec2 Y', 1.0)\n\n# 4) Compact control layout (white background)\ncontrols = widgets.HBox([\n    widgets.VBox([s_x1, s_y1]),\n    widgets.VBox([s_x2, s_y2])\n], layout=widgets.Layout(background_color='white', padding='4px'))\n\n# 5) White Output widget\nout = widgets.Output(layout=widgets.Layout(background_color='white', border='0px'))\n\n# 6) Plotting function\ndef plot_vectors(x1, y1, x2, y2):\n    with out:\n        clear_output(wait=True)\n\n        emb1, emb2 = np.array([x1, y1]), np.array([x2, y2])\n        n1, n2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n\n        # cosine & angle\n        if n1&gt;0 and n2&gt;0:\n            cos_sim = float(np.clip(np.dot(emb1,emb2)/(n1*n2), -1,1))\n            angle_deg = np.degrees(np.arccos(cos_sim))\n        else:\n            cos_sim, angle_deg = 0.0, 0.0\n\n        # absolute angles for arc\n        a1 = np.degrees(np.arctan2(y1, x1)) % 360\n        a2 = np.degrees(np.arctan2(y2, x2)) % 360\n\n        # Create a truly white figure in the Output widget\n        fig, ax = plt.subplots(figsize=(6,6), facecolor='white')\n        fig.patch.set_facecolor('white')\n        ax.set_facecolor('white')\n\n        # Draw vectors\n        ax.quiver(0,0,x1,y1, color=color1, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  label=f'v₁ (|v₁|={n1:.2f})')\n        ax.quiver(0,0,x2,y2, color=color2, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  alpha=0.85, label=f'v₂ (|v₂|={n2:.2f})')\n\n        # Draw angle arc if valid\n        if n1&gt;0 and n2&gt;0 and abs(cos_sim)&lt;0.9999:\n            r = 0.5\n            t0, t1 = sorted((a1, a2))\n            arc = Arc((0,0), 2*r, 2*r, theta1=t0, theta2=t1, color=arc_color, linewidth=2)\n            ax.add_patch(arc)\n            mid = np.radians((t0 + t1)/2)\n            ax.text(r*np.cos(mid)+0.02, r*np.sin(mid)+0.02,\n                    f'θ={angle_deg:.1f}°', color=arc_color,\n                    fontsize=13, fontweight='bold')\n        else:\n            ax.text(0.6,0.6, f'θ={angle_deg:.1f}°',\n                    color=arc_color, fontsize=13, fontweight='bold')\n\n        # Cosine similarity label\n        ax.text(0.05,2.9, f'Cosine = {cos_sim:.2f}', fontsize=14,\n                bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n\n        # Grid, no spines, equal aspect\n        ax.set_xlim(0,3); ax.set_ylim(0,3)\n        ax.set_aspect('equal','box')\n        ax.grid(True, linestyle='--', alpha=0.4)\n        for sp in ax.spines.values(): sp.set_visible(False)\n        ax.set_xlabel('Dim 1'); ax.set_ylabel('Dim 2')\n        ax.set_title('Visualizing Cosine Similarity', fontsize=14, fontweight='bold')\n        ax.legend(loc='upper right')\n\n        # **Zero margins** around the figure\n        fig.tight_layout(pad=0)\n        plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n        plt.show()\n\n# 7) Hook up interactive output\nwidgets.interactive_output(\n    plot_vectors,\n    {'x1': s_x1, 'y1': s_y1, 'x2': s_x2, 'y2': s_y2}\n)\n\n# 8) Display everything\ndisplay(controls, out)\n\n\n\n\n\n\n\n\n%matplotlib widget\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Arc\nfrom matplotlib import rcParams\nimport numpy as np\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\n\n# 1) Force white everywhere\nrcParams['figure.facecolor']    = 'white'\nrcParams['axes.facecolor']      = 'white'\nrcParams['axes.edgecolor']      = 'white'\nrcParams['figure.edgecolor']    = 'white'\nrcParams['font.family']         = 'serif'\nrcParams['font.serif']          = ['DejaVu Serif']\ncolor1, color2, arc_color = '#1f78b4', '#ff7f00', '#33a02c'\n\n# 2) Sliders (allow zero)\ndef make_slider(label, init):\n    return widgets.FloatSlider(\n        value=init, min=0.0, max=3.0, step=0.1,\n        description=label, continuous_update=True,\n        layout=widgets.Layout(width='250px')\n    )\n\ns_x1 = make_slider('Vec1 X', 1.2)\ns_y1 = make_slider('Vec1 Y', 2.4)\ns_x2 = make_slider('Vec2 X', 2.0)\ns_y2 = make_slider('Vec2 Y', 1.0)\n\ncontrols = widgets.HBox([\n    widgets.VBox([s_x1, s_y1]), widgets.VBox([s_x2, s_y2])\n], layout=widgets.Layout(background_color='white', padding='4px'))\n\nout = widgets.Output(layout=widgets.Layout(background_color='white', border='0'))\n\n# 3) Plot fn\ndef plot_vectors(x1, y1, x2, y2):\n    with out:\n        clear_output(wait=True)\n\n        emb1, emb2 = np.array([x1,y1]), np.array([x2,y2])\n        n1, n2 = np.linalg.norm(emb1), np.linalg.norm(emb2)\n\n        if n1&gt;0 and n2&gt;0:\n            cos_sim = float(np.clip(np.dot(emb1,emb2)/(n1*n2), -1,1))\n            angle_deg = np.degrees(np.arccos(cos_sim))\n        else:\n            cos_sim, angle_deg = 0.0, 0.0\n\n        a1 = np.degrees(np.arctan2(y1,x1)) % 360\n        a2 = np.degrees(np.arctan2(y2,x2)) % 360\n\n        # create widget‐backed figure\n        fig, ax = plt.subplots(figsize=(5,5), facecolor='white')\n        # remove canvas borders\n        canvas = fig.canvas\n        if hasattr(canvas, 'layout'):\n            canvas.layout.border = '0'\n            canvas.layout.margin = '0'\n            canvas.layout.padding = '0'\n\n        ax.set_facecolor('white')\n        ax.spines['left'].set_color('white')\n        ax.spines['bottom'].set_color('white')\n        ax.spines['right'].set_color('white')\n        ax.spines['top'].set_color('white')\n\n        # vectors\n        ax.quiver(0,0,x1,y1, color=color1, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  label=f'v₁ (|v₁|={n1:.2f})')\n        ax.quiver(0,0,x2,y2, color=color2, angles='xy', scale_units='xy', scale=1, width=0.01,\n                  alpha=0.85, label=f'v₂ (|v₂|={n2:.2f})')\n\n        # angle arc\n        if n1&gt;0 and n2&gt;0 and abs(cos_sim)&lt;0.9999:\n            r = 0.5\n            t0, t1 = sorted((a1,a2))\n            arc = Arc((0,0),2*r,2*r,theta1=t0,theta2=t1,color=arc_color,linewidth=2)\n            ax.add_patch(arc)\n            mid = np.radians((t0+t1)/2)\n            ax.text(r*np.cos(mid)+0.02, r*np.sin(mid)+0.02,\n                    f'θ={angle_deg:.1f}°', color=arc_color,\n                    fontsize=13, fontweight='bold')\n        else:\n            ax.text(0.6,0.6, f'θ={angle_deg:.1f}°',\n                    color=arc_color, fontsize=13, fontweight='bold')\n\n        # cosine label\n        ax.text(0.4,2.8, f'Cosine Similarity = {cos_sim:.2f}', fontsize=14,\n                bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n\n        # tidy up\n        ax.set_xlim(0,3); ax.set_ylim(0,3)\n        ax.set_aspect('equal','box')\n        ax.grid(True, linestyle='--', alpha=0.4)\n        ax.set_xlabel('Dim 1'); ax.set_ylabel('Dim 2')\n        ax.set_title('Visualizing Cosine Similarity', fontsize=14, fontweight='bold')\n        ax.legend(loc='upper right')\n\n        # zero margins\n        fig.tight_layout(pad=0)\n        plt.subplots_adjust(left=0,right=1,top=1,bottom=0)\n        plt.show()\n\n# 4) Link and display\nwidgets.interactive_output(\n    plot_vectors,\n    {'x1': s_x1, 'y1': s_y1, 'x2': s_x2, 'y2': s_y2}\n)\ndisplay(controls, out)\n\n\n\n\n\n\n\n\nimport plotly.graph_objects as go\nimport numpy as np\nimport math\nimport panel as pn\npn.extension()\n\ndef cosine_similarity_plot(x1=1.2, y1=2.4, x2=2.0, y2=1.0):\n    v1 = np.array([x1, y1])\n    v2 = np.array([x2, y2])\n    n1 = np.linalg.norm(v1)\n    n2 = np.linalg.norm(v2)\n\n    if n1 == 0 or n2 == 0:\n        cos_sim = 0\n        angle_deg = 0\n    else:\n        cos_sim = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n        angle_deg = np.degrees(np.arccos(cos_sim))\n\n    fig = go.Figure()\n\n    # Vector 1\n    fig.add_trace(go.Scatter(x=[0, x1], y=[0, y1],\n                             mode='lines+markers+text',\n                             line=dict(color='#1f78b4', width=4),\n                             name='Vector 1'))\n\n    # Vector 2\n    fig.add_trace(go.Scatter(x=[0, x2], y=[0, y2],\n                             mode='lines+markers+text',\n                             line=dict(color='#ff7f00', width=4),\n                             name='Vector 2'))\n\n    # Angle arc (circular sector)\n    if n1 &gt; 0 and n2 &gt; 0 and not np.isclose(abs(cos_sim), 1.0):\n        angle1 = np.arctan2(y1, x1)\n        angle2 = np.arctan2(y2, x2)\n        theta = np.linspace(angle1, angle2, 100)\n        if angle2 &lt; angle1:\n            theta = np.linspace(angle2, angle1, 100)\n        arc_radius = 0.5\n        arc_x = arc_radius * np.cos(theta)\n        arc_y = arc_radius * np.sin(theta)\n        fig.add_trace(go.Scatter(x=arc_x, y=arc_y,\n                                 mode='lines',\n                                 line=dict(color='#33a02c', dash='dot'),\n                                 name='Angle θ'))\n\n        # Label angle\n        mid = theta[len(theta)//2]\n        fig.add_trace(go.Scatter(x=[arc_radius * np.cos(mid)],\n                                 y=[arc_radius * np.sin(mid)],\n                                 text=[f'θ = {angle_deg:.1f}°'],\n                                 mode='text',\n                                 textposition='top center',\n                                 showlegend=False))\n\n    # Cosine similarity text\n    fig.add_annotation(x=0.05, y=max(y1, y2, 1.5),\n                       text=f\"&lt;b&gt;Cosine Similarity = {cos_sim:.2f}&lt;/b&gt;\",\n                       showarrow=False, font=dict(size=14))\n\n    # Layout tweaks\n    fig.update_layout(title='Cosine Similarity (Interactive)',\n                      xaxis=dict(range=[-0.1, 3.1], zeroline=True),\n                      yaxis=dict(range=[-0.1, 3.1], zeroline=True),\n                      width=600, height=600,\n                      plot_bgcolor='white',\n                      margin=dict(t=60, l=30, r=30, b=30),\n                      showlegend=True)\n    return fig\n\n# Interactive panel UI\npn.interact(cosine_similarity_plot,\n            x1=(0.0, 3.0, 0.1),\n            y1=(0.0, 3.0, 0.1),\n            x2=(0.0, 3.0, 0.1),\n            y2=(0.0, 3.0, 0.1))\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nimport numpy as np\nimport plotly.graph_objects as go\nimport ipywidgets as widgets\nfrom IPython.display import display\n\ndef plot_cosine_similarity(x1, y1, x2, y2):\n    v1 = np.array([x1, y1])\n    v2 = np.array([x2, y2])\n    n1 = np.linalg.norm(v1)\n    n2 = np.linalg.norm(v2)\n\n    if n1 == 0 or n2 == 0:\n        cos_sim = 0.0\n        angle_deg = 0.0\n    else:\n        cos_sim = np.clip(np.dot(v1, v2) / (n1 * n2), -1.0, 1.0)\n        angle_deg = np.degrees(np.arccos(cos_sim))\n\n    fig = go.Figure()\n\n    # Draw vector lines (for visual clarity)\n    fig.add_trace(go.Scatter(\n        x=[0, x1], y=[0, y1],\n        mode='lines+markers',\n        name=f'Image 1 (|v₁|={n1:.2f})',\n        line=dict(color='#1f78b4', width=2)\n    ))\n    fig.add_trace(go.Scatter(\n        x=[0, x2], y=[0, y2],\n        mode='lines+markers',\n        name=f'Image 2 (|v₂|={n2:.2f})',\n        line=dict(color='#ff7f00', width=2)\n    ))\n\n    # Add true arrows using annotations\n    fig.update_layout(annotations=[\n        dict(\n            x=x1, y=y1, ax=0, ay=0,\n            xref=\"x\", yref=\"y\", axref=\"x\", ayref=\"y\",\n            showarrow=True, arrowhead=3, arrowwidth=2,\n            arrowcolor='#1f78b4', opacity=0.9, text=\"\"\n        ),\n        dict(\n            x=x2, y=y2, ax=0, ay=0,\n            xref=\"x\", yref=\"y\", axref=\"x\", ayref=\"y\",\n            showarrow=True, arrowhead=3, arrowwidth=2,\n            arrowcolor='#ff7f00', opacity=0.9, text=\"\"\n        ),\n        dict(\n            text=f\"&lt;b&gt;Cosine Similarity = {cos_sim:.2f}&lt;/b&gt;\",\n            xref=\"paper\", yref=\"paper\", x=0.35, y=1.05,\n            showarrow=False,\n            font=dict(size=14, color='black'),\n            bgcolor=\"white\", borderpad=4\n        ),\n        # Optional: vector magnitude text at tip\n        dict(\n            x=x1, y=y1, text=f\"|v₁|={n1:.2f}\",\n            showarrow=False, font=dict(color='#1f78b4', size=11),\n            xanchor='left', yanchor='bottom', xshift=5, yshift=5\n        ),\n        dict(\n            x=x2, y=y2, text=f\"|v₂|={n2:.2f}\",\n            showarrow=False, font=dict(color='#ff7f00', size=11),\n            xanchor='left', yanchor='bottom', xshift=5, yshift=5\n        )\n    ])\n\n    # Draw angle arc\n    if n1 &gt; 0 and n2 &gt; 0 and not np.isclose(abs(cos_sim), 1.0):\n        angle1 = np.arctan2(y1, x1)\n        angle2 = np.arctan2(y2, x2)\n        theta = np.linspace(min(angle1, angle2), max(angle1, angle2), 100)\n        arc_radius = 0.5\n        arc_x = arc_radius * np.cos(theta)\n        arc_y = arc_radius * np.sin(theta)\n        fig.add_trace(go.Scatter(\n            x=arc_x, y=arc_y,\n            mode='lines', name='θ',\n            line=dict(color='#33a02c', dash='dot')\n        ))\n\n        mid_angle = (angle1 + angle2) / 2\n        fig.add_annotation(\n            x=arc_radius * np.cos(mid_angle),\n            y=arc_radius * np.sin(mid_angle),\n            text=f\"θ = {angle_deg:.1f}°\",\n            showarrow=False,\n            font=dict(size=13, color='#33a02c')\n        )\n\n    # Layout styling\n    fig.update_layout(\n        width=520, height=520,\n        margin=dict(l=30, r=20, t=50, b=30),\n        xaxis=dict(range=[-0.2, 3.2], zeroline=True, showgrid=True, gridcolor='lightgray'),\n        yaxis=dict(range=[-0.2, 3.2], zeroline=True, showgrid=True, gridcolor='lightgray'),\n        plot_bgcolor='white',\n        paper_bgcolor='white',\n        title='Cosine Similarity Between Two Images',\n        showlegend=True\n    )\n\n    fig.show()\n\n# Sliders\nx1_slider = widgets.FloatSlider(value=1.2, min=0.0, max=3.0, step=0.1, description='Vec1 X')\ny1_slider = widgets.FloatSlider(value=2.4, min=0.0, max=3.0, step=0.1, description='Vec1 Y')\nx2_slider = widgets.FloatSlider(value=2.0, min=0.0, max=3.0, step=0.1, description='Vec2 X')\ny2_slider = widgets.FloatSlider(value=1.0, min=0.0, max=3.0, step=0.1, description='Vec2 Y')\n\nui = widgets.VBox([\n    widgets.HBox([x1_slider, y1_slider]),\n    widgets.HBox([x2_slider, y2_slider])\n])\n\nout = widgets.interactive_output(\n    plot_cosine_similarity,\n    {'x1': x1_slider, 'y1': y1_slider, 'x2': x2_slider, 'y2': y2_slider}\n)\n\ndisplay(ui, out)\n\n\n\n\n\n\n\n\nimport pandas as pd\n\ndf = pd.read_csv(\"../data/complete_sculpture_dataset_labeled.csv\")\n\ndf['material']\n\n0     Marble\n1     Marble\n2     Marble\n3     Marble\n4       Lead\n       ...  \n57    Marble\n58    Marble\n59    Marble\n60    Bronze\n61    Bronze\nName: material, Length: 62, dtype: object\n\n\n\ndf['material'] = df['material'].where(df['material'].isin(['Marble', 'Bronze']), 'Other')\n\n\nprint(df)\n\n                     filename  page           group           era material\n0   page188_img01_photo13.jpg   188   SOUNION GROUP  615 - 590 BC   Marble\n1    page202_img01_photo3.jpg   202   SOUNION GROUP  615 - 590 BC   Marble\n2    page202_img01_photo4.jpg   202   SOUNION GROUP  615 - 590 BC   Marble\n3    page205_img01_photo4.jpg   205   SOUNION GROUP  615 - 590 BC   Marble\n4   page211_img01_photo12.jpg   211   SOUNION GROUP  615 - 590 BC    Other\n..                        ...   ...             ...           ...      ...\n57  page357_img01_photo12.jpg   357     MELOS GROUP  555 - 540 BC   Marble\n58  page358_img01_photo10.jpg   358     MELOS GROUP  555 - 540 BC   Marble\n59   page358_img01_photo4.jpg   358     MELOS GROUP  555 - 540 BC   Marble\n60   page363_img01_photo4.jpg   363  PTOON 20 GROUP  520 - 485 BC   Bronze\n61   page365_img01_photo3.jpg   365        EPILOGUE  485 - 460 BC   Bronze\n\n[62 rows x 5 columns]\n\n\n\ndf.to_csv('../data/complete_sculpture_dataset_labeled.csv', index = False)"
  },
  {
    "objectID": "pages/team.html",
    "href": "pages/team.html",
    "title": "prAxIs Team",
    "section": "",
    "text": "The prAxIs project is a true team effort; the learning materials that you see here have taken input from many writers, coders, editors, and reviewers from a variety of disciplines."
  },
  {
    "objectID": "pages/team.html#principal-investigators",
    "href": "pages/team.html#principal-investigators",
    "title": "prAxIs Team",
    "section": "Principal Investigators",
    "text": "Principal Investigators\n\nJonathan Graves\nLaura Nelson"
  },
  {
    "objectID": "pages/team.html#research-assistants",
    "href": "pages/team.html#research-assistants",
    "title": "prAxIs Team",
    "section": "Research Assistants",
    "text": "Research Assistants\n\n Summer 2025 \n\n\nIrene Berezin\n\n\nAlex Ronczewski\n\n\nNathan Zhang\n\n\nJalen Faddick\n\n\nYash Mali\n\n\nAnna Kovtunenko\n\n\nKrishaant Pathmanathan\n\n\nADD"
  },
  {
    "objectID": "pages/team.html#praxis-partners",
    "href": "pages/team.html#praxis-partners",
    "title": "prAxIs Team",
    "section": "prAxIs+ Partners",
    "text": "prAxIs+ Partners\n\nUBC’s Department of Economics: Vancouver School of Economics\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson"
  },
  {
    "objectID": "pages/team.html#special-thanks",
    "href": "pages/team.html#special-thanks",
    "title": "prAxIs Team",
    "section": "Special Thanks",
    "text": "Special Thanks\n\nThe UBC TLEF team, especially Jeff Miller and Jason Myers.\nThe team behind QuantEcon, especially Jesse Perla and Peifan Wu for their advice and support.\nThe UBC CTLT, Arts ISIT, and LT Hub Teams, for their support with Jupyter and GitLab, especially Stephen Michaud, Nausheen Shafiq, and Michael Ha.\nThe UBC DataScience Slack and the Jupyter team, especially Tiffany Timbers, Phil Austin, Firas Moosvi, and the presenters and attendees at JupyterDays 2020\nThe staff at the VSE for facilitating events, payments, and space use, especially Maria Smith and Caroline Gatchalian\nPlus many, many, more!"
  },
  {
    "objectID": "pages/quickstart.html",
    "href": "pages/quickstart.html",
    "title": "Quickstart Guide",
    "section": "",
    "text": "Want to get started using prAxIs as soon as possible? It’s as easy as 1-2-3!\nAnd select your JupyterHub\nYou’re all ready to go! You can see more details in our using prAxis guide."
  },
  {
    "objectID": "pages/quickstart.html#using-locally",
    "href": "pages/quickstart.html#using-locally",
    "title": "Quickstart Guide",
    "section": "Using Locally",
    "text": "Using Locally\nWant to run prAxIs on your own computer? Don’t have a reliable internet connection? Want to be a power user or have a favourite IDE? You don’t have to rely on a JupyterHub to use prAxIs.\n\nInstall Jupyter or RStudio locally on your own computer, including R and any other packages necessary.\nSelect “Launch Locally” from the “Launch prAxIs” menu and download the repository files.\nUnzip the files and open them in your local IDE.\n\nYou’re all ready to go!"
  },
  {
    "objectID": "pages/installation/rstudio_setup.html",
    "href": "pages/installation/rstudio_setup.html",
    "title": "Using RStudio",
    "section": "",
    "text": "This page explains how to set up RStudio in order to access and use the COMET notebooks."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-r",
    "href": "pages/installation/rstudio_setup.html#installing-r",
    "title": "Using RStudio",
    "section": "1. Installing R",
    "text": "1. Installing R\nThe first thing we’ll need to do is install R onto our computer.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and `associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-rstudio",
    "href": "pages/installation/rstudio_setup.html#installing-rstudio",
    "title": "Using RStudio",
    "section": "2. Installing RStudio",
    "text": "2. Installing RStudio\nWe’ll now install RStudio, our IDE of choice.\n\nWindowsMacOS\n\n\n\nHead to RStudio and press Download RStudio Desktop for windows.\nOpen the RStudio setup and press Next &gt; Next &gt; Install.\n\n\n\n\n\nRStudio, scroll down to “OS”, and select the Mac installer.\nOpen the Rstudio setup and install."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#installing-package-compilers",
    "href": "pages/installation/rstudio_setup.html#installing-package-compilers",
    "title": "Using RStudio",
    "section": "3. Installing package compilers",
    "text": "3. Installing package compilers\nWe’ll need to install a package compiler to activate the renv.lock, a package bundle made specifically for the COMET notebooks.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages from source.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#downloading-and-opening-the-comet-notebooks",
    "href": "pages/installation/rstudio_setup.html#downloading-and-opening-the-comet-notebooks",
    "title": "Using RStudio",
    "section": "4. Downloading and opening the COMET notebooks",
    "text": "4. Downloading and opening the COMET notebooks\n\n4.1. Downloading the COMET notebooks\n\nOpen comet.arts.ubc.ca and, in the top right corner, select Launch Comet &gt; Launch Locally. This will download a .zip file to your computer.\nOpen the file in file explorer and extract the zip folder to your preferred destination."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#importing-the-comet-notebooks-into-rstudio",
    "href": "pages/installation/rstudio_setup.html#importing-the-comet-notebooks-into-rstudio",
    "title": "Using RStudio",
    "section": "4.2. Importing the COMET notebooks into RStudio",
    "text": "4.2. Importing the COMET notebooks into RStudio\nWe can now import the COMET notebooks into RStudio. When we say “import”, what we really mean is setting the working directory. The working directory is the location in your computer where you are working in. Unlike VSCode and other high-level IDEs, setting our working directory is done manually by specifiying our location using the R console.\n\nLocate the downloaded COMET notebooks and copy the absolute file path by right-clicking on the folder and pressing copy as path. If you are on a Mac, hold the option key and select Copy (file name) as Pathname.\nLocate the current working directory in RStudio by entering getwd() in the R console (bottom left of the screen).\n\n\n\n\nThe output should look something like this on windows, but won’t necessarily give the same file path.\n\n\n\nWe’ll now set our working directory to the COMET folder. To do so, enter the following command into the console:\n\nsetwd(\"C:/your/file/path/goes/here\")\nWhere “C:/your/file/path/goes/here” is the absolute file path you copied earlier, with the backshashes (\\) set to forward slashes (/). For example, the file path (on windows) C:\\users\\i_love_econ will be changed to C:/users/i_love_econ.\n\n\n\n\n\n\nWarning\n\n\n\nRStudio requires file paths to have a forward slash instead of a back slash. If you don’t adjust the absolute file path accordingly, you won’t be able to set/change your working directory.\n\n\nTo check that you’ve got the right working directory, run getwd() again.\n\n4.3. Changing your working directory in the files tab.\nOn the right-hand bottom side of Rstudio there is a display that shows your files. If you’d like to change it to your current working directory, press the small button ... button called “go to directory”. This will allow you to pick the COMET modules folder from your file system and navigate it from within R."
  },
  {
    "objectID": "pages/installation/rstudio_setup.html#activating-the-comet-environment",
    "href": "pages/installation/rstudio_setup.html#activating-the-comet-environment",
    "title": "Using RStudio",
    "section": "5. Activating the COMET environment",
    "text": "5. Activating the COMET environment\nWe’re now ready to activate the COMET environment in RStudio.\n\nIn the R console, enter the following line of code: install.packages(\"renv\"). This will install the renv package, which will allow RStudio to read the custom environment file for the notebooks.\nIn the R console, run renv::restore(). You should get a message that reads: “It looks like you’ve called renv::restore() in a project that hasn’t been activated yet. How would you like to proceed?”. Press 1. This should restart R. If it doesn’t, run renv::activate().\nTo check that everything is installed properly, run renv::status(). This command will give you a the list of packages in your environment (Note that this might take some time to run)."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html",
    "href": "pages/installation/local_terminal_setup.html",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "",
    "text": "Some students may prefer to use the local version of Jupyter that is accessible via browser, rather than through Jupyter desktop. The latter allows for more customizability at the expense of a more intuitive installation and activation process."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-r",
    "href": "pages/installation/local_terminal_setup.html#installing-r",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "1. Installing R",
    "text": "1. Installing R\nBefore we install Jupyter, we’ll need to install R and Rtools.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-the-r-package-compiler",
    "href": "pages/installation/local_terminal_setup.html#installing-the-r-package-compiler",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "2. Installing the R package Compiler",
    "text": "2. Installing the R package Compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-conda-and-jupyter",
    "href": "pages/installation/local_terminal_setup.html#installing-conda-and-jupyter",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "3. Installing Conda and Jupyter",
    "text": "3. Installing Conda and Jupyter\nWe can now install Jupyter. To do so, we’ll need to install minconda, a python environment manager, in order to create an envrionment in which we will use our required packages and activate Jupyter.\n\nHead to anaconda.com and select the installer that meets your computer requirements.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that, during the installation process, you select the following options:\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\n\n\n\n\nOnce installed, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet_env jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env."
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#installing-the-r-kernel",
    "href": "pages/installation/local_terminal_setup.html#installing-the-r-kernel",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "4. Installing the R kernel",
    "text": "4. Installing the R kernel\nWe’ll need to install the R kernel in order to use the R programming language in Jupyter. To do so, in the miniconda terminal, run r. This should result in the following output:\n\n\n\n\n\nIf you recieved this output, it means that everything has been set up correctly. Lastly, run the following lines of code in the terminal:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThis will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, this would be “Canada (MB)”.)"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#opening-jupyter",
    "href": "pages/installation/local_terminal_setup.html#opening-jupyter",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "5. Opening Jupyter",
    "text": "5. Opening Jupyter\nFinally, to open Jupyter, open a new miniconda terminal. In the terminal, run the following command: jupyter lab. This will open up Jupyter as a local copy on your search engine.\n\n\n\n\n\n\nWarning\n\n\n\nThis terminal acts as your local Jupyter server. Closing it will shut down your server!"
  },
  {
    "objectID": "pages/installation/local_terminal_setup.html#opening-the-comet-modules",
    "href": "pages/installation/local_terminal_setup.html#opening-the-comet-modules",
    "title": "Install and Use COMET locally Through the Terminal",
    "section": "6. Opening the COMET modules",
    "text": "6. Opening the COMET modules\nLastly, you’ll want to download the COMET files. In the COMET website, press launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer. Extract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. Within Jupyter, find this directory, and open it."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html",
    "href": "pages/installation/jupyter_desktop_setup.html",
    "title": "Running Via Jupyter Desktop",
    "section": "",
    "text": "One alternative to running on a JupyterHub is running Jupyter locally though Jupyter Desktop. This is particularly helpful if you have a machine with a good CPU/GPU and would like to use it to it’s full abilities, or when you need to run commands that require more memory than UBC’s JupyterHub options offer.\nIn 2021, the developers of JupyterLab launched JupyterLab Desktop, a desktop application for JupyterLab, which this tutorial will demonstrate how to install and use."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-and-launching-jupyterlab",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-and-launching-jupyterlab",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing and Launching JupyterLab",
    "text": "Installing and Launching JupyterLab\nTo install JupyterLab on your machine, head to github.com/jupyterlab/jupyterlab-desktop and scroll down to installations. Then, select the version of the installer that you require. From there, open the installer and follow the instructions.\n\n\n\n\n\n\nWarning\n\n\n\nIf you are on a Windows device, running the installer may prompt you with a warning. To bypass it, press Learn More &gt; Run Anyway.\n\n\nTo launch JupyterLab, simply open the application."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-r",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-r",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing R",
    "text": "Installing R\nAdditionally, we’ll need to install R and Rtools.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-the-r-package-compiler",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-the-r-package-compiler",
    "title": "Running Via Jupyter Desktop",
    "section": "3. Installing the R package compiler",
    "text": "3. Installing the R package compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#installing-conda-and-required-packages",
    "href": "pages/installation/jupyter_desktop_setup.html#installing-conda-and-required-packages",
    "title": "Running Via Jupyter Desktop",
    "section": "Installing conda and required packages",
    "text": "Installing conda and required packages\nAdditionally, we’ll need to install minconda, a python environment manager, in order to create an envrionment in which we will use our required packages.\n\nHead to anaconda.com and select the installer that meets your computer requirements.\n\n\n\n\n\n\n\nWarning\n\n\n\nEnsure that, during the installation process, you select the following options:\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\n\n\n\n\nOnce installed, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet_env jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env.\n\nInstalling the R kernel\nWe’ll need to install the R kernel in order to use the R programming language in Jupyter. To do so, in the miniconda terminal, run r. This should result in the following output:\n\n\n\n\n\nIf you recieved this output, it means that everything has been set up correctly. Lastly, run the following lines of code in the terminal:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThis will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, this would be “Canada (MB)”.)\n\n\nSelecting our environment in Jupyter Desktop\nLastly, we’ll need to select our environment. To do so, open jupyter desktop and press on the two blue bars at the top right of the application. This will prompt you to select your new environment. If the previous installation steps were successfull, you should see the COMET environment.\nOnce pressed, this will restart your current session. To check that everything is been set up properly, select the R kernel and run installed.packages() in a new notebook. If the output gives a list of R packages, the installation has been successful."
  },
  {
    "objectID": "pages/installation/jupyter_desktop_setup.html#additional-resources",
    "href": "pages/installation/jupyter_desktop_setup.html#additional-resources",
    "title": "Running Via Jupyter Desktop",
    "section": "Additional resources",
    "text": "Additional resources\nIf you would like to learn more about how to use Jupyter Desktop, consider checking out the Jupyter github page found here."
  },
  {
    "objectID": "pages/installation/installing_locally.html",
    "href": "pages/installation/installing_locally.html",
    "title": "Install and Use COMET",
    "section": "",
    "text": "We have designed COMET to be easy to use and install for a wide range of students. If there’s one thing we know about teaching data and econometrics, it’s that everyone finds their own routine and tools that work best for them. We encourage you to explore and try different things - see what works best for you!\n\nFor most students, we recommend using Jupyter Notebooks via a cloud server, called a JupyterHub which is the easiest to use, and the simplest to get started, since you don’t have to install anything. See Accessing COMET using a JupyterHub for a in-depth explanation.\nAlternatively, you can run JupyterLab directly on your device as a stand-alone application. See Running Via Jupyter Desktop.\nIf you have experience with VSCode, or prefer using a general-purpose IDE, you can install Comet using VSCode.\nIf you have experience with R Studio, you can check out our guide Using RStudio.\n\nIf you’re not sure, start with the cloud-based option, then get more sophisticated in the future.\n\n\n\n\n\n\n\n\n\n\nAccessing COMET using a JupyterHub\n\n\n\n\n\n\n\n\n\n6 Jun 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInstall and Use COMET locally Through the Terminal\n\n\n\n\n\n\n\n\n\n13 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRunning Via Jupyter Desktop\n\n\n\n\n\n\n\n\n\n13 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing RStudio\n\n\n\n\n\n\n\n\n\n13 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing VSCode\n\n\n\n\n\n\n\n\n\n13 Aug 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_text_analysis.html",
    "href": "pages/index/index_text_analysis.html",
    "title": "Text Analysis",
    "section": "",
    "text": "The modules in this unit are for the topics with Text Analysis. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_text_analysis.html#modules",
    "href": "pages/index/index_text_analysis.html#modules",
    "title": "Text Analysis",
    "section": "Modules",
    "text": "Modules"
  },
  {
    "objectID": "pages/index/index_SOCI415.html",
    "href": "pages/index/index_SOCI415.html",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "",
    "text": "The modules in this unit are for SOCI 415 - Theories of Family and Kinship. Major theoretical approaches to the study of the family. Each approach is assessed for its strengths and weaknesses on the basis of empirical data. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_SOCI415.html#modules",
    "href": "pages/index/index_SOCI415.html#modules",
    "title": "SOCI415 - Theories of Family and Kinship",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n22 Jun 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - CBDB Dataset\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n12 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - KINMATRIX Dataset\n\n\nUsing the KINMATRIX Dataset to explore network analysis for SOCI 415\n\n\n\n27 Jun 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_HIST-414.html",
    "href": "pages/index/index_HIST-414.html",
    "title": "Constitutions in Canadian History: From Pre-Contact to the Charter of Rights (HIST 414)",
    "section": "",
    "text": "This section contains material to support UBC’s Constitutions in Canadian History: From Pre-Contact to the Charter of Rights (HIST 414). This course covers European precedents, Colonial self-government, Canadian Confederation, and issues such as gay rights, abortion, and First Nations land rights.\nCHANGE\n\nThese modules in particular are designed to be used in 1 or 2 one-hour long lab environments, with TA or Instructor supervision\nThey can also be used for self-study, with some additional effort\n\nThese materials are a review of basic probability, which contains overlap with AP and IB Statistics from high school.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nText Embeddings for Regina V Wing Chong (1885)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing pyLDAvis for Analysis for HIST-414 Alex R\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext embeddings + conceptual axes\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_CNN.html",
    "href": "pages/index/index_CNN.html",
    "title": "CNN",
    "section": "",
    "text": "The modules in this unit are for the topics with CNN’s. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_CNN.html#modules",
    "href": "pages/index/index_CNN.html#modules",
    "title": "CNN",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nIntroduction to Convolutional Neural Networks (CNNs)\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/all.html",
    "href": "pages/index/all.html",
    "title": "All Modules",
    "section": "",
    "text": "This section contains all materials and case studies to support a variety of classes. Filter by topic using the categories on the right. Filter by class by choosing the appropriate class category."
  },
  {
    "objectID": "pages/index/all.html#modules",
    "href": "pages/index/all.html#modules",
    "title": "All Modules",
    "section": "Modules",
    "text": "Modules\n\n\n\n\n\n\n\n\n\n\nECON 227 - How Do Large Language Models Predict?\n\n\nThis notebook aims to explain how large language models (LLMs) work and make predictions through an example of stock price prediction based on historical price data and news…\n\n\n\n4 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Convolutional Neural Networks (CNNs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Convolutions\n\n\nThis notebook provides an intuitive explanation of convolution and its application in multiple fields with some simple examples.\n\n\n\n22 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP\n\n\n\n\n\n\n24 Jul 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n22 Jun 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - CBDB Dataset\n\n\nThis notebook is an introduction to basic network analysis in Python.\n\n\n\n12 Aug 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOCI 415 Network Analysis - KINMATRIX Dataset\n\n\nUsing the KINMATRIX Dataset to explore network analysis for SOCI 415\n\n\n\n27 Jun 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#about-praxis",
    "href": "index.html#about-praxis",
    "title": "Praxis",
    "section": "About prAxIs",
    "text": "About prAxIs\n\nprAxIs is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2025 that seeks to bring Machine Learning and AI into arts classes at UBC.\nBased at UBC’s Vancouver School of Economics, our team consists of faculty and students (at all levels) working in partnership to develop hands-on learning modules that explore the real-world applications of economic theory using statistics.\n\n\n\nUBC Arts"
  },
  {
    "objectID": "index.html#getting-started-with-praxis",
    "href": "index.html#getting-started-with-praxis",
    "title": "Praxis",
    "section": "Getting Started with prAxIs",
    "text": "Getting Started with prAxIs\n\n\n\n\n For Learners \nThese modules cover topics from beginner to advanced applications of statistical methods in economics. Topics include:\n\nHIST 414\nAMNE 170\nAMNE 376\nSOCI 415\nSOCI 217\nSOCI 280\nECON 227\n\nADD HERE\n\nModules can be accessed on the left-hand side of this page. They are written as JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself, complete the exercises and get automatic feedback throughout your learning. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n For Educators \nThese modules, focused on economic questions, models, and data, use the interactive notebook software, JupyterNotebooks to synthesize theoretical learning, practice, and analysis into a single learning experience that is appropriate for either laboratory or flipped classroom instruction.\nOur work adapts proven pedagogical strategies to integrate practical learning throughout economics curriculum in order to provide students with the opportunity to improve their technical data skills.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating prAxIs resources into your instruction, check out our Using prAxIs for Teaching page."
  },
  {
    "objectID": "index.html#citing-praxis",
    "href": "index.html#citing-praxis",
    "title": "Praxis",
    "section": "Citing prAxIs",
    "text": "Citing prAxIs\n\nThis project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teachings’. https://comet.arts.ubc.ca/.\n\n\n\n\n\nHowever, some notebooks have an additional suggested attribution (especially those in GEOG 374). Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page."
  },
  {
    "objectID": "index.html#get-involved",
    "href": "index.html#get-involved",
    "title": "Praxis",
    "section": "Get Involved",
    "text": "Get Involved\n\nprAxIs is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with prAxIs!\n\nSome partners from other fields have made substantial contributions to this project. We call these people part of  prAxIs+. These materials are particularly relevant for interdisciplinary students in economics or other fields who need to combine insights from econometrics with other statistical approaches.\n\nprAxIs+ Partners\n\n\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe prAxIs Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  },
  {
    "objectID": "docs/intro_to_convolution/intro_to_convolution.html",
    "href": "docs/intro_to_convolution/intro_to_convolution.html",
    "title": "Introduction to Convolutions",
    "section": "",
    "text": "Figure 1. An example of convolution operation (Created by the author)\n\n\nBefore you start, make sure you have the required libraries installed, if not, simply uncomment the lines below (remove the #) and run the cell to install them:\n\n\nCode\n# !pip install opencv-python numpy matplotlib pandas scikit-learn\n\n\n\nWhy Convolutions?\nMathematically speaking, Convolution is an operation that combines two functions to produce a third function, which has a variety of applications in signal processing, image analysis and more. While this may sound complex, we can minimize the math behind it and explain it in a less harmful and vivid way.\nLet’s begin by imagining a simple scenario: you took a picture of a cute dog, and you want to apply a filter to it so that it looks more vibrant and colorful. Now that you input the image into a computer, how does a computer “see” it? The computer would see the image as a grid of numbers, where the combination of 3 numbers (R, G, B) in a grid represents the color of a pixel, and with all the colored pixels combined, it forms the image. Given the numeric nature of a computer image, we say that the image is digitalized.\nHere, let’s read in the image into Python using OpenCV and display it using matplotlib:\n\n\nCode\n# Load necessary libraries\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# Read the image in BGR format\n# Note this is the default input format of Python OpenCV\ndoge_bgr = cv2.imread('images/Original_Doge_meme.jpg')\n\n# Therefore we need to convert the color mapping to RGB\ndoge_rgb = cv2.cvtColor(doge_bgr, cv2.COLOR_BGR2RGB)\n\n# Display the image using matplotlib\nplt.figure(figsize=(4, 4))\nplt.imshow(doge_rgb)\nplt.axis('off')  \nplt.title('Input Image \\n Sato (2010).\"Kabosu the Shiba Inu\"')\nplt.show()\n\n\nThe image doesn’t look like numbers, does it? In fact, if we zoom in close enough, we can clearly see that the image is made up of pixels. But we still don’t see the numbers, this is because the numerical information is decoded by the computer and displayed as a colored pixel. However, we can easily convert the image into a numerical representation.\nFor demonstration purpose, the image is rescaled to a 10 \\(\\times\\) 10 grid, where within each cell, the numbers represents the R, G, B values (0-255) of each pixel.\nNote that compressing images to a smaller size is always easier comparing to enhancing images to a larger size, as compression can be done by going through the image pixel by pixel and averaging the color values in each cell, while enhancement usually requires more complex operations. This gives us a hint on why convolution is important.\n\n\nCode\nimg_rgb = cv2.resize(doge_rgb, (10, 10), interpolation=cv2.INTER_NEAREST)\n\n# Plot the RGB matrix\nfig, ax = plt.subplots(figsize=(6, 6))\n\n\nh, w = img_rgb.shape[:2]\nax.imshow(img_rgb, extent=[0, w, h, 0]) \n\n# Set ticks to show grid at pixel boundaries\nax.set_xticks(np.arange(0, w + 1, 1))\nax.set_yticks(np.arange(0, h + 1, 1))\nax.set_xticklabels([])\nax.set_yticklabels([])\nax.grid(color='black', linewidth=1)\n\nfor i in range(h):\n    for j in range(w):\n        r, g, b = img_rgb[i, j]\n        brightness = int(r) + int(g) + int(b)\n        color = 'white' if brightness &lt; 380 else 'black'\n        ax.text(j + 0.5, i + 0.5, f'({r},{g},{b})',\n                ha=\"center\", va=\"center\", fontsize=6, color=color)\n        \n        \n# Display the Grid\nax.set_title(\"RGB Value Grid of Doge Image Resized to 10x10\")\nplt.tight_layout()\nplt.show()\n\n\nThe image has become a little abstract. Can you still identify the original image out of it?\nWhile the resized image looked significantly different, it still contains the necessary information, and same ideas also applies to larger images. That is, all images can be represented as a grid of numbers, where the 3 numbers in each cell corresponds to the color of a pixel. Computers can’t see colors like we do, the way they see colors is as if they were mixing colors using a palette that only has red, green and blue (RGB), where each color has an “amount” of intensity between 0 and 255. With the 3 values for red, green and blue, computers can create any color we see in the world.\nBack to the dog picture, it is easy to see that resizing the image to a smaller grid loses a lot of details, especially the rich color that makes the image vibrant. If we want to keep the complete color information, alternatively, we can plot out the distribution of the RGB values and frequencies in the image using a histogram. While this gives us a good idea of the color distribution, it does not tell us much about the spatial relationships between the pixels.\n\n\nCode\n# Compute and plot the color histogram\ncolors = ('r', 'g', 'b')\nplt.figure(figsize=(6, 4))\n\nfor i, col in enumerate(colors):\n    hist = cv2.calcHist(\n        images=[doge_bgr],       # source image (still in BGR)\n        channels=[i],           # which channel: 0=B, 1=G, 2=R\n        mask=None,              \n        histSize=[256],         \n        ranges=[0, 256]         \n    )\n    plt.plot(hist, color=col)\n    plt.xlim([0, 256])\n    \n# Display the histogram\nplt.title('RGB Histogram')\nplt.xlabel('Pixel Value')\nplt.ylabel('Frequency')\nplt.tight_layout()\nplt.show()\n\n\nCan you still identify the original image out of it?\nThe example above shows us what images are like in the eyes of a computer. Computers do not understand images in the same way that humans do, they can only see them as a collection of numbers. It thus make sense that we need to apply some math to these numbers to either change the image or extract some useful information from it, and that’s where convolution comes in.\n\n\nHow Does Convolution Work?\nBefore we dive into the application of it, let’s first understand how convolution operate on an image in a more intuitive way:\nThink of copying a painting by first sketching its outline at the same size. To add your own flair, you use a patterned brush: wherever the brush touches, it brightens the paint. You move this brush methodically across your sketch, from left to right, top to bottom so every spot is stamped with the pattern, tweaking the original image into something familiar yet distinctly styled.\nHere, the brush you used is called a kernel in the context of convolution, and the process of applying the brush is what we call convolution operation. We would define the kernel as a small matrix of numbers that represents the pattern of the brush, and the convolution operation as the process of transforming the original image by applying the kernel to it.\nHere is a gif illustrating how our filter (the kernel) will work on the image mathematically. You can see it as the small brush that slides over the image, operating on a small region of the image at a time, and eventually producing a new image that was completely transformed by the filter.\n\n\n\nFigure 2. Visual explanation of sharpening filter (Michael Plotke, Wikipedia, 2013)\n\n\nNow, let’s return to the example of the cute dog picture. What we are going to apply is a brush called sharpening filter, it is a 3 \\(\\times\\) 3 matrix that looks like this:\n\\[\n\\text{Sharpening Filter} = \\begin{bmatrix}\n0 &-1 & 0 \\\\\n-1 & 5 & -1 \\\\\n0 & -1 & 0\n\\end{bmatrix}\n\\]\nDon’t panic as we are not going to do any math here, we will just let computer do the math for us. The only thing you need to know is that this kernel will enhance the edges of the image, making it look sharper and more defined. For better understanding, let’s visualize it to see what this kernel look like in grey-scale:\n\n\nCode\nsharp_kernel = np.array([\n    [ 0, -1,  0],\n    [-1,  5, -1],\n    [ 0, -1,  0]\n], dtype=np.float32)\n\nfig, ax = plt.subplots(figsize=(4, 4))\n\n# Display with true gray-scale between kernel's min and max\nim = ax.imshow(sharp_kernel, cmap='gray_r',\n               vmin=sharp_kernel.min(), vmax=sharp_kernel.max())\nax.set_title('Sharpening Kernel')\nax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nIf you take a closer look at the kernel, you will see that it has a positive value in the center and negative values around it, and it has 0 values on the corners. This exactly looks like a brush that enhances the center of a region while reducing the intensity of the surrounding pixels, which is exactly what we want to achieve with the filter.\n\n\nCode\n# Define the sharpening filter\nkernel = np.array([\n    [ 0, -1,  0],\n    [-1,  5, -1],\n    [ 0, -1,  0]\n], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nfiltered = cv2.filter2D(doge_rgb, ddepth=-1, kernel=kernel)\n\nfiltered = np.clip(filtered, 0, 255).astype(np.uint8)\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\n# Input\nax1.imshow(doge_rgb)\nax1.set_title('Input Image \\n Sato (2010).\"Kabosu the Shiba Inu\"')\nax1.axis(\"off\")\n\n# Output\nax2.imshow(filtered)\nax2.set_title(\"Filtered Image (Sharper & More Vibrant)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nThe difference is quite obvious now. You can clearly see that the sharpened image (R) has more contrast and the edges are more defined, making it look more vibrant and colorful. This is the power of convolution, it allows us to apply filters to images and transform them in a way that is not possible with simple pixel manipulation. Similarly, we can blur an image quite easily, for which the “brush” we are going to use looks like this\n\\[\\text{Box Blur Filter} = \\begin{bmatrix}\n\\frac{1}{9} &\\frac{1}{9} & \\frac{1}{9} \\\\\n\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9} \\\\\n\\frac{1}{9} & \\frac{1}{9} & \\frac{1}{9}\n\\end{bmatrix}\n\\]\nVisually, this kernel is a filter that looks like:\n\n\nCode\nid_kernel = np.array([\n  [ 1, 1, 1],\n  [ 1, 1, 1],\n  [ 1, 1, 1]\n])\n\nblur_kernel = np.array([\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9]\n], dtype=np.float32)\n\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor ax, kernel, title in zip(axes, [id_kernel, blur_kernel],\n                             ['Input Image', 'Box Blur Kernel']):\n    # Show actual values in gray-scale\n    im = ax.imshow(kernel, cmap='gray_r', vmin=0, vmax=1)\n    ax.set_title(title)\n    ax.axis('off')\n\n    # Draw horizontal and vertical boundary lines at half‐integer positions\n    for i in range(1, kernel.shape[0]):\n        ax.axhline(i - .5, color='white', linewidth=1.5, zorder=1)\n        ax.axvline(i - .5, color='white', linewidth=1.5, zorder=1)\n\nplt.tight_layout()\nplt.show()\n\n\nLet’s see what it will make on our input image.\n\n\nCode\n# Define the box blur filter\nkernel = np.array([\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9],\n    [1/9, 1/9, 1/9]\n], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nfiltered = cv2.filter2D(doge_rgb, ddepth=-1, kernel=kernel)\n\nfiltered = np.clip(filtered, 0, 255).astype(np.uint8)\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n\n# Input\nax1.imshow(doge_rgb)\nax1.set_title('Input Image\\n Sato (2010).\"Kabosu the Shiba Inu\"')\nax1.axis(\"off\")\n\n# Output\nax2.imshow(filtered)\nax2.set_title(\"Filtered Image (Blurred)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nIt worked as we expected, now the dog image becomes more blurry.\nHowever, convolution on image is not limited to filtering, it can also be used to extract features from the image. For example, we can use a kernel to detect edges, lines and texts in images, we can even use specific kernels to detect specific shapes or patterns in images, such as a kernel that detects anything that looks like a dog. As you can imagine, this is a very powerful tool in many applications, in fact, you most likely have already used it in your daily life. For example, when you use a photo editing app to apply a filter to your picture, the app is using convolution. When you use a search engine to search do image searches, the search engine is using convolution to extract features from the images and match them with your search query. Let’s say, the thing you are trying to find is eyes, the gif below shows how a convolution kernel detects “eyes” in an image:\n\n\n\nFigure 3. Eye detection kernel (Neuromatch Academy, 2025)\n\n\nThis is a very simple example, and it is implemented exactly the same way as we did with the sharpening filter. The only difference is that to extract a specific features, we need to use a “brush” designed to detect that feature, which usually requires some knowledge of the feature we want to extract. For example, if we want to detect eyes in the painting, we would need our “brush” to understand what eyes look like and what typical colors they have. This could be way too complicated for a single “brush”, so we often use multiple brushes to detect different features when it comes to the task of feature extraction.\nTo demonstrate how convolution extracts a specific feature from an image, let’s take a look at a different “art tool”. Let’s say this time you don’t want to color the painting differently, but rather you want to sketch a line art based on the original painting. You would use a fineliner pen that detects the edges of a painting and draw a line along them. In the eyes of a computer, these are the tools it is going to use:\n\\[\\text{Horizontal Sobel} = \\begin{bmatrix}\n1 & 0 & -1 \\\\\n2 & 0 & -2 \\\\\n1 & 0 & -1\n\\end{bmatrix}\n\\text{, }\n\\text{Vertical Sobel} = \\begin{bmatrix}\n1 & 2 & 1 \\\\\n0 & 0 & 0 \\\\\n-1 & -2 & -1\n\\end{bmatrix}\n\\]\nStill, don’t panic, we won’t do any math in this notebook. All you need to know is these two kernels together are called Sobel filter, and what they do is highlighting the edges in the image, making them more visible. We can also visualize them as follows:\n\n\nCode\nh_kernel = np.array([\n    [ 1, 0, -1],\n    [ 2, 0, -2],\n    [ 1, 0, -1]\n])\n\nv_kernel = np.array([\n    [ 1, 2, 1],\n    [ 0, 0, 0],\n    [ -1, -2, -1]\n])\n\n# Plot the kernels\nfig, axes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor ax, kernel, title in zip(axes, [np.abs(h_kernel), np.abs(v_kernel)],\n                             ['Horizontal Kernel', 'Vertical Kernel']):\n    # Show actual values in gray-scale\n    im = ax.imshow(kernel, cmap='gray_r', vmin=0, vmax=2)\n    ax.set_title(title)\n    ax.axis('off')\n\nplt.tight_layout()\nplt.show()\n\n\nIf you take a closer look at the kernels, you will see that the first one has positive values on the left and negative values on the right, while the second one has positive values on the top and negative values on the bottom. This pattern intuitively tells us that the first “pen” will scan through the image horizontally and extract the horizontal edges, while the second “pen” will scan through the image vertically and extract the vertical edges.\nLet’s now look at a different example and see what happens when we apply the Sobel filter.\n\n\nCode\n# Generate a greyscale version of the image\nhill_bgr = cv2.imread('images/xiangbishan.jpg')\n\nhill_rgb = cv2.cvtColor(hill_bgr, cv2.COLOR_BGR2RGB)\n\nhill_gray = cv2.cvtColor(hill_bgr, cv2.COLOR_BGR2GRAY)\n\n# Define the Sobel filter kernel\nsb_kernel_h = np.array([\n    [ 1, 0, -1],\n    [ 2, 0, -2],\n    [ 1, 0, -1]\n], dtype=np.float32)\n\nsb_kernel_v = np.array([\n    [ 1, 2, 1],\n    [ 0, 0, 0],\n    [-1, -2, -1]], dtype=np.float32)\n\n# Apply the kernel to the color image using filter2D.\nhoriz = cv2.filter2D(hill_gray, -1, sb_kernel_h)\n\nvert = cv2.filter2D(hill_gray, -1, sb_kernel_v)\n\ncombined = cv2.convertScaleAbs(np.sqrt(vert.astype(np.float32)**2 + horiz.astype(np.float32)**2))\n\n# Display the original and filtered images side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n\n# Input\nax1.imshow(hill_rgb)\nax1.set_title('Input Image \\n xiquinhosilva (2018). \"Elephant Trunk Hill\"')\nax1.axis(\"off\")\n\n# Output\nax2.imshow(combined, cmap = 'gray')\nax2.set_title(\"Filtered Image (Edges Highlighted)\")\nax2.axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n\n\nAs we can see in the example above, the Sobel filter detects the edges in the image and highlights them like a fineliner pen. This is a very useful technique in image processing, as it allows us to extract features from the image that can be used for further analysis or classification. Let’s do more explorations with the example above:\n\n\nCode\n# Compute the magnitude and orientation of the gradient.\n# To put it simpler, they represents the length and direction of the edges at each pixel.\n# For those who are familiar with pythagorean theorem, the magnitude is exactly the length of the\"long edge\" calculated with pythagorean theorem.\nmagnitude = np.sqrt(vert.astype(np.float32)**2 + horiz.astype(np.float32)**2)\norientation = np.arctan2(vert.astype(np.float32), horiz.astype(np.float32))\n\n# Generate an edge binary map\nmag_norm = cv2.normalize(magnitude, None, 0,255, cv2.NORM_MINMAX).astype(np.uint8)\n_, edge_binary = cv2.threshold(mag_norm, 50, 255, cv2.THRESH_BINARY)\n\n# Create the Contour plot\ncontours, _ = cv2.findContours(edge_binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\ncontour_img = hill_rgb.copy()\ncv2.drawContours(contour_img, contours, -1, (255,0,0), 1)\n\n# Create an edge density heatmap\nblock = 32\nh, w = edge_binary.shape\nint_img = cv2.integral(edge_binary)\n\nheatmap = np.zeros_like(edge_binary, dtype=np.float32)\n\nfor y in range(0, h - block + 1):\n    for x in range(0, w - block + 1):\n        y1, x1 = y,       x\n        y2, x2 = y + block, x + block\n        total = (int_img[y2, x2]\n               - int_img[y1, x2]\n               - int_img[y2, x1]\n               + int_img[y1, x1])\n        # center the block’s density back into the heatmap\n        heatmap[y + block//2, x + block//2] = total\n        \nheatmap = cv2.normalize(heatmap, None, 0,255, cv2.NORM_MINMAX).astype(np.uint8)\n\n# Plot out the visualizations\nfig, axs = plt.subplots(2, 2, figsize=(8, 5))\nax1, ax2, ax3, ax4 = axs.flatten()\n\n# Original Image\nax1.imshow(hill_rgb)\nax1.set_title(\"Input Image\")\nax1.axis(\"off\")\n\n# Contours on Original Image\nax2.imshow(contour_img)\nax2.set_title(\"Contours on Original Image\")\nax2.axis(\"off\")\n\n# Edge Density Heatmap\nax3.imshow(heatmap, cmap=\"hot\")\nax3.set_title(\"Edge Density Heatmap\")\nax3.axis(\"off\")\n\n# Edge Orientation Histogram\nangles_deg = np.degrees(orientation[magnitude &gt; 50].flatten())\n\nax4.hist(angles_deg, bins=36, range=(0, 90), color='purple')\nax4.set_title(\"Edge Orientation Histogram\")\nax4.set_xlabel(\"Angle (degrees)\")\nax4.set_ylabel(\"Frequency\")\n\n\nplt.tight_layout()\nplt.show()\n\n\nHere, we have generated three visualizations based on the extracted edge data.\n\nThe first visualization is a Contour Map on the original image. It circles contours from the image that are distinguishable from others. In many cases, this implies the circled regions have different visual patterns or belong to different objects, which is very useful in tasks such as image recognition.\nThe second visualization is the Edge Density Heatmap that assigns corresponding heat values to regions on the image based on the density distribution of the edges. The lighter the color, the higher the edge density of the region. This helps us to understand which area of the image carries the most information that may be of interest to us.\nThe third visualization is a histogram showing the distribution of edge orientations. edge orientation is the direction of the edge in the graph, expressed as the angle between the edge and the x-axis (horizontal line). Understanding the distribution of edge orientation can help us better recognize the graphical features of objects in an image for tasks such as classification. For example, based on this image featuring a hill and a water surface, we now know that the edges of these two objects typically have either a horizontal or vertical orientation.\n\nThe examples above demonstrated the power of convolution in both image processing and image analysis, and more importantly, convolution is very efficient, as it is easy for computers to understand and process, and can be applied to images of any size without losing information. This is why convolution has become a fundamental operation in computer vision and image processing.\nBut as you are probably aware, implementing convolution from scratch can be quite tedious, especially when we need to perform more specific tasks such as detecting texts or some specific shapes. A more advanced, adaptable and efficient way of applying convolution has been developed, which is called Convolutional Neural Network (CNN), and we will discuss it in the next notebook.\n\n\nKey takeaways:\n\nComputers see images as grids of numbers, where each grid cell contains the RGB values of a pixel. With the spatial relationships and the color information, computers can understand images without losing information.\nConvolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\nThe kernel that sharpens an image is called a sharpening filter, which enhances the edges of the image and makes it look more vibrant. The kernel that detects edges is called Sobel filter, which highlights the edges in the image and makes them more visible.\nConvolution is a powerful tool that can be used in many applications, such as photo editing, image search and feature extraction. It is efficient and can be applied to images of any size without losing information.\n\n\n\nGlossary\n\nComputer Vision: Computer vision is a field of artificial intelligence that enables computers to “see” and interpret images and videos in a way that is similar to human vision.\nConvolution: Convolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\nKernel/Filter/Mask: A convolution kernel, also known as a filter or mask, is a small matrix (array of numbers) used in image processing and computer vision to perform operations like blurring, sharpening, edge detection, and more.\n\n\n\nAdditional Resources\n\n3Blue1Brown. (2018, August 15). But what is a convolution? [Video]. YouTube. https://www.youtube.com/watch?v=KuXjwB4LzSA This video graphically explains the mathematical operation of convolution.\n\n\n\n\nReferences\n\nWikipedia contributors. (2025, June 18). Convolution. In Wikipedia, The Free Encyclopedia. Retrieved June 18, 2025, from https://en.wikipedia.org/wiki/Convolution\nSato, A. (2010, September 13). Kabosu the Shiba Inu (“Doge”) [Photograph]. Flickr. This photo is cited and used under   https://creativecommons.org/licenses/by-sa/2.0/\nxiquinhosilva. (2018, June 14), Elephant Trunk Hill [Photograph]. Flickr. This photo is cited and used under   https://creativecommons.org/licenses/by-sa/2.0/\nGeeksforGeeks. (n.d.). Types of Convolution Kernels. Retrieved July 16, 2025, from https://www.geeksforgeeks.org/deep-learning/types-of-convolution-kernels\nOpenCV.org. (n.d.). Image Processing in OpenCV. Retrieved July 16, 2025, from https://docs.opencv.org/4.x/d2/d96/tutorial_py_table_of_contents_imgproc.html"
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html",
    "href": "docs/SOCI-415/soci_415_network_analysis.html",
    "title": "SOCI 415 Network Analysis",
    "section": "",
    "text": "This notebook introduces key concepts in network analysis pertaining to sociology and provides a hands-on tutorial to using the NetworkX Python library."
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html#what-is-network-analysis",
    "href": "docs/SOCI-415/soci_415_network_analysis.html#what-is-network-analysis",
    "title": "SOCI 415 Network Analysis",
    "section": "1. What is Network analysis?",
    "text": "1. What is Network analysis?\nNetwork Analysis is a set of techniques used to study the structure and dynamics of networks. Networks are collections of objects/locations/entities (called nodes) connected by relationships (called edges). Network analysis has applications in many fields, including sociology, biology, economics, computer science, and more.\n\n\n\nRegional networks of ceramic similarity across time in the greater Arizona/New Mexico area of the US (From Mills et al. 2013, Fig. 2)\n\n\nNetwork science in Sociology Network analysis in sociology is the systematic study of social structure through the mapping and measurement of relationships among individuals, groups, or organizations. It is not a discipline on its own, but rather a methodological and theoretical approach within sociology that helps conceptualize, describe, and model society as interconnected sets of actors linked by specific relationships. Network analysis in sociology is a core tool for understanding how relationships and social structures impact individuals and groups,\n\n1.1 Key terms\n\n\n\n\n\nAn example of a graph with nodes and edges.\n\n\n\nNode: A node is a representation of an individual entity or actor in a network. In different contexts, nodes can be people, organizations, cities, or any other unit of analysis.\nEdge: An edge represents the relationship or connection between two nodes. Edges can be directed (having a specific direction from one node to another) or undirected (no direction, implying a mutual relationship).\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nDegree: The degree of a node is the number of edges connected to it. In directed networks, this can be further divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges).\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nThe network above is an example of a Undirected graph, a graph with no direction. This means that if there is a connection between node A and node B, it is bidirectional - A is connected to B, and B is connected to A.\nThe example to the left is a directed graph: the edges between nodes have a specific direction. This means that if there is an edge from node A to node B, it does not imply there is an edge from B to A unless explicitly stated.\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nDensity: Density is a measure that indicates how closely connected the nodes in a network are. Specifically, it refers to the ratio of the number of actual edges in the network to the maximum possible number of edges between nodes.\n\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\nCentrality: Centrality measures the importance, influence, or prominence of nodes (entities) within a network. The centrality of a node tells us how “important” a node is to the aggregate network. There are many different kinds of centrality, but the four most well-known ones are degree, betweenness, closeness, and eigenvector centrality.\n\n\n\n\n1.2 NetworkX\nNetworkX is a Python library that is used for the creation, manipulation, and visualization of complex networks. It provides tools to work with both undirected and directed networks, perform network-related calculations, and visualize the results.\nA library in Python is a collection of code that makes everyday tasks more efficient. In this case working with networks becomes much simpler when using NetworkX.\nIf you want to read the NetworkX documentation you can follow the NetworkX documentation link. This link shows what kind of commands exist within the NetworkX library.\n\n1.2.1 Importing NetworkX\nWe can import NetworkX using the import command. At the same time, we’ll also import the matplotlib.pyplot library, for plotting graphs. Additionally, we’ll import pandas for basic data wrangling, and numpy for math. The as command allows us to use networkx commands without needing to type out networkx each time. Along with some other libraries.\n\nimport matplotlib.pyplot as plt #allows us to call the matplotlib.pyplot library as 'plt'\nimport matplotlib.patches as mpatches #imports mpatches matplotlib subpackage \nimport networkx as nx #allows us to call the networkx library as 'nx'\nimport pandas as pd #allows us to call the pandas library as 'pd'\nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\n\n\n\n1.2.2 Creating simple networks using NetworkX\nWe’ll start by creating a simple graph:\nBelow in the code we choose our nodes and edges between them.\n\nG = nx.Graph() #creates an empty network graph\n\nnodes = (1, 2, 3, 4, 5, 6) #our nodes, labeled 1,2,3,4,5,6.\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\n        #the connections between our nodes are stored in an array, containing pairs of numbers called tuples.\nG.add_edges_from(edges) #the `add_edges_from()` command adds edges to the network\nG.add_nodes_from(nodes) #the `add_nodes_from()` command adds nodes to the network\n\nnx.draw(G, with_labels = True) #renders the graph in the notebook\n        #the `with_labels = True` argument specifies that we want labels on the nodes.\n\nLet’s create a directed graph using nx.DiGraph(). We’ll also set our node positions using a seed: this will ensure that each time the nodes are rendered they hold the same position on the graph. You can set the seed to any number.\n\nG = nx.DiGraph() #creates an empty directed graph object\nnodes = (1, 2, 3, 4, 5, 6) #our nodes\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)] #our tuples stored in an array which represent our nodes\nG.add_edges_from(edges) #connects edges to nodes\nG.add_nodes_from(nodes) #connects edges to nodes\n\nposition = nx.spring_layout(G, seed=100)\n\n#nx.draw plots our network\nnx.draw(G, pos = position, with_labels = True) # `pos` argument assigns a position to each node\n\n\n\n\n1.3 Creating Random Graphs\nInstead of creating a graph with predetermined positions of nodes and edges we can also generate a random graph with a set amount of nodes and edges. Below you can change the amount of nodes and edges by changing n and d which correspond to the number of nodes and the degree (number of edges) that each node has. Creating a random graph could be more helpful for testing or when you want to try something and don’t wish to spend time plotting a real network and determining paths for all edges and nodes.\nThe command we will use is the nx.random_regular_graph command. Which generates a random regular graph.\n\n# Set a seed for reproducibility so that everytime the code runs we get the same random graph\nrandom.seed(42)\n\n# Parameters\nn = 20  # number of nodes\nd = 3   # degree of each node\n\n# Generate the random regular graph\nrr_graph = nx.random_regular_graph(d, n)\n\n# Visualize the graph, you can change the size, color, font and node size. \nplt.figure(figsize=(8, 6)) \nnx.draw(rr_graph, with_labels=True, node_color='lightgreen', node_size=500, font_size=10, font_weight='bold')\nplt.title(\"Random Regular Graph\")\nplt.show()\n\n# Print some basic information about the graph\nprint(f\"Number of nodes: {rr_graph.number_of_nodes()}\")\nprint(f\"Number of edges: {rr_graph.number_of_edges()}\")\nprint(f\"Degree of each node: {d}\")"
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html#degrees-density-and-weights",
    "href": "docs/SOCI-415/soci_415_network_analysis.html#degrees-density-and-weights",
    "title": "SOCI 415 Network Analysis",
    "section": "2. Degrees, Density and Weights",
    "text": "2. Degrees, Density and Weights\n\n2.1 Degrees\nThe degree of a node is the number of edges that are connected to a node.\nWe can see the degree of each node by running dict(G.degree()). This create a dictionary of key-value pairs for our network, where each key is the name of the node and the value is it’s respective degree.\n\ndegrees = dict(G.degree())\n\nIf we want to see the degree of node \\(n\\), we can do so by running print(degrees[n]). For instance:\n\nprint(degrees[1])\n\nLet’s color the nodes of our graph based on their degree. We’ll create a function called get_node_colors which takes in the degree dictionary of each node and returns a color. We’ll then create a for-loop that iterates over each nodes in the list of nodes, gets the color of each node using the get_node_colors function we defined earlier, and appends it to an empty list called color_map.\n\ndegrees = dict(G.degree())\nnodes = list(G.nodes())\n\ndef get_node_colors(degree):\n    if degree in [1, 2]:\n        return 'blue'\n    elif degree in [3, 4]:\n        return 'green'\n    elif degree in [5, 6]:\n        return 'yellow'\n    else:\n        return 'red' \n\ncolor_map = [] #`color_map` is an empty list\n\nfor node in nodes:\n  color = get_node_colors(degrees[node]) # get color of current node using node_colors according to degree of node\n  color_map.append(color) # appends color of each node to color_map for each node in nodes\n\nprint(degrees)\nprint(nodes)\nprint(color_map)\n\nThe \\(n\\)-th entry in color_map corresponds to the \\(n\\)-th node in nodes. For instance, color_map[0] returns the color of the first node (1).\n\ncolor_map[0]\n\nWe can now color the nodes of our graph, using the color map we defined above. The node_color argument takes in an array or list of colors that it uses to color each node.\n\nG = nx.DiGraph() # creates an empty directed graph object\nnodes = (1, 2, 3, 4, 5, 6) \nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nG.add_edges_from(edges) \nG.add_nodes_from(nodes) \n\nposition = nx.spring_layout(G, seed=100)\n\nnx.draw(G, pos = position, node_color=color_map, with_labels=True) \n    # node_color argument colors the nodes based on a given list or array of colors, \n    # with the first color corresponding to the first node, second to the second node, etc.\n\nLet’s also add a legend to our graph, which gives information about the meaning of each color. We’ll do this using the mpatches subpackage we imported earlier.\n\nblue_patch = mpatches.Patch(color='blue', label='1-2 edges') \ngreen_patch = mpatches.Patch(color='green', label='3-4 edges')\nyellow_patch = mpatches.Patch(color='yellow', label='5-6 edges')\nplt.legend(handles=[blue_patch, green_patch, yellow_patch]) #adds legend to the plot\n\nnx.draw(G, pos = position, node_color=color_map, with_labels=True)\n\n\n\n2.2 Density\nDensity refers to the proportion of actual edges in a network compared to the total possible connections. It indicates how interconnected the nodes in a network are, with a higher density suggesting a more connected network. Density is defined from [0,1], meaning a network with a density of 0.95 is very interconncected. Note that self-loops (edges from and to the same node) are counted in the total number of edges but not in the maximum number of edges so graphs can have a density greater than 1.\nWe can calculate the density of our graph:\n\nnx.density(G)\n\n\n\n2.3 Weights\nOften times, you may end up working with weighted graphs: for instance, these weights could correspond to popularity of roads in road networks, or the size of pipes in a sewage network.\nWe’ll standardize our weights to be between 1 and 2 (as otherwise the results are messy). We’ll do this using a for-loop, like we did with the degrees.\n\nG_weights = nx.DiGraph() #creating a new graph object called G_weights\nnodes = [1, 2, 3, 4, 5, 6]\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nweights = [100, 50, 75, 50, 60, 100, 100, 75, 40, 50, 50, 100, 100] #add list of weights\nG_weights.add_edges_from(edges) \nG_weights.add_nodes_from(nodes) \n\nadjusted_weights = []\nfor weight in weights:\n    adjusted_weight = 1+ (max(weights)-weight)/(max(weights)-min(weights)) #standardizes weights to be between 1 and 2\n    adjusted_weights.append(adjusted_weight)\n\nposition = nx.spring_layout(G, seed=100)\n\nprint(adjusted_weights)\nnx.draw(G_weights, pos = position, width = adjusted_weights, with_labels = True) \n    # width argument take in a list or array of numbers corresponding to weights\n\nThis is great, but the results aren’t very clear. Let’s add a color gradient to the edges to represent different weights.\n\nnorm = plt.Normalize(min(weights), max(weights), clip=False) \n    #`plot.normalizes` normalizes the weights such that they are evenly distributed across the gradient spectrum\nedge_colors = plt.cm.Greys(norm(weights)) \n    # norm(weights) normalizes the weights \n    # plot.cm.greys() assigns the weights to color values\n    # edge_colors is a multidimensional array of RGBA color values corresponding to each edge\n\nfig, ax = plt.subplots() #explicitly specifying figure and axes in order to create a color bar\n\nnx.draw(G_weights, pos=position, edge_color=edge_colors, width=adjusted_weights, with_labels=True, ax=ax) \n    #ax = ax argument needed for color bar\n\n# Adding color bar\nsm = plt.cm.ScalarMappable(cmap=\"Greys\", norm=norm) # creates a scalarmappable object which acts \n                                                    # as a bridge between the numerical weight values and color map\nplt.colorbar(sm, ax=ax) #plotting color bar"
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html#adjacency-matrices",
    "href": "docs/SOCI-415/soci_415_network_analysis.html#adjacency-matrices",
    "title": "SOCI 415 Network Analysis",
    "section": "3. Adjacency matrices",
    "text": "3. Adjacency matrices\nAn Adjacency matrix is a method of representing graphs in matrix form. In an adjacency matrix, the rows and columns correspond to the vertices (or nodes) of the graph. The entries of the matrix indicate whether pairs of vertices are adjacent or not in the graph. Normally, a value of 1 is assigned to entries where an edge is present, and 0 is assigned to entries where an edge is not. For a weighed graph, the weight of the edge is represented as a numerical value for entries where an edge is present.\nWe can convert our simple graph to an adjacency matrix:\n\nnx.to_pandas_adjacency(G)\n\nIf we want to use our weighted graph, we can use the following code:\n\n# len(edges) returns the total number of entries in the list of edges.\n# range(len(edges)): This generates a sequence of numbers from 0 to n-1 where n is len(edges), \n    #so the for-loop will run n times with i taking each value in that range, one at a time.\n\nfor i in range(len(edges)):\n    edge = edges[i] # retrieves the edge at position i in the list of edges\n    weight = weights[i] # retrieves the weight at position i in the list of weights\n    G_weights.add_edge(edge[0], edge[1], weight=weight) # adds an edge with a weight to the graph \n    \nnx.to_pandas_adjacency(G_weights, nodelist=nodes, weight='weight') #converts to pandas adjacency matrix with the weights in place\n\nWe can visualize our matrix using the code below. Note that instead of using nx.to_pandas_adjacency we use nx.to_numpy_array: this allows us to store the matrix in the form of an array.\n\nadj_matrix = nx.to_numpy_array(G_weights, nodelist=nodes, weight='weight')\n\n\nplt.figure(figsize=(8, 8)) #displays data as an image on a 2d raster; in our case, a numpy array\n\nplt.imshow(adj_matrix, cmap='gray_r')\n\nfor i in range(adj_matrix.shape[0]): #loops through each row of the matrix\n    for j in range(adj_matrix.shape[1]): #for each row, loops through each column of the matrix\n        plt.text(j, i, int(adj_matrix[i, j]),\n                 ha='center', va='center', color='red', size=30) #prints the value at that position in the matrix on the graph\n\nplt.title('Adjacency Matrix Visualization')\nplt.xlabel('Node Index')\nplt.ylabel('Node Index')"
  },
  {
    "objectID": "docs/SOCI-415/soci_415_network_analysis.html#measures-of-centrality",
    "href": "docs/SOCI-415/soci_415_network_analysis.html#measures-of-centrality",
    "title": "SOCI 415 Network Analysis",
    "section": "4.0 Measures of Centrality",
    "text": "4.0 Measures of Centrality\nCentrality is defined as the set of metrics used to determine the importance or influence of a particular node within a network. It helps to identify which nodes hold strategic significance in terms of connectivity, information flow, or influence over other nodes. Various centrality metrics, such as degree, betweenness, and eigenvector centrality, provide different perspectives on the role each node plays within the network’s overall structure.\n\n4.1 Network Distance and Eccentricity\nBefore talking about centrality, we first need to talk a bit about distance. Distance, also known as Geodesic distance, is defined as the number of edges traversed by the shortest path between two nodes.\n\nThe distance between a node and itself is 0.\nThe distance between a node and a node for which no shortest path exists (such as a node that is disconnected from other nodes) is \\(\\infty\\).\nThe distance between a node and it’s neighbor is 1.\n\nA node’s eccentricity is the maximum distance from said node to all other nodes in the graph. For instance, in the following network, the eccentricity of node \\(A\\) is 2, but the eccentricity of node \\(B\\) is 1.\n\nnodes = (\"A\",\"B\", \"C\")\nedges = [(\"A\",\"B\"), (\"B\", \"C\")]\n\nG_example = nx.Graph()\nG_example.add_edges_from(edges)\nG_example.add_nodes_from(nodes)\n\ncolor_map = [\"salmon\", \"lightblue\", \"salmon\"]\n\n\nred_patch = mpatches.Patch(color='salmon', label='eccentricity = 1') \nblue_patch = mpatches.Patch(color='lightblue', label='eccentricity = 2') \nplt.legend(handles=[blue_patch, red_patch])\n\nnx.draw(G_example, node_color=color_map, with_labels=True)\n\nIf we color the nodes of our random graph by eccentricity, we can see:\n\n#Create random graph\nrandom.seed(415) #Course code\nn = 20 \nd = 3\nrr_graph = nx.random_regular_graph(d, n)\n\n# Compute eccentricity\necc = nx.eccentricity(rr_graph)\nunique_ecc = sorted(set(ecc.values()))\n\n# Choose a color for each unique eccentricity using matplotlib\ncolors = plt.get_cmap('tab10', len(unique_ecc))\necc_to_color = {e: colors(i) for i, e in enumerate(unique_ecc)}\ncolor_map = [ecc_to_color[ecc[node]] for node in rr_graph.nodes()]\n\n# Create a legend\npatches = [mpatches.Patch(color=colors(i), label=f\"eccentricity = {e}\") for i, e in enumerate(unique_ecc)]\n\nplt.figure(figsize=(8, 6))\nnx.draw(rr_graph, node_color=color_map, with_labels=True, node_size=500, font_size=10, font_weight='bold')\nplt.legend(handles=patches, bbox_to_anchor=(1, 1))\nplt.title(\"Random Regular Graph with Eccentricity Coloring\")\nplt.tight_layout()\nplt.show()\n\n# Print summary\nprint(f\"Number of nodes: {rr_graph.number_of_nodes()}\")\nprint(f\"Number of edges: {rr_graph.number_of_edges()}\")\nprint(f\"Eccentricity distribution: {ecc}\")\n\nWe can see that the nodes in the furthest corners have an eccentricity of 6 and more central nodes have a lower eccentricity.\n\n\n4.2 Degree Centrality\nDegree centrality is simple: Recall that the degree of a node is the number of nodes directly connected to it. In degree centrality, the more adjacent nodes, the more important the network is considered to be. Degree centrality is used primarily in social networks, where nodes with higher degrees are commonly major channels of information. A high degree means a node has many direct ties with other nodes, and has better access to resources within the network.\nNote that the NetworkX nx.degree_centrality() function normalizes each node’s degree by dividing by the maximum possible degree in the network. Therefore for graphs without self-loops the degree centrality is always \\(\\leq 1\\). For educational purposes, we un-normalize the degree values, but this is not common practice.\nWe can not use the same random graph technique as each node will have a degree equal to 3, instead we will use the first graph from this notebook.\nWe can calculate the degree centrality of all our nodes in our network:\n\n# Define the graph\nnodes = (1, 2, 3, 4, 5, 6)\nedges = [\n    (1, 2), (2, 3), (3, 1), (1, 5), (3, 5),\n    (4, 5), (4, 6), (6, 1), (6, 3), (6, 4),\n    (4, 3), (5, 5), (3, 5)\n]\nG = nx.Graph()\nG.add_edges_from(edges)\nG.add_nodes_from(nodes)\n\n# Get degree for each node\ndegree_dict = dict(G.degree())\ndegrees = list(degree_dict.values())\nunique_degrees = sorted(set(degrees))\n\n# Assign a unique color per degree\ncolors = plt.get_cmap('viridis', len(unique_degrees))\ndegree_to_color = {deg: colors(i) for i, deg in enumerate(unique_degrees)}\ncolor_map = [degree_to_color[degree_dict[n]] for n in G.nodes()]\n\n# Create legend for each degree\npatches = [mpatches.Patch(color=colors(i), label=f\"degree = {deg}\") for i, deg in enumerate(unique_degrees)]\n\nplt.figure(figsize=(6, 4))\nnx.draw(G, with_labels=True, node_color=color_map, node_size=800, edge_color='gray', font_weight='bold')\nplt.legend(handles=patches, bbox_to_anchor=(1, 1))\nplt.title(\"Degree Centrality Graph\")\nplt.tight_layout()\nplt.show()\n\n\n\n4.3 Closeness Centrality\nCloseness centrality is a measure of how close a node is to all other nodes in the network. It can be computed as the “sum of the geodesic distances of a node to all other nodes in the network”. A node is important if it is close to all other nodes in the network. One flaw of closeness centrality is that while it is a useful indicator of node importance in small networks, it produces little variation in large networks with many edges.\n\n# Define the graph\nnodes = (1, 2, 3, 4, 5, 6)\nedges = [(1, 2), (2, 3), (3, 1), (1, 5), (3, 5),(4, 5), (4, 6), (6, 1), (6, 3), (6, 4),(4, 3), (5, 5), (3, 5)]\nG = nx.Graph()\nG.add_edges_from(edges)\nG.add_nodes_from(nodes)\n\n# Calculate closeness centrality\ncentrality = nx.closeness_centrality(G)\n\n# Normalize centrality values for color mapping (so 0 = min, 1 = max)\ncentralities = np.array(list(centrality.values()))\nnorm_centrality = (centralities - centralities.min()) / (centralities.max() - centralities.min() + 1e-9)\n\n# Map normalized centrality to colormap\ncmap = plt.cm.plasma\ncolor_map = [cmap(val) for val in norm_centrality]\n\nplt.figure(figsize=(6, 4))\nnx.draw(G, with_labels=True, node_color=color_map, node_size=800, edge_color='gray', font_weight='bold')\nplt.title(\"Graph Colored by Closeness Centrality\")\nplt.tight_layout()\nplt.show()\n\n# Print node centrality table\nprint(\"Node\\tCloseness Centrality\")\nfor node in G.nodes():\n    print(f\"{node}\\t{centrality[node]:.4f}\")\n\n\n\n4.4 Betweenness Centrality\nBetweenness Centrality is a measure of the importance of a node based on how well it serves as a bridge between nodes in a network. The mathematical representation of the betweeness centrality of a node is the number of times each node has to pass through that node to reach every other node in a network. Nodes with high betweenness thus serve as “bridges” within a network.\nConsider the graph below:\n\n#Define our network\nG_betweenness_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_betweenness_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_betweenness_example, seed=1000)\n\n#Draw our graph\nnx.draw(G_betweenness_example,pos=pos, with_labels=True, edgecolors=\"black\", node_color=\"bisque\", node_size=800)\n\nNode \\(4\\) serves as a bridge between nodes 5 and 6 to the rest of the nodes in the network. For a path to be drawn between nodes 6 or 5 to nodes 0,1,2,3, the path must go through node 4. Let’s calculate the betweenness centrality of this network, and label nodes by centrality:\n\n#Define our network\nG_betweenness_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_betweenness_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_betweenness_example, seed=1000)\n\n#Find the centrality values for our nodes\ncentrality = nx.betweenness_centrality(G_betweenness_example, normalized=False)\ncentrality_values = np.array(list(centrality.values()))\ncmap=\"BuPu\"\n\n#Put labels on our network\nlabels = {}\nfor node in G_betweenness_example.nodes():\n    labels[node] = centrality_values[node]\n\n\n#Draw our graph using `nx.draw`\nnx.draw(G_betweenness_example,pos=pos, node_color=centrality_values, edgecolors=\"black\", cmap=cmap, node_size=800)\nnx.draw_networkx_labels(G_betweenness_example, pos, labels=labels, font_color=\"orangered\")\n\nWe can see that node 4 does indeed have the highest betweenness centrality. The values of 0 for nodes 0, 1, 2, 3 and 6 indicate that each node can reach every other node without passing through those nodes. The value of 5.0 for node 5 indicates that five nodes must pass through node 5 in order to reach another node.\n\n\n4.5 Eigenvector centrality\nEigenvector centrality is a measure of the influence of a node in a network by considering not just how many connections it has (as we did with degree centrality), but also the importance of those connections: A node with high eigenvector centrality is connected to many nodes that themselves have high centrality, making it more influential in spreading information or resources. Unlike simpler measures like degree centrality, which only counts connections, eigenvector centrality looks at the overall structure of the network. It helps identify key players in a network who might not have the most connections but are well-connected to other important nodes.\n\n# Create a larger network \nG = nx.barabasi_albert_graph(n=20, m=2, seed=42)  # 20 nodes and each connects to 2\n\n# Calculate eigenvector centrality\ncentrality = nx.eigenvector_centrality(G)\n\n# Normalize values for coloring\ncentrality_values = np.array(list(centrality.values()))\nnorm_centrality = (centrality_values - centrality_values.min()) / (centrality_values.max() - centrality_values.min() + 1e-9)\ncmap = plt.cm.plasma\ncolor_map = [cmap(val) for val in norm_centrality]\n\n# Draw the graph\nplt.figure(figsize=(8, 6))\npos = nx.spring_layout(G, seed=42)\nnx.draw(G, pos=pos, with_labels=True, node_color=color_map,\n        node_size=800, font_weight='bold', edge_color='gray')\nplt.title('Large Graph Colored by Eigenvector Centrality')\nplt.tight_layout()\nplt.show()\n\n# Print node eigenvector centrality table\nprint(\"Node\\tEigenvector Centrality\")\nfor node in G.nodes():\n    print(f\"{node}\\t{centrality[node]:.4f}\")"
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html",
    "href": "docs/SOCI-415/cbdb_dataset.html",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "",
    "text": "SOCI 415 Network Analysis Intro Notebook\nKinmatrix Dataset Notebook\n\nThe China Biographical Database Abstract: The China Biographical Database is a freely accessible relational database with biographical information about approximately 641,568 individuals as of August 2024, currently mainly from the 7th through 19th centuries. With both online and offline versions, the data is meant to be useful for statistical, social network, and spatial analysis as well as serving as a kind of biographical reference."
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html#network-level-analysis-clusters-and-clustering-coefficients",
    "href": "docs/SOCI-415/cbdb_dataset.html#network-level-analysis-clusters-and-clustering-coefficients",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "2.1 Network-level analysis: Clusters and clustering coefficients",
    "text": "2.1 Network-level analysis: Clusters and clustering coefficients\nA cluster (also known as a community) is a set of nodes in a graph that are densely connected to each other but sparsely connected to nodes in other clusters. For example, in a social network, a cluster might represent a group of people who frequently interact with each other but have fewer interactions with people outside the group. Community detection is the process of finding such communities within nodes.\nBefore diving into community detection, we first need to understand modularity. Modulaity is a numerical measure for the community structure of a graph: it compares the density of edges within the communities of a network to the density of edges between communities. A positive modularity value suggests a strong community structure, while values closer to zero or negative indicate that the divisions are no better than random.\nThe Louvain algorithm is a community detection method in networks that aims to optimize modularity. By optimizing modularity, the Louvain algorithm effectively uncovers natural divisions in the network where connections are dense within clusters and sparse between them, thus identifying meaningful community structures.\nFirst, each node is assigned to its own community, and nodes are then iteratively moved to neighboring communities if it increases the modularity. In the second phase, the algorithm creates a new network where each community from the first phase is treated as a single node, and the process is repeated. This hierarchical approach continues until no further modularity improvements can be made, resulting in a final set of communities that maximize modularity.\nLet’s first try running the Louvain Algorithm on a random graph to demonstrate how it works before running it on our real data.\n\n#Set the seed so it is reproducable\nrandom.seed(1)\n\nn = 20  # number of nodes\nd = 3   # degree of each node\n\n# Generate the random regular graph\nrr_graph = nx.random_regular_graph(d, n)\n\npartition = community_louvain.best_partition(rr_graph)\npos = nx.spring_layout(rr_graph, seed=42)\nnum_communities = max(partition.values()) + 1\ncmap = cm.get_cmap('viridis', num_communities)\nnx.draw_networkx_nodes(\n    rr_graph, pos, node_size=40, cmap=cmap, node_color=list(partition.values())\n)\nnx.draw_networkx_edges(rr_graph, pos, alpha=0.5)\nplt.show()\n\nWe can see by the node coloring that by optimizing modularity the Louvain Algorithm has found smaller subgroups within our random network. Now we can try it on our real data."
  },
  {
    "objectID": "docs/SOCI-415/cbdb_dataset.html#louvain-run-on-real-data",
    "href": "docs/SOCI-415/cbdb_dataset.html#louvain-run-on-real-data",
    "title": "SOCI 415 Network Analysis - CBDB Dataset",
    "section": "2.2 Louvain Run on Real Data",
    "text": "2.2 Louvain Run on Real Data\nFirst lets look at some metrics from the algorithm and a distribution of community sizes.\n\ndb_path = r'C:\\Users\\alexr\\OneDrive\\Desktop\\WORK\\Summer2025\\latest.db'\n\n# Load kinship data into DataFrame\nconn = sqlite3.connect(db_path)\ndf = pd.read_sql_query(\"SELECT c_personid, c_kin_id, c_kin_code FROM KIN_DATA\", conn)\nconn.close()\n\n# Build the Graph\nG = nx.Graph()\nfor _, row in df.iterrows():\n    person = row['c_personid']\n    kin = row['c_kin_id']\n    kin_type = row['c_kin_code']\n    G.add_edge(person, kin, kinship=kin_type)\n\n# Work with the largest connected component\nlargest_cc = max(nx.connected_components(G), key=len)\nG_sub = G.subgraph(largest_cc).copy()\nprint(f\"Largest connected component nodes: {G_sub.number_of_nodes()}\")\n\n# Run Louvain Community Detection\nprint(\"Running Louvain algorithm...\")\npartition = community_louvain.best_partition(G_sub)\n\n# Community Analysis Output\nnum_communities = len(set(partition.values()))\nprint(f\"In our data Louvain has detected {num_communities} communities.\")\n\n# Count community sizes\ncommunity_sizes = Counter(partition.values())\nprint(f\"\\nMetrics about Community size:\")\nprint(f\"Average community size: {np.mean(list(community_sizes.values())):.1f}\")\nprint(f\"Largest community: {max(community_sizes.values())} people\")\nprint(f\"Smallest community: {min(community_sizes.values())} people\")\n\n#Show top 10 largest communities\nprint(f\"\\nLargest Communities:\")\nfor i, (comm_id, size) in enumerate(community_sizes.most_common(10)):\n    print(f\"Community {comm_id}: {size:,} people\")\n\n# Calculate modularity\nmodularity = community_louvain.modularity(partition, G_sub)\nprint(f\"\\nModularity Score: {modularity:.4f}\")\nprint(\"(Higher modularity indicates stronger community structure)\")\n\n#Distributon Histogram of Community Size\nfig, ax = plt.subplots(figsize=(15, 12))\nfig.suptitle('CBDB Kinship Network Community Analysis', fontsize=16, fontweight='bold')\n\n# Community size histogram\nax.hist(list(community_sizes.values()), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\nax.set_xlabel('Community Size (number of people)')\nax.set_ylabel('Number of Communities')\nax.set_title('Distribution of Community Sizes')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nWe can see most communities are around 200 - 400 nodes in size.\nLets look at one-two of these smaller community networks in depth and see how they are structured compared to the KINMATRIX dataset.\nThe two we will use are:\n\nCommunity 68: 655 people\nCommunity 125: 872 people\n\n\ndef visualize_cbdb_community(G_sub, partition, community_id, max_nodes=300):\n    # Ensure all node IDs are strings\n    community_nodes = [str(node) for node, comm_id in partition.items() if comm_id == community_id]\n    \n    print(f\"Community {community_id}: {len(community_nodes)} people\")\n    \n    # Sample nodes if too large\n    if len(community_nodes) &gt; max_nodes:\n        print(f\"Sampling {max_nodes} nodes for performance...\")\n        subgraph_full = G_sub.subgraph([int(node) for node in community_nodes])\n        degrees = dict(subgraph_full.degree())\n        sorted_nodes = sorted(community_nodes, key=lambda x: degrees.get(int(x), 0), reverse=True)\n        community_nodes = sorted_nodes[:max_nodes//2] + random.sample(sorted_nodes[max_nodes//2:], max_nodes//2)\n    \n    # Create subgraph with string node IDs, but degree lookups need int\n    subgraph = G_sub.subgraph([int(node) for node in community_nodes])\n    print(f\"Showing {len(community_nodes)} people, {subgraph.number_of_edges()} relationships\")\n    \n    net = Network(height=\"700px\", width=\"100%\", bgcolor=\"#ffffff\", notebook=True)\n    \n    for node in community_nodes:\n        degree = subgraph.degree[int(node)]  # degree lookup; node as int\n        size = max(15, min(35, 15 + degree))\n        net.add_node(str(node), \n                     label=str(node), \n                     size=size, \n                     color=\"#3498db\",\n                     title=f\"Person {node}\\nConnections: {degree}\")\n    \n    for u, v, data in subgraph.edges(data=True):\n        net.add_edge(str(u), str(v), color=\"#cccccc\", title=f\"Kinship: {data.get('kinship','family')}\")\n    \n    filename = f\"community_{community_id}.html\"\n    net.show(filename)\n    print(f\"Interactive network: {filename}\")\n\n# Visualize communities as before\nvisualize_cbdb_community(G_sub, partition, 68)\nvisualize_cbdb_community(G_sub, partition, 125)\n\nWe will again use a PyVis visualization, just like with the KINMATRIX Visualizations we can zoom and pan around and hover on the nodes. This time you can also drag the nodes and the surrounding nodes will move like bacteria under a microscope.\nMORE STUFF\n\nCan add a section on coloring by gender or someother variable of interest for these visualizations."
  },
  {
    "objectID": "docs/HIST-414/pyLDAvis/pyLDAvis.html",
    "href": "docs/HIST-414/pyLDAvis/pyLDAvis.html",
    "title": "Using pyLDAvis for Analysis for HIST-414 Alex R",
    "section": "",
    "text": "Legal judgments are complex documents that draw upon many facets of legal reasoning, including the interpretation of applicable law, evaluation of evidence presented in court, and consideration of prior precedents. This makes them difficult to systemically analyze as they often defy single membership classification. Rice (2019) introduces Latent Dirichlet Allocation as a method of unsupervised structured topic modelling (STM), allowing for underlying themes in legal texts to be uncovered via a computational approach and capture the proportionate attention given to multiple legal dimensions within each judgement. A continuation of notebook 2 the goal of this notebook is to create interactive visualizations in order to explore and understand these relationships.\n#Import packages \nimport pandas as pd\nimport json\nimport pathlib\nimport re, string\nimport multiprocessing\nimport requests\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom gensim import corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models.phrases import Phrases, Phraser\nfrom gensim.corpora import Dictionary\nfrom gensim.models.ldamulticore import LdaMulticore\nfrom gensim.models import CoherenceModel\nfrom gensim.models import LdaMulticore\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport multiprocessing\nExplanation of LDA from notebook 2 or if this is attached to notebook 2 then this is unnecessary"
  },
  {
    "objectID": "docs/HIST-414/pyLDAvis/pyLDAvis.html#references",
    "href": "docs/HIST-414/pyLDAvis/pyLDAvis.html#references",
    "title": "Using pyLDAvis for Analysis for HIST-414 Alex R",
    "section": "References",
    "text": "References\nbmabey , and marksusol . “Pyldavis.” PyPI, 23 Apr. 2023, pypi.org/project/pyLDAvis/.\nMattingly, W.J.B. “Introduction to Topic Modeling and Text Classification.” 1. Introduction to Topic Modeling - Introduction to Topic Modeling and Text Classification, Feb. 2021, topic-modeling.pythonhumanities.com/01_01_introduction_to_topic_modeling.html.\n“Supreme Court of Canada Bulk Decisions Dataset.” Refugee Law Lab - Refugee Law Lab, 27 June 2024, refugeelab.ca/bulk-data/scc/.\nTran, Khuyen. “Pyldavis: Topic Modeling Exploration Tool.” Neptune.Ai, 20 May 2025, neptune.ai/blog/pyldavis-topic-modeling-exploration-tool.\n“What Is Pyldavis Library in Python?” HowDev, how.dev/answers/what-is-pyldavis-library-in-python. Accessed 9 June 2025."
  },
  {
    "objectID": "docs/HIST-414/NLI/development/NLI.html",
    "href": "docs/HIST-414/NLI/development/NLI.html",
    "title": "Text Embeddings for Regina V Wing Chong (1885)",
    "section": "",
    "text": "# Data Wrangling\nimport os\nimport numpy as np\nimport pandas\nfrom nltk.tokenize import word_tokenize\n\n\nwith open('data/Regina_V_Wing_Chong.txt', encoding='utf-8') as f:\n    full_text = f.read()\nprint(full_text)\n\n\nBERT Word Embeddings\n\nimport re\n\ndef clean_text(text):\n    \n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    \n    return text.strip()\n\ntext_cleaned = clean_text(full_text)\nprint(text_cleaned[:500])  # Print the first 500 characters of the cleaned text\n\n\n# Load pre-trained BERT tokenizer and model\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\nbert_model = BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')\n\n\n# Create the word embeddings\n# Tokenize the cleaned text into words\ntokens = word_tokenize(text_cleaned)\n\ntoken_frequencies = {}\n\nfor token in tokens:\n    token_frequencies[token] = token_frequencies.get(token, 0) + 1\n\n\nsorted_tokens = sorted(token_frequencies.items(), key=lambda x: x[1], reverse=True)\n\n# Example: print top 10 most frequent tokens\nprint(\"Most frequent tokens:\")\nfor token, freq in sorted_tokens[:20]:\n    print(f\"{token}: {freq}\")\n\n\nimport re\n# Build ethnicity vocabulary\nethnicities = [\n    \"chinese\", \"japanese\", \"black\", \"white\", \"yellow\", \"chinamans\", \"hong kong\",\n    \"canada\", \"american\", \"americans\", \"european\", \"china\", \"chinaman\", \"britain\",\n    \"canadian\", \"latino\", \"mongolian\", \"asian\", \"indian\", \"india\", \"english\",\n    \"british\", \"america\", \"columbia\", \"ontario\", \"australia\", \"australian\",\n    \"germans\", \"german\", \"chinamen\", \"italian\", \"italy\", \"french\", \"france\"\n]\n\npattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, ethnicities)) + r\")\\b\", flags=re.IGNORECASE)\n\n# Mask in any string\ndef mask_ethnicity(tokens):\n    masked = []\n    for tok in tokens:\n        masked.append(pattern.sub(\"[MASK]\", tok))\n        \n    return masked\n\n\nexample_word = [\"chinaman\", \"chinese women\"]\n\nmask_ethnicity(example_word)\n\n\ntokens = mask_ethnicity(tokens)\n\n# Get unique words to avoid redundant computation\nunique_tokens = list(set(tokens))\n\n\n# Include the word \"chinese\" as our target\nunique_tokens.append(\"chinese\")\n\n# Print the shape of unique tokens\nprint(f'There are {len(unique_tokens)} unique tokens in this corpus.')\n\n\n# Prepare a dictionary to store word embeddings\nbert_word_embeddings = {}\n\n# For each word, get its BERT embedding by feeding it as a single-token input\nfor word in unique_tokens:\n    word_inputs = tokenizer(word, return_tensors='pt', truncation=True, max_length=10)\n    with torch.no_grad():\n        word_outputs = bert_model(**word_inputs)\n        # Use the [CLS] token embedding as the word embedding\n        word_embedding = word_outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n        bert_word_embeddings[word] = word_embedding\n\n\n# Print embedding for the word of interest 'chinese'\n\nprint(f\"BERT embedding for 'chinese':\\n{bert_word_embeddings.get('chinese')}\")\n\n\n# Compute cosine similarity between all words with Chinese in the model\nfrom scipy.spatial.distance import cosine\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    if other_word != \"chinese\":\n        similarity = 1 - cosine(bert_word_embeddings[\"chinese\"], bert_word_embeddings[other_word])\n        similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'chinese':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    if other_word != \"commerce\":\n        similarity = 1 - cosine(bert_word_embeddings[\"commerce\"], bert_word_embeddings[other_word])\n        similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'commerce':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\nemd = np.array(bert_word_embeddings.get('chinese')) - np.array(bert_word_embeddings.get('alien'))\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    similarity = 1 - cosine(emd, bert_word_embeddings[other_word])\n    similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'chinese - alien':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\n# Generate a 2D PCA for visualiaztion\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\n\nword_embeddings = np.array(list(bert_word_embeddings.values()))\npca_results = pca.fit_transform(word_embeddings)\n\n\nimport plotly.express as px\ndf_pca = pandas.DataFrame(pca_results, columns = ['x', 'y'])\ndf_pca['word'] = list(bert_word_embeddings.keys())\n# Highlight the word 'chinese' in the plot\ndf_pca['highlight'] = df_pca['word'].apply(lambda x: 'chinese' if x == 'chinese' else '')\n\nfig = px.scatter(\n    df_pca,\n    x='x',\n    y='y',\n    title=' Visualization of 2D PCA of the legal-BERT Word Embeddings',\n    color='highlight',                        \n    hover_data=['word'], \n    text= 'highlight'\n)\n\nfig.show()\n\n\n# Generate a t-SNE plot for visualization\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=42)\n\ntsne_results = tsne.fit_transform(word_embeddings)\n\n\n# Create a DataFrame for visualization\ndf_tsne = pandas.DataFrame(tsne_results, columns=['x', 'y'])\ndf_tsne['word'] = list(bert_word_embeddings.keys())\n# Highlight the word 'chinese' in the plot\ndf_tsne['highlight'] = df_tsne['word'].apply(lambda x: 'chinese' if x == 'chinese' else '')\n\nfig = px.scatter(\n    df_tsne,\n    x='x',\n    y='y',\n    title='t-SNE Visualization of legal-BERT Word Embeddings',\n    color='highlight',                        \n    hover_data=['word'], \n    text= 'highlight'\n)\n\nfig.show()\n\n\n\nSentence Embeddings\n\nfrom pathlib import Path\n\n# Read the txt file as lines\nlines = Path(\"data/Regina_V_Wing_Chong.txt\").read_text(encoding=\"utf-8\").splitlines()\n\n# Extract line 67 as the target\ntarget = lines[91]\nprint(\"Line 91:\", target)\n\n\nparagraphs = [p.strip() for p in full_text.split(\"\\n\\n\") if p.strip()]\n\nfor paragraph in paragraphs[:5]:\n    print(paragraph)\n\n\nfrom sentence_transformers import SentenceTransformer\n\n# Import the sentence transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Calculate embeddings by calling model.encode()\nparagraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)\nprint(paragraph_embeddings.shape)\n\n\n# We also want to encode the target separately\ntarget_embedding = model.encode(target, convert_to_tensor=True)\n\n\nimport torch\nfrom torch.nn.functional import cosine_similarity\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), paragraph_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\ntop_paragraphs = []\n\nfor score, idx in zip(topk.values, topk.indices):\n    top_paragraphs.append(paragraphs[idx])\n    print(f\"{score:.4f}\\t{paragraphs[idx]}\")\n\n\nimport spacy\n\n# Tokenize the text into sentences\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(full_text)\nsentences = [sent.text.strip() for sent in doc.sents]\n\nprint(sentences)\n\n\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\nprint(sentence_embeddings.shape)\n\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), sentence_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\nfor score, idx in zip(topk.values, topk.indices):\n    print(f\"{score:.4f}\\t{sentences[idx]}\")\n\nWe apply a trained model to mask key words related to ethnicity and nationality identities.\n\nfrom transformers import pipeline\nimport numpy\n\nner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n\ndef mask_ethnicity_hf(text):\n    entities = ner(text)\n    spans_to_mask = [e for e in entities if e[\"entity_group\"] == \"MISC\" or e[\"entity_group\"] == \"ORG\" or e[\"entity_group\"] == \"PER\" or e[\"entity_group\"] == \"LOC\" or e[\"entity_group\"] == \"NORP\"]\n    # typically nationality is in MISC or NORP depending on the model\n    masked = text\n    for ent in sorted(spans_to_mask, key=lambda e: e[\"start\"], reverse=True):\n        masked = masked[:ent[\"start\"]] + \"[MASK]\" + masked[ent[\"end\"]:]\n    return masked\n\n\ndef mask_ethnicity(texts):\n    masked_list = []\n    for sent in texts:\n        sent = mask_ethnicity_hf(sent)\n        masked_list.append(sent)\n        \n    return masked_list\n\n\n# Example output applying this pre-trained model\nexample_text = \"\"\"And when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, \nfrom every state and every city, we will be able to speed up that day when all of God's children, Black men and white men, \nJews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. \nFree at last. Thank God almighty, we are free at last.\"\"\"\n\nmasked_example = mask_ethnicity_hf(example_text)\n\nprint(masked_example)\n\n\nmasked_paragraphs = mask_ethnicity(paragraphs)\n\nmasked_paragraphs[40]\n\n\n# Calculate embeddings by calling model.encode()\nmasked_paragraph_embeddings = model.encode(masked_paragraphs, convert_to_tensor=True)\nprint(masked_paragraph_embeddings.shape)\n\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), masked_paragraph_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\ntop_masked_paragraphs = []\n\nfor score, idx in zip(topk.values, topk.indices):\n    top_masked_paragraphs.append(masked_paragraphs[idx])\n    print(f\"{score:.4f}\\t{masked_paragraphs[idx]}\")\n\n\n\nNatural Language Inference\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n\n# Choose a strong NLI model\nmodel_name = \"lexlms/legal-roberta-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel     = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Create an NLI pipeline\nnli = pipeline(\n    \"text-classification\",\n    model=model,\n    tokenizer=tokenizer,\n    device=-1,                   \n    return_all_scores=True        \n)\n\n# Define the premise\npremise = \"Chinese immigrants should enjoy equal rights and legal protections.\"\n\nresults = []\nfor sent in top_masked_paragraphs:\n    inputs = tokenizer.encode_plus(premise, sent, return_tensors=\"pt\", truncation=True)\n    out = model(**inputs).logits.softmax(dim=-1).tolist()[0]\n    label_idx = out.index(max(out))\n    label = [\"FAVOR\", \"NEUTRAL\", \"AGAINST\"][label_idx]\n    results.append((sent, label, dict(zip([\"favor\",\"neutral\",\"against\"], out))))\n\n# Print stance results\nfor sent, label, probs in results:\n    print(f\"{label.lower():&gt;12}  {probs[label.lower()]:.2f}  -&gt;  {sent}\")\n\n\nfrom transformers import pipeline\nimport pandas as pd\n\n# Load the MNLI‑based zero‑shot classifier\nclassifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"facebook/bart-large-mnli\",\n    device=-1\n)\n\n# Use the NLI labels as your “candidate labels”\ncandidate_labels = [\"entailment\", \"neutral\", \"contradiction\"]\n\nrecords = []\nfor para in paragraphs:\n    out = classifier(\n        sequences=para,\n        candidate_labels=candidate_labels,\n        hypothesis_template=\"Given the context that the texts for classification are from a legal ruling in 1885, this paragraph is {} of the premise 'Chinese immigrants should enjoy equal rights and legal protections'.\"\n    )\n    # out['labels'] is sorted by score descending\n    scores = dict(zip(out[\"labels\"], out[\"scores\"]))\n    pred = out[\"labels\"][0]\n\n    records.append({\n        \"paragraph\": para,\n        \"entailment\":   scores.get(\"entailment\", 0.0),\n        \"neutral\":      scores.get(\"neutral\",    0.0),\n        \"contradiction\":scores.get(\"contradiction\", 0.0),\n        \"predicted\":    pred\n    })\n\n#  Build a DataFrame\ndf_nli = pd.DataFrame(records)\n\n# Inspect the first few rows\nprint(df_nli.head())\n\n\ndf_nli.shape\n\n\ncounts = df_nli['predicted'].value_counts()\n\nproportions = df_nli['predicted'].value_counts(normalize=True)\n\nresult = pd.DataFrame({\n    'count': counts,\n    'proportion': proportions\n})\n\nprint(result)\n\n\n\nTopic Modelling Through BERTopic\n\nfrom bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\nvectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1,2), max_df=0.85, min_df=2)\n\ntopic_model = BERTopic(\n    vectorizer_model=vectorizer, \n    ctfidf_model= ctfidf_model\n)\n\ntopics, probs = topic_model.fit_transform(paragraphs)\n\ndf_topic = topic_model.get_topic_info()\nprint(df_topic)\n\n\nfor para in df_topic[\"Representative_Docs\"][1]:\n    print(para)\n\n\nrep_list = []\n\nfor list in df_topic[\"Representation\"]:\n    rep_list.extend(list)\n    \nprint(rep_list)\n\n\ntopic_labels = topic_model.generate_topic_labels(\n    nr_words=3,       \n    separator=\" \",     \n    topic_prefix=False \n)\n\ntopic_list = []\n\nfor label in topic_labels:\n    phrases = label.split(\" \")\n    topic_list.extend(phrases)\n    \nprint(topic_list)\n\n\nfrom umap import UMAP\nimport pandas as pd\nimport plotly.express as px\n\n# Get your topic-term embeddings\nembeddings = topic_model.c_tf_idf_.toarray()\n\n# 1) Build a UMAP reducer with random init\numap_model = UMAP(\n    n_neighbors=15,\n    n_components=2,\n    metric=\"cosine\",\n    init=\"random\",\n    random_state=42\n)\nreduced_embeddings = umap_model.fit_transform(embeddings)\n\n# 2) Build a DataFrame and scatter\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\ndf[\"topic\"] = topic_model.get_topic_info()[\"Topic\"].values\n\nfig = px.scatter(\n    df,\n    x=\"x\",\n    y=\"y\",\n    text=\"topic\",\n    title=\"Topic visualization\"\n)\nfig.show()\n\n\nfrom sentence_transformers import SentenceTransformer\n\nembedding_model = SentenceTransformer(\"nlpaueb/legal-bert-base-uncased\")\n\n\nfrom bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\n\ntopic_model = BERTopic(embedding_model=embedding_model,\n                       vectorizer_model= vectorizer,\n                       ctfidf_model= ctfidf_model)\n\ntopics, probs = topic_model.fit_transform(masked_paragraphs)\n\ndf_topic = topic_model.get_topic_info()\nprint(df_topic)\n\n\nfor para in df_topic['Representative_Docs']:\n    print(para)\n\n\ntopic_labels = topic_model.generate_topic_labels(\n    nr_words=5,       \n    separator=\" \",     \n    topic_prefix=False \n)\n\ntopic_labels\n\n\nfrom umap import UMAP\nimport pandas as pd\nimport plotly.express as px\n\n# Get your topic-term embeddings\nembeddings = topic_model.c_tf_idf_.toarray()\n\n# 1) Build a UMAP reducer with random init\numap_model = UMAP(\n    n_neighbors=15,\n    n_components=2,\n    metric=\"cosine\",\n    init=\"random\",\n    random_state=42\n)\nreduced_embeddings = umap_model.fit_transform(embeddings)\n\n# 2) Build a DataFrame and scatter\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\ndf[\"topic\"] = topic_model.get_topic_info()[\"Topic\"].values\n\nfig = px.scatter(\n    df,\n    x=\"x\",\n    y=\"y\",\n    text=\"topic\",\n    title=\"Topic visualization\"\n)\nfig.show()"
  },
  {
    "objectID": "docs/ECON-227/llm_distributions.html",
    "href": "docs/ECON-227/llm_distributions.html",
    "title": "ECON 227 - How Do Large Language Models Predict?",
    "section": "",
    "text": "Prerequisite\nBefore you start, make sure you have the required libraries installed, if not, uncomment the lines below (i.e. remove the #) and run the cell to install them:\n\n\nCode\n# !pip install yfinance finvizfinance transformers pandas numpy statsmodels holidays plotly ipywidgets \n\n\nImportant: Run this cell to load the libraries we need for running this notebook.\n\n\nCode\n# load libraries we need to run this notebook\nimport os\nimport glob\nimport warnings\nfrom datetime import datetime, timedelta\nfrom math import sqrt\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport yfinance as yf\nfrom finvizfinance.quote import finvizfinance\nfrom transformers import pipeline\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nimport holidays\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom forecast_plot import create_plot\n\n\n\n\nIntroduction: Distribution of LLM Predictions\nLarge language models like ChatGPT do something that seems very simple: Next word prediction.\nWhat does that mean? It means that given a sequence of words, the model predicts the next word in the sequence. For example, if the input is “The cat sat on the”, the model might predict “mat” as the next word.\n\nWe saw an example of predicting just one word. These models predict only one word at a time, but they can do this for very long sequences of words.\nFor example: “bob went to the store” to buy some milk.\n\nHere, let’s say we will output a total of \\(T\\) words to form a sentence, \\(w_1, w_2, \\ldots, w_{t-1}, w_t\\) are the words in the sentence and the subscript \\(t\\) means the word is at the position \\(t\\).\nWhat the model does is learning the probability distribution of the next word given the previous words. It is to create a list of non-negative numbers, one for each possible next word, that add up to 1.\nGiven the previous words \\(w_1, w_2, \\ldots, w_{t-1}\\), the probability of predicting the next word \\(w_{t}\\) is called conditional probability: it essentially measures for all words in the vocabulary, “the probability of the next word being this word, based on what was already said”.\nMathematically, this is defined as below:\n\\[\nP(w_t \\mid w_1, w_2, \\ldots, w_{t-1}) = \\frac{P(w_1, w_2, \\ldots, w_{t-1}, w_t)}{P(w_1, w_2, \\ldots, w_{t-1})}\n\\]\nLLMs can approximate this probability by learning from training datasets. Then, when they make predictions, they will select the word at position \\(t\\) as the word with the maximum \\(P(w_t \\mid w_1, w_2, \\ldots, w_{t-1})\\).\nWith chain rule, the model can also use the conditionals for every position to calculate the probability of a whole sentence:\n\\[\nP(w_1, w_2, \\ldots, w_T) = \\prod_{t=1}^T P(w_t\\mid w_1, \\ldots w_{t-1})\n\\]\nThese models give a probability distribution over the entire vocabulary (all the words the model was trained on). We can then pick the word with the highest probability as the next word or we can sample from this distribution to get more varied (creative) outputs.\nLet’s look at an example of how this works in practice:\n\n\nCode\nwarnings.filterwarnings(\"ignore\")\n\n# Example vocabulary\nvocab = ['buy', 'some', 'milk', 'along', 'the', 'way']\n\n# Probabilities at each step (toy example)\nprobs_step1 = [0.8, 0.05, 0.05, 0.03, 0.04, 0.03]  # 'buy' high\nprobs_step2 = [0.05, 0.7, 0.1, 0.05, 0.05, 0.05]   # 'some' high\nprobs_step3 = [0.05, 0.05, 0.75, 0.05, 0.05, 0.05] # 'milk' high\n\nprob_distributions = [probs_step1, probs_step2, probs_step3]\nstep_labels = ['Step 1: Predict \"buy\"', 'Step 2: Predict \"some\"', 'Step 3: Predict \"milk\"']\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 8), sharey=True)\n\nfor i, ax in enumerate(axes):\n    sns.barplot(x=vocab, y=prob_distributions[i], palette='muted', ax=ax)\n    ax.set_title(step_labels[i])\n    ax.set_ylim(0, 1)\n    ax.set_ylabel('Probability' if i == 0 else '')\n    ax.set_xlabel('Vocabulary')\n    # Highlight the max prob bar in gold\n    max_idx = prob_distributions[i].index(max(prob_distributions[i]))\n    ax.bar(max_idx, prob_distributions[i][max_idx], color='gold')\n\n    for patch, token, prob in zip(ax.patches, vocab, prob_distributions[i]):\n        height = patch.get_height()\n        ax.annotate(\n            f\"{prob:.2f}\",                            \n            xy=(patch.get_x() + patch.get_width() / 2, height),  \n            xytext=(0, 3),       \n            textcoords=\"offset points\",\n            ha='center', va='bottom',\n            fontsize=9\n        )\n\nplt.tight_layout()\nplt.show()\n\n\nThe example above shows us the process of generating \\(T=3\\): at each step, the model calculates the conditional probability of the next word and then selects the word with the highest probability to insert into the sentence. The final output we obtain is “buy some milk”.\nTo get more creative responses you change the distribution at the output where you pick the next word. Very simply this involves making the distribution sharper or flatter. If you make the distribution sharper, you are more likely to pick the word with the highest probability. If you make it flatter, you are more likely to pick a word that is not the most probable one.\nThis is called temperature. A higher temperature makes the distribution flatter, while a lower temperature makes it sharper. You would want to use a temperature of more than 1 \\((1.2-1.5)\\) for creative responses, and a temperature of less than 1 \\((0.1 - 0.5)\\) for more focused responses. For a balanced response, you can use a temperature of \\(0.7-1\\). Another set of parameters are called top-p and top-k sampling.\n\n\nCode\nwarnings.filterwarnings(\"ignore\")\n\nnp.random.seed(42)\n\n# Example vocabulary\nvocab = [\n    'apple', 'banana', 'cherry', 'date', 'elderberry',\n    'fig', 'grape', 'honeydew', 'kiwi', 'lemon',\n    'mango', 'nectarine', 'orange', 'papaya', 'quince',\n    'raspberry', 'strawberry', 'tangerine', 'ugli', 'watermelon'\n]\n\n# Normalize to sum to 1\nvocab_size = len(vocab)\nbase_probs = np.random.rand(vocab_size)\nbase_probs /= base_probs.sum()  \n\ndef apply_temperature(probs, temp):\n    logits = np.log(probs + 1e-20)\n    scaled_logits = logits / temp\n    exp_logits = np.exp(scaled_logits)\n    return exp_logits / exp_logits.sum()\n\ntemperatures = [0.5, 1.0, 1.5]\ndistributions = [apply_temperature(base_probs, t) for t in temperatures]\n\nfig, axes = plt.subplots(3, 1, figsize=(8, 8), sharey=True)\nfor i, (ax, dist, temp) in enumerate(zip(axes, distributions, temperatures)):\n    sns.barplot(x=vocab, y=dist, palette='muted', ax=ax)\n    ax.set_title(f'Temperature = {temp}')\n    ax.set_ylim(0, 0.25)\n    ax.set_ylabel('Probability' if i == 0 else '')\n    ax.set_xlabel('Vocabulary')\n    ax.tick_params(axis='x', rotation=90)\n\n    # Highlight the max-prob bar in gold\n    max_idx = dist.argmax()\n    ax.patches[max_idx].set_color('gold')\n\n    # Annotate each bar with its probability\n    for patch, prob in zip(ax.patches, dist):\n        height = patch.get_height()\n        ax.annotate(\n            f\"{prob:.2f}\",\n            xy=(patch.get_x() + patch.get_width() / 2, height),\n            xytext=(0, 3),            # 3 points vertical offset\n            textcoords=\"offset points\",\n            ha='center', va='bottom',\n            fontsize=9\n        )\n\nplt.tight_layout()\nplt.show()\n\n\nIn the example above, we see how the probability distribution changes with different temperatures. A high temperature (1.5) results in a flatter distribution, meaning the model is more likely to sample from less probable tokens, while a low temperature (0.5) results in a sharper distribution, favoring the most probable tokens.\nNote that while this is with words and language, the same idea applies to any sequential data, like stock prices, weather data, etc. The model looks at what happened before and tries to guess what comes next.\nIf you are interested in understanding the inner workings of these models, take a look at the interactive visualization at The Illustrated Transformer . It provides an excellent, hands-on way to explore the core ideas behind modern language models.\nSo, just like it guesses the next word in a sentence, it can guess the next day’s temperature or the next movement in a stock price, based on the pattern it sees in the earlier numbers.\n\n\nCode\nn = 10\nm = 9\n\n# Generate actual data: random walk + small trend\nactual_data = np.cumsum(np.random.normal(0, 1, n)) + 50\n\n# Predictor approximates entire data closely with small noise everywhere\npredicted = actual_data + np.random.normal(0, 0.2, n)\n\n# Posterior uncertainty: low and roughly constant over entire period\nposterior_std = np.full(n, 0.3)\n\nupper = predicted + posterior_std\nlower = predicted - posterior_std\n\nplt.figure(figsize=(6,6))\nplt.plot(range(n), actual_data, label=\"Actual Data\", color='blue')\nplt.plot(range(n), predicted, label=\"Predicted\", color='orange')\nplt.axvline(x=m-1, color='black', linestyle='--', label=\"Observed / Future Split\")\n\nplt.xlabel(\"Time\")\nplt.ylabel(\"Value\")\nplt.title(\"Stock price over time.\")\nplt.legend()\nplt.show()\n\n\nTypically, when building a model to predict stock prices, you would use more information than just the past prices. For example, you might include things like public sentiment (how people feel about the stock), news headlines, or other features that could influence the price.\nIn the examples below, that’s exactly what we’re going to try! We’ll see how adding these extra features can help the model make better predictions about what happens next.\n\n\nPredicting Stock Prices from News Headlines with AI\nJust like LLMs predict the next word based on the context of prior words:\n\\[\nP(w_t \\mid w_1, w_2, \\ldots, w_{t-1})\n\\]\nWe can use similar models to predict the next value in a time series, like stock prices or percentage changes in returns.\nWhile prediction of next word in language models is inherently univariate that the model predicts the next word based solely on the sequence of previous words, predicting daily stock returns is often a multivariate problem as more exogenous factors must be taken into account. Here, we don’t just use past stock prices (returns) as context, but also incorporate additional features such as public sentiment from news headlines.\nIn other words, instead of predicting the next token from a single stream (words), we predict the next value in a time series using multiple sources of information: historical price data and external signals like news sentiment. This richer, multivariate context allows the model to capture more complex relationships and potentially make more accurate forecasts.\nThe comparison of word prediction and stock price prediction is given as follows:\n\n\n\n\n\n\n\nWord Prediction\nStock Price Prediction\n\n\n\n\nPrevious words\nPast daily returns + aggregated news sentiment\n\n\nNext word prediction\nFuture return prediction\n\n\nAttention to important words\nFeature weights on returns and sentiment\n\n\nTemperature to control randomness\nConfidence or prediction intervals in forecasts\n\n\nWord probability distribution\nForecasted return distribution\n\n\n\nIn this case study, we use a language model to analyze real-time news headlines alongside historical stock prices in order to forecast short-term changes in stock value.\nYou will:\n\nCollect news headlines about real companies (like Amazon or Starbucks)\nUse a pre-trained AI model to classify the sentiment (positive or negative) of these news headlines\nCombine that with stock prices\nUse a forecasting model to predict future price changes\nVisualize your results interactively\n\n\n\nPreview the News Data\nHere we use the finvizfinance packages to retrieve real-time news headlines for companies like Starbucks (SBUX).\n\n\nCode\n# The code below will give us a snapshot of the 100 most recent news headlines for a particular stock in the last 30 days.  \n\ndef get_news_data(ticker):\n    stock = finvizfinance(ticker)\n    news_df = stock.ticker_news()\n    news_df = pd.DataFrame(news_df)\n\n    # Drop NaN and clean whitespace\n    news_df = news_df.dropna(subset=[\"Title\"])\n    news_df = news_df[news_df[\"Title\"].str.strip() != \"\"]\n    news_df['Title'] = news_df['Title'].str.lower()\n    news_df['Date'] = pd.to_datetime(news_df['Date'])\n    news_df['DateOnly'] = news_df['Date'].dt.date\n    news_df[\"Ticker\"] = ticker.upper()\n    # Remove the 'Date' column\n    news_df = news_df.drop(columns=['Date'])\n\n    return news_df.reset_index(drop=True)\n\n# For the sake of reproducibility (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n\n# Uncomment the line below get a more recent snapshot of the data! \n# SBUX_news_df = get_news_data(\"SBUX\")\n# SBUX_news_df.to_csv(\"data/SBUX_news.csv\", index=False)\n\n\nLet’s look at what the cleaned news data looks like. We’ll start with Starbucks (SBUX).\nEach row is a headline, what website it was from, the date it was published and a ticker indicating what stock it is for.\n\n\nCode\nSBUX_news_df = pd.read_csv(\"data/SBUX_news.csv\")\nSBUX_news_df.head()\n\n\nLet’s take a closer look at the news titles. What would you say about their sentiment?\n\n\nCode\ntitles = SBUX_news_df['Title'].tolist()\nprint(\"News on Starbucks:\\n\")\nfor i in range(5):\n    print(titles[i])\n\n\n\n\nClassifying Headline Sentiment with LLM\nEarlier, we explored how large language models (LLMs) predict the next word by learning the probability distribution of possible outcomes based on context.\nNow, we apply a similar idea to entire sentences: in this case, financial news headlines. Instead of predicting the next word, the model assigns a probability to each sentiment category (e.g. POSITIVE, NEGATIVE, or NEUTRAL).\n\nHow it works:\n\nA pre-trained model reads the headline.\nIt assigns probabilities to the sentiment labels.\nWe keep only positive or negative headlines, since those are more likely to affect stock prices.\n\nThis is like asking:\n\nGiven the words in this sentence, what is the most likely emotion behind it?\n\nWe will rely on some AI tools to figure out.\nBERT (short for Bidirectional Encoder Representations from Transformers) is a pretrained language model that learns a word’s meaning by looking at the words before and after it, so it understands context and tone well. And Hugging Face is an open-source platform and Python library that hosts many pretrained models (like BERT) and provides easy tools.\nIn this notebook, we are going to use Hugging Face’s pipeline() function to load a RoBERTa model: an optimized version of the BERT model trained to understand the tone of text. Although it is specifically trained on tweets and social media text. It’s well-suited to handling short, informal writing like news headlines.\nThis builds directly on our earlier discussion of LLMs predicting probability distributions, but here, the prediction is over sentiment classes rather than words.\nWe now apply a pre-trained large language model to each headline.\nIt returns:\n\nPOSITIVE: news that sounds good (e.g., “profits surge”)\nNEGATIVE: news that sounds bad (e.g., “lawsuit filed”)\n\n\nCan you think of a positive and negative news headline?\n\nNote that we will skip NEUTRAL news to focus on strong market signals.\n\n\nCode\nwarnings.filterwarnings(\"ignore\")\n# Here we are just specifying the classifier (AI model) that decides on a sentiment \n\nclassifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", device=-1)\n\n# define a function to classify sentiment of each text \ndef classify_sentiment(text):\n    return classifier(text)[0][\"label\"].upper()\n\n# define classify_sentiment to entire dataframe\ndef apply_sentiment(news_df):\n    news_df[\"Sentiment\"] = news_df[\"Title\"].apply(classify_sentiment)\n    return news_df # [news_df[\"Sentiment\"] != \"NEUTRAL\"] # remove neutral headlines\n\ndef process_sentiment(news_df):\n    grouped = news_df.groupby([\"DateOnly\", \"Sentiment\"]).size().unstack(fill_value=0)\n    grouped = grouped.reindex(columns=[\"POSITIVE\", \"NEGATIVE\"], fill_value=0)\n    # Calculate a rolling 7-day total of positive headlines\n    grouped[\"7day_avg_positive\"] = grouped[\"POSITIVE\"].rolling(window=7, min_periods=1).sum()\n    # Calculate a rolling 7-day total of negative headlines\n    grouped[\"7day_avg_negative\"] = grouped[\"NEGATIVE\"].rolling(window=7, min_periods=1).sum()\n    # Calculate the percentage of positive headlines each day (out of total positive + negative)\n    grouped[\"7day_pct_positive\"] = grouped[\"POSITIVE\"] / (grouped[\"POSITIVE\"] + grouped[\"NEGATIVE\"])\n\n    return grouped.reset_index()\n\n\nLet’s use our model to see wether the sentence “I hate bananas” is negative or positive\n\nTry changing the words inside classify sentiment to see wether its is classified positive or negative\n\n\n\nCode\nclassify_sentiment(\"I hate bananas\")        # You can change the words inside the function to test anything you want! \n\n\nNow let’s apply this to our entire SBUX_news_df and see how each news headline is classified.\n\n\nCode\nnews_df = apply_sentiment(SBUX_news_df)       # Classify sentiment of each Starbucks news headline\nnews_df.head()\n\n\n\n\nCode\nprint(\"News Titles and Sentiments\\n\")\n\nfor i, row in news_df.iterrows():\n    if i &lt; 5:\n        print(f\"Title: {row['Title']}\\nSentiment: {row['Sentiment']}\\n\")\n\n\nLet’s summarize the sentiment results by date. For each day we count the number of positive and negative headlines, then calculate 7-day moving averages and the daily percentage of positive news. This gives us a quick overview of news sentiment trends over time.\n\n\nCode\nsentiment_df = process_sentiment(news_df)       # Summarize daily sentiment statistics\nsentiment_df.head()                             # Display the first 5 rows of the dataframe\n\n\n\n\nWhat this table shows?\nThis table is the output of a function that summarizes news sentiment over time.\nEach row corresponds to a specific date and gives us a snapshot of how positive or negative the news headlines were for that day and the surrounding week.\nMarkets move not just based on today’s headlines, but on short-term trends in public sentiment.\nThis table lets us track how optimism or pessimism is building up over time, which we can later use to help predict stock price movements.\n\n\nWhy is this useful?\n\nWe know that in the last 100 news stories about NVIDIA 24 have been positive and 5 have been negative.\nIf the 7day_pct_positive is rising, the overall tone of news is getting more optimistic.\nIf it’s dropping, it could mean public or investor concern is growing.\nWe can later plot this and compare it against stock price to see if sentiment influences market behavior.\n\n\n\n\nGetting Stock Price Data\nWe are using the yfinance package to get real stock price data directly from Yahoo Finance. The function below helps us download historical stock prices and compute the daily percentage change in the stock’s closing price. To make sure our results are replicable, this data has been saved as a .csv file “data/NVDA_snapshot.csv”\nThis allows us to analyze how stock prices change over time.\n\n\nCode\n# The code below will give us a snapshot of stock prices for the duration in we have news headlines for.  \n\ndef get_stock_data(ticker, start, end):\n    stock = yf.download(ticker, start=start, end=end)\n\n    # Flatten columns if multi-indexed (e.g., multiple tickers)\n    if isinstance(stock.columns, pd.MultiIndex):\n        stock.columns = ['_'.join(col).strip() for col in stock.columns]  # \"Close_SBUX\", etc.\n        close_col = f\"Close_{ticker}\"\n    else:\n        close_col = \"Close\"\n\n    stock[\"Pct_Change\"] = stock[close_col].pct_change() * 100\n    stock.reset_index(inplace=True)\n    stock[\"DateOnly\"] = pd.to_datetime(stock[\"Date\"])\n    return stock[[\"DateOnly\", \"Pct_Change\"]]\n\n# merges sentiment and stock data by date, and lags sentiment by one day to align with price changes.\ndef combine_data(sent_df, stock_df):\n    sent_df = sent_df.reset_index(drop=True)\n    stock_df = stock_df.reset_index(drop=True)\n\n    sent_df[\"DateOnly\"] = pd.to_datetime(sent_df[\"DateOnly\"])\n    stock_df[\"DateOnly\"] = pd.to_datetime(stock_df[\"DateOnly\"])\n\n    return (\n        pd.merge(sent_df, stock_df, on=\"DateOnly\", how=\"inner\")\n          .assign(lagged_sentiment=lambda df: df[\"7day_pct_positive\"].shift(1))\n    )\n\n\n# For the sake of reproducibility (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n\n# Uncomment the line below get a more recent snapshot of the data ! \n\nSBUX_news_df[\"DateOnly\"] = pd.to_datetime(SBUX_news_df[\"DateOnly\"])\nstart_date = SBUX_news_df[\"DateOnly\"].min() - pd.Timedelta(days=1) \nend_date = SBUX_news_df[\"DateOnly\"].max() + pd.Timedelta(days=1) \nstock_df = get_stock_data(\"SBUX\", start_date, end_date)\nstock_df.to_csv(\"data/SBUX_price.csv\", index=False)\n\nSBUX_price_df = pd.read_csv(\"data/SBUX_price.csv\")\nSBUX_price_df.head()\n\n\nNow we bring together the sentiment summary data and the stock price changes. By merging these two datasets, we can analyze how changes in news sentiment might be related to changes in Starbucks’ stock price. This combined dataset will help us answer the question: “Does positive news sentiment lead to an increase in stock price?”\n\n\nCode\ncombined_df = combine_data(sentiment_df, stock_df)\ncombined_df\n\n\n\nForecasting Future Stock Changes with Sentiment\nIn this notebook, we will try to predict future stock price changes using the SARIMAX model, a powerful forecasting model that allows us to include external information, in our case, public sentiment. For those who are interested in learning more about the SARIMAX model and its implementation in Python, see this comprehensive guide on GeeksforGeeks.\nBelow, we define two key functions for our forecasting workflow:\n\nget_future_dates(): Returns the next business days for which we want to make predictions.\nfit_and_forecast(): Uses both historical stock prices and recent news sentiment to predict how Starbucks’ (SBUX) stock price might change over the next few days. This function fits a SARIMAX model, which incorporates both past price data and the influence of news sentiment, and then generates forecasts along with confidence intervals.\n\nNext, we will use a pre-defined create_plot() function to generate an interactive line chart that visualizes both stock price percentage changes and sentiment trends over time. This allows us to explore the relationship between market sentiment and stock performance in a clear, interactive way.\nBy combining these functions, we can see how shifts in news sentiment may impact SBUX’s future stock movements. The forecasting approach essentially answers the question: “Given recent sentiment, what does the model predict for this stock’s price in the coming days?”\n\n\nCode\n# business days we should forecast for \ndef get_future_dates(start_date, num_days):\n    if not isinstance(start_date, pd.Timestamp):\n        start_date = pd.to_datetime(start_date)\n\n    us_holidays = holidays.US()\n    future_dates = []\n    current_date = start_date + pd.Timedelta(days=1)\n\n    while len(future_dates) &lt; num_days:\n        if current_date.weekday() &lt; 5 and current_date.date() not in us_holidays:\n            future_dates.append(current_date)\n        current_date += pd.Timedelta(days=1)\n\n    return future_dates\n\n# prediction model\ndef fit_and_forecast(combined_df, forecast_steps=3):\n    combined_df = combined_df.dropna(subset=['Pct_Change', 'lagged_sentiment'])\n\n    endog = combined_df['Pct_Change']\n    exog = combined_df['lagged_sentiment']\n\n    model = SARIMAX(endog, exog=exog, order=(1, 1, 1))\n    fit = model.fit(disp=False)\n\n    future_dates = get_future_dates(combined_df.index[-1], forecast_steps)\n    future_exog = np.tile(combined_df['lagged_sentiment'].iloc[-1], forecast_steps).reshape(-1, 1)\n\n    forecast = fit.get_forecast(steps=forecast_steps, exog=future_exog)\n    return forecast.predicted_mean, forecast.conf_int(), future_dates\n\n\n\n\nPlot: Sentiment vs Stock % Change Forecast\nThis chart shows how news sentiment about a company relates to its stock price changes over time, and how we can use this relationship to make simple predictions.\n\nThe blue line shows the standardized 7-day average of positive sentiment extracted from financial news headlines. A higher value means news sentiment was more positive.\nThe green line shows the actual daily percentage change in the company’s stock price.\nThe red line shows our simple forecast of future stock movement based on past sentiment trends. The shaded red area represents uncertainty around the forecast (a 95% confidence interval).\n\nWe want to see whether the emotions in the news (blue) can help us predict price changes (green and red). If they move together, it suggests that public mood might influence investor behavior.\n\nThis plot helps us visualize correlations and test basic forecasting using real-world data like stock prices and media sentiment.\n\n\n\nCode\ncombined_df['DateOnly'] = pd.to_datetime(combined_df['DateOnly'])  # convert to datetime\ncombined_df.set_index('DateOnly', inplace=True)  # use as index\ncombined_df.sort_index(inplace=True)  # ensure time order\n\nforecast_mean, forecast_ci, forecast_index = fit_and_forecast(combined_df)\ncreate_plot(combined_df, forecast_mean, forecast_ci, forecast_index)\n\n\n\nDisclaimer: This is a simplified model. In reality, stock prices are influenced by many factors, such as interest rates, earnings reports, geopolitical events, and investor speculation. This chart only considers one variable: news sentiment. It should not be used for actual trading decisions.\n\n\nThink about what you would include in a model other than news to help us predict how a stock price might change ?\n\n\n\n\nHow Do AIs Feel About AI?\nIn this section of the notebook, we explore how our AI Sentiment Analysis model** feels about AI-related stocks. That’s a mouthful!\nThe goal is to see if public sentiment (as captured by the headlines) is generally optimistic or pessimistic toward leading AI companies — as interpreted by another AI (we are using BERT here!).\nWe’ll start by selecting the top 10 AI stocks in 2025 as suggested by financial news magazine, Forbes - ACN (Accenture) - ADBE (Adobe) - AMD (Advanced Micro Devices) - APP (Applovin) - AVGO (Broadcom) - CRM (Salesforce) - MRVL (Marvell Technology) - MU (Micron Technology) - NVDA (NVIDIA) - QCOM (Qualcomm)\nFor each company, we will: 2. Classify the sentiment using our RoBERTa-based model 1. Pull recent news headlines 3. Analyze the 7-day rolling trends in public sentiment 4. Compare results across companies\nLet’s find out if the machines love themselves ?\n\n\nCode\n# This code collects recent news headlines for each company in our AI stock list. It uses our  `get_news_data()` which we defined above. \n\nai_tickers = [\"ACN\", \"ADBE\", \"AMD\", \"APP\", \"AVGO\", \"CRM\", \"MRVL\", \"MU\", \"NVDA\", \"QCOM\"] # Top 10 AI stocks \n\nall_news = []\n\ndef fetch_all_news(ticker_list):\n    all_news = []\n    for ticker in ticker_list:\n        try:\n            news = get_news_data(ticker)\n            all_news.append(news)\n        except Exception as e:\n            print(f\"Failed to get news for {ticker}: {e}\")\n    if all_news:\n        return pd.concat(all_news, ignore_index=True)\n    else:\n        return pd.DataFrame()\n\n# For the sake of reproducability (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n\n# Uncomment the line below get a more recent snapshot of the data !\n\n# Usage :\nAI_combined_news_df = fetch_all_news(ai_tickers)\n#AI_combined_news_df = pd.concat(all_news, ignore_index=True)\nAI_combined_news_df.to_csv(\"data/AI_news_snapshot.csv\", index=False)\nAI_sentiment_news_df = apply_sentiment(AI_combined_news_df)         \nAI_sentiment_news_df.to_csv(\"data/AI_news_sentiment.csv\", index=False)\n\n\n\n\nCode\nAI_sentiment_news_df = pd.read_csv(\"data/AI_news_sentiment.csv\")\nAI_sentiment_news_df.head()\n\n\nThisAI_process_sentiment does for a bunch of companies what we did for SBUX. It automatically collect all their news headlines and put them in one place for analysis.\n\n\nCode\ndef AI_process_sentiment(news_df):\n    filtered = news_df[news_df[\"Sentiment\"].isin([\"POSITIVE\", \"NEGATIVE\"])].copy()\n\n    # Group by DateOnly, Ticker, Sentiment → count headlines\n    grouped = (\n        filtered\n        .groupby([\"DateOnly\", \"Ticker\", \"Sentiment\"])\n        .size()\n        .unstack(fill_value=0)\n        .reset_index()\n    )\n\n    # Ensure both sentiment columns exist\n    if \"POSITIVE\" not in grouped.columns:\n        grouped[\"POSITIVE\"] = 0\n    if \"NEGATIVE\" not in grouped.columns:\n        grouped[\"NEGATIVE\"] = 0\n\n    # Sort for rolling computation\n    grouped = grouped.sort_values([\"Ticker\", \"DateOnly\"])\n\n    # 7-day rolling sums by ticker\n    grouped[\"7day_avg_positive\"] = (\n        grouped.groupby(\"Ticker\")[\"POSITIVE\"]\n        .transform(lambda x: x.rolling(window=7, min_periods=1).sum())\n    )\n    grouped[\"7day_avg_negative\"] = (\n        grouped.groupby(\"Ticker\")[\"NEGATIVE\"]\n        .transform(lambda x: x.rolling(window=7, min_periods=1).sum())\n    )\n\n    # Compute % positive\n    grouped[\"7day_pct_positive\"] = grouped[\"7day_avg_positive\"] / (\n        grouped[\"7day_avg_positive\"] + grouped[\"7day_avg_negative\"]\n    )\n\n    return grouped[[\"DateOnly\", \"Ticker\", \"7day_pct_positive\"]]\n\n\nAI_sentiment_df = AI_process_sentiment(AI_sentiment_news_df)\nAI_sentiment_df.head()\n\n\n\nLike we did with the SBUX stock above let’s get stock prices for all of our AI stocks now.\n\n\nCode\n# Match date range to your sentiment dataset\nstart_date = pd.to_datetime(AI_combined_news_df[\"DateOnly\"]).min() - pd.Timedelta(days=1)\nend_date = pd.to_datetime(AI_combined_news_df[\"DateOnly\"]).max()\n\ndef fetch_and_save_stock_data(ticker_list, start_date, end_date, save_dir=\"data/ai_prices\"):\n    \"\"\"\n    Fetches stock price data for each ticker in the list and saves it as a CSV file.\n    Prints a success or error message for each ticker.\n    \"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    for ticker in ticker_list:\n        try:\n            stock_df = get_stock_data(ticker, start_date, end_date)\n            stock_df[\"Ticker\"] = ticker\n            file_path = f\"{save_dir}/{ticker}_price.csv\"\n            stock_df.to_csv(file_path, index=False)\n        except Exception as e:\n            print(f\"Failed to get stock data for {ticker}: {e}\")\n\n\n# For the sake of reproducability (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n\n# Uncomment the line below get a more recent snapshot of the data !\n\n# Usage :\n# fetch_and_save_stock_data(ai_tickers, \"start_date\", \"end_date\")\n# price_files = glob.glob(\"data/ai_prices/*.csv\")\n# price_dfs = [pd.read_csv(f) for f in price_files]\n# AI_combined_price_df = pd.concat(price_dfs, ignore_index=True)\n# AI_combined_price_df.to_csv(\"data/AI_top10_price_snapshot.csv\", index=False)\n\nAI_stock_prices = pd.read_csv(\"data/AI_top10_price_snapshot.csv\")\nAI_stock_prices.head()\n\n\nAs we did before let’s merge everything into one dataframe to analyse it!\n\n\nCode\n# Ensure both DateOnly columns are datetime\nAI_sentiment_df['DateOnly'] = pd.to_datetime(AI_sentiment_df['DateOnly'])\nAI_stock_prices['DateOnly'] = pd.to_datetime(AI_stock_prices['DateOnly'])\n\n# Merge on DateOnly and Ticker\nfinal_df = pd.merge(\n    AI_stock_prices,\n    AI_sentiment_df,\n    on=['DateOnly', 'Ticker'],\n    how='inner'                  \n)\n\nfinal_df.head()\n\n\nSimilar to our approach with the SBUX stock price, we can fit a SARIMAX model to this combined dataset and forecast the percentage change in AI stock prices for the next 7 business days, using both historical price trends and recent news sentiment as inputs.\n\n\nCode\ndef fit_and_forecast(final_df, forecast_steps=7):\n    from statsmodels.tsa.statespace.sarimax import SARIMAX\n    import numpy as np\n    import pandas as pd\n\n    # Drop missing values\n    final_df = final_df.dropna(subset=['Pct_Change', 'lagged_sentiment'])\n\n    # Define endogenous and exogenous variables\n    endog = final_df['Pct_Change']\n    exog = final_df['lagged_sentiment']\n\n    # Fit SARIMAX\n    model = SARIMAX(endog, exog=exog, order=(1, 1, 1), enforce_stationarity=False, enforce_invertibility=False)\n    fit = model.fit(disp=False)\n\n    # Future exog (use last known lagged sentiment)\n    last_sentiment = exog.iloc[-1]\n    future_exog = np.full(shape=(forecast_steps,), fill_value=last_sentiment)\n\n    # Forecast\n    forecast = fit.get_forecast(steps=forecast_steps, exog=future_exog)\n    forecast_mean = forecast.predicted_mean\n    forecast_ci = forecast.conf_int()\n\n    # Create future dates\n    last_date = final_df.index[-1]\n    forecast_index = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_steps, freq='B')\n\n    return forecast_mean, forecast_ci, forecast_index\n\n\n\n\nCode\nfinal_df = final_df.copy()\n\n# Sort and index\nfinal_df['DateOnly'] = pd.to_datetime(final_df['DateOnly'])\nfinal_df.set_index('DateOnly', inplace=True)\nfinal_df.sort_index(inplace=True)\n\n# Create lagged and standardized sentiment\nfinal_df['sentiment_std'] = (\n    final_df['7day_pct_positive'] - final_df['7day_pct_positive'].mean()\n) / final_df['7day_pct_positive'].std()\n\nfinal_df['lagged_sentiment'] = final_df['sentiment_std'].shift(1)\nfinal_df.dropna(subset=['lagged_sentiment', 'Pct_Change'], inplace=True)\n\n# Now call\nforecast_mean, forecast_ci, forecast_index = fit_and_forecast(final_df)\ncreate_plot(final_df, forecast_mean, forecast_ci, forecast_index)\n\n\n\n\nHow well does the model predict?\nWe can fetch the actual data to see how well our model predicts the AI stocks.\n\n\nCode\n# Create function to generate the compare window\ndef get_compare_window(compare_start, compare_target_end):\n\n    # Define today to avoid fetching beyond range\n    today = pd.Timestamp(datetime.utcnow().date())  # use UTC date\n\n    compare_end = min(today, compare_target_end)\n    return compare_start, compare_end\n\n# Create function to fetch the actual percentage change\ndef fetch_actual_pct_changes(ticker_list, start_date, end_date, buffer_days=7):\n    # This function fetches the pct change of the given list of tickers and return as a DataFrame\n    rows = []\n    # fetch one ticker at a time to avoid group_by complexity\n    start_fetch = (start_date - pd.Timedelta(days=buffer_days)).strftime('%Y-%m-%d')\n    end_fetch = (end_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n\n    for t in ticker_list:\n        try:\n            df = yf.download(t, start=start_fetch, end=end_fetch, progress=False, interval='1d', auto_adjust=False)\n            if df.empty:\n                # no data for ticker in window\n                continue\n            df.index = pd.to_datetime(df.index).normalize()\n            # compute percent change on Close\n            if 'Adj Close' in df.columns:\n                price_col = 'Adj Close'\n            else:\n                price_col = 'Close'\n            df['pct_change'] = df[price_col].pct_change() * 100.0\n            # select only rows in [start_date, end_date]\n            sel = df.loc[(df.index &gt;= start_date) & (df.index &lt;= end_date)]\n            # Fix: Properly select the pct_change column\n            sel = sel[['pct_change']].dropna()\n            for idx, r in sel.iterrows():\n                rows.append({'DateOnly': idx, 'Ticker': t, 'Actual_Pct_Change': r['pct_change']})\n        except Exception as e:\n            print(f\"yfinance fetch failed for {t}: {e}\")\n\n    actuals_df = pd.DataFrame(rows)\n    if not actuals_df.empty:\n        actuals_df['DateOnly'] = pd.to_datetime(actuals_df['DateOnly']).dt.normalize()\n    return actuals_df\n\n# Function to compare predictions with actuals\ndef compare_predictions(pred_mean, pred_index, actuals_df, ticker=None):\n\n    pred_df = pd.DataFrame({\n        'DateOnly': pd.to_datetime(pred_index).normalize(),\n        'Predicted_Pct_Change': np.asarray(pred_mean).astype(float)\n    })\n    if ticker is not None:\n        actuals_df = actuals_df[actuals_df['Ticker'] == ticker].copy()\n    # If actuals contain multiple tickers and ticker=None, will compare using all actual rows\n    merged = pd.merge(pred_df, actuals_df, on='DateOnly', how='inner')\n\n    if merged.empty:\n        print(\"No overlapping observed days to compare (maybe market closed or today &lt; start).\")\n        return merged, {}\n\n    # If actuals has multiple tickers for same DateOnly (if ticker=None), aggregated handling:\n    if 'Ticker' in merged.columns and ticker is None:\n        # average actuals across tickers for that day\n        aggregated = merged.groupby('DateOnly').agg({\n            'Predicted_Pct_Change': 'first',  \n            'Actual_Pct_Change': 'mean'\n        }).reset_index()\n        mdf = aggregated\n    else:\n        mdf = merged[['DateOnly', 'Predicted_Pct_Change', 'Actual_Pct_Change']].copy()\n\n    # metrics\n    mae = mean_absolute_error(mdf['Actual_Pct_Change'], mdf['Predicted_Pct_Change'])\n    rmse = sqrt(mean_squared_error(mdf['Actual_Pct_Change'], mdf['Predicted_Pct_Change']))\n    metrics = {'count': len(mdf), 'MAE': mae, 'RMSE': rmse}\n\n    return mdf, metrics\n\n\nThe evaluation metrics we use are the Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE), which measure how far off our predictions are from the actual stock price changes.\nThe MAE is the average of the absolute differences between predicted and actual values, while the RMSE is the square root of the average of squared differences. Usually, lower values of these metrics indicate better model performance.\nThe MAE is calculated as follows: \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\] And the RMSE is calculated as follows: \\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]\nHowever, we must note that these metrics are not unitless, meaning they depend on the scale of the data. In our case, since we are predicting percentage changes in stock prices, the MAE and RMSE will be in percentage points.\n\n\nCode\n# Fit the previous example for comparison\ncompare_start = forecast_index[0]\ncompare_target_end = forecast_index[-1]\n\ncompare_start, compare_end = get_compare_window(compare_start, compare_target_end)\n\n# Create the actual comparison DataFrame\nactuals_df = fetch_actual_pct_changes(ai_tickers, compare_start, compare_end, buffer_days=7)\n\n# Calculate the metrics\npred_mean_values = forecast_mean.values  # array-like\nmdf, metrics = compare_predictions(pred_mean_values, forecast_index, actuals_df)\n\nprint(\"Comparison metrics:\")\nprint(f\"Count: {metrics['count']}\\nMAE: {metrics['MAE']:.2f}\\nRMSE: {metrics['RMSE']:.2f}\\n\")\n\nif not mdf.empty:\n    print(mdf.head())\n\n\nNow, with the actual stock price data, let’s visualize it in the plot alongside our predictions to see if our confidence intervals capture the actual movements well.\n\n\nCode\n# Visualize the comparison and evaluate the confidence intervals\ncreate_plot(final_df, forecast_mean, forecast_ci, forecast_index, actuals_df)\n\n\n\n\nDo you think the model’s predictions are accurate enough for practical use?\n\nWhat strategies could you use to:\n\nReduce the width of the confidence intervals?\n\nImprove the accuracy of the predictions?\n\nIncrease the model’s robustness to outliers?\n\nShare your ideas with your classmates!\n\n\n\n\n\nConclusion\nThis notebook provides an overview of how large language models (LLMs) make predictions, typically with an example showing the distribution of a LLM’s prediction and an example of stock price forecasting based on historical data and news sentiment. It demonstrates how LLMs work and how they can be applied to real-world problems like stock price prediction. The use of LLM and sentiment analysis allows us to incorporate additional qualitative information into our predictions, potentially allowing for better inference and forecasting of economic variables like stock prices.\nHowever, it is important to note that the model’s predictions are not perfect and should be used with caution. The notebook also highlights the importance of evaluating the model’s performance using metrics like MAE and RMSE, and encourages users to think critically about how to improve the model’s accuracy and robustness.\n\n\nKey Takeaways\n\nLarge language models (LLMs) predict the next word in a sequence by learning the probability distribution of possible outcomes based on context.\nLLMs can also be used to predict future values in a time series, such as stock prices, by incorporating additional features like public sentiment.\nThe SARIMAX model is a powerful forecasting model that allows us to include external information, such as news sentiment, in our predictions.\nWe must evaluate the model’s performance using metrics like MAE and RMSE to ensure its predictions are reliable.\n\n\n\nGlossary\n\nLarge Language Model (LLM): A type of AI model that predicts the next word in a sequence based on the context of previous words.\nNext Word Prediction: The task of predicting the next word in a sequence given the previous words.\nSentiment Analysis: The process of determining the emotional tone behind a series of words, used to understand the sentiment expressed in text.\nSARIMAX Model: A statistical model used for forecasting time series data that can incorporate external variables.\nMean Absolute Error (MAE): A measure of prediction accuracy that calculates the average absolute difference between predicted and actual values.\nRoot Mean Squared Error (RMSE): A measure of prediction accuracy that calculates the square root of the average of squared differences between predicted and actual values.\n\n\n\nReferences\n\nKirsch, N. (2024, April 29). 10 Best AI Stocks Of August 2024. Forbes Advisor. https://www.forbes.com/advisor/investing/best-ai-stocks/\nChen, J. (2023, October 6). Efficient Market Hypothesis (EMH): Forms and criticisms. Investopedia. https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp\nYahoo Finance. (n.d.). Yahoo Finance — Stocks, financial news, quotes, and market data. Retrieved August 1, 2025, from https://ca.finance.yahoo.com/\nAroussi, R. (n.d.). yfinance [Python package]. GitHub. Retrieved August 1, 2025, from https://github.com/ranaroussi/yfinance\nLi, T. (2025). finvizfinance (Version 1.1.1) [Python package]. PyPI. Retrieved August 1, 2025, from https://pypi.org/project/finvizfinance/\nGeeksforGeeks. (2023, September 27). Complete guide to SARIMAX in Python. https://www.geeksforgeeks.org/python/complete-guide-to-sarimax-in-python/\nSprenger, T. O., Tumasjan, A., Sandner, P. G., & Welpe, I. M. (2014, February 5). Twitter sentiment and stock market movements: The predictive power of social media. VoxEU. https://cepr.org/voxeu/columns/twitter-sentiment-and-stock-market-movements-predictive-power-social-media"
  },
  {
    "objectID": "docs/AMNE-376/development/cosine_demo.html",
    "href": "docs/AMNE-376/development/cosine_demo.html",
    "title": "Cosine Similarity Between Two Embeddings",
    "section": "",
    "text": "Vec1 X: \n  Vec1 Y: \n  Vec2 X: \n  Vec2 Y:"
  },
  {
    "objectID": "docs/AMNE-376/notebook/amne376_image_embedding.html",
    "href": "docs/AMNE-376/notebook/amne376_image_embedding.html",
    "title": "AMNE 376: A Study of Richter’s Kouroi Through Image Embedding",
    "section": "",
    "text": "Learning Outcomes\nIn the notebook you will\n\nFamiliarize yourself with concepts such as computer vision, convolution, convolutional neural network (CNN) and image embeddings.\nUnderstand how computers “see” and distinguish between different images by identifying unique visual elements and quantifying their similarity.\nExplore photographs of Kouroi from Richter’s book Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942) and create image embeddings for these photographs using pre-trained models.\nLearn how to cluster and classify Kouroi based solely on photographs, and critically analyze the advantages and limitations of these techniques and their potential applications.\n\n\n\nPrerequisites\nBefore you start, make sure you\n\nHave at least 200 MB storage available on your device.\nDownload the 2 folders from SharePoint by hovering around the three points next to the folder name, then click download (You will only have access if you are an enrolled UBC student with an activated student email. If you are a UBC student but don’t have a student email, please follow this link to activate it). They will be saved locally as .zip files.\nFind the downloaded zipped files in your device and upload them to Jupyter in the same directory as this notebook.\nHave the required libraries installed, if not, uncomment the lines below (i.e. remove the #) and run the cell to install them:\n\n\n\nCode\n# !pip install matplotlib numpy pandas opencv-python scikit-learn torch torchvision transformers datasets grad-cam tensorflow keras\n\n\nImportant: Please run the following cells before continuing, as it sets up the libraries and image folders required for this notebook.\n\n\nCode\n# Standard library\nimport os\nfrom collections import defaultdict\nfrom pathlib import Path\n\n# Imaging / computer vision\nfrom PIL import Image\nimport cv2\nimport numpy as np\nfrom torchvision import transforms\n\n# Visualization\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.colors import ListedColormap\nimport plotly.express as px\nimport plotly.io as pio\n\n# Data handling\nimport pandas as pd\n\n# SciPy / numerical utilities\nfrom scipy.ndimage import convolve\n\n# scikit-learn\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\n\n# TensorFlow / Keras (VGG16)\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.preprocessing import image as keras_image\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\n\n# PyTorch / torchvision / transformers\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torchvision.models import convnext_base, ConvNeXt_Base_Weights\nfrom transformers import AutoImageProcessor, ConvNextV2Model\n\n# Misc\nfrom tqdm.notebook import tqdm\n\n# Local helpers\nfrom zip_extractor import ZipExtractor\n\n\n\n\nCode\n# Don't change this cell\n# Define the source path and the destination\nsources = [\"example_images.zip\", \"richter_kouroi_complete_front_only.zip\"]\ndest = \"../data/\"\nfor src in sources:\n    extractor = ZipExtractor(src, dest)\n    extractor.extract_all()\n\n\n\n\n1. Introduction: How Do Computers “See” Visual Art?\nIn this notebook, we explore a dataset of photographs collected from Gisela Richter’s Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942).\nGisela Richter’s 1942 book was one of the first systematic efforts to catalog and classify kouroi based on their stylistic evolution. Her work combined archaeological evidence with visual comparison, laying the foundation for how we study ancient sculpture today.\nIn this project, we aim to apply computer vision techniques to digitally analyze and group images from this dataset. Just as Richter used her trained eye to identify patterns and typologies, we’ll explore how machines can “see” these sculptures through their eyes (image embeddings, clustering, and convolutional neural networks).\nHave you ever wondered how images are stored in computers, how computers see them and distinguish the difference between them?\nMany of you probably know that digital images are stored based on pixels as a grid of figures, but when we are doing image searches using a search engine or uploading them to a Generative AI model, how exactly do computers interpret, distinguish, and process them? Here is a brief introduction that introduces you to some of the basic forms and methods.\n\n1.1 Digital Representations of Images\nHave you ever heard of the RGB primary colour model? For those who are unfamiliar with the concept, the model uses numbers in a range 0 ~ 255 to represent the colour intensity of red, green and blue and add up the three colour channels to generate any colour that’s visible to human. In a colorful digital image, each pixel is characterized by its colour stored in the form (R, G, B), so knowing the distribution of colour intensity gives you a lot of information about the image.\nHowever, for monochrome images, there is only one colour channel, the grayscale. We can still use the distribution of grayscale intensities to represent the image. Since all of our images (cropped from scanned pdf books) are printed in monochrome, we can represent them using a grayscale colour histogram.\nLet’s start with a three-view of the New York Kouros, here we read in the images and present them together.\n\n\nCode\n# Define the folder path where the images are stored\nimage_path = '../data/example_images' \n\nfig, axes = plt.subplots(1, 3, figsize=(8, 5))\n\n# List of specific image filenames\nimage_names = {'page188_img01_photo12.jpg': \"Left\", 'page188_img01_photo13.jpg': \"Front\", 'page189_img01_photo3.jpg': \"Back\"}\n\n# Display the images side by side\naxes = axes.flatten()\nfor i, img_name in enumerate(image_names):\n    img_path = f\"{image_path}/{img_name}\"\n    image = Image.open(img_path)\n    axes[i].imshow(image)\n    axes[i].set_title(image_names[img_name])\n    axes[i].axis('off')\n\nplt.suptitle(\"Selected Images from Richter's Kouroi Dataset\")\nplt.tight_layout()\nplt.show()\n\n\nEach of these images, when loaded into the computer, becomes a 2D array of numbers representing intensity values. We then plot the colour histogram for each image representing the distribution of grayscale intensity.\n\nDiscussion: What do you notice by looking at the three histograms?\n\n\n\nCode\n# Generate and plot greyscale histograms for the selected images\nfig, axes = plt.subplots(1, 3, figsize=(7, 4))\n\nfor i, img_name in enumerate(image_names):\n    img_path = f\"{image_path}/{img_name}\"\n    image = Image.open(img_path)\n    histogram = image.histogram()\n\n    axes[i].plot(histogram, color='black')\n    axes[i].set_title(f'{image_names[img_name]}')\n    axes[i].set_xlim([0, 255])\n    axes[i].set_xlabel(\"Intensity\")\n    if i == 0:\n        axes[i].set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\n\nThey look very similar! This result is not surprising given that the three images were taken at the same time with the same equipment of the same Kouros. The above example shows us that comparing the similarity of colour distributions is one way that computers understand the similarity of images.\nHowever, one can quickly realize the drawbacks of this approach. First, it relies on the correct representation of colour, so two identical images with color differences may not be recognized as similar. Second, since it focuses only on colour, it ignores the fundamental information for object recognition such as spatial, shape and texture in the image. Last but not least, there may exist two completely different images with exactly the same color distribution. Therefore, we need better methods to consider the similarity between images.\nBag of Visual Words (BoVW) is a more practical method for recognizing similarity. The rationale behind this is very complicated, but to put it simply, it treats a “feature” in an image as a “word” (a set of numbers containing information about the feature) and calculates how often each word appears in the image. Here, we created a visual vocabulary containing 20 “words” using three-view photos of the New York Kouros, and visualized what a visual word represents on the left-view image. Here, we pick the visual word with ID 6:\n\n\nCode\n# Define the number of clusters for KMeans\nn_clusters   = 20\nword_to_show = 6\nmax_patches  = 30\n\n# Initialize ORB detector\norb = cv2.ORB_create(nfeatures=500)\nall_descriptors = []      # for stacking\nimage_data      = []      # (img_name, kps, descs)\n\n# Detect and describe all images\nfor img_name in image_names:\n    img_path = os.path.join(image_path, img_name)\n    img      = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n    keypoints, descriptors = orb.detectAndCompute(img, None)\n\n    if descriptors is None:\n        descriptors = np.zeros((0, orb.descriptorSize()), dtype=np.uint8)\n\n    all_descriptors.append(descriptors)\n    image_data.append((img_name, keypoints, descriptors))\n\n# Build the visual vocabulary\nall_descriptors_stacked = np.vstack(all_descriptors)\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\nkmeans.fit(all_descriptors_stacked)\n\n# Compute BoVW histograms\nhistograms = []\nfor img_name, _, descriptors in image_data:\n    if descriptors.shape[0] &gt; 0:\n        words = kmeans.predict(descriptors)\n        hist, _ = np.histogram(words, bins=np.arange(n_clusters + 1))\n    else:\n        hist = np.zeros(n_clusters, dtype=int)\n    histograms.append((img_name, hist))\n\n# Find the locations matching visual word ID = 6\nlocations = []\nfor img_idx, (_, keypoints, descriptors) in enumerate(image_data):\n    if descriptors.shape[0] == 0:\n        continue\n    assignments = kmeans.predict(descriptors)\n    for kp, w in zip(keypoints, assignments):\n        if w == word_to_show:\n            x, y = map(int, kp.pt)\n            locations.append((img_idx, x, y))\n            if len(locations) &gt;= max_patches:\n                break\n    if len(locations) &gt;= max_patches:\n        break\n\n# Group by image and visualize\nimgs = defaultdict(list)\nfor idx, x, y in locations:\n    imgs[idx].append((x, y))\n\nif imgs:\n    img_idx, pts = next(iter(imgs.items()))\n    fname = image_data[img_idx][0]\n    img   = cv2.imread(os.path.join(image_path, fname), cv2.IMREAD_GRAYSCALE)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n    for x, y in pts:\n        cv2.circle(img_rgb, (x, y), radius=25, color=(0,255,0), thickness=2)\n\n    plt.figure(figsize=(6,6))\n    plt.imshow(img_rgb)\n    plt.title(f\"Word {word_to_show} Keypoints on the Left-View Photo\")\n    plt.axis('off')\n    plt.tight_layout()\n    plt.show()\n\n\nWe note that the word “6” could represent the beads in the beadwork worn by the Kouros.\n\nDiscussion: Based on your knowledge of the various Kouros, do you think this visual word can be the key to differentiating between different Kouros, or even different sculptural subjects?\n\n\n\n1.2 Measurement of Similarity\nAs you may have realized, the visual word frequency distributions of different images are not exactly the same, so how can we determine if these images are similar? More importantly, what can we use as a criterion to categorize different images based on visual words? Here, we will use something called cosine similarity to make a measurement. \nYou can think of the visual word frequency histogram for each image as an arrow in space, and cosine similarity is a measure of how much those arrows are pointing in the same direction. The criterion is very intuitive: the closer the cosine similarity of two images is to 1, the more similar the two images are; the closer the cosine similarity is to 0, the less similar the two images are.\nHere, we perform pairwise cosine similarity measurements on left-view, front-view, and back-view photographs of New York Kouros, and show the results.\n\n\nCode\nhist_list = []\nfor img_name, _, descriptors in image_data:\n    if descriptors.shape[0] &gt; 0:\n        words = kmeans.predict(descriptors)\n        hist, _ = np.histogram(words, bins=np.arange(n_clusters + 1))\n    else:\n        hist = np.zeros(n_clusters, dtype=int)\n    hist = hist.astype(float)\n    if hist.sum() &gt; 0:\n        hist /= hist.sum()\n    hist_list.append(hist)\n    \nhistograms = np.array(hist_list)\n\nsim_matrix = cosine_similarity(histograms)\n\nimage_keys = list(image_names.keys())\nimage_labels = list(image_names.values())\n\n# Display the similarity matrix\nfig, ax = plt.subplots(figsize=(8, 5))\ncax = ax.imshow(sim_matrix, interpolation='nearest', cmap='viridis')\nax.set_title('BoVW Cosine Similarity between Images')\nax.set_xticks(np.arange(len(image_labels)))\nax.set_yticks(np.arange(len(image_labels)))\nax.set_xticklabels(image_labels, rotation=45, ha='right')\nax.set_yticklabels(image_labels)\nfig.colorbar(cax, ax=ax, label='Cosine Similarity')\nplt.tight_layout()\nplt.show()\n\nprint(\"Pairwise Cosine Similarity Matrix:\")\nfor i in range(len(image_labels)):\n    for j in range(i + 1, len(image_labels)):\n        print(f\"{image_labels[i]} vs {image_labels[j]}: {sim_matrix[i, j]:.3f}\")\n\n\nAs you can see, the pairwise cosine similarities are all very high, even though the BoVW histograms look very different! This is good evidence that they are photographs of the same object, and computers can understand this by setting the appropriate threshold.\nHowever, would this also work for photos of different objects? Let’s find out by calculating the cosine similarity between existing images and another Kouros currently exhibited in the Piraeus Archaeological Museum.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_image_path = '../data/richter_kouroi_complete_front_only/page312_img01_photo4.jpg'\nnew_image_label = 'A Kouros from Athens' # Suppose this is a new artifact we just discovered\n\nimg_new = cv2.imread(new_image_path, cv2.IMREAD_GRAYSCALE)\norb = cv2.ORB_create(nfeatures=500)\nkp_new, desc_new = orb.detectAndCompute(img_new, None)\n\nif desc_new is not None and len(desc_new) &gt; 0:\n    words_new = kmeans.predict(desc_new)\n    hist_new, _ = np.histogram(words_new, bins=np.arange(kmeans.n_clusters + 1))\nelse:\n    hist_new = np.zeros(kmeans.n_clusters, dtype=int)\n    \nhist_new = hist_new.astype(float)\nif hist_new.sum() &gt; 0:\n    hist_new /= hist_new.sum()\n\nsims = cosine_similarity(histograms, hist_new.reshape(1, -1)).flatten() \n\n# Print the cosine similarity of the new image with existing images\nprint(f\"\\nCosine Similarity of '{new_image_label}' with existing images:\")\nfor i, label in enumerate(image_labels):\n    print(f\"{label} vs {new_image_label}: {sims[i]:.3f}\")\n\n# Show the new image\nplt.figure(figsize=(5, 5))\nplt.imshow(img_new, cmap='gray')\nplt.title(f\"{new_image_label}\")\nplt.axis('off')\nplt.show()\n\n\nBy looking at the results, we see that it has a lower but still relatively high cosine similarity to the previous images, albeit with different textures and poses. Although computers do not understand what “Kouroi” are simply by collecting visual words, they can still see the similarity! To support this view, let’s look at an example that is also a standing figure, but from a different culture (China, Sanxingdui). If our conjecture is correct, its cosine similarity to the previous images will decrease significantly.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_image_path2 = '../data/example_images/sanxingdui.jpeg'\nnew_image_label2 = 'A Bronze Figure from Sanxingdui' # Suppose this is a new artifact we just discovered\n\nimg_new2 = cv2.imread(new_image_path2, cv2.IMREAD_GRAYSCALE)\norb = cv2.ORB_create(nfeatures=500)\nkp_new, desc_new = orb.detectAndCompute(img_new2, None)\n\nif desc_new is not None and len(desc_new) &gt; 0:\n    words_new = kmeans.predict(desc_new)\n    hist_new, _ = np.histogram(words_new, bins=np.arange(kmeans.n_clusters + 1))\nelse:\n    hist_new = np.zeros(kmeans.n_clusters, dtype=int)\n    \nhist_new = hist_new.astype(float)\nif hist_new.sum() &gt; 0:\n    hist_new /= hist_new.sum()\n\nsims = cosine_similarity(histograms, hist_new.reshape(1, -1)).flatten()  \n\n# Print the cosine similarity of the new image with existing images\nprint(f\"\\nCosine Similarity of '{new_image_label2}' with existing images:\")\nfor i, label in enumerate(image_labels):\n    print(f\"{label} vs {new_image_label2}: {sims[i]:.3f}\")\n\n# Show the new image\nplt.figure(figsize=(5, 5))\nplt.imshow(img_new2, cmap='gray')\nplt.title(f\"{new_image_label2}\")\nplt.axis('off')\nplt.show()\n\n\nThe results were exactly as we expected. The cosine similarity measured for different artistic styles, different poses and different angles of the Sanxingdui sculpture is significantly lower.\nThis provides us with a hint on how to build an automatic image-based classifier for art and artifacts of different genres, cultures, and textures. Although BoVW also has some obvious limitations (lack of spatial relationships, lack of ability to detect specific objects in complex images), the examples above demonstrate the fundamentals of computer vision, and with the help of more advanced techniques we can do much more in analyzing artwork based on digitized images.\n\n\n\n2. Convolutions on Images\n\n2.1 What are convolutions?\nBefore diving into applying a convolutional neural network, let’s first make an intuitive introduction to the concept convolution.\nImagine sliding a tiny image over an image as a filter to make the actual image appear the same as the filter. Convolution is the mathematical operation to achieve such an effect.\nBelow is one of such filters, or its professional term, a kernel, how do you think it will filter an image to make the image look like it?\n\n\nCode\n# Kernel\nkernel = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\n# Map: 1 -&gt; 1.0 (white), 0 -&gt; 0.0 (black), -1 -&gt; 1.0 (white)\ndisplay_kernel = np.where(kernel == 0, 0, 1)\n\nfig, ax = plt.subplots()\ncax = ax.matshow(display_kernel, cmap='gray', vmin=0, vmax=1)\nplt.colorbar(cax)\n\n# Annotate the kernel values\n\nax.set_title('Example of a Kernel')\nplt.show()\n\n\nThis is how an actual image Convolved with the filter:\n\n\nCode\n# Load the image as grayscale\nimg_path = \"../data/example_images/page300_img01_photo8.jpg\"\nimage = Image.open(img_path).convert('L')\nimg_array = np.array(image)\n\n# Define the horizontal edge detection kernel\nkernel = np.array([\n    [-1, -1, -1],\n    [ 0,  0,  0],\n    [ 1,  1,  1]\n])\n\n# Convolve the image with the kernel\nconvolved = convolve(img_array, kernel, mode='reflect')\n\n# Display the original and convolved images\nfig, ax = plt.subplots(1, 2, figsize=(8, 4))\nax[0].imshow(img_array, cmap='gray')\nax[0].set_title(\"Original Image\")\nax[0].axis('off')\nax[1].imshow(convolved, cmap='gray')\nax[1].set_title(\"Convolved with Kernel\")\nax[1].axis('off')\nplt.show()\n\n\nThe above is just one example of a convolutional kernel that extracts horizontal edges in an image. In fact, there are many different kernels with different effects. For example, here is a filter that blurs all images:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\ndef gaussian_kernel(size=21, sigma=5):\n    ax = np.linspace(-(size-1)//2, (size-1)//2, size)\n    xx, yy = np.meshgrid(ax, ax)\n    kernel = np.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n    return kernel / np.sum(kernel)\n\nkernel = gaussian_kernel(21, 5)\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"21x21 Gaussian Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Heavily Blurred\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nBelow is a kernel that preserves the input image as it is; it is also known as the identity kernel:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Identity kernel (3x3)\nkernel = np.zeros((3, 3))\nkernel[1, 1] = 1\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray', vmin=0, vmax=1); ax[1].set_title(\"Identity Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nThere is also a kernel that sharpens the images, known as the sharpening filter:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Sharpen kernel\nkernel = np.array([[ 0, -1,  0],\n                   [-1,  5, -1],\n                   [ 0, -1,  0]])\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"Sharpen Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nOther than sharpening, there is even a filter to emboss the image:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\n# Emboss kernel\nkernel = np.array([[-2, -1, 0],\n                   [-1,  1, 1],\n                   [ 0,  1, 2]])\n\nconv = convolve(img, kernel)\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\nax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"Emboss Kernel\"); ax[1].axis('off')\nax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\nOther than only detecting vertical or horizontal edges, a filter named after Laplace was discovered to detect all edges:\n\n\nCode\nimg = np.array(Image.open(img_path).convert('L'))\n\nlaplacian = np.array([[0,-1,0],[-1,8,-1],[0,-1,0]])\nedge = convolve(img, laplacian)\n\nfig, ax = plt.subplots(1, 3, figsize=(8, 3))\nax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off'\n)\nax[1].imshow(laplacian, cmap='gray'); ax[1].set_title(\"Laplacian Kernel\"); ax[1].axis('off')\nax[2].imshow(edge, cmap='gray'); ax[2].set_title(\"Edges\"); ax[2].axis('off')\nplt.tight_layout(); plt.show()\n\n\n\n\n2.2 What can Machine Learning do?\nOver the years, people have discovered these tiny images or “kernels” or “filters”. In machine learning, we discover or learn these potentially useful filters directly from the data, rather than through mathematical derivation. In a word, machine learning can “learn” the filters from data what would be useful for downstream tasks like classifying images or identifying things in an image.\n\n\nCode\n# 1. Load a pretrained conv model (VGG16 without top)\nmodel = VGG16(weights='imagenet', include_top=False)\n\n# 2. Choose three conv layers: early, middle, late\nearly_layer = model.get_layer('block1_conv2')\nmid_layer   = model.get_layer('block3_conv3')\nlate_layer  = model.get_layer('block5_conv3')\n\n# 3. Extract one kernel from each layer\n# Each kernel has shape (k, k, in_channels, out_channels)\nkernel_early = early_layer.get_weights()[0][:, :, 0, 0]\nkernel_mid   = mid_layer.get_weights()[0][:, :, 0, 0]\nkernel_late  = late_layer.get_weights()[0][:, :, 0, 0]\n\n# 4. Load and preprocess an image\ndef load_and_gray(path, target_size=(224,224)):\n    img = keras_image.load_img(path, target_size=target_size)\n    img_arr = keras_image.img_to_array(img)\n    # convert to grayscale\n    gray = cv2.cvtColor(img_arr.astype('uint8'), cv2.COLOR_RGB2GRAY)\n    # normalize\n    gray = gray.astype('float32') / 255.0\n    return gray\n\ngray = load_and_gray(img_path)\n\n# 5. Convolve the image with each kernel\ndef apply_filter(img, kernel):\n    # Flip kernel for convolution\n    k = kernel.shape[0]\n    # OpenCV uses correlation; flip kernel to perform convolution\n    flipped = np.flipud(np.fliplr(kernel))\n    filtered = cv2.filter2D(img, -1, flipped)\n    return filtered\n\nout_early = apply_filter(gray, kernel_early)\nout_mid   = apply_filter(gray, kernel_mid)\nout_late  = apply_filter(gray, kernel_late)\n\n# 6. Visualize\nplt.figure(figsize=(8, 8))\n\nplt.subplot(2, 2, 1)\nplt.title('Original Gray')\nplt.imshow(gray, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 2)\nplt.title('Early Layer Kernel')\nplt.imshow(out_early, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 3)\nplt.title('Mid Layer Kernel')\nplt.imshow(out_mid, cmap='gray')\nplt.axis('off')\n\nplt.subplot(2, 2, 4)\nplt.title('Late Layer Kernel')\nplt.imshow(out_late, cmap='gray')\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()  # In Colab this will display inline\n\n\nDifferent parts of the model highlight different things. The important thing to note is that no one wrote the filters themselves. The network learned that the features highlighted by these filters are useful.We simply wrote the learning algorithm, then the model learned from data by itself.\nTypically, a model used for image classification can (on its own) learn filters to highlight things the model needs, such as edges and lines, and more importantly, in addition to these simple features, the model can learn filters to detect heads, eyes, ears, and other abstract concepts, and this is exactly how convolution makes it possible to detect, characterize, and categorize complex objects in complex images. In order to achieve these amazing features, we usually need to employ models such as Convolutional Neural Networks.\n\n\n\n3. Data Exploration\n\n3.1 Exploring the Metadata\nFor the rest of the notebook, we will use a small selection of photographs from Richter’s Kouroi (1942), which contain frontal shots of Kouroi with a full torso and recognizable facial features. We have also prepared a labeled metadata that shows information about which group and era these Kouroi belong to and what materials they are made of. We can begin by looking at some basic information from the metadata:\n\n\nCode\n# Read in the metadata CSV file\n# Note that we are only going to investigate a subset of the full dataset\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')\n\ndf = df.drop(columns = 'page')\n\nprint(df.head())\n\n\n\n\nCode\nprint(\"Information of the dataset:\")\nprint(f\"Number of images: {df.shape[0]}\")\nprint(f\"Number of distinct eras: {df['era'].nunique()}\")\nprint(f\"Number of distinct materials: {df['material'].nunique()}\")\n\n\nWe can also see the distribution of each label by plotting histograms:\n\n\nCode\ndef bar_plot(df, column1, column2):\n    # Calculate counts of each value in the specified columns\n    label_counts1 = df[column1].value_counts()\n    label_counts2 = df[column2].value_counts()\n\n    # Create a figure with a fixed size\n    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n    \n    # Plot the bar chart\n    label_counts1.plot(kind='bar', ax=axes[0], color='steelblue')\n    axes[0].set_title(f'Distribution of {column1.capitalize()}')\n    axes[0].set_xlabel(column1.capitalize())\n    axes[0].set_ylabel('Count')\n    axes[0].tick_params(axis='x', rotation=90)\n    label_counts2.plot(kind='bar', ax=axes[1], color='darkorange')\n    axes[1].set_title(f'Distribution of {column2.capitalize()}')\n    axes[1].set_xlabel(column2.capitalize())\n    axes[1].set_ylabel('Count')\n    axes[1].tick_params(axis='x', rotation=90)\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot the distribution of labels in the dataset\nbar_plot(df, 'era', 'material')\n\n\n\n\n3.2 Exploring the Images\nTo get a direct idea of the general characteristics of this subset of photographs, we read the photographs from the image directory and show the first 4 images in this dataset.\n\n\nCode\n# Read in the images as a list \ndata_dir = Path(\"../data/richter_kouroi_complete_front_only\")\nimage_paths = sorted(data_dir.glob(\"*.jpg\"))\n\nimages = []\nfor p in image_paths:\n    img = Image.open(p).convert(\"RGB\")   # ensure 3‑channel\n    img_arr = np.array(img)\n    images.append(img_arr)\n    \nfig, axes = plt.subplots(1, 4, figsize=(6, 4))\nfor ax, img in zip(axes, images[:4]):\n    ax.imshow(img)\n    ax.axis(\"off\")\nplt.suptitle(\"First 4 images in the dataset\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n4. Image Embedding Using ConvNeXt V2\n\n4.1 CNN and Models See Images\nWhat we’ll do next is we will bring in ConvNeXt V2, it is a CNN model trained on millions of images based on ImageNet.\nWe’re sure everyone has used some Large Language Models (LLMs) and gotten a feel for how these models mimic the way humans think. This imitation of the human way of thinking comes from Artificial Neural Networks (ANN). Just as LLMs process and generate natural language, CNN models process visual images, examining the features in those images through convolutional layers and generating their own digital representations of the images.\nHere, we will create a visualization that shows the early, middle, and late feature layers of the 4 images after convolution, what commonality and difference did you notice from the extracted features in these layers?\n\n\nCode\nmodel = VGG16(weights='imagenet', include_top=False)\n\n# Grab kernels from the first and last conv layers\nearly_layer = model.get_layer('block1_conv2')\nmid_layer = model.get_layer('block3_conv3')\nlate_layer  = model.get_layer('block5_conv3')\n\n# choose the (0,0) filter for each\nkernel_early = early_layer.get_weights()[0][:, :, 0, 0]\nkernel_mid  = mid_layer.get_weights()[0][:, :, 0, 0]\nkernel_late  = late_layer.get_weights()[0][:, :, 0, 0]\n\n# Convolution helper (flip kernel for true conv)\ndef apply_filter(img, kernel):\n    flipped = np.flipud(np.fliplr(kernel))\n    return cv2.filter2D(img, -1, flipped)\n\nfig, axes = plt.subplots(3, 4, figsize=(8, 8))\n\nfor col, img in enumerate(images[:4]):\n    gray = (\n        cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2GRAY)\n          .astype('float32') / 255.0\n    )\n    out_early = apply_filter(gray, kernel_early)\n    out_early = (out_early - out_early.min()) / (out_early.max() - out_early.min())\n\n    out_mid = apply_filter(gray, kernel_mid)\n    out_mid = (out_mid - out_mid.min()) / (out_mid.max() - out_mid.min())\n    \n    out_late = apply_filter(gray, kernel_late)\n    out_late = (out_late - out_late.min()) / (out_late.max() - out_late.min())\n\n    # plot\n    axes[0, col].imshow(out_early, cmap='gray')\n    axes[0, col].set_title(f'Early Layer #{col+1}')\n    axes[0, col].axis('off')\n\n    axes[1, col].imshow(out_mid, cmap='gray')\n    axes[1, col].set_title(f'Mid Layer #{col+1}')\n    axes[1, col].axis('off')\n\n    axes[2, col].imshow(out_late, cmap='gray')\n    axes[2, col].set_title(f'Late Layer #{col+1}')\n    axes[2, col].axis('off')\n\nfig.suptitle('First, Middle vs. Last Conv Layer Responses', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n4.2 Creating Image Embeddings\nImage embedding is a process where images are transformed into numerical representations, specifically, lists of numbers that carry informations about the images. While this sounds somewhat similar to the idea of visual words, they are not the same. Think of BoVW as counting how many times specific words appear in a book without caring about grammar or sentence structure– this can identify simple patterns, but cannot summarize the big picture of the book. Image embeddings, on the other hand, are like reading the entire book and summarizing its meaning in a well crafted passage, they capture the bigger picture, context, and nuance.\nWe can build a vocabulary of visual words quite easily, but creating image embeddings usually require using deep neural networks pretrained on millions of images. These networks process the entire image and learn hierarchical, abstract features that are more semantically meaningful.\nHere, we will load the pre-trained ConvNeXt V2 model, pass the image folder to generate embeddings, and save the embeddings in a grid of numbers.\n\n\nCode\n# Read in the pre-trained ConvNeXtV2 model\n# Load the pre-trained ConvNeXtV2 model and image processor\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-base-22k-224\") \n\n# Move the model to the appropriate device (GPU or CPU)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = ConvNextV2Model.from_pretrained(\"facebook/convnextv2-base-22k-224\")\n\n# Move the model to the appropriate device (GPU or CPU)\n_ = model.to(device)\n\n# Define the image directory for later use\nimage_directory = \"../data/richter_kouroi_complete_front_only\"\n\n\n\n\nCode\nbatch_size = 16\nembeddings = []\nvalid_filenames = []\n\nfilenames = df['filename'].tolist()\n\nfor i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n    batch_filenames = filenames[i : i + batch_size]\n    images = []\n    for filename in batch_filenames:\n        path = os.path.join(image_directory, filename)\n        try:\n            img = Image.open(path).convert(\"RGB\")\n            images.append(img)\n        except FileNotFoundError:\n            print(f\"Missing: {path}\")\n        except Exception as e:\n            print(f\"Error with {filename}: {e}\")\n\n    if not images:\n        continue\n\n    # Prepare inputs\n    inputs = processor(images=images, return_tensors=\"pt\")\n    pixel_values = inputs[\"pixel_values\"].to(device)\n\n    # Forward through feature extractor\n    with torch.no_grad():\n        outputs = model(pixel_values)   \n    # Global average pool\n    if isinstance(outputs, torch.Tensor):\n        hidden = outputs           \n    else:\n        hidden = outputs.last_hidden_state \n\n    batch_emb = hidden.mean(dim=(2, 3)).cpu().numpy()\n\n    embeddings.extend(batch_emb)   \n    \nembeddings = np.stack(embeddings, axis=0) \n\nnp.save('../data/embeddings/convnextv2_image_embeddings.npy', embeddings)\n\n\n\n\nCode\n# Load embeddings from the saved file\nembeddings = np.load('../data/embeddings/convnextv2_image_embeddings.npy')\n\nprint(embeddings)\n\nprint(f\"The embedding has {embeddings.shape[0]} rows and {embeddings.shape[1]} numbers each row.\")\n\n\nWe printed the embedded results above to see what they look like, and we also displayed the shape of the grid. As you can see, it contains 62 rows representing the 62 images in the dataset, and each row has 1024 numbers representing all the information extracted from each image. From now on, we will use this embedded data instead of the original image data.\n\n\n\n5. Analysis of Image Embeddings\n1024 is way too many numbers for us to examine and understand with our brains. So, we use something techniques called dimensionality reduction to squish the data down to just 2 or 3 dimensions, making it easy to visualize in 2D.\nPrincipal Component Analysis (PCA) is one of such techniques, it is essentially finding the two or three major axes through our huge data along with which the data has the most variations. By PCA we decompose our data with 1024 dimensions (number of cells in each row) to 2 dimensions and represent each image as a point on our scatterplots. We then colour the data with “era” and “material” respectively.\nHere, we will use plotly to create interactive visualizations, feel free to play with it and discuss patterns that you notice.\n\n\nCode\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\n# load metadata\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')   \n\n# PCA to 2 components\npca = PCA(n_components=2)\npc2 = pca.fit_transform(embeddings)\n\n# build DataFrame\npc_df = pd.DataFrame(pc2, columns=['PC1','PC2'])\npc_df['filename'] = df['filename'].values\npc_df['era'] = df['era'].values\n\n# interactive scatter\nfig = px.scatter(\n        pc_df,\n        x='PC1',\n        y='PC2',\n        color='era',\n        hover_data=['filename'], \n        title='Interactive PCA of Image Embeddings Colored by Era',\n        width=700, height=500\n    )\n\nfig.show()\n\n\n\n\nCode\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\npc_df['material'] = df['material'].values\n\nfig = px.scatter(\n        pc_df,\n        x='PC1',\n        y='PC2',\n        color='material',\n        hover_data=['filename'], \n        title='Interactive PCA of Image Embeddings Colored by Material',\n        width=700, height=500\n    )\n\nfig.show()\n\n\n\nDiscussion: Did you see any clear patterns of distributions by looking at the visualizations above? How can you interpret the results? What primary “features” do you think the PCA embedding captured in the first two dimensions?\n\n\n\n6. Classification of Kouroi\nArchaeological classification has always been an important issue in archaeology and artifact research. This problem is especially challenging when faced with a large amount of artifact data, or when faced with new artifacts with insufficient information. With the development of machine learning and image recognition technology, the use of computer technology to assist classification has become a trend in the new era of information archaeology. In this section, we would like to provide an example of classifying Kouroi by visual element for your reference.\n\n6.1 Traditional Approach\nIn addition to observing how the labels are clustered based on the embeddings, we can train classifiers to categorize objects into appropriate labels based on the image embeddings directly. A traditional approach is through a technique called logistic regression.\n\n\nCode\nX = embeddings\ny1 = pc_df['era'].tolist()\ny2 = pc_df['material'].tolist()\n\ny1 = np.array(y1)\ny2 = np.array(y2)\n\n\nNote that here we use image embeddings and labels as training data and test data respectively, this is because we want to evaluate the effectiveness of the classifier when dealing with unseen data. After training the classifier for eras, we use it to predict the labels of the test data and compare the results with the real labels. The report is printed below:\n\n\nCode\n# Perform the classification of eras\n# Split the data into training and testing sets\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X, y1, test_size=0.25, random_state=42, stratify=y1)\n\n# Create a logistic regression model\nclf1 = LogisticRegression(max_iter=1000, random_state=42)\n\nclf1.fit(X_train1, y_train1)\n\ny_pred1 = clf1.predict(X_test1)\n\n# Calculate classification metrics\nclassification_report1 = metrics.classification_report(y_test1, y_pred1, zero_division=0)\n\n# Print the classification report \nprint(\"Classification Report for Eras:\")\nprint(classification_report1)\n\n\nThe key metrics we especially care about here is the accuracy of our classifier, it is defined by\n\\[\n\\text{Accuracy} = \\frac{\\text{True Predictions}}{\\text{True Predictions} + \\text{Flase Predictions}}\n\\]\nIt reflects the proportion of true predictions out of all predictions made using the classifier. However, as shown above in the report, the accuracy of predicting era based on image embedding is not satisfactory. Still, we can visualize the decision boundary of this classifier on our 2D PCA of image embeddings to see what went wrong:\n\n\nCode\n# Prepare your 2D data + labels\nX_pca = pc_df[['PC1','PC2']].values\ny_era = pc_df['era'].values\n\n# Encode eras as integers\nle = LabelEncoder()\ny_enc = le.fit_transform(y_era)\n\n# Train the logistic on the encoded labels\nclf_2d = LogisticRegression(max_iter=1000, random_state=42)\nclf_2d.fit(X_pca, y_enc)\n\n# Build a mesh grid over the plotting area\nx_min, x_max = X_pca[:,0].min() - 1, X_pca[:,0].max() + 1\ny_min, y_max = X_pca[:,1].min() - 1, X_pca[:,1].max() + 1\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\n\n# Predict integer labels on the mesh\nZ = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(7,7))\n\n# Number of classes\nn_classes = len(le.classes_)\n\n# Pick a sequential colormap name:\nbase_cmap_red = plt.cm.Reds\nbase_cmap_blue = plt.cm.Blues\n\n# Sample 7 colors evenly from the light part of the cmap for regions\n# and the dark part for points.\nregion_colors = base_cmap_red(np.linspace(0.1, 0.6, n_classes))\npoint_colors  = base_cmap_blue(np.linspace(0.1, 1.0, n_classes))\n\ncmap_light = ListedColormap(region_colors)\ncmap_bold  = ListedColormap(point_colors)\n\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n\n# scatter original points, mapping back to string labels in the legend\nfor class_int, era_label in enumerate(le.classes_):\n    mask = (y_enc == class_int)\n    plt.scatter(\n        X_pca[mask,0], X_pca[mask,1],\n        color=cmap_bold(class_int),\n        label=era_label,\n        edgecolor='k', s=40\n    )\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Logistic Decision Boundary on 2D PCA Embedding\\n(colored by era)')\nplt.legend(title='Era')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.grid(True)\nplt.show()\n\n\n\nDiscussion: What can you say about this decision boundary?\n\nSimilarly, we can use the same approach to classify material of Kouroi. We first perform a train-test split, then train the classifier, use the trained classifier to predict the labels of the test set, and print out the classification report for quality evaluation.\n\n\nCode\n# Perform the classification of materials\n# Split the data into training and testing sets\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2, test_size=0.25, random_state=42, stratify=y2)\n\n# Create a logistic regression model\nclf2 = LogisticRegression(max_iter=1000, random_state=42)\n\nclf2.fit(X_train2, y_train2)\n\ny_pred2 = clf2.predict(X_test2)\n\n# Calculate classification metrics\nclassification_report2 = metrics.classification_report(y_test2, y_pred2, zero_division=0)\n\n# Print the classification report\nprint(\"Classification Report for Materials:\")\nprint(classification_report2)\n\n\nAs you can see, the accuracy is much higher now, but does that mean the classifier is good? You may have noticed that bronze and marble are classified almost perfectly, but other materials are not. This means that the classifier, while having a high accuracy, may have low precision or recall, as defined below:\n\nPrecision: The ratio of the number of true positives to the number of positive predictions. Precision tells us how often the model predicts correctly.\nRecall: The ratio of the number of true positives to the number of actual positives. Recall answers the question, “What percentage of positive results did we correctly predict?”\n\nWe can also visualize the decision boundary on the 2D PCA of materials\n\n\nCode\n# Prepare 2D data and labels\nX_pca = pc_df[['PC1','PC2']].values\ny_era = pc_df['material'].values\n\n# Encode eras as integers\nle = LabelEncoder()\ny_enc = le.fit_transform(y_era)\n\n# Train the logistic on the encoded labels\nclf_2d = LogisticRegression(max_iter=1000, random_state=42)\nclf_2d.fit(X_pca, y_enc)\n\n# Build a mesh grid over the plotting area\nx_min, x_max = X_pca[:,0].min() - 1, X_pca[:,0].max() + 1\ny_min, y_max = X_pca[:,1].min() - 1, X_pca[:,1].max() + 1\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\n\n# Predict integer labels on the mesh\nZ = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n\n# Plot the decision boundary\nplt.figure(figsize=(7,7))\n\n# light colors for regions\ncmap_light = ListedColormap(['#FFCCCC','#CCFFCC','#CCCCFF','#FFE5CC','#E5CCFF'][:len(le.classes_)])\n# bold colors for points\ncmap_bold  = ListedColormap(['#FF0000','#00AA00','#0000FF','#FF8000','#8000FF'][:len(le.classes_)])\n\nplt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n\n# Scatter original points, mapping back to string labels in the legend\nfor class_int, era_label in enumerate(le.classes_):\n    mask = (y_enc == class_int)\n    plt.scatter(\n        X_pca[mask,0], X_pca[mask,1],\n        color=cmap_bold(class_int),\n        label=era_label,\n        edgecolor='k', s=40\n    )\n\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('Logistic Decision Boundary on 2D PCA Embedding\\n(colored by material)')\nplt.legend(title='Era')\nplt.xlim(x_min, x_max)\nplt.ylim(y_min, y_max)\nplt.grid(True)\nplt.show()\n\n\n\nDiscussion: Now, going back to the classification reports shown above, do you think the classifier trained on era is a good chronological classifier? What about materials?\n\n\n\n6.2 CNN Classification of Materials\nThe last classifier we’re going to visit today is a neural network classifier, using a Multi-layer Perceptron (MLP) network architecture, which means we’re going to add a classification layer to the ConvNeXt V2 model to classify the material. All the model does here is act as a backbone to observe and extract features of interest in the input image. The MLP process, on the other hand, can be visualized as a number of experts examining different features on an image, then discussing them with each other, and finally voting to reach a final conclusion.\nWe begin by creating the data loader and load the Kouroi data directly from the folder:\n\n\nCode\n# Map the materials to integers\nMAT2IDX = {\n    'Marble': 0,\n    'Bronze': 1,\n    'Other': 2\n}\n\n# Create a custom dataset class for the Kouroi dataset\nclass KouroiDataset(Dataset):\n    def __init__(self, df, img_dir, processor, mat2idx):\n        self.df = df\n        self.img_dir = image_directory\n        self.processor = processor\n        self.mat2idx = MAT2IDX\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img = Image.open(os.path.join(self.img_dir, row.filename)).convert(\"RGB\")\n\n        inputs = self.processor(images=img, return_tensors=\"pt\")\n\n        for k,v in inputs.items():\n            inputs[k] = v.squeeze(0)\n        label = self.mat2idx[row.material]\n        return inputs, label\n\n\n\n\nCode\n# Tain-test split\ntrain_df, test_df = train_test_split(\n    df,\n    test_size=0.25,                 # 20% held out for testing\n    stratify=df[\"material\"],       # preserve class proportions\n    random_state=42\n)\n\n# Initialize the dataset and dataloader\ntrain_ds = KouroiDataset(\n    df=train_df,\n    img_dir=image_directory,\n    processor=processor,\n    mat2idx=MAT2IDX\n)\ntest_ds  = KouroiDataset(\n    df=test_df,\n    img_dir=image_directory,\n    processor=processor,\n    mat2idx=MAT2IDX\n)\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\ntest_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)\n\n\nAs mentioned above, here we freeze the model so that training does not change the way it understands the input image, but we are going to add a new classification layer on top of the network so that it can now use the additional knowledge about Kouroi for classification.\n\n\nCode\n# Build the model with convnextv2 as the backbone and a linear layer for classification\nclass Classifier(nn.Module):\n    def __init__(self, backbone_name, num_classes):\n        super().__init__()\n        # Load the ConvNeXtV2 backbone correctly and freeze it\n        self.backbone = ConvNextV2Model.from_pretrained(\n            backbone_name,\n            output_hidden_states=False,\n            output_attentions=False\n        )\n        for p in self.backbone.parameters():\n            p.requires_grad = False\n\n        embed_dim = self.backbone.config.hidden_sizes[-1]\n\n        # Build a simple 2-layer MLP head\n        self.head = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.ReLU(inplace=True),\n            nn.Dropout(0.2),\n            nn.Linear(embed_dim // 2, num_classes)\n        )\n\n    def forward(self, pixel_values):\n        # Forward through the frozen backbone\n        outputs = self.backbone(pixel_values=pixel_values)\n        x = outputs.pooler_output\n\n        # Classification head\n        logits = self.head(x)\n        return logits\n\n# Instantiate and move to device\nmodel = Classifier(\n    backbone_name=\"facebook/convnextv2-base-22k-224\",\n    num_classes=len(MAT2IDX)\n).to(device)\n\n\nHere, after setting up the new model, we will define the training loop and train the model. Please note that this process may take some time, especially when running on devices without a GPU. This also suggests that the high computational power requirement is a drawback when using CNNs for classification.\n\n\nCode\n# Set up the loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.head.parameters(), lr=1e-4, weight_decay=0.01)\nepochs = 20  \n\nfor epoch in range(1, epochs+1):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n        inputs, labels = batch\n        # move to device\n        inputs = {k:v.to(device) for k,v in inputs.items()}\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        logits = model(**inputs)\n        loss   = criterion(logits, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item() * labels.size(0)\n\n    avg_loss = total_loss / len(train_ds)\n    print(f\" Epoch {epoch} avg loss: {avg_loss:.4f}\")\n\ntorch.save(model.state_dict(), \"../data/models/mlp_model.pth\")\n\n\nAfter training, we set the model to evaluation mode and assessed the classification quality by printing the confusion matrix and classification report. It is clear that the accuracy does improve with the MLP architecture. However, we still lacked samples of materials other than bronze and marble, which undoubtedly harmed the quality of our training.\n\n\nCode\n# load the trained model\nmodel.load_state_dict(torch.load(\"../data/models/mlp_model.pth\"))\nmodel.to(device)  # move to the right device \n\n# Run one pass over your data in eval mode\nmodel.eval()\n\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        if inputs is None: continue\n        inputs = {k:v.to(device) for k,v in inputs.items()}\n        logits = model(**inputs)\n        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# Print the classification report\nfrom sklearn.metrics import classification_report\nreport = classification_report(all_labels, all_preds, target_names=list(MAT2IDX.keys()), zero_division=0)\nprint(\"Classification Report for Materials:\")\nprint(report)\n\n\n\nDiscussion: What difference did you notice in the result? Does this mean that MLP is not applicable to classifying materials?\n\nWhile the MLP classifier via CNN has advantages in dealing with more complex data structures (especially high-dimension nonlinear data), we note that it has two serious drawbacks: 1. it is more susceptible to the randomness of the training-testing split, with greater model variability; and 2. it is more susceptible to overfitting, whereas logistic regression is more susceptible to underfitting (for more detailed information on these concepts discussion can be found in Appendix B). This tells us that no one model is perfect for all situations. We must remain cautious in our choice of models.\n\n\n6.3 Example: Predicting the Material of Unseen Kouroi Photos\nNow, let’s imagine a scenario where we find a new Kouros, but we are not sure what it is made of, and we want to use our trained classifier to classify it based on its image features.\n\n\nCode\n# Define a new image to compare with the existing ones\nnew_artifact_path = '../data/example_images/NAMA_3938_Aristodikos_Kouros.jpeg'\nnew_artifact_label = 'A Kouros from Athens' # Suppose this is a new artifact we just discovered\n\nart_new = cv2.imread(new_artifact_path, cv2.IMREAD_GRAYSCALE)\n\n# Show the new image\nplt.figure(figsize=(6, 6))\nplt.imshow(art_new, cmap='gray')\nplt.title(f\"{new_artifact_label}\")\nplt.axis('off')\nplt.show()\n\n\n\nDiscussion: Looking at this photo, how would you classify the era and material of this Kouros?\n\nWe pass it into our trained logistic regression classifier and see how would its era be classified:\n\n\nCode\n# reload the processor and the model\nprocessor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-base-22k-224\") \n\nmodel = ConvNextV2Model.from_pretrained(\"facebook/convnextv2-base-22k-224\")\n\n# Open as new img\nimg_new = Image.open(new_artifact_path).convert(\"RGB\")\n\ninputs = processor(images=img_new, return_tensors=\"pt\")\npixel_values = inputs[\"pixel_values\"].to(device)\n\n# Forward through feature extractor\nwith torch.no_grad():\n    outputs = model(pixel_values) \n\n# Global average pool\nif isinstance(outputs, torch.Tensor):\n    hidden = outputs           \nelse:\n    hidden = outputs.last_hidden_state \n\nemb_new = hidden.mean(dim=(2, 3)).cpu().numpy()\n\n\npred_era = clf1.predict(emb_new)[0]\n\nprint(\"Predicted Era:\", pred_era)\n\n\nWe then pass it into our trained CNN classifier and see how its material would be classified:\n\n\nCode\n# reload the model\nmodel = Classifier(\n    backbone_name=\"facebook/convnextv2-base-22k-224\",\n    num_classes=len(MAT2IDX)\n).to(device)\nmodel.load_state_dict(torch.load(\"../data/models/mlp_model.pth\"))\nmodel.to(device) \n\n# Resize the model to an appropriate size\npreprocess = transforms.Compose([\n    transforms.Resize(256),                \n    transforms.CenterCrop(224),            \n    transforms.ToTensor(),\n    transforms.Normalize(                   \n        mean=[0.485, 0.456, 0.406],\n        std= [0.229, 0.224, 0.225]\n    ),\n])\n\nidx2mat = {idx: mat for mat, idx in MAT2IDX.items()}\n\ndef predict_image(image_path, model, device):\n    # Load\n    img = Image.open(image_path).convert(\"RGB\")\n    # Preprocess\n    x = preprocess(img)\n    x = x.unsqueeze(0).to(device)\n    # Inference\n    model.eval()\n    with torch.no_grad():\n        logits = model(**{\"pixel_values\": x}      \n                       if isinstance(x, torch.Tensor) else x)\n        pred_idx = logits.argmax(dim=1).item()\n\n    return idx2mat[pred_idx]\n\nmodel.to(device)\n\npredicted_material = predict_image(new_artifact_path, model, device)\nprint(\"Predicted Material:\", predicted_material)\n\n\n\nDiscussion: Do you think the predictions made above are correct? What is your evidence?\n\n\n\n6.4 Additional Note on Fine-tuning\nWhat we didn’t really include here are the more advanced applications of fine-tuning. Fine-tuning refers to the process of taking a pre-trained model (like ConvNeXt V2) and continuing its training on your specific dataset, allowing the model to adapt its learned features to better suit your task. Why we did not cover fine-tuning because it requires more computational resources, careful hyperparameter tuning, and a larger dataset to avoid overfitting. Additionally, fine-tuning can be time-consuming and is often not practical in an introductory or resource-limited setting.\nHowever, fine-tuning has the potential to significantly improve classification quality, especially for irregular data. By allowing the model to update its internal representations based on the unique characteristics of your images, it can learn more relevant features for distinguishing between subtle differences in style, era, or material. For research or production applications with sufficient data and compute, fine-tuning is a powerful next step to achieve higher accuracy and more robust results.\n\n\n\n7. Conclusion\nThrough this notebook, you’ve taken a journey with the example of Richter’s Kouroi from the basics of how computers “see” images to advanced techniques for analyzing and classifying images. You’ve explored how simple pixel values can be transformed into powerful representations using convolution, image embeddings, and neural networks. Along the way, you learned to visualize, cluster, and classify artworks…… these are all skills that are at the heart of modern computer vision.\nRemember, the tools and concepts you’ve practiced here are not just limited to art history or archaeology: they are widely used in fields ranging from medicine to astronomy, and beyond. As you continue your studies, keep experimenting, stay curious, and don’t be afraid to explore new datasets or try more advanced models. The intersection of technology and the humanities is full of exciting possibilities, and your creativity is the key to unlocking them.\nCongratulations on completing this exploration, and we hope you feel inspired to keep learning and discovering the field of Machine Learning!\n\n\nKey Takeaways\n\nThere are multiple different ways for computers to represent and understand content in images, including intensity histograms, BoVW distribution and image embeddings.\nConvolution is a common technique to process and extract different features in input images.\nConvolutional Neural Networks (CNN) are trained models that mimic the way people view images and understand them through convolutional layers.\nImage embeddings produced by a pretrained CNN map each kouros image into a high‑dimensional feature space. We can then apply dimensionality‑reduction techniques such as Principal Component Analysis to visualize clusters among different archaic sculptures.\nFor formal classification of kouroi, such as distinguishing groups/eras or identifying materials, both logistic regression and neural-network classification provide robust methods to assign style labels based on extracted image features, but we should always be aware of issues such as underfitting and overfitting.\nDifferent models may yield different results for image embedding, and sometimes the features recognized from an image are not exactly what we want. We should always be cautious about our results.\n\n\n\nGlossary\n\nComputer Vision: Computer Vision is a field of artificial intelligence that enables computers to “see” and interpret images and videos, mimicking human vision.\nConvolution: In the context of Computer Vision, Convolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\nConvolutional Neural Network (CNN): Convolutional Neural Network is a type of feedforward neural network that learns features via filter (or kernel) optimization. It is distinguished from other neural networks by its superior performance with image, speech or audio signal inputs.\nImage Embedding: Image Embedding is a process where images are transformed into numerical representations, called vectors, that capture the semantic meaning of the image.\nPrincipal Component Analysis (PCA): Principal Component Analysis is a statistical technique that simplifies complex data sets by reducing the number of variables while retaining key information. It does so by finding the major axes where the data sets vary the most.\nLogistic Regression: Logistic Regression is a statistical model used for binary or multiclass classification tasks. It estimates the probability that an input belongs to a particular class by applying the logistic (sigmoid) function to a weighted sum of the input features, making it well-suited for problems where outputs are discrete categories.\nMultilayer Perceptron (MLP): A Multilayer Perceptron is a class of feedforward artificial neural network composed of an input layer, one or more hidden layers of nonlinear activation units, and an output layer. It learns complex patterns by adjusting the weights of connections through backpropagation and is versatile for both classification and regression tasks.\nUnderfitting: Underfitting in machine learning occurs when a model is too simple to capture the underlying patterns in the training data.\nOverfitting: Overfitting in machine learning occurs when a model learns the training data too well, including its noise and random fluctuations, leading to poor performance on new, unseen data.\n\n\n\nAppendix A: Image Data Collection and Preprocessing from .pdf Files\nThis part provides a brief overview of how the data was collected and preprocessed for the analysis, typically how we cropped the images and prepared the metadata.\nWe used the following python script to convert a scanned pdf of Richter (1942) to image files in .jpg format.\nimport fitz  \n\n# Change the filename here if you want to reuse the script for your own project\ndoc = fitz.open(\"kouroiarchaicgre0000rich_1.pdf\") \n\nimport os\nout_dir = \"extracted_images\"\nos.makedirs(out_dir, exist_ok=True)\n\n# Iterate pages\nfor page_index in range(len(doc)):\n    page = doc[page_index]\n    image_list = page.get_images(full=True)  # get all images on this page\n\n    # Skip pages without images\n    if not image_list:\n        continue\n\n    # Extract each image\n    for img_index, img_info in enumerate(image_list, start=1):\n        xref = img_info[0]                   \n        base_image = doc.extract_image(xref)  \n        image_bytes = base_image[\"image\"]     \n        image_ext   = base_image[\"ext\"]      \n\n        # Write to file\n        out_path = os.path.join(\n            out_dir,\n            f\"page{page_index+1:03d}_img{img_index:02d}.{image_ext}\"\n        )\n        with open(out_path, \"wb\") as f:\n            f.write(image_bytes)\n\nprint(f\"Saved all images to {out_dir}\")\nThe following script cropped the photos by applying convolution.\nimport cv2\nimport glob\nimport os\n\n# Folder containing your page images\ninput_folder = \"extracted_images\"\noutput_folder = \"cropped_photos\"\nos.makedirs(output_folder, exist_ok=True)\n\ndef extract_photos_from_page(image_path, min_area=5000):\n    img = cv2.imread(image_path)\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # Blur and threshold to get binary image\n    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n    _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY_INV)\n    \n    # Dilate to merge photo regions\n    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n    dilated = cv2.dilate(thresh, kernel, iterations=2)\n    \n    # Find contours\n    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    \n    crops = []\n    for cnt in contours:\n        x, y, w, h = cv2.boundingRect(cnt)\n        area = w * h\n        # Filter by area to remove small artifacts\n        if area &gt; min_area:\n            crop = img[y:y+h, x:x+w]\n            crops.append((crop, (x, y, w, h)))\n    return crops\n\n# Process all pages\nfor img_path in glob.glob(os.path.join(input_folder, \"*.*\")):\n    base = os.path.splitext(os.path.basename(img_path))[0]\n    photos = extract_photos_from_page(img_path)\n    for idx, (crop, (x, y, w, h)) in enumerate(photos, start=1):\n        out_file = os.path.join(output_folder, f\"{base}_photo{idx}.jpg\")\n        cv2.imwrite(out_file, crop)\n\nprint(f\"Saved all images at {output_folder}\")\nThe following script employed tesseract OCR engine to detect pure text images and filter out photos of Kouros. You may want to visit the GitHub repository https://github.com/tesseract-ocr/tesseract to see how to install and setup appropriately.\nimport os\nimport shutil\nfrom PIL import Image\nimport pytesseract\n# Ensure you have Tesseract installed and pytesseract configured correctly.\n# On Windows, you might need:\n# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n\n# Folders\ninput_folder = \"cropped_photos\"\ntext_folder = \"text_crops\"\nphoto_folder = \"filtered_photos\"\nos.makedirs(text_folder, exist_ok=True)\nos.makedirs(photo_folder, exist_ok=True)\n\n# Threshold for text length to consider as \"text-only\"\n# You can also adjust this threshold based on your specific needs.\nTEXT_CHAR_THRESHOLD = 2 # Be careful with this threshold, do remember to check the results manually\n\n\nfor filename in os.listdir(input_folder):\n    path = os.path.join(input_folder, filename)\n    img = Image.open(path)\n\n    # Perform OCR to extract text\n    extracted_text = pytesseract.image_to_string(img)\n\n    # Classify based on length of extracted text\n    if len(extracted_text.strip()) &gt;= TEXT_CHAR_THRESHOLD:\n        dest = os.path.join(text_folder, filename)\n    else:\n        dest = os.path.join(photo_folder, filename)\n\n    shutil.move(path, dest)\n    print(f\"Moved {filename} -&gt; {os.path.basename(dest)}\")\n\nprint(\"Filtering complete\")\nThis script creates a .csv file for mannual labelling.\nimport os, re\nimport pandas as pd\n\n# Scan your filtered_photos folder\nrecords = []\n\n# Updated regex to match \"page&lt;number&gt;_img&lt;number&gt;_photo&lt;number&gt;.&lt;ext&gt;\"\npattern = re.compile(r\"page(\\d+)_img\\d+_photo(\\d+)\\.(?:png|jpe?g)\", re.IGNORECASE)\n\nfor fn in os.listdir(\"richter_kouroi_head_front_only\"):\n    m = pattern.match(fn)\n    if not m:\n        continue\n    page = int(m.group(1))\n    photo_idx = int(m.group(2))\n    records.append({\n        \"filename\": fn,\n        \"page\": page,\n        \"group\": \"\",    # blank for manual entry\n        \"era\": \"\",  # blank for manual entry\n        \"material\": \"\"  # blank for manual entry\n    })\n\n# Build DataFrame\ndf = pd.DataFrame(records)\n\ndf.sort_values([\"page\", \"filename\"], inplace=True)\n\n# Save out to CSV for manual labeling\ndf.to_csv(\"label_template.csv\", index=False)\nYou can try out the scripts with your interested pdf files yourself by running them in a python environment.\n\n\nAppendix B: The Risk of Underfitting and Overfitting\nIn the context of machine learning, two common pitfalls are underfitting and overfitting.\nUnderfitting occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test sets. This is often seen when using models like logistic regression on complex, non-linear datasets, as shown in the left panel above, where the decision boundary fails to separate the classes effectively. On the other hand, overfitting happens when a model is excessively complex, such as a deep neural network with many layers, and learns not only the true patterns but also the noise in the training data. This leads to excellent performance on the training set but poor generalization to new, unseen data, as illustrated in the right panel where the decision boundary is overly intricate.\nLet’s examine the two problems with a simulated two-class data:\n\n\nCode\n# Generate a simulater two-class data\nX, y = make_moons(n_samples=500, noise=0.40, random_state=0)\n\n# Visualize the simulated data\nplt.figure(figsize=(6, 4))\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolor='k')\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('Simulated Two-Class Data')\nplt.show()\n\n\nWe can clearly see that the data is very non-linear, which gives us a hint that the right model should be able to account for non-linear relationships. However, for demonstration purposes, I will be using logistic regression to generate an underfitting classifier; and while MLP is suitable for use here, I will let it generate an overfitting classifier by significantly oversizing the neurons and iterations.\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=1\n)\nunder = LogisticRegression()\n\nover  = MLPClassifier(hidden_layer_sizes=(200,200,200),\n                      max_iter=2000,\n                      random_state=1)\n\n# Train both models\nunder.fit(X_train, y_train)\nover.fit(X_train, y_train)\n\n\nThen, we get the decision boundaries of the two classifiers and visualize them with the test data, what do you find about their accuracy?\n\n\nCode\n# Build grid\nx_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\ny_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\nxx, yy = np.meshgrid(\n    np.linspace(x_min, x_max, 300),\n    np.linspace(y_min, y_max, 300)\n)\ngrid = np.c_[xx.ravel(), yy.ravel()]\n\n# Get decision boundary\nZu = under.predict_proba(grid)[:,1].reshape(xx.shape)\nZo = over.predict_proba(grid)[:,1].reshape(xx.shape)\n\n# Plot side by side\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n\nfor ax, Z, title in [\n    (ax1, Zu, 'Extreme Underfit'),\n    (ax2, Zo, 'Extreme Overfit')\n]:\n    ax.contourf(xx, yy, Z&gt;0.5, alpha=0.3)\n    # draw precise decision boundary P=0.5\n    ax.contour(   xx, yy, Z, levels=[0.5], colors='k', linewidths=1.5)\n\n    ax.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor='k')\n    ax.set_title(title)\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n\nfig.suptitle('Underfitting vs. Overfitting in Classification', fontsize=16)\nplt.tight_layout(rect=[0,0.03,1,0.95])\nplt.show()\n\n\nWe can tell easily from the above visualization that both scenarios are harmful: underfitting prevents the model from making meaningful predictions, while overfitting results in unreliable predictions on real-world data. The example above also highlights the necessity to validate classifier quality using test data.\nWhile underfitting and overfitting can seem scary, there are many tools that have been developed to address this issue. We can adjust the complexity of the model, use regularization techniques, collect more data, or employ cross-validation to find a balance that generalizes well to new examples. These are left for you to explore on your own.\n\n\nReferences\n\nRichter, G. M. A. (1970). Kouroi: Archaic Greek youths: A study of the development of the Kouros type in Greek sculpture. Phaidon. Accessed through Internet Archive https://archive.org/details/kouroiarchaicgre0000rich.\nPinecone. Embedding Methods for Image Search. Accessed through Pinecone https://www.pinecone.io/learn/series/image-search/.\nIBM. What are convolutional neural networks? https://www.ibm.com/think/topics/convolutional-neural-networks\nHugging Face. Image Classification. https://huggingface.co/docs/transformers/tasks/image_classification\nColeman, C., Lyon, S., & Perla, J. (2020). Introduction to Economic Modeling and Data Science. QuantEcon. Retrieved from https://datascience.quantecon.org/\nWoo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I. S., & Xie, S. (2024). ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders. arXiv preprint arXiv:2301.00808v1."
  },
  {
    "objectID": "docs/HIST-414/NLI/NLI.html",
    "href": "docs/HIST-414/NLI/NLI.html",
    "title": "Text Embeddings for Regina V Wing Chong (1885)",
    "section": "",
    "text": "# Data Wrangling\nimport os\nimport numpy as np\nimport pandas\nfrom nltk.tokenize import word_tokenize\n\n\nwith open('data/Regina_V_Wing_Chong.txt', encoding='utf-8') as f:\n    full_text = f.read()\nprint(full_text)\n\n\nBERT Word Embeddings\n\nimport re\n\ndef clean_text(text):\n    \n    text = text.lower()\n    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n    \n    return text.strip()\n\ntext_cleaned = clean_text(full_text)\nprint(text_cleaned[:500])  # Print the first 500 characters of the cleaned text\n\n\n# Load pre-trained BERT tokenizer and model\nfrom transformers import BertTokenizer, BertModel\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\nbert_model = BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')\n\n\n# Create the word embeddings\n# Tokenize the cleaned text into words\ntokens = word_tokenize(text_cleaned)\n\ntoken_frequencies = {}\n\nfor token in tokens:\n    token_frequencies[token] = token_frequencies.get(token, 0) + 1\n\n\nsorted_tokens = sorted(token_frequencies.items(), key=lambda x: x[1], reverse=True)\n\n# Example: print top 10 most frequent tokens\nprint(\"Most frequent tokens:\")\nfor token, freq in sorted_tokens[:20]:\n    print(f\"{token}: {freq}\")\n\n\nimport re\n# Build ethnicity vocabulary\nethnicities = [\n    \"chinese\", \"japanese\", \"black\", \"white\", \"yellow\", \"chinamans\", \"hong kong\",\n    \"canada\", \"american\", \"americans\", \"european\", \"china\", \"chinaman\", \"britain\",\n    \"canadian\", \"latino\", \"mongolian\", \"asian\", \"indian\", \"india\", \"english\",\n    \"british\", \"america\", \"columbia\", \"ontario\", \"australia\", \"australian\",\n    \"germans\", \"german\", \"chinamen\", \"italian\", \"italy\", \"french\", \"france\"\n]\n\npattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, ethnicities)) + r\")\\b\", flags=re.IGNORECASE)\n\n# Mask in any string\ndef mask_ethnicity(tokens):\n    masked = []\n    for tok in tokens:\n        masked.append(pattern.sub(\"[MASK]\", tok))\n        \n    return masked\n\n\nexample_word = [\"chinaman\", \"chinese women\"]\n\nmask_ethnicity(example_word)\n\n\ntokens = mask_ethnicity(tokens)\n\n# Get unique words to avoid redundant computation\nunique_tokens = list(set(tokens))\n\n\n# Include the word \"chinese\" as our target\nunique_tokens.append(\"chinese\")\n\n# Print the shape of unique tokens\nprint(f'There are {len(unique_tokens)} unique tokens in this corpus.')\n\n\n# Prepare a dictionary to store word embeddings\nbert_word_embeddings = {}\n\n# For each word, get its BERT embedding by feeding it as a single-token input\nfor word in unique_tokens:\n    word_inputs = tokenizer(word, return_tensors='pt', truncation=True, max_length=10)\n    with torch.no_grad():\n        word_outputs = bert_model(**word_inputs)\n        # Use the [CLS] token embedding as the word embedding\n        word_embedding = word_outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n        bert_word_embeddings[word] = word_embedding\n\n\n# Print embedding for the word of interest 'chinese'\n\nprint(f\"BERT embedding for 'chinese':\\n{bert_word_embeddings.get('chinese')}\")\n\n\n# Compute cosine similarity between all words with Chinese in the model\nfrom scipy.spatial.distance import cosine\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    if other_word != \"chinese\":\n        similarity = 1 - cosine(bert_word_embeddings[\"chinese\"], bert_word_embeddings[other_word])\n        similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'chinese':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    if other_word != \"commerce\":\n        similarity = 1 - cosine(bert_word_embeddings[\"commerce\"], bert_word_embeddings[other_word])\n        similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'commerce':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\nemd = np.array(bert_word_embeddings.get('chinese')) - np.array(bert_word_embeddings.get('alien'))\n\nsimilarity_scores = {}\n\nfor other_word in bert_word_embeddings.keys():\n    similarity = 1 - cosine(emd, bert_word_embeddings[other_word])\n    similarity_scores[other_word] = similarity\n\n# Sort by cosine similarity\nsorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n\n# Print the top 10 most similar words\nprint(\"Top 10 most similar words to 'chinese - alien':\")\nfor word, score in sorted_similarity[:10]:\n    print(f\"{word}: {score:.4f}\")\n\n\n# Generate a 2D PCA for visualiaztion\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\n\nword_embeddings = np.array(list(bert_word_embeddings.values()))\npca_results = pca.fit_transform(word_embeddings)\n\n\nimport plotly.express as px\ndf_pca = pandas.DataFrame(pca_results, columns = ['x', 'y'])\ndf_pca['word'] = list(bert_word_embeddings.keys())\n# Highlight the word 'chinese' in the plot\ndf_pca['highlight'] = df_pca['word'].apply(lambda x: 'chinese' if x == 'chinese' else '')\n\nfig = px.scatter(\n    df_pca,\n    x='x',\n    y='y',\n    title=' Visualization of 2D PCA of the legal-BERT Word Embeddings',\n    color='highlight',                        \n    hover_data=['word'], \n    text= 'highlight'\n)\n\nfig.show()\n\n\n# Generate a t-SNE plot for visualization\nfrom sklearn.manifold import TSNE\ntsne = TSNE(n_components=2, random_state=42)\n\ntsne_results = tsne.fit_transform(word_embeddings)\n\n\n# Create a DataFrame for visualization\ndf_tsne = pandas.DataFrame(tsne_results, columns=['x', 'y'])\ndf_tsne['word'] = list(bert_word_embeddings.keys())\n# Highlight the word 'chinese' in the plot\ndf_tsne['highlight'] = df_tsne['word'].apply(lambda x: 'chinese' if x == 'chinese' else '')\n\nfig = px.scatter(\n    df_tsne,\n    x='x',\n    y='y',\n    title='t-SNE Visualization of legal-BERT Word Embeddings',\n    color='highlight',                        \n    hover_data=['word'], \n    text= 'highlight'\n)\n\nfig.show()\n\n\n\nSentence Embeddings\n\nfrom pathlib import Path\n\n# Read the txt file as lines\nlines = Path(\"data/Regina_V_Wing_Chong.txt\").read_text(encoding=\"utf-8\").splitlines()\n\n# Extract line 67 as the target\ntarget = lines[91]\nprint(\"Line 91:\", target)\n\n\nparagraphs = [p.strip() for p in full_text.split(\"\\n\\n\") if p.strip()]\n\nfor paragraph in paragraphs[:5]:\n    print(paragraph)\n\n\nfrom sentence_transformers import SentenceTransformer\n\n# Import the sentence transformer model\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n# Calculate embeddings by calling model.encode()\nparagraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)\nprint(paragraph_embeddings.shape)\n\n\n# We also want to encode the target separately\ntarget_embedding = model.encode(target, convert_to_tensor=True)\n\n\nimport torch\nfrom torch.nn.functional import cosine_similarity\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), paragraph_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\ntop_paragraphs = []\n\nfor score, idx in zip(topk.values, topk.indices):\n    top_paragraphs.append(paragraphs[idx])\n    print(f\"{score:.4f}\\t{paragraphs[idx]}\")\n\n\nimport spacy\n\n# Tokenize the text into sentences\nnlp = spacy.load(\"en_core_web_sm\")\ndoc = nlp(full_text)\nsentences = [sent.text.strip() for sent in doc.sents]\n\nprint(sentences)\n\n\nsentence_embeddings = model.encode(sentences, convert_to_tensor=True)\nprint(sentence_embeddings.shape)\n\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), sentence_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\nfor score, idx in zip(topk.values, topk.indices):\n    print(f\"{score:.4f}\\t{sentences[idx]}\")\n\nWe apply a trained model to mask key words related to ethnicity and nationality identities.\n\nfrom transformers import pipeline\nimport numpy\n\nner = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\", grouped_entities=True)\n\ndef mask_ethnicity_hf(text):\n    entities = ner(text)\n    spans_to_mask = [e for e in entities if e[\"entity_group\"] == \"MISC\" or e[\"entity_group\"] == \"ORG\" or e[\"entity_group\"] == \"PER\" or e[\"entity_group\"] == \"LOC\" or e[\"entity_group\"] == \"NORP\"]\n    # typically nationality is in MISC or NORP depending on the model\n    masked = text\n    for ent in sorted(spans_to_mask, key=lambda e: e[\"start\"], reverse=True):\n        masked = masked[:ent[\"start\"]] + \"[MASK]\" + masked[ent[\"end\"]:]\n    return masked\n\n\ndef mask_ethnicity(texts):\n    masked_list = []\n    for sent in texts:\n        sent = mask_ethnicity_hf(sent)\n        masked_list.append(sent)\n        \n    return masked_list\n\n\n# Example output applying this pre-trained model\nexample_text = \"\"\"And when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, \nfrom every state and every city, we will be able to speed up that day when all of God's children, Black men and white men, \nJews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. \nFree at last. Thank God almighty, we are free at last.\"\"\"\n\nmasked_example = mask_ethnicity_hf(example_text)\n\nprint(masked_example)\n\n\nmasked_paragraphs = mask_ethnicity(paragraphs)\n\nmasked_paragraphs[40]\n\n\n# Calculate embeddings by calling model.encode()\nmasked_paragraph_embeddings = model.encode(masked_paragraphs, convert_to_tensor=True)\nprint(masked_paragraph_embeddings.shape)\n\n\n# Calculate the cosine similarity\nsims = cosine_similarity(target_embedding.unsqueeze(0), masked_paragraph_embeddings)\n\nk = min(10, sims.shape[0])\n\ntopk = torch.topk(sims, k=k-1)\n\ntop_masked_paragraphs = []\n\nfor score, idx in zip(topk.values, topk.indices):\n    top_masked_paragraphs.append(masked_paragraphs[idx])\n    print(f\"{score:.4f}\\t{masked_paragraphs[idx]}\")\n\n\n\nNatural Language Inference\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n\n# Choose a strong NLI model\nmodel_name = \"lexlms/legal-roberta-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel     = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# Create an NLI pipeline\nnli = pipeline(\n    \"text-classification\",\n    model=model,\n    tokenizer=tokenizer,\n    device=-1,                   \n    return_all_scores=True        \n)\n\n# Define the premise\npremise = \"Chinese immigrants should enjoy equal rights and legal protections.\"\n\nresults = []\nfor sent in top_masked_paragraphs:\n    inputs = tokenizer.encode_plus(premise, sent, return_tensors=\"pt\", truncation=True)\n    out = model(**inputs).logits.softmax(dim=-1).tolist()[0]\n    label_idx = out.index(max(out))\n    label = [\"FAVOR\", \"NEUTRAL\", \"AGAINST\"][label_idx]\n    results.append((sent, label, dict(zip([\"favor\",\"neutral\",\"against\"], out))))\n\n# Print stance results\nfor sent, label, probs in results:\n    print(f\"{label.lower():&gt;12}  {probs[label.lower()]:.2f}  -&gt;  {sent}\")\n\n\nfrom transformers import pipeline\nimport pandas as pd\n\n# Load the MNLI‑based zero‑shot classifier\nclassifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"facebook/bart-large-mnli\",\n    device=-1\n)\n\n# Use the NLI labels as your “candidate labels”\ncandidate_labels = [\"entailment\", \"neutral\", \"contradiction\"]\n\nrecords = []\nfor para in paragraphs:\n    out = classifier(\n        sequences=para,\n        candidate_labels=candidate_labels,\n        hypothesis_template=\"Given the context that the texts for classification are from a legal ruling in 1885, this paragraph is {} of the premise 'Chinese immigrants should enjoy equal rights and legal protections'.\"\n    )\n    # out['labels'] is sorted by score descending\n    scores = dict(zip(out[\"labels\"], out[\"scores\"]))\n    pred = out[\"labels\"][0]\n\n    records.append({\n        \"paragraph\": para,\n        \"entailment\":   scores.get(\"entailment\", 0.0),\n        \"neutral\":      scores.get(\"neutral\",    0.0),\n        \"contradiction\":scores.get(\"contradiction\", 0.0),\n        \"predicted\":    pred\n    })\n\n#  Build a DataFrame\ndf_nli = pd.DataFrame(records)\n\n# Inspect the first few rows\nprint(df_nli.head())\n\n\ndf_nli.shape\n\n\ncounts = df_nli['predicted'].value_counts()\n\nproportions = df_nli['predicted'].value_counts(normalize=True)\n\nresult = pd.DataFrame({\n    'count': counts,\n    'proportion': proportions\n})\n\nprint(result)\n\n\n\nTopic Modelling Through BERTopic\n\nfrom bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\nctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\nvectorizer = CountVectorizer(stop_words=\"english\", ngram_range=(1,2), max_df=0.85, min_df=2)\n\ntopic_model = BERTopic(\n    vectorizer_model=vectorizer, \n    ctfidf_model= ctfidf_model\n)\n\ntopics, probs = topic_model.fit_transform(paragraphs)\n\ndf_topic = topic_model.get_topic_info()\nprint(df_topic)\n\n\nfor para in df_topic[\"Representative_Docs\"][1]:\n    print(para)\n\n\nrep_list = []\n\nfor list in df_topic[\"Representation\"]:\n    rep_list.extend(list)\n    \nprint(rep_list)\n\n\ntopic_labels = topic_model.generate_topic_labels(\n    nr_words=3,       \n    separator=\" \",     \n    topic_prefix=False \n)\n\ntopic_list = []\n\nfor label in topic_labels:\n    phrases = label.split(\" \")\n    topic_list.extend(phrases)\n    \nprint(topic_list)\n\n\nfrom umap import UMAP\nimport pandas as pd\nimport plotly.express as px\n\n# Get your topic-term embeddings\nembeddings = topic_model.c_tf_idf_.toarray()\n\n# 1) Build a UMAP reducer with random init\numap_model = UMAP(\n    n_neighbors=15,\n    n_components=2,\n    metric=\"cosine\",\n    init=\"random\",\n    random_state=42\n)\nreduced_embeddings = umap_model.fit_transform(embeddings)\n\n# 2) Build a DataFrame and scatter\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\ndf[\"topic\"] = topic_model.get_topic_info()[\"Topic\"].values\n\nfig = px.scatter(\n    df,\n    x=\"x\",\n    y=\"y\",\n    text=\"topic\",\n    title=\"Topic visualization\"\n)\nfig.show()\n\n\nfrom sentence_transformers import SentenceTransformer\n\nembedding_model = SentenceTransformer(\"nlpaueb/legal-bert-base-uncased\")\n\n\nfrom bertopic import BERTopic\nfrom bertopic.vectorizers import ClassTfidfTransformer\n\ntopic_model = BERTopic(embedding_model=embedding_model,\n                       vectorizer_model= vectorizer,\n                       ctfidf_model= ctfidf_model)\n\ntopics, probs = topic_model.fit_transform(masked_paragraphs)\n\ndf_topic = topic_model.get_topic_info()\nprint(df_topic)\n\n\nfor para in df_topic['Representative_Docs']:\n    print(para)\n\n\ntopic_labels = topic_model.generate_topic_labels(\n    nr_words=5,       \n    separator=\" \",     \n    topic_prefix=False \n)\n\ntopic_labels\n\n\nfrom umap import UMAP\nimport pandas as pd\nimport plotly.express as px\n\n# Get your topic-term embeddings\nembeddings = topic_model.c_tf_idf_.toarray()\n\n# 1) Build a UMAP reducer with random init\numap_model = UMAP(\n    n_neighbors=15,\n    n_components=2,\n    metric=\"cosine\",\n    init=\"random\",\n    random_state=42\n)\nreduced_embeddings = umap_model.fit_transform(embeddings)\n\n# 2) Build a DataFrame and scatter\ndf = pd.DataFrame(reduced_embeddings, columns=[\"x\", \"y\"])\ndf[\"topic\"] = topic_model.get_topic_info()[\"Topic\"].values\n\nfig = px.scatter(\n    df,\n    x=\"x\",\n    y=\"y\",\n    text=\"topic\",\n    title=\"Topic visualization\"\n)\nfig.show()"
  },
  {
    "objectID": "docs/HIST-414/NLI/development/word_embeddings_irene_attempt.html",
    "href": "docs/HIST-414/NLI/development/word_embeddings_irene_attempt.html",
    "title": "text embeddings + conceptual axes",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport torch\nfrom transformers import AutoTokenizer, AutoModel\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sentence_transformers import SentenceTransformer\nimport torch\nimport torch.nn.functional as F\nimport textwrap\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n\ndef mean_pooling(model_output, attention_mask):\n    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n\ntokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B')\nmodel = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')\n\nmodel.eval()\n\n\naxes_map = {'social_issues': ['justice', 'equality', 'rights', 'fairness', 'discrimination', 'prejudice', 'persecution', 'humanity', 'dignity', 'compassion', 'liberty', 'protection', 'civil rights', 'human rights', 'alien', 'person', 'power', 'unconstitutional'],\n            'economics': ['economy', 'economic', 'labor', 'commerce', 'trade', 'industry', 'railway', 'development', 'profit', 'wages', 'capital', 'prosperity', 'cheap labor', 'economic necessity', 'business', 'license', 'tax', 'fee', 'revenue', 'labour', 'employment', 'wage', 'miner', 'mining', 'land', 'goods', 'property', 'economic', 'industry', 'regulate', 'forfeit', 'penalty', 'pay']}\n\n\ndf = pd.read_csv(\"../data/metadata.csv\")\n\ndf.head()\n\n\ncrease_texts = df[df['author'] == 'Crease']['text'].tolist()\nbegbie_texts = df[df['author'] == 'Begbie']['text'].tolist()\nother_texts = df[df['author'] =='Others']['text'].tolist() # the chinese regulation act -- much more racist\n\njudge_dict = {\n    'Crease': crease_texts,\n    'Begbie': begbie_texts,\n    'other': other_texts}\n\n\nimport torch\n\nimport torch\n\ndef get_mean_embedding(text, model, tokenizer):\n\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n    \n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    last_hidden_state = outputs.last_hidden_state\n    attention_mask = inputs['attention_mask']s\n    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape).float()\n    masked_embeddings = last_hidden_state * mask\n    \n    summed_embeddings = torch.sum(masked_embeddings, dim=1)\n    summed_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n    mean_embedding = summed_embeddings / summed_mask\n\n    return mean_embedding.squeeze().cpu().numpy()\n\n\ndef get_token_embedding(text, model, tokenizer):\n    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=32768, padding=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    \n    attention_mask = inputs['attention_mask']\n    last_token_index = attention_mask.sum(dim=1) - 1\n    \n    batch_size = outputs.last_hidden_state.shape[0]\n    last_token_index = last_token_index.unsqueeze(-1).expand(-1, outputs.last_hidden_state.shape[-1]).unsqueeze(1)\n    last_token_embedding = torch.gather(outputs.last_hidden_state, 1, last_token_index).squeeze(1)\n\n    return last_token_embedding.squeeze().cpu().numpy()\n\n\nimport numpy as np\n\ndef create_axis_vector(keywords, corpus_texts, model, tokenizer, get_embedding_for_text):\n    context_sentences = []\n\n    for doc in corpus_texts:\n        sentences = [s.strip() for s in doc.split('.') if len(s.strip()) &gt; 10]\n        for sentence in sentences:\n            if any(keyword in sentence.lower() for keyword in keywords):\n                context_sentences.append(sentence)\n\n    context_embeddings = [get_embedding_for_text(sent, model, tokenizer) for sent in context_sentences]\n    axis_vector = np.mean(context_embeddings, axis=0)\n    axis_vector_normalized = axis_vector / np.linalg.norm(axis_vector)\n    \n    return axis_vector_normalized\n\n\nall_texts = df['text'].tolist()\n\ncontextual_social_axis = create_axis_vector(keywords=axes_map['social_issues'],corpus_texts=all_texts,model=model,tokenizer=tokenizer,get_embedding_for_text=get_token_embedding )\n\ncontextual_economic_axis = create_axis_vector(keywords=axes_map['economics'],corpus_texts=all_texts,model=model,tokenizer=tokenizer,get_embedding_for_text=get_token_embedding )\n\n\nresults = []\nprint(\"Processing texts and projecting onto axes...\")\n\nfor judge, texts in judge_dict.items():\n    for text in texts:\n        sentences = [s.strip() for s in text.split('.') if len(s.strip()) &gt; 10] \n\n        for sentence in sentences:\n            sentence_embedding = get_token_embedding(sentence, model, tokenizer)\n        \n            sentence_vec = sentence_embedding.reshape(1, -1)\n            social_axis_vec = contextual_social_axis.reshape(1, -1)\n            econ_axis_vec = contextual_economic_axis.reshape(1, -1)\n\n            social_score = cosine_similarity(sentence_vec, social_axis_vec)[0][0]\n            econ_score = cosine_similarity(sentence_vec, econ_axis_vec)[0][0]\n            \n            results.append({'Judge': judge,\n                'Sentence': sentence,\n                'Social_Score': social_score,\n                'Econ_Score': econ_score})\nresults_df = pd.DataFrame(results)\n\nprint(results_df.head())\n\n\nresults_df = pd.DataFrame(results)\n\naggregate_scores = results_df.groupby('Judge')[['Social_Score', 'Econ_Score']].mean()\n\n\ng = sns.displot(\n    data=results_df,\n    x='Econ_Score',\n    y='Social_Score',\n    col='Judge',        \n    kind='kde',         # Using Kernel Density Estimate \n    fill=True,          \n    cbar=True,         \n    height=6,           \n    aspect=1           \n)\n\nfor i, judge_name in enumerate(aggregate_scores.index):\n    ax = g.axes.flat[i]\n    avg_scores = aggregate_scores.loc[judge_name]\n    \n    ax.scatter(\n        avg_scores['Econ_Score'],\n        avg_scores['Social_Score'],\n        s=400,\n        marker='*',\n        color='white',\n        edgecolor='black',\n        label=f'Average ({judge_name})' )\n    ax.legend()\n\ng.fig.suptitle('Judicial Language Density on Concept Axes', y=1.03, fontsize=16)\ng.set_axis_labels('Alignment with Economic Interests', 'Alignment with Social Justice', fontsize=12)\n\nplt.show()\n\n\njudges = results_df['Judge'].unique()\n\npalette = sns.color_palette()\n\nfor i, judge_name in enumerate(judges):\n    fig, ax = plt.subplots(figsize=(10, 8))\n    judge_data = results_df[results_df['Judge'] == judge_name]\n    sns.scatterplot(\n        data=judge_data,\n        x='Econ_Score',\n        y='Social_Score',\n        s=100,\n        alpha=0.7,\n        ax=ax,\n        color=palette[i % len(palette)],\n        label='Individual Decisions')\n    \n    avg_scores = aggregate_scores.loc[judge_name]\n    ax.scatter(\n        avg_scores['Econ_Score'], \n        avg_scores['Social_Score'], \n        s=400, \n        marker='*', \n        edgecolor='black', \n        label=f'Average ({judge_name})',\n        c=[palette[i % len(palette)]])\n    \n    ax.set_title(f'Judge {judge_name}', fontsize=16)\n    ax.set_xlabel('Alignment with Economic Interests', fontsize=12)\n    ax.set_ylabel('Alignment with Social Justice', fontsize=12)\n    ax.legend()\n    ax.grid(True)\n    \n    plt.show()\n\n\nfrom transformers import pipeline \nprint(\"Loading sentiment analysis model...\")\nsentiment_pipeline = pipeline(\n    'sentiment-analysis', \n    model='distilbert-base-uncased-finetuned-sst-2-english'\n)\n\nsentiments = []\n\nfor sentence in tqdm(results_df['Sentence']):\n    result = sentiment_pipeline(sentence)[0]\n    \n    score = result['score']\n    if result['label'] == 'NEGATIVE':\n        score = -score\n    else: score = score\n    sentiments.append(score)\n\nresults_df['Sentiment'] = sentiments\n\nprint(\"\\nSentiment analysis complete.\")\nprint(results_df.head())\n\n\nprint(results_df.groupby('Judge')[['Econ_Score', 'Social_Score', 'Sentiment']].count())\n\n\nprint(pd.crosstab(results_df['Judge'], results_df['Sentiment']))\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\ndef plot_score_distribution(ax, data, score_col, title):\n    sentiment_palette = {1: \"green\", -1: \"red\"} \n    \n    sns.violinplot(\n        data=data,\n        x='Judge',\n        y=score_col,\n        hue='Sentiment',\n        split=False,\n        inner='quartile',\n        palette=sentiment_palette,\n        linewidth=1.5,\n        ax=ax\n    )\n    \n    ax.set_title(title, fontsize=16, weight='bold')\n    ax.set_ylabel(f\"{score_col.replace('_', ' ')}\", fontsize=12)\n    ax.set_xlabel(None) \n    ax.get_legend().remove() \n    \nsns.set_theme(style=\"whitegrid\", font_scale=1.1)\nfig, axes = plt.subplots(1, 2, figsize=(20, 8), sharey=True)\nfig.suptitle('Distribution of Judicial Scores by Sentiment', fontsize=20, weight='bold')\n\n\nplot_score_distribution(\n    ax=axes[0], \n    data=results_df, \n    score_col='Econ_Score', \n    title='Economic Score Analysis')\nplot_score_distribution(\n    ax=axes[1], \n    data=results_df, \n    score_col='Social_Score', \n    title='Social Score Analysis')\n\nfig.text(0.5, 0.02, 'Judge', ha='center', va='center', fontsize=14, weight='bold')\nhandles, labels = axes[0].get_legend_handles_labels()\nfig.legend(handles, ['Negative', 'Positive'], title='Sentiment', loc='upper right', bbox_to_anchor=(0.98, 0.95))\nsns.despine(left=True, bottom=True)\nplt.tight_layout(rect=[0, 0.05, 1, 0.95])\nplt.show()\n\n\nneg_df = pd.read_csv(\"words_negative.csv\") \npos_df = pd.read_csv(\"words_positive.csv\")\nnegative_words = neg_df.iloc[:, 0].dropna().tolist()\npositive_words = pos_df.iloc[:, 0].dropna().tolist()\n\nsentiment_map = {\n    \"negative\": negative_words,\n    \"positive\": positive_words}\n\n\npos_axis = create_axis_vector(keywords=sentiment_map['positive'],corpus_texts=all_texts,model=model,tokenizer=tokenizer,get_embedding_for_text=get_token_embedding )\n\nneg_axis = create_axis_vector(keywords=sentiment_map['negative'],corpus_texts=all_texts,model=model,tokenizer=tokenizer,get_embedding_for_text=get_token_embedding )\n\n\nresults = []\nprint(\"Processing texts and projecting onto axes...\")\n\nfor judge, texts in judge_dict.items():\n    for text in texts:\n        sentences = [s.strip() for s in text.split('.') if len(s.strip()) &gt; 10] \n\n        for sentence in sentences:\n            sentence_embedding = get_token_embedding(sentence, model, tokenizer)\n        \n            sentence_vec = sentence_embedding.reshape(1, -1)\n            pos_axis_vec = pos_axis.reshape(1, -1)\n            neg_axis_vec = neg_axis.reshape(1, -1)\n\n            positive_score = cosine_similarity(sentence_vec, pos_axis_vec)[0][0]\n            negative_score = cosine_similarity(sentence_vec, neg_axis_vec)[0][0]\n            \n            results.append({'Judge': judge,\n                'Sentence': sentence,\n                'Positive_Score': positive_score,\n                'Negative_Score': negative_score})\nresults_sent = pd.DataFrame(results)\n\nprint(results_sent.head())\n\n\nall_scores = pd.merge(results_df, results_sent, on=['Judge', 'Sentence'])\nall_scores['Sentiment_Score'] = all_scores['Positive_Score'] - all_scores['Negative_Score']\n\nfinal_df = all_scores[['Judge', 'Sentence', 'Econ_Score', 'Social_Score', 'Sentiment_Score']]\nfinal_df = final_df.rename(columns={\n    'Econ_Score': 'Economic_Alignment_Score',\n    'Social_Score': 'Social_Score'})\n\nprint(final_df.head())\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, 1))\nfinal_df['Sentiment_Normalized'] = scaler.fit_transform(final_df['Sentiment_Score'].values.reshape(-1, 1))\n\ncrease_df = final_df[final_df['Judge'] == 'Crease']\nbegbie_df = final_df[final_df['Judge'] == 'Begbie']\nother_df = final_df[~final_df['Judge'].isin(['Crease', 'Begbie'])]\n\nglobal_sentiment_min = final_df['Sentiment_Score'].min()\nglobal_sentiment_max = final_df['Sentiment_Score'].max()\n\ndatasets_to_plot = {\n    'Crease': crease_df,\n    'Begbie': begbie_df,\n    'Other Judges': other_df}\n\nfor name, data in datasets_to_plot.items():\n    plt.figure(figsize=(10, 8))\n    \n    ax1 = sns.scatterplot(\n        data=data,\n        x='Economic_Alignment_Score',\n        y='Social_Score',\n        hue='Sentiment_Normalized',  \n        palette='RdYlGn',\n        s=40,\n        legend=False \n\n    ax1.set_title(f'Semantic Map for {name}', fontsize=16, pad=20)\n    ax1.set_xlabel('Economic Alignment', fontsize=12)\n    ax1.set_ylabel('Social Alignment', fontsize=12)\n    ax1.axhline(0, color='grey', linestyle='--', linewidth=1)\n    ax1.axvline(0, color='grey', linestyle='--', linewidth=1)\n    \n    norm = plt.Normalize(vmin=global_sentiment_min, vmax=global_sentiment_max)\n    sm = plt.cm.ScalarMappable(cmap=\"RdYlGn\", norm=norm)\n    sm.set_array([])\n    \n    cbar = plt.colorbar(sm, ax=ax1)\n    cbar.set_label('Original Sentiment Score', size=12)\n\n    plt.tight_layout()\n    plt.show()\n\n\nfig = px.scatter(\n    final_df,\n    x='Economic_Alignment_Score',\n    y='Social_Score',\n    color='Sentiment_Score',\n    color_continuous_scale='RdYlGn',  # Red-Yellow-Green color scale\n    range_color=[-0.02, 0.02], # You can adjust this range to better fit your data's spread\n    title='Interactive Semantic Map of Sentences',\n    # This part defines what appears when you hover over a point\n    hover_data={\n        'Sentence': True, # Show the full sentence text\n        'Economic_Alignment_Score': ':.3f', # Format score to 3 decimal places\n        'Social_Score': ':.3f',\n        'Sentiment_Score': ':.3f'\n    }\n)\n\n# Customize the hover label appearance\nfig.update_traces(hoverlabel=dict(bgcolor=\"white\", font_size=12))\n\n# Add quadrant lines for context\nfig.add_vline(x=0, line_width=1, line_dash=\"dash\", line_color=\"grey\")\nfig.add_hline(y=0, line_width=1, line_dash=\"dash\", line_color=\"grey\")\n\n# Update layout for a cleaner look\nfig.update_layout(\n    xaxis_title=\"Economic Alignment\",\n    yaxis_title=\"Social Alignment\",\n    coloraxis_colorbar_title_text='Sentiment'\n)\n\nfig.show()\n\n\nimport umap\n\n# --- UMAP and Plotting ---\nnumeric_cols = ['Economic_Alignment_Score', 'Social_Score', 'Sentiment_Score']\ndata_for_umap = final_df[numeric_cols].values\nreducer = umap.UMAP(n_components=2, random_state=42)\nembedding = reducer.fit_transform(data_for_umap)\nfinal_df['UMAP_1'] = embedding[:, 0]\nfinal_df['UMAP_2'] = embedding[:, 1]\n\nplt.figure(figsize=(10, 8))\nax2 = sns.scatterplot(\n    data=final_df,\n    x='UMAP_1',\n    y='UMAP_2',\n    hue='Sentiment_Normalized',\n    palette='RdYlGn',\n    s=150\n)\nax2.set_title('UMAP Projection (with Normalized Color)', fontsize=16, pad=20)\nax2.set_xlabel('UMAP Component 1', fontsize=12)\nax2.set_ylabel('UMAP Component 2', fontsize=12)\n\n# Manually create and add the colorbar\nnorm = plt.Normalize(final_df['Sentiment_Score'].min(), final_df['Sentiment_Score'].max())\nsm = plt.cm.ScalarMappable(cmap=\"RdYlGn\", norm=norm)\nsm.set_array([])\n\n# Remove the original legend created by seaborn\nax2.get_legend().remove()\n\n# Add the new colorbar, specifying the axes\ncbar = ax2.figure.colorbar(sm, ax=ax2) # &lt;-- THIS IS THE CORRECTED LINE\ncbar.set_label('Original Sentiment Score', size=12)\n\nplt.tight_layout()\nplt.show()\n\n\nsocial_justice_keywords = [\"rights\", \"equality\", \"morality\", \"race\", \"welfare\"]\n\n# Hand-code a small set of keywords for economics\neconomic_keywords = [\"tax\", \"development\", \"trade\", \"business\", \"economy\"]\n\n\nsocial_justice_similarities = {}\nfor keyword in social_justice_keywords:\n    if keyword in word_embeddings:\n        social_justice_similarities[keyword] = {}\n        for other_word, embedding in word_embeddings.items():\n            if other_word != keyword:\n                similarity = 1 - cosine(word_embeddings[keyword], embedding)\n                social_justice_similarities[keyword][other_word] = similarity\n                \n# Sort by similarity score\nsocial_justice_tokens = social_justice_keywords\n\nfor keyword, similarities in social_justice_similarities.items():\n    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n    print(f\"Top 5 most similar words to '{keyword}':\")\n    for word, score in sorted_similarities[:5]:\n        social_justice_tokens.append(word)\n        print(f\"{word}: {score:.4f}\")\n    print()\n\n\nsocial_justice_anchor = np.mean([word_embeddings[word] for word in social_justice_tokens if word in word_embeddings], axis=0)\neconomic_anchor = np.mean([word_embeddings[word] for word in economic_tokens if word in word_embeddings], axis=0)\n\n\ndef align_with_axis(embeddings, anchor, top_n=10):\n    similarities = []\n    for auth, snippets in judge_snippets.items():\n        for snippet, emb in zip(snippets, embeddings_dict[auth]):\n            similarity = cosine_similarity(emb.reshape(1, -1), anchor.reshape(1, -1))[0][0]\n            similarities.append((auth, snippet, similarity))\n    \n    # Sort by similarity score and return top n results with author and snippet\n    sorted_similarities = sorted(similarities, key=lambda x: x[2], reverse=True)\n    return sorted_similarities[:top_n]\n\nsocial_justice_top_sentences = align_with_axis(embeddings_dict, social_justice_anchor)\nprint(\"Top 10 sentences aligned with the social justice axis:\\n\")\nfor auth, snippet, score in social_justice_top_sentences:\n    wrapped_para = textwrap.fill(snippet, width=100)\n    print(f\"Author: {auth}\\nSentence: {wrapped_para}\\nSimilarity Score: {score:.4f}\\n\")\n\n\neconomic_top_sentences = align_with_axis(embeddings_dict, economic_anchor)\n\nprint(\"Top 10 sentences aligned with the economic axis:\\n\")\nfor auth, snippet, score in economic_top_sentences:\n    wrapped_para = textwrap.fill(snippet, width=100)\n    print(f\"Author: {auth}\\nSentence: {wrapped_para}\\nSimilarity Score: {score:.4f}\\n\")\n\n\nimport plotly.graph_objects as go\n\n# Build the alignment DataFrame\nalignment_rows = []\nfor auth, snippets in act_snippets.items():\n    for snippet, emb in zip(snippets, embeddings_dict[auth]):\n        sj_score = cosine_similarity(emb.reshape(1, -1), social_justice_anchor.reshape(1, -1))[0][0]\n        econ_score = cosine_similarity(emb.reshape(1, -1), economic_anchor.reshape(1, -1))[0][0]\n        alignment_rows.append({\n            'Author': auth,\n            'Text': wrap_text(snippet, width=60),\n            'Social Justice Alignment': sj_score,\n            'Economic Alignment': econ_score\n        })\nalignment_df = pd.DataFrame(alignment_rows)\n\n# Compute mean alignment per author\nmean_df = alignment_df.groupby('Author')[['Social Justice Alignment', 'Economic Alignment']].mean().reset_index()\n\n# Base scatter plot of all snippets\nfig = px.scatter(\n    alignment_df,\n    x='Social Justice Alignment',\n    y='Economic Alignment',\n    color='Author',\n    hover_data=['Text'],\n    title='Alignment of Sentences with Social Justice and Economic Axes',\n    labels={\n        'Social Justice Alignment': 'Social Justice Alignment',\n        'Economic Alignment': 'Economic Alignment'\n    },\n    width=800,\n    height=600\n)\n\n# Add mean points for each author\nfor _, row in mean_df.iterrows():\n    fig.add_trace(\n        go.Scatter(\n            x=[row['Social Justice Alignment']],\n            y=[row['Economic Alignment']],\n            mode='markers+text',\n            marker=dict(symbol='star', size=10, line=dict(width=1, color='Black')),\n            text=[row['Author'] + ' Mean'],\n            textposition='top center',\n            showlegend=False\n        )\n    )\n\nfig.update_traces(marker=dict(size=6), selector=dict(mode='markers'))\nfig.update_layout(\n    title='Alignment of Sentences with Social Justice and Economic Axes (with Author Means)',\n    xaxis_title='Social Justice Alignment',\n    yaxis_title='Economic Alignment'\n)\n\nfig.show()\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 4))\n\n# Prepare data for boxplots\nauthors = alignment_df['Author'].unique()\nsocial_justice_data = [alignment_df[alignment_df['Author'] == author]['Social Justice Alignment'].values for author in authors]\neconomic_data = [alignment_df[alignment_df['Author'] == author]['Economic Alignment'].values for author in authors]\n\nn_authors = len(authors)\npositions_sj = [i - 0.2 for i in range(1, n_authors + 1)]\npositions_econ = [i + 0.2 for i in range(1, n_authors + 1)]\n\n# Create boxplots\nbp1 = ax.boxplot(social_justice_data, positions=positions_sj, widths=0.3, patch_artist=True, \n                 tick_labels=None, boxprops=dict(facecolor='lightblue'))\nbp2 = ax.boxplot(economic_data, positions=positions_econ, widths=0.3, patch_artist=True,\n                 tick_labels=None, boxprops=dict(facecolor='lightgreen'))\n\n# Set labels\nax.set_xticks(range(1, n_authors + 1))\nax.set_xticklabels(authors)\n\n# Add labels and title\nax.set_xlabel('Author', fontsize=12)\nax.set_ylabel('Alignment Score', fontsize=12)\nax.set_title('Social Justice vs Economic Alignment by Author', fontsize=14)\nax.grid(True, alpha=0.3)\nax.legend([bp1[\"boxes\"][0], bp2[\"boxes\"][0]], ['Social Justice Alignment', 'Economic Alignment'])\n\nplt.tight_layout()\nplt.show()\n\n\nRegressions\n\nprint(results_df)\n\nWe can test to what extent the effect of social score on sentiment depends on the Judge\n\nresults_df['is_crease'] = (results_df['Judge'] == 'Crease').astype(int)\n\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nreg = \"Sentiment ~ Social_Score * is_crease + Econ_Score * is_crease\"\nmodel = smf.ols(reg, data=results_df)\nmodel_fit = model.fit()\nprint(model_fit.summary())\n\n\nresults_df['is_begbie'] = (results_df['Judge'] == 'Begbie').astype(int)\n\nreg = \"Sentiment ~ Social_Score * is_begbie + Econ_Score * is_begbie\"\nmodel = smf.ols(reg, data=results_df)\nmodel_fit = model.fit()\nprint(model_fit.summary())\n\npretty terrible\n\n\nnon-ml tf-idf\n\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\n\nimport nltk\n\nnltk.download('punkt')\nnltk.download('stopwords')\n\nsomeone please fix to use nltk libeary, nltk.download() doesnt work for me\n\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\n\ntry:\n    stopwords.words('english')\nexcept LookupError:\n    nltk.download('stopwords')\n\n# --- Main Function ---\ndef preprocess_text(text_string):\n    \"\"\"\n    Cleans and preprocesses text by:\n    1. Converting to lowercase\n    2. Removing punctuation and numbers\n    3. Tokenizing\n    4. Removing stop words (standard + custom)\n    5. Removing words with 4 or fewer characters\n    \"\"\"\n    # Start with the standard English stop words\n    stop_words = set(stopwords.words('english'))\n    \n    # Add custom domain-specific stop words if needed\n    custom_additions = {'would', 'may', 'act', 'mr', 'sir', 'also', 'upon', 'shall'}\n    stop_words.update(custom_additions)\n    \n    # 1. & 2. Lowercase and remove non-alphabetic characters\n    processed_text = text_string.lower()\n    processed_text = re.sub(r'[^a-z\\s]', '', processed_text)\n    \n    # 3. Tokenize\n    tokens = processed_text.split()\n    \n    # 4. & 5. Filter out stop words AND short words in a single step\n    filtered_tokens = [\n        word for word in tokens \n        if word not in stop_words and len(word) &gt; 4\n    ]\n    \n    # Re-join the words into a single string\n    return \" \".join(filtered_tokens)\n\ndf['processed_text'] = df['text'].apply(preprocess_text)\n\nprint(df)\n\n\nprint(df)\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Assuming 'df' is your pre-existing DataFrame with 'processed_text' and 'author' columns\n# df = pd.read_csv('your_data.csv') \n# df['processed_text'] = df['text'].apply(preprocess_text) # Your preprocessing function\n\n# 1. Create the 'group' column as before\ndf['group'] = 'Other'\ndf.loc[df['author'] == 'Crease', 'group'] = 'Crease'\ndf.loc[df['author'] == 'Begbie', 'group'] = 'Begbie'\n\n# --- The Improved Method ---\n\n# 2. Vectorize the entire corpus of individual documents FIRST\n# This calculates IDF based on word rarity across ALL individual texts.\nvectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 3))\ntfidf_matrix = vectorizer.fit_transform(df['processed_text'])\n\n# 3. Create a new DataFrame with the TF-IDF scores\n# Each row is a document, each column is a word/ngram\nfeature_names = vectorizer.get_feature_names_out()\ntfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n\n# 4. Add the 'group' column to this TF-IDF DataFrame for aggregation\ntfidf_df['group'] = df['group'].values\n\n# 5. Group by author and calculate the MEAN TF-IDF score for each word\n# This finds the words that are, on average, most important for each group.\nmean_tfidf_by_group = tfidf_df.groupby('group').mean()\n\n# 6. Display the top words for each group\nfor group_name in ['Crease', 'Begbie', 'Other']:\n    print(f' author: {group_name}')\n    # Sort the mean scores for the current group\n    top_words = mean_tfidf_by_group.loc[group_name].sort_values(ascending=False).head(10)\n    print(top_words)\n    print(\"\\n\")\n\n\n# 6. Collect top words and arrange them into a side-by-side DataFrame\nlist_of_author_dfs = []\nfor group_name in ['Crease', 'Begbie', 'Other']:\n    # Get the top 10 terms and scores for the current author\n    top_words = mean_tfidf_by_group.loc[group_name].sort_values(ascending=False).head(10)\n    \n    # Convert the Series to a DataFrame\n    top_words_df = top_words.reset_index()\n    \n    # Rename the columns to be specific to this author\n    # This creates one column for the term and one for the score\n    top_words_df.columns = [group_name, f'{group_name}_score']\n    \n    list_of_author_dfs.append(top_words_df)\n\n# Concatenate the list of DataFrames horizontally (axis=1)\nfinal_wide_df = pd.concat(list_of_author_dfs, axis=1)\n\n# Display the final combined DataFrame\nfinal_wide_df"
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html",
    "href": "docs/SOCI-280/soci_270_bert.html",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "",
    "text": "Module adapted from UBC COMET, prepared originally by Anneke Dresselhuis and Irene Berezin, by Irene Berezin, Anna Kovtunenko, Jalen Faddick and the prAxIs UBC team."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#disinformation-in-the-information-age",
    "href": "docs/SOCI-280/soci_270_bert.html#disinformation-in-the-information-age",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Disinformation in the Information Age",
    "text": "Disinformation in the Information Age\nDisinformation is defined as being deliberately false information, created with the intention to mislead it’s reader. Disinformation has been weaponized since the early middle ages: for example, in the 19th century, New Ywork-based newspaper The Sun published a series of articles, about the discovery of life on the Moon, with the purpose of increasing sales of The Sun. The papers claimed that, using a massive telescope, an english astronomer had discovered vegetation, bipedal beavers, and human-like aliens, dubbed “man-bats”, that were four feet tall, had wings, and could fly (Zielinski, 2015). Whether-or-not the great Moon Hoax lead to The Sun becoming a successfull paper remains uncertain; some accounts claim that the series of papers brought The Sun to international fame, however it’s likely that rumors of The Sun’s hoax increasing the paper’s circulation were exaggerated.\n\n\n&lt;img \n  src=\"soci_270_images/disinformation_example_1.png\" \n  alt=\"An example of disinformation spread on twitter urnign voters to vote via twitter\" \n  style=\"width: 100%; height: auto; display: block;\"&gt;\n\n\n&lt;img \n  src=\"soci_270_images/muller_report_1.png\" \n  alt=\"Middle example\" \n  style=\"width: 100%; height: auto; display: block;\"&gt;\n\n\nThis is an example of a (mostly) harmless disinformation campaign; However, disinformation campaigns can, and have, been used instead to sway public opinion on critical matters. For example, during the Cold War, the KGB orchestrated a widespread disinformation campaign, alledging that HIV/AIDs was a bioeapon engineedred by the United States, in an effort to stoke global mistrust of public health authorities and foster anti-americanism (Selvage & Nehring, 2019). A particularly relevant example is political elections: State-sponsored Russian actors have mounted disinformation campaigns in ever single US federal election since 2016, at the latest. In 2016, for instance, the Russian state-sponsored Internet Research Agency (IRA) ran hundreds of facebook and Twitter groups that amplified divisive content, and organized astroturf rallies in key US states, most notably Pennsylvania (Menn and Dave, 2018). The extent to which these coordinated campains influenced the 2016 United-States election remains unclear: initial findings by the DOJ suggested that Russia coordinated a sweeping, large scale multi-million dollar online campaign aimed to praise Donald Trump and disparage Hillary Clinton (Muller, 2019). However, multiple studies have found that even under generous assumptions about presuasion rates, the vote-shifts caused by the IRA’s disinformation campaigns were too small to sway the election’s outcome (Allcot & Gentzkow, 2017, Eady et al., 2023).\nWith the rise of digital platforms and generative AI, the scale, speed, and sophistication of disinformation have grown exponentially. From elections and pandemics to social justice movements and international conflicts, false or misleading content is being spread online to manipulate emotions and polarize public opinion. The challenge today is not just the volume of disinformation, but how convincing and targeted it has become. Former U.S. Director of National Intelligence Avril Haines describes how state-sponsored campaigns, like Russia’s Kremlin, now operate using “a vast multimedia influence apparatus,” including bots, cyber-actors, fake news websites, and social media trolls. Large language models (LLMs) can now generate human-like tweets, comments, and articles at scale. Combined with deepfakes, doppelgänger sites, and AI-generated personas, these tools allow bad actors to craft propaganda that appears authentic, emotionally resonant, and difficult to detect.\nIn this notebook, we’ll use machine learning — specifically, pretrained large language models — to study the language of disinformation in a real dataset of English and Russian-language tweets. These tweets include both propagandist and non-propagandist content."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#learning-outcomes",
    "href": "docs/SOCI-280/soci_270_bert.html#learning-outcomes",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this module, you will be able to better understand disinformation and propaganda techniques using the following computational tools:\n\nSentiment analysis to detect emotional tone (positive, negative, neutral)\nToxicity analysis to identify harmful or aggressive language (e.g., insults, threats)\nStatistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian)\n\nYou’ll learn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than normal political tweets?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\nThrough this analysis, we’ll explore various dimmensions of AI applications, critically examining how it can be used to better understand and detect the patterns of disinformation when working with large amounts of social data."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#key-terms-and-concepts",
    "href": "docs/SOCI-280/soci_270_bert.html#key-terms-and-concepts",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Key Terms and Concepts",
    "text": "Key Terms and Concepts\nDisinformation: Disinformation is generally understood as the intentional spreading of false, misleading, or incorrect information, in order to influence the public or create an outcome, usually political.\nPropaganda: Propaganda is similar to disinformation in it’s intent to spread a cause or doctrine, but can differ in how systematically or overtly it is speread. The two concepts are both forms of misinformation, with propaganda generally being employed to promote a cause.\nLarge Language Model (LLM): A Large Language Model is a langauge model trained on a very large set of text data, accessing the features of the text by converting units of text into quanitative representations that can be used for various tasks, such as chatbotting, or in the case of this notebook, Natural Language Processing.\nNatural Language Processing (NLP): Natural Langauge Processing encompasses a wide variety of techniques and practices wtih the goal of enabling computers to understand, interpret, and produce human language. The key NLP techniques in this notebook are sentiment analysis, a technique that analayzes the relative positivity or negativity of langauge, and toxitcity analyis which analyzes the relative aggressiveness of language.\nBERT: to enable these analyses we will be using two BERT models. BERT is an open-source framework for machine learning, whcih is used for NLP. BERT is well-suited to understanding langauge, rather than generating it, which is what this notebook is concerned with. The specific BERT models we are using are a multi-lingual model, which can analyze tweets in different languages, and a model trained for analyzing tweet sentiments.\nFine Tuning: The multi-lingual model is fine-tuned, or trained specifically on the Russian Disinformation tweet dataset. This is done by inputting a training subset of the data, where the tweets are labeled as Disinformation, into the BERT model to create a model familiar with our data, and well-suited to producing analyses."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#dataset",
    "href": "docs/SOCI-280/soci_270_bert.html#dataset",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Dataset",
    "text": "Dataset\nOur data for this notebook comes from four sources:\n\n1) The analysis data:\nThe data used from our analysis is a combination of four datasets: 1) The Russian troll tweets dataset, AKA the Internet Research Agency (IRA) tweets dataset is a large, verified collection of social media posts created by the Internet Research agency, the Russian state-sponsored troll farm we talked about earlier. The data was collected and provided by FiveThirtyEight (RIP) an America political news website that focused on providing detailed statistical analyses of the political climate in the United States. Compared to other russian troll datasets, IRA dataset is unparalleled in terms of accuracy as because the data labels were directly provided by Twitter’s internal security teams, which identified the IRA troll accounts and turned over to the United States Congress as evidence for the Senate Select Committee on Intelligence’s investigation into foreign election interference. Hence, every single tweet in the dataset is a known, verified russian disinformation account. 2) The sentiment140 dataset is a massive english language datasets of tweets, created by Standford with the purpose of training sentiment analysis detection models. This dataset serves as our english control group. 3) Likewise, theRusentimental dataset is a dataset of modern-day Russian tweets, collected by Russian NLP researchers for sentiment analysis purposes. This serves as our Russian control group. 4) Lastly, we added tweets taken from a collection of known russian disinformation accounts that were active during the start of the Russia-Ukraine War. Researchers selected these tweets based on the accounts’ “association with state-backed media and a history of spreading disinformation and misinformation”.\n\n\n2) The model training data:\nFor our analysis, we trained a tweet propaganda detection model off of the paper “Labeled Datasets for Research on Information Operations”, which provides the same verified collection of disinformation accounts from the IRA, as well as a “control” dataset of legitimate, topically-related accounts, which allowed for a direct, comparative analysis between malicious and organic online posts, within the same conversational political context. To avoid including training data in our in-class analysis, the IRA dataset was randomly split in two, with one half being used for model training and the other half being used for the in-class demo."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#hashtag-analysis",
    "href": "docs/SOCI-280/soci_270_bert.html#hashtag-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "0.1 Hashtag analysis",
    "text": "0.1 Hashtag analysis\nWhat hashtags are being used in tweets from propagandist vs. non-propagandist accounts? For the next part of our data analysis, we’ll find the freqency of the 10 most common hashtags used by the two account types. As you explore the tables below, notice the content and language of the hashtags.\nOur earlier language analysis suggests that most disinformation in this dataset is likely in English. By examining the hashtags used in these tweets, we can better understand the specific words, topics, and narratives that disinformation accounts are trying to amplify.\nTo begin, we’ll extract hashtags from our textual data by finding phrases beginning with the character #, and ignoring hashtags shorter than 10 letters for the sake of making things interesting.\n\n\nCode\ntweets['content'] = tweets['content'].astype(str) # because some of the tweets aren't strings, apparently\ndef extract_hashtags(text):\n    return re.findall(r'#(\\w+)', text.lower())\n\n\ntweets['hashtags'] = tweets['content'].apply(extract_hashtags)\ntweets['hashtags'] = tweets['hashtags'].apply(lambda hashtag_list: [tag for tag in hashtag_list if len(tag) &gt; 10])\n\nprint(tweets[['is_propaganda', 'content', 'hashtags']].head())\n\n\nNow we can visualize the frequency of the 10 most common hashtags longer than 10 characters used by propagandist accounts over time.\n\n🔎 Engage Critically\n❓ Key Questions\n\nWith reference to the timeline of tweets, and the hashtags below, describe some of the main targets of Russian disinformation.\nGiven what you know about disinformation, what are the intentions of these accounts, and what outcomes are they attempting to create?\nWhat do the hashtags not tell us about the disinformation accounts? Where might our ability to conclude the intentions and outcomes of these accounts be limited by the data we have examined?\nWhat additional data could we collect to better understand this type of disinformation.\n\n\n\n\nCode\npropaganda_tweets = tweets[tweets['is_propaganda'] == 1]\npropaganda_hashtags = propaganda_tweets.explode('hashtags')\ntop_20_propaganda = propaganda_hashtags['hashtags'].value_counts().head(10)\n\ntop_prop_list = top_20_propaganda.index.tolist()\ntop_prop_df = propaganda_hashtags[propaganda_hashtags['hashtags'].isin(top_prop_list)]\ntop_prop_df['date'] = pd.to_datetime(top_prop_df['date'], errors='coerce')\ntop_prop_df.dropna(subset=['date'], inplace=True)\n\nprop_weekly = top_prop_df.groupby('hashtags').resample('W', on='date').size().reset_index(name='count')\n\nfig, ax = plt.subplots(1, 1, figsize=(18, 10))\nfig.suptitle('Weekly spread of Top 10 propaganda hashtags over time', fontsize=20)\n\nsns.lineplot(ax=ax, data=prop_weekly, x='date', y='count', hue='hashtags', palette='viridis')\nax.set_title('Propaganda Hashtags (IRA only)', fontsize=16)\nax.set_xlabel('Date (by week)')\nax.set_ylabel('Weekly Count')\nax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\nax.grid(True, which='both', linestyle='--', linewidth=0.5)\n\nplt.tight_layout(rect=[0, 0, 0.9, 0.96])\nplt.show()\n\n\nTo explore how these hashtags trend over time, you can use the interactive tool below. Enter one or more hashtags (separated by commas) to visualize their weekly frequency in the dataset. This can help reveal how certain narratives gain momentum or fade in relevance.\n\n\nCode\nall_hashtags_df = tweets.explode('hashtags')\nall_hashtags_df['date'] = pd.to_datetime(all_hashtags_df['date'], errors='coerce')\nall_hashtags_df.dropna(subset=['date', 'hashtags'], inplace=True)\nall_hashtags_df['hashtags'] = all_hashtags_df['hashtags'].str.lower()\n\n\n\n\nCode\nhashtag_input = widgets.Text(value='news,russia,syria',placeholder='Enter hashtags, separated by commas',description='Hashtags:',layout={'width': '50%'})\n\nplot_output = widgets.Output()\n\ndef update_plot(change):\n    hashtags_to_plot = [tag.strip().lower() for tag in change['new'].split(',') if tag.strip()]\n    with plot_output:\n        clear_output(wait=True)\n        if not hashtags_to_plot:\n            print(\"enter at least one hashtag\")\n            return\n        filtered_data = all_hashtags_df[all_hashtags_df['hashtags'].isin(hashtags_to_plot)]\n\n        if filtered_data.empty:\n            print(\"No data found for the specified hashtags\")\n            return\n        weekly_counts = filtered_data.groupby('hashtags').resample('W', on='date').size().reset_index(name='count')\n        fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n        sns.lineplot(data=weekly_counts, x='date', y='count', hue='hashtags', ax=ax)\n        \n        ax.set_title('Weekly Frequency of Selected Hashtags')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Weekly Mentions')\n        ax.grid(True, linestyle='--', linewidth=0.5)\n        plt.tight_layout()\n        plt.show()\n\nhashtag_input.observe(update_plot, names='value')\n\nprint(\"Enter a comma-separated list of hashtags to see their trends over time.\")\ndisplay(widgets.VBox([hashtag_input, plot_output]))\nupdate_plot({'new': hashtag_input.value})\n\n\n\n🛑 Stop and Reflect\nNow that we have gone through the dataset and examined a variety of its features, take a few minutes and disucss the following questions with a partner or small group.\n\nWhat are the conclusions we have come to regarding disinformation? How are they influenced or limited by the dataset?\nSo far we have only looked at statistical aspects of the data without using machine learning techniques. Make some predictions on how the machine learning techniques we will use next might change our understanding of online disinformation. How might machine learning, specifically NLP be used to enrich our understanding of disinformation?\nHas your understanding of online disinformation changed after looking at this data? Write down a few questions you have about online disinformation, the dataset, or the computational methods we have been using."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#can-we-tune-models-to-detect-online-disinformation-campaigns-classifying-current-tweets-with-a-model-finetuned-on-the-russian_disinformation_tweets-dataset",
    "href": "docs/SOCI-280/soci_270_bert.html#can-we-tune-models-to-detect-online-disinformation-campaigns-classifying-current-tweets-with-a-model-finetuned-on-the-russian_disinformation_tweets-dataset",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "1. Can we tune models to detect online disinformation campaigns? Classifying current tweets with a model finetuned on the russian_disinformation_tweets dataset",
    "text": "1. Can we tune models to detect online disinformation campaigns? Classifying current tweets with a model finetuned on the russian_disinformation_tweets dataset\n\nExploring Our Model\nNow that we have examined our data and looked at some of the key features in the Russian Disinformation Dataset, we can start thinking about ways to use machine learning to answer questions, classify features, and make predictions about our dataset. To do any of these tasks we first require a way to interpret the text data and assign numeric qualities to our tweets.\nThe model we are using to do this is a multilingual model which maps sentences and paragraphs into multi-dimensional vector space. In other words, it takes the sentences and paragraphs of our tweets and assigns them a position associated with their meaning. This is done based on the context of the token (the unit of text, like a word or sentence). The model we are using is capable of interpreting multiple languages and is fine-tuned, or specifically trained, on the data we are examining. The code below is going to call upon a pre-built classifier which uses this fine-tuned model to predict whether a tweet is likely Russian propaganda. The two sample tweets are:\n\n“#qanon #trump Hunter Biden is a Ukranian Shill”\n“What great weather we have today”\n\nThe model is going to take these text inputs, represent them in vector space, and then report whether their respective values are similar to those of disinformation tweets.\n\n\nClassification and It’s Discontents\nBefore we explore the possibilities of our model to classify, we should first consider some of the main concerns and limitations regarding classification. Classification is an essential element of how machine learning operates. At its core it is the method of finding features that are central to a class and assigning units to that class based on those features. As you may already see, this “in-or-out” framework necessarily flattens some of the richness of human life, in order to effectively create these incredibly useful classes.\n\nExample:\nYou might say a cat and a dog are really easy to classify. Most people know what a dog looks like and that it looks different than a cat. But if all I tell you is that there are two animals that commonly live with humans, that have a tail and paws, and make noise you might have a hard time classifying them, because they share common features.\nIt is important to think deeply about how we are classifying, especially as many datasets are labeled by people, who carry their own understandings of what belongs to each class.\nAny class or classifier will be informed by the balance of abstraction to specificity, and we should always keep this in mind when we are classifying. It is important to be specific enough to ascertain the qualities we are interested in, but not so specific we end up with thousands of classes.\n\nNow that we have explored some of the trickiness of classification as a concept, we can look at how machine learning can help us work through some of these challenges. By using data that is labeled as disinformation our model can be trained to associated certain numerical feautres with disinformation, and when we give it text data that is similar to what it knows to be disinformation, it will classify it as such.\nFor this analysis, we trained a model on a dataset very similar to ours, with the purpose of detecting propaganda tweets. Recall, however, that this model was trained soley on tweets around and before the 2016 presidential election- meaning it has never seen any tweets posted after this point. Given this information, what kinds of tweets do you think the model will struggle with the most? What kinds of propaganda tweets will it excel at detecting? Try using the model below, and see if you can get it to label a string of text as disinformation.\n\n\nCode\nclassifier = pipeline(\"text-classification\",model=\"IreneBerezin/soci-280-model\")\n\nprint(classifier(\"crooked hillary is trying to rig the election! #MAGA!\")) #put your text in here\n\n\nLet’s now apply the model to a random sample of the dataset we’ve been studying. How well do you think the model will perform?\n\n\nCode\ntweets_sample = tweets.sample(n=2000, random_state=60)\ntweets_inference = tweets_sample[[\"content\", \"is_propaganda\"]].dropna()\ntweets_inference['is_propaganda'] = 1 - tweets_inference['is_propaganda'] # for whatever reasons the labels are switched in the training data\n\nclassifier = pipeline(\"text-classification\",model=\"IreneBerezin/soci-280-model\",device=-1)\npredictions = classifier(tweets_inference['content'].tolist(),batch_size=32,truncation=True)\npredicted_labels = [int(p['label'].split('_')[1]) for p in predictions]\ntrue_labels = tweets_inference['is_propaganda'].tolist()\n\n\n\n\nCode\ndisplay_labels = ['Disinformation', 'Normal Tweet']\n\ncm = confusion_matrix(y_true=true_labels,y_pred=predicted_labels,labels=[0, 1])\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.set_title(\"Confusion Matrix\", fontsize=16)\ndisp.plot(ax=ax, cmap='Greys') \nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nPretty terribly, apparently! Above is what’s called a confusion matrix: it shows the number of labels that were correctly predicted, and incorrectly predicted. We see that, out of a random sample of 2000 tweets:\n\n797 tweets were marked as disinformation, and were, in fact, disinformation\n504 tweets were labelled as normal tweets while they were, in fact, disinformation\n282 tweets were labelled disinformation when they were in fact normal\n417 tweets were correctly labelled as normal tweets\n\nSo, better than blindly guessing (which would put us on average as 500 tweets in each category) but still very bad.\nThis is an example of overfitting: instead of actually learning what constitutes a propagandist tweet, the model simply memorized the specific writing styles of the troll accounts present in the training set. Hence, when it was shown new tweets from different troll accounts in the test set, it failed, as it never learned the general patterns that define an account as part of a coordinated disinformation campaign. The model did the ML equivalent of memorizing in great detail all the solutions to questions in a math textbook instead of actually learning how to solve them."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#what-is-sentiment-analysis",
    "href": "docs/SOCI-280/soci_270_bert.html#what-is-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "1. What is Sentiment Analysis?",
    "text": "1. What is Sentiment Analysis?\n\n“Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text” (Hussein, 2018).\n\nAs this definition alludes, sentiment analysis is a part of natural language processing (NLP), a field at the intersection of human language and computation. Because humans are complex, emotional beings, the language we use is often shaped by our affective (emotional) dispositions. Sentiment analysis, sometimes referred to as “opinion mining”, is one way researchers can methodologically understand the emotional intentions, typically positive, negative, or neutral sentiments, that lie in textual datasets.\n\n🔎 Engage Critically\nAt the heart of sentiment analysis is the assumption that language reveals interior, affective states, and that these states can be codified and generalized to broader populations. AI scholar Kate Crawford, in her book Atlas of AI, explores how many assumptions found in contemporary sentiment research (i.e., that there are 7 universal emotions) are largely unsubstantiated notions that emerged from mid-20th century research funded by the US Department of Defense. Rather than maintaining that emotions can be universally categorized, her work invites researchers to think about how emotional expression is highly contextualized by social and cultural factors and the distinct subject positions of content makers.\n\n❓ Consider the research question for your sentiment analysis. How might the text you are working with be shaped by the distinct groups that have generated it?\n\n\n❓ Are there steps you can take to educate yourself around the unique language uses of your dataset (for example, directly speaking with someone from that group or learning from a qualified expert on the subject)?\n\nIf you’re interested, you can learn more about data justice in community research in a guide created by UBC’s Office for Regional and International Community Engagement.\n\nThe rise of web 2.0 has produced prolific volumes of user-generated content (UGC) on the internet, particularly as people engage in a variety of social platforms and forums to share opinions, ideas and express themselves. Maybe you are interested in understanding how people feel about a particular political candidate by examining tweets around election time, or you wonder what people think about a particular bus route on reddit. UGC is often unstructured data, meaning that it isn’t organized in a recognizable way.\nStructured data for opinions about a political candidate might look like this:\n\n\n\n\n\n\n\n\nPro\nCon\nNeutral\n\n\n\n\nSupports climate action policies\nNo plan for lowering the cost of living\nUBC Graduate\n\n\nExpand mental health services\n\n\n\n\n\nWhile unstructured data might look like this:\n\nlove that she’s trying to increase mental health services + actually cares abt the climate 👏 but what’s up w rent n grocieries?? i dont wanna go broke out here 😭 a ubc alum too like i thought she’d understand\n\nIn the structured data example above, the reviewer defines which parts of the feedback are positive, negative or neutral. In the unstructured example on the other hand, there are many typos and a given sentence might include a positive and a negative review as well as more nuanced contextual information (i.e. mentioning being a UBC alum when discussing cost of living). While messy, this contextual information often carries valuable insights that can be very useful for researchers.\nThe task of sentiment analysis is to make sense of these kinds of nuanced textual data - often for the purpose of understanding people, predicting human behaviour, or even in some cases, manipulating human behaviour.\nDisinformation campaigns often aim to sway public opinion by influencing the emotional tone of online conversations. Sentiment analysis allows us to detect and understand these patterns by identifying whether large volumes of text express positive, negative, or neutral sentiment.\nOur model is pretrained, meaning it has already learnt from millions of labelled examples how to distinguish different sentiments. Specifically, because the model we’ll be using was trained on English tweets, it’s tuned to the language and syntax common on Twitter/X, and is limited to analyzing English-language text.\nLanguage is complex and always changing.\nIn the English language, for example, the word “present” has multiple meanings which could have positive, negative or neutral connotations. Further, a contemporary sentiment lexicon might code the word “miss” as being associated with negative or sad emotional experiences such as longing; if such a lexicon were applied to a 19th century novel which uses the word “miss” to describe single women, then, it might incorrectly associate negative sentiment where it shouldn’t be. While sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\nNow, we’re ready to get back to our analysis. Below, we’ll load in our model and tokenizer and start playing around with identifying the sentiment of different phrases.\n\n\nCode\nsentiment = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n\nprint(sentiment(\"I hate everyone and everything\"))\nprint(sentiment(\"Life is great!\"))\nprint(sentiment(\"Hello world\"))\n\n\nLet’s breakdown this output. There are two parts to what the model returns:\n\nLabel → a classification labelling the text as either having positive, negative, or neutral sentiment\nScore → the model’s confidence in it’s classification\n\n\n🔎 Engage Critically\nTry using the interactive tool below to explore how a machine learning model detects sentiment in short texts like tweets. The model classifies each input as positive, neutral, or negative, and assigns a probability score to each label. Type a sentence (like a tweet or short message) into the box below and click “Analyze” to see how the model interprets its emotional tone.\n\n\n\nCode\ntext_input = widgets.Text(\n    value=\"hello world!\",\n    placeholder=\"Type a sentence here\",\n    description=\"Input:\",\n    layout=widgets.Layout(width=\"70%\")\n)\nanalyze_btn = widgets.Button(description=\"Analyze\", button_style=\"primary\")\noutput_area = widgets.Output()\n\ndef on_analyze_clicked(b):\n    with output_area:\n        clear_output(wait=True)\n        scores = sentiment(text_input.value)\n        labels = [item[\"label\"] for item in scores]\n        probs  = [item[\"score\"] for item in scores]\n        fig, ax = plt.subplots(figsize=(6,4))\n        bars = ax.bar(labels, probs)\n        ax.set_ylim(0, 1)\n        ax.set_ylabel(\"Probability\")\n        ax.set_title(\"Sentiment Probability Distribution\")\n        for bar, prob in zip(bars, probs):\n            height = bar.get_height()\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,  \n                height + 0.02,                    \n                f\"{prob:.2f}\",                   \n                ha=\"center\", va=\"bottom\",\n                color=\"red\", fontsize=12)\n        plt.show()\n\nanalyze_btn.on_click(on_analyze_clicked)\n\ndisplay(widgets.VBox([text_input, analyze_btn, output_area]))\n\n\n\nBatch Sentiment Analysis\nNow, let’s start running sentiment analysis on our dataset. The general steps to run our analysis include:\n\nLoading a pretrained model and tokenizer\nWe load a RoBERTa model that has been fine-tuned for sentiment analysis on tweets, along with its corresponding tokenizer.\nCreating sentiment analysis pipeline\nWe set up a Hugging Face pipeline that handles finer steps in our sentiment analysis, such as tokenization (breaking up text into smaller units, called tokens), batching (processing multiple texts at once for efficiency), and prediction (predicting the overall sentiment).\nRunning batch sentiment analysis on the dataset\nTo efficiently analyze large numbers of tweets, we split the dataset into batches of 1,000 tweets and process them one batch at a time. To store the predictions, we extract the predicted sentiment labels and save them in a column named sentiment.\nPreviewing the results\n\n\n\nCode\ntweets_small = tweets.groupby('source').sample(n=1000, random_state=42)\ntweets_small = tweets_small.sample(frac=1, random_state=42).reset_index(drop=True) #you might want to change this value\ntweets_small[\"sentiment\"] = \"\" \n\n\n\n\nCode\nbatch_size = 1000\nn = 0\nfor start in range(0, len(tweets_small), batch_size):\n    end = start + batch_size\n    batch_texts = tweets_small[\"content\"].iloc[start:end].tolist()\n    results = sentiment(batch_texts)           \n    labels  = [res[\"label\"] for res in results]\n    tweets_small.loc[start:end-1, \"sentiment\"] = labels \n    n = n+1\n    print(f'batch {n} done')\n\n\n\n\nCode\nprint(tweets_small[[\"content\", \"sentiment\"]].head())\n\n\nWe can see the first 5 tweets and their predicted sentiment above.\nNow that we know how to run sentiment analysis to identify the overarching sentiment of a tweet, we are now in good position to ask and investigate whether emotionally charged language is more common in propaganda. Let’s explore this by forming a hypothesis and testing it statistically.\n\n🔎 Engage Critically\nHypothesis: Propagandist tweets (is_propaganda == 1) are more emotionally charged — that is, they are more likely to be classified as Positive or Negative (non-neutral) compared to non-propagandist tweets (is_propaganda == 0).\nWe will test whether the difference in sentiment category frequencies between the two groups is statistically significant.\n\nFirst, let’s examine the sentiment distribution for each group:\n\n\nCode\ndist = pd.crosstab(tweets_small['sentiment'], tweets_small['is_propaganda'], normalize='columns')\ndist.columns = ['Non-propagandist', 'Propagandist']\nprint(dist * 100)\n\n\nReading the table, we can see that the majority of non-propagandist tweets are either negative (~43%) or neutral (~44%), while the majority of propagandist tweets (~63%) express neutral sentiment.\n\n\nCode\nprint(tweets_small['sentiment'])\n\n\n\n\nCode\n# We define 'charged' sentiment as Positive or Negative\ntweets_small['charged'] = tweets_small['sentiment'].isin(['positive', 'negative']).astype(int)\n\n# Constructing a  contingency table: rows = propagandist/non propagandist group, columnss = charged vs neutral\ncontingency = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged'])\nprint(\"Contingency table:\\n\", contingency)\n\n# Chi-squared test\nchi2, p, dof, expected = chi2_contingency(contingency)\nprint(f\"p-value = {p:.3e}\")\n\n\n\n🔎 Engage Critically\nTry interpreting the output. What are our results telling us? Based on the p-value, what can we conclude about our hypothesis?"
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#multilingual-sentiment-analysis",
    "href": "docs/SOCI-280/soci_270_bert.html#multilingual-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "3. Multilingual Sentiment Analysis",
    "text": "3. Multilingual Sentiment Analysis\nOur dataset of tweets isn’t entirely in English — many of the tweets are written in Russian. Could this be skewing our results? How is our model actually handling Russian-language tweets compared to English ones?\n\n🔎 Engage Critically\nRecall our discussion on sentiment lexicons in Section 1:\n\nWhile sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\n\n❓ How might the use of a monolingual sentiment model introduce bias into our results? Are non-English tweets being misclassified as neutral, negative, or positive when they shouldn’t be?\n\nWith this in mind, let’s explore below. We’ll use the Unicode values of Cyrillic characters to identify Russian-language tweets, and run sentiment analysis seperately on Russian and and English tweets.\n\n\nCode\ntweets_lang = tweets_small.copy()\ntweets_small['language'] = tweets_small['language'].str.lower().str[:2]\n\nen_tweets = tweets_lang[tweets_lang['language'] == 'en']\nru_tweets  = tweets_lang[tweets_lang['language'] == 'ru']\neng_dist = pd.crosstab(en_tweets['sentiment'], en_tweets['is_propaganda'], normalize='columns') * 100\nru_dist  = pd.crosstab(ru_tweets['sentiment'],  ru_tweets['is_propaganda'],  normalize='columns') * 100\n\n# Renaming columns for clarity\ncol_map = {0: 'Non-propagandist', 1: 'Propagandist'}\n\neng_dist = eng_dist.rename(columns=lambda c: col_map.get(c, str(c)))\nru_dist  = ru_dist.rename(columns=lambda c: col_map.get(c, str(c)))\n\nprint(\"English Tweets Sentiment (%):\\n\", eng_dist, \"\\n\")\nprint(\"Russian Tweets Sentiment (%):\\n\", ru_dist)\n\n\n\n🔎 Engage Critically\nTake a moment to interpret the results before continuing. What do they tell us about the performance of our model on Russian-language tweets? Why do you think that is?\n\nFrom the table above, we can see that our model is performing very poorly on Russian-language tweets, as nearly all of the Russian tweets are being marked as neutral regardless of if they are propagandist or not. This means that the pretrained model we were using before is not an appropriate choice based on the characteristics of our data, namely that a significant portion of the tweets are written in Russian, a language the model was not trained to make reliable predictions on.\nLet’s try re-running our sentiment analysis using a different model. This time, we’ll use a model trained on 198 million tweets that were not filtered by language. As a result, the training data reflects the most commonly used languages on the platform at the time of collection, with Russian conveniently ranking as the 11th most frequent.\nWe’ll follow the same steps for batch sentiment analysis that we did in Section 2:\n\n\nCode\nsentiment_multi = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\")\n\nbatch_size = 1000\nfor start in range(0, len(tweets_small), batch_size):\n    end = start + batch_size\n    batch_texts = tweets_small[\"content\"].iloc[start:end].tolist()\n    results = sentiment_multi(batch_texts)           \n    labels  = [res[\"label\"] for res in results]\n    tweets_small.loc[start:end-1, \"sentiment\"] = labels \n\nprint(tweets_small[[\"content\", \"sentiment\"]].head())\n\n\nTo make the results easier to visualize, let’s create a table that shows the percentage distribution of sentiment labels (positive, neutral, negative) within propagandist and non-propagandist tweets.\n\n\nCode\nmulti_dist = pd.crosstab(tweets_small['sentiment'], tweets['is_propaganda'], normalize='columns') * 100\nmulti_dist.columns = ['Non-propagandist', 'Propagandist']\nprint(\"Multilingual Model Sentiment (%):\\n\", multi_dist)\n\n\nThe sentiment distribution between propagandist and non-propagandist tweets is quite similar when using the multilingual model. Both groups are predominantly neutral (around 50%), with roughly equal proportions of negative and positive sentiment.\nNow, let’s run a statistical test to see if there’s a meaningful difference in sentiment between propagandist and non-propagandist tweets. Specifically, we want to know:\n\nAre propagandist tweets more likely to be emotionally charged (positive or negative) than neutral, compared to non-propagandist tweets?\n\nTo answer this, we’ll use a chi-squared test, which helps us check whether the differences we see in the data are likely due to chance or if they’re statistically significant.\n\n\nCode\ntweets_small['charged_multi'] = tweets_small['sentiment'].isin(['positive','negative']).astype(int)\ncontingency_multi = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged_multi'])\nchi2_multi, p_multi, *_ = chi2_contingency(contingency_multi)\nprint(f\"Chi-squared p-value with multilingual model: {p_multi:.3e}\")\n\n\n\n🔎 Engage Critically\nTake a moment to interpret the results before continuing. What does our p-value tell us about our inital research question above?\n\nOur p-value (0.00003727) is much smaller than the common significance level of 0.05, indicating that the difference in how emotionally charged tweets are distributed between propagandist and non-propagandist groups is very unlikely to be due to random chance.\nThis means there is strong evidence that propagandist tweets are more likely to be emotionally charged compared to non-propagandist tweets, according to the multilingual model’s sentiment analysis."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert.html#introduction-to-toxicity-analysis",
    "href": "docs/SOCI-280/soci_270_bert.html#introduction-to-toxicity-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "4. Introduction to Toxicity Analysis",
    "text": "4. Introduction to Toxicity Analysis\nWarning: this section contains examples of potentially offensive or profane text\nToxicity analysis is another type of classification task that uses machine learning to detect whether a piece of text contains toxic speech. Jigsaw, a Google subsidary and leader in technological solutions for threats to civil society, uses the following definition for “toxic speech” proposed by Dixon et al. (2018):\n\n”[R]ude, disrespectful, or unreasonable language that is likely to make someone leave a discussion”\n\n\n🔎 Engage Critically\nThis definition is widely considered by the NLP community to be ill-defined and vague. Why do you think? What issues could potentially arise from this definition, and how could they impact (for example) a comment flagging tool that gives warnings to social media users whose comments meet this definition of toxic speech?\n\nA core issue defined by Berezin, Farahbakhsh, and Crespi (2023) is that the definition “gives no quantitative measure of the toxicity and operates with highly subjective cultural terms”, yet still remains widely used by researchers and developers in the field. We’ll explore some of the ways this definition is influencing toxicity analysis briefly below.\n\n4.1 Positive profanity\n\n\n\nFuck dude, nurses are the shit (Mauboussin, 2022)\n\n\nConsider the Reddit post above. Is the comment an example of toxic speech? Probably not, right?\nNow imagine you are Perspective API, Google’s AI toxicity moderation tool, with your scope of “toxic speech” limited solely to the definition of ”rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion”. Because of your architecture, you are limited in the way you can understand a message in context. You process the comment and immediately detect two profanities that meet your requirement for being rude language, and assign it a subsequent toxicity score:\n\n\n\nFuck dude, nurses are the shit with Toxicity Score (98.62%) (Mauboussin, 2022)\n\n\nThis is where, in the NLP community, there has been a growing discussion to ensure toxicity analysis tools, especially detectors used in online discussion and social media platforms, are more robust than simply being ‘profanity detectors’. They must be able to interpret a word in context.\n\n\n4.2 In-group language\nConsider in-group words used by distinct communities. Many of these words, once used as derogatory slurs against a group of people (such as Black or LGBTQ+ folk), have now largely been reclaimed and are prevalent in the lexicons of individuals identifying within these communities, no longer considered offensive when used by the in-group. However, if human annotators label textual data that ML models then are trained on, biases can permeate the models and lead to the classification of non-toxic, in-group language as harmful or offensive. Notably, African-American Vernacular English (AAVE) has been found to be flagged as toxic due to linguistic bias. XX frames how the challenge impacts toxicity detectors well:\n\n🔎 Engage Critically\nHow do you think this challenge can impact toxicity detectors? Resende et al. (2024) underscore this tension, noting that:\n\n…[S]uch a fine line between causal speaking and offensive discourse is problematic from a human and computational perspective. In that case, these interpretations are confounding to automatic content moderation tools. In other words, toxicity/sentiment analysis tools are usually developed using manual rules or supervised ML techniques that employ human-labeled data to extract patterns. The disparate treatment embodied by machine learning models usually replicates discrimination patterns historically practiced by humans when interacting with processes in the real world. Due to biases in this process, a lack of context leads both rule-based and machine learning-based models to a concerning scenario where minorities do not receive equal treatment.  Resende et al., 2024, p. 2\n\n\nResende et al. (2024) also conducted a comparison analysis of toxicity models, including Google’s Perspective API and Detoxify (the model we’ll be using for our own analysis soon).\n\n\n\nComparing the Toxicity Scoring Models (Resende et al., 2024)\n\n\nThis bias shown in this model’s performance can come from many factors in its structure, from data provenance and annotation to model architecture and processing, to a combination of many.\n\n🔎 Engage Critically\nIf you could, what questions would you want to ask the people who build these models?\n\n\n\nToxicity analysis using Detoxify\nThe model we’ll be using is called Detoxify (you can read more about it here). It was trained on large datasets of online comments across seven languages, including English and Russian. Detoxify provides an overall toxicity score for each text and can also detect five specific subtypes of toxicity: identity_attack, insult, obscene, sexual_explicit, and threat.\nIn the context of our dataset, propagandist tweets often aim to provoke strong emotions, spread hate, or stir conflict. Running toxicity analysis can help us investigate questions like:\n\nAre propagandist tweets more toxic than non-propagandist ones?\nWhat types of toxic language are most common?\nAre there patterns in how toxicity is used to influence or manipulate public discourse?\n\nToxicity analysis gives us another lens to understand how language and emotion are used in disinformation campaigns. Let’s begin by importing the necessary libraries and tools:\n\n\nCode\nfrom detoxify import Detoxify\n\nmodel = Detoxify('original', device='cpu')\n\n\nBefore we throw this model at our dataset, let’s take a look at what ‘toxic’ really means.\nWe’ll be repeating the same hypothesis test that we performed using our sentiment analysis models, this time trying to answer:\n\nAre tweets deemed toxic more likely to originate from propagandists relative to non-propagandist tweets?\n\nHere, we’ll define a tweet toxic if it meets or exceeds a toxicity theshold of 0.5.\n\n\nCode\ntexts = tweets_small['content'].tolist()\nresults = model.predict(texts)\ntoxicity_df = pd.DataFrame(results)\ntweets_small = tweets_small.join(toxicity_df)\n\n\n\n\nCode\nthreshold = 0.5\ntweets_small['charged'] = (tweets_small['toxicity'] &gt; threshold).astype(int)\n\ncontingency = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged'])\nprint(\"Contingency table:\\n\", contingency)\n\nchi2, p, dof, expected = chi2_contingency(contingency)\nprint(f\"Chi-squared statistic: {chi2:.2f}\")\nprint(f\"p-value: {p:.3e}\")\n\nprint(\"\\nSample of results:\")\nprint(tweets_small[['content', 'is_propaganda', 'toxicity', 'charged']].head())\n\n\nFollowing the logic from the sentiment analysis results, what do these results tell us about our hypothesis? How do you think the results would change if we used a different threshold to define toxicity?"
  },
  {
    "objectID": "docs/SOCI-415/kinmatrix.html",
    "href": "docs/SOCI-415/kinmatrix.html",
    "title": "SOCI 415 Network Analysis - KINMATRIX Dataset",
    "section": "",
    "text": "SOCI 415 Network Anlysis Notebook\n\n\nThe KINMATRIX dataset represents families as ego-centric networks of younger adults aged 25 to 35, collecting extensive data about both nuclear and extended kin across ten countries. The data include over 12,000 anchor respondents and more than 252,000 anchor-kin dyads, encompassing a wide range of relatives such as parents, siblings, grandparents, aunts, uncles, cousins, and complex kin (e.g., step- and half-relatives).\nAnchor respondents refer to the ones directly sampled from. These anchors filled out the survey about their families and kin meaning that they will always be at the center of these family networks.\nThe countries in the dataset are the United Kingdom, Germany, Poland, Italy, Sweeden, Denmark, Finland, Norway, the Netherlands and the USA.\n\n\nWe will begin by looking at broader patterns across different countries. This will serve as an introduction into the dataset and will be a foundation for more focused analysis later on. First we import all the necessary Python libraries for data manipulation, network analysis, and visualization. This will allow us to get results for each country. The output of this cell will be a list of countries in the data.\nImport libraries and the data\n\n#Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.patches as mpatches \nimport networkx as nx \nimport pandas as pd \nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\nimport plotly.graph_objects as go\nfrom pyvis.network import Network\nimport re\nimport matplotlib.cm as cm\n\nfile_path = r'C:\\Users\\alexr\\OneDrive\\Desktop\\WORK\\Summer2025\\temp\\ZA8825_v1-0-0.dta' #change here once we know how to present the notebook, or change here for a different path\ndf = pd.read_stata(file_path)\n\n#Print country codes\nprint(df['anc_cou'].unique())\n\n#Make a list of countries\ncountry_dfs = {}\nfor country in df['anc_cou'].unique():\n    country_dfs[country] = df[df['anc_cou'] == country]\n\n\n\nHere, we systematically construct a kinship network for each country in the dataset using the NetworkX Python package. For each country, we filter the data, create a new graph, and add nodes representing anchor individuals and their kin. Then we draw edges between the nodes. Finally, we print out the number of nodes and edges for each country’s network, providing a quick overview of network size and complexity across the dataset.\n\n#Set up a dictionary of countries\ncountry_map = {\n    'UK': '1. UK',\n    'Germany': '2. Germany',\n    'Poland': '3. Poland',\n    'Italy': '4. Italy',\n    'Sweden': '5. Sweden',\n    'Denmark': '6. Denmark',\n    'Finland': '7. Finland',\n    'Norway': '8. Norway',\n    'Netherlands': '9. Netherlands',\n    'USA': '10. USA'\n}\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store the graph and anchor nodes\n    anchor_nodes = list(filtered_df['anc_id'].unique())\n    graphs[country] = {\n        'graph': G_filtered,\n        'anchor_nodes': anchor_nodes\n    }\n    \n    print(f\"{country}:\")\n    print(f\"  Number of nodes: {G_filtered.number_of_nodes()}\")\n    print(f\"  Number of edges: {G_filtered.number_of_edges()}\")\n    print()\n\nWe can see that the Kinmatrix dataset has a wide range of networks ranging from smaller countries like Norway and Denmark (~3000 Nodes) to expansive networks like the United States with 118,702 nodes. This is a good overview, but it does not really give any information on the nature of the networks, their size, family relations and other interesting statistics.\n\n\n\nThis block computes and displays key network statistics for each country’s kinship network. We calculate the mean degree (average number of connections per node), and the gender distribution among anchor nodes.\n\nfor country, data in graphs.items():\n    G = data['graph']\n    anchor_nodes = data['anchor_nodes']\n    \n    # Mean degree for anchor nodes only\n    anchor_degrees = [G.degree(n) for n in anchor_nodes]\n    mean_anchor_degree = np.mean(anchor_degrees) if anchor_degrees else 0\n    \n    # Gender distribution among anchor nodes\n    female_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '2. Female')\n    male_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '1. Male')\n    other_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '3. Other gender or no gender')\n    \n    print(f\"{country}:\")\n    print(f\"  Mean degree (anchor nodes): {mean_anchor_degree:.2f}\")\n    print(f\"  Female anchor nodes: {female_count}\")\n    print(f\"  Male anchor nodes: {male_count}\")\n    print(f\"  Other/No gender anchor nodes: {other_count}\")\n    print()\n\nThe results are interesting we can see that the smallest family networks are in Germany (16.33 nodes per anchor) and the largest are in the United States (22.72 nodes per anchor). We can also see that there are more women than men in most countries espcially in the US 2,992 compared to 1,956.\n\n\n\nHere we have enough information to do a visualization of our findings. We will use the Pyvis package. This package will make an interactive visualization which you can zoom in and pan around. This will be the most simple visualization we will make, and it will just be on the shape and size of the network. The code loads the NetworkX graph into Pyvis, cleans any missing attribute values, and provides interactive controls for exploring the network.\n\n# Retrieve the Norway Network\nG_Norway = graphs['Norway']['graph']\nanchor_nodes_norway = graphs['Norway']['anchor_nodes']\n\n# Clean None attributes\nfor n, attrs in G_Norway.nodes(data=True):\n    for k, v in attrs.items():\n        if v is None:\n            G_Norway.nodes[n][k] = \"NA\"\n\nfor u, v, attrs in G_Norway.edges(data=True):\n    for k, val in attrs.items():\n        if val is None:\n            G_Norway.edges[u, v][k] = \"NA\"\n\n# Create a new Pyvis Network\nnet = Network(height='800px', width='100%', notebook=True)\n\n# Create simple anchor mapping\nanchor_to_label = {}\nfor i, anchor in enumerate(anchor_nodes_norway, 1):\n    anchor_to_label[anchor] = f\"Anchor-{i}\"\n\n# Add nodes manually \nfor node in G_Norway.nodes():\n    if node in anchor_nodes_norway:\n        # Anchor nodes with labels\n        net.add_node(node, label=anchor_to_label[node], color='#4ECDC4', size=15)\n    else:\n        # Kin nodes\n        net.add_node(node, color='#4ECDC4', size=8)\n\n# Add edges manually\nfor u, v in G_Norway.edges():\n    net.add_edge(u, v)\n\nfor node in net.nodes:\n    if node['id'] not in anchor_nodes_norway:\n        node['label'] = '' \n\n# Minimal settings\nnet.show('norway_updated_network.html')\n\nprint(\"All blue nodes version saved as 'norway_all_blue_nodes.html'\")\nprint(f\"Total nodes: {G_Norway.number_of_nodes()}\")\nprint(f\"Total edges: {G_Norway.number_of_edges()}\")\nprint(f\"Anchor nodes labeled: {len(anchor_nodes_norway)}\")\n\n\n\n\n\nThis code block constructs kinship network graphs for each country in the KINMATRIX dataset and then analyzes how the number of nodes in the family networks vary by age and gender within each country. This analysis helps reveal patterns and differences in family network structure across demographic groups and between countries, providing insight into how kinship connectivity varies by age and gender in different countries.\n\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store only the graph\n    graphs[country] = G_filtered\n\ndef analyze_family_degree_by_demographics(G_filtered):\n    degree_by_age = {}\n    degree_by_gender = {}\n\n    # Identify anchor nodes by prerequisite attribute\n    anchor_nodes = [n for n, d in G_filtered.nodes(data=True)\n                    if ('age' in d and 'gender' in d)]\n\n    for anchor_id in anchor_nodes:\n        # Degree of the anchor node (number of kin ties)\n        deg = G_filtered.degree(anchor_id)\n        age = G_filtered.nodes[anchor_id].get('age')\n        gender = G_filtered.nodes[anchor_id].get('gender')\n\n        if age is not None:\n            age_group = age // 10 * 10  # e.g., 27 -&gt; 20s\n            degree_by_age.setdefault(age_group, []).append(deg)\n        if gender is not None:\n            degree_by_gender.setdefault(gender, []).append(deg)\n\n    print(\"Mean Degree by Age Group:\")\n    for age_group, degrees in sorted(degree_by_age.items()):\n        print(f\"  Age {age_group}s: Mean Degree = {np.mean(degrees):.2f}, Count = {len(degrees)}\")\n\n    print(\"\\nMean Degree by Gender:\")\n    for gender, degrees in degree_by_gender.items():\n        print(f\"  Gender {gender}: Mean Degree = {np.mean(degrees):.2f}, Count = {len(degrees)}\")\n\n# Use this updated function for each country's graph:\nfor country in country_map:\n    if country in graphs:\n        print(f\"\\n--- {country} ---\")\n        analyze_family_degree_by_demographics(graphs[country])\n    else:\n        print(f\"No graph available for {country}\")\n\n\n\nInteractive part: Have a look at our findings from sections 1 and 2, based on frameworks taught in class explain a reason for them.\nSome example findings are listed below, you can try to answer these 3, or look for your own and try to link it to a concept from class.\n\nAge does not seem to be a factor in mean degree. In six out of ten countries, anchors in their 20’s have a higher degree than anchors in their 30’s. Why is this, we would expect older anchors to have larger families?\nWomen tend to have a higher mean degree than men in all countries (Poland: Male = 17.97, Female = 20.25). Why is this?\nThe USA has the largest mean degree out of all countries in the dataset by both age and gender. Is it purely because of the large sample size in the US or are there other factors? We might expect the US to have a lower mean degree especially among anchors aged 20-30 as their parents or grandparents are likely to be immigrants disconnected from their families in their original countries."
  },
  {
    "objectID": "docs/SOCI-415/kinmatrix.html#kinmatrix-data-description",
    "href": "docs/SOCI-415/kinmatrix.html#kinmatrix-data-description",
    "title": "SOCI 415 Network Analysis - KINMATRIX Dataset",
    "section": "",
    "text": "The KINMATRIX dataset represents families as ego-centric networks of younger adults aged 25 to 35, collecting extensive data about both nuclear and extended kin across ten countries. The data include over 12,000 anchor respondents and more than 252,000 anchor-kin dyads, encompassing a wide range of relatives such as parents, siblings, grandparents, aunts, uncles, cousins, and complex kin (e.g., step- and half-relatives).\nAnchor respondents refer to the ones directly sampled from. These anchors filled out the survey about their families and kin meaning that they will always be at the center of these family networks.\nThe countries in the dataset are the United Kingdom, Germany, Poland, Italy, Sweeden, Denmark, Finland, Norway, the Netherlands and the USA.\n\n\nWe will begin by looking at broader patterns across different countries. This will serve as an introduction into the dataset and will be a foundation for more focused analysis later on. First we import all the necessary Python libraries for data manipulation, network analysis, and visualization. This will allow us to get results for each country. The output of this cell will be a list of countries in the data.\nImport libraries and the data\n\n#Import libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport matplotlib.patches as mpatches \nimport networkx as nx \nimport pandas as pd \nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\nimport plotly.graph_objects as go\nfrom pyvis.network import Network\nimport re\nimport matplotlib.cm as cm\n\nfile_path = r'C:\\Users\\alexr\\OneDrive\\Desktop\\WORK\\Summer2025\\temp\\ZA8825_v1-0-0.dta' #change here once we know how to present the notebook, or change here for a different path\ndf = pd.read_stata(file_path)\n\n#Print country codes\nprint(df['anc_cou'].unique())\n\n#Make a list of countries\ncountry_dfs = {}\nfor country in df['anc_cou'].unique():\n    country_dfs[country] = df[df['anc_cou'] == country]\n\n\n\nHere, we systematically construct a kinship network for each country in the dataset using the NetworkX Python package. For each country, we filter the data, create a new graph, and add nodes representing anchor individuals and their kin. Then we draw edges between the nodes. Finally, we print out the number of nodes and edges for each country’s network, providing a quick overview of network size and complexity across the dataset.\n\n#Set up a dictionary of countries\ncountry_map = {\n    'UK': '1. UK',\n    'Germany': '2. Germany',\n    'Poland': '3. Poland',\n    'Italy': '4. Italy',\n    'Sweden': '5. Sweden',\n    'Denmark': '6. Denmark',\n    'Finland': '7. Finland',\n    'Norway': '8. Norway',\n    'Netherlands': '9. Netherlands',\n    'USA': '10. USA'\n}\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store the graph and anchor nodes\n    anchor_nodes = list(filtered_df['anc_id'].unique())\n    graphs[country] = {\n        'graph': G_filtered,\n        'anchor_nodes': anchor_nodes\n    }\n    \n    print(f\"{country}:\")\n    print(f\"  Number of nodes: {G_filtered.number_of_nodes()}\")\n    print(f\"  Number of edges: {G_filtered.number_of_edges()}\")\n    print()\n\nWe can see that the Kinmatrix dataset has a wide range of networks ranging from smaller countries like Norway and Denmark (~3000 Nodes) to expansive networks like the United States with 118,702 nodes. This is a good overview, but it does not really give any information on the nature of the networks, their size, family relations and other interesting statistics.\n\n\n\nThis block computes and displays key network statistics for each country’s kinship network. We calculate the mean degree (average number of connections per node), and the gender distribution among anchor nodes.\n\nfor country, data in graphs.items():\n    G = data['graph']\n    anchor_nodes = data['anchor_nodes']\n    \n    # Mean degree for anchor nodes only\n    anchor_degrees = [G.degree(n) for n in anchor_nodes]\n    mean_anchor_degree = np.mean(anchor_degrees) if anchor_degrees else 0\n    \n    # Gender distribution among anchor nodes\n    female_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '2. Female')\n    male_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '1. Male')\n    other_count = sum(1 for n in anchor_nodes if G.nodes[n].get('gender') == '3. Other gender or no gender')\n    \n    print(f\"{country}:\")\n    print(f\"  Mean degree (anchor nodes): {mean_anchor_degree:.2f}\")\n    print(f\"  Female anchor nodes: {female_count}\")\n    print(f\"  Male anchor nodes: {male_count}\")\n    print(f\"  Other/No gender anchor nodes: {other_count}\")\n    print()\n\nThe results are interesting we can see that the smallest family networks are in Germany (16.33 nodes per anchor) and the largest are in the United States (22.72 nodes per anchor). We can also see that there are more women than men in most countries espcially in the US 2,992 compared to 1,956.\n\n\n\nHere we have enough information to do a visualization of our findings. We will use the Pyvis package. This package will make an interactive visualization which you can zoom in and pan around. This will be the most simple visualization we will make, and it will just be on the shape and size of the network. The code loads the NetworkX graph into Pyvis, cleans any missing attribute values, and provides interactive controls for exploring the network.\n\n# Retrieve the Norway Network\nG_Norway = graphs['Norway']['graph']\nanchor_nodes_norway = graphs['Norway']['anchor_nodes']\n\n# Clean None attributes\nfor n, attrs in G_Norway.nodes(data=True):\n    for k, v in attrs.items():\n        if v is None:\n            G_Norway.nodes[n][k] = \"NA\"\n\nfor u, v, attrs in G_Norway.edges(data=True):\n    for k, val in attrs.items():\n        if val is None:\n            G_Norway.edges[u, v][k] = \"NA\"\n\n# Create a new Pyvis Network\nnet = Network(height='800px', width='100%', notebook=True)\n\n# Create simple anchor mapping\nanchor_to_label = {}\nfor i, anchor in enumerate(anchor_nodes_norway, 1):\n    anchor_to_label[anchor] = f\"Anchor-{i}\"\n\n# Add nodes manually \nfor node in G_Norway.nodes():\n    if node in anchor_nodes_norway:\n        # Anchor nodes with labels\n        net.add_node(node, label=anchor_to_label[node], color='#4ECDC4', size=15)\n    else:\n        # Kin nodes\n        net.add_node(node, color='#4ECDC4', size=8)\n\n# Add edges manually\nfor u, v in G_Norway.edges():\n    net.add_edge(u, v)\n\nfor node in net.nodes:\n    if node['id'] not in anchor_nodes_norway:\n        node['label'] = '' \n\n# Minimal settings\nnet.show('norway_updated_network.html')\n\nprint(\"All blue nodes version saved as 'norway_all_blue_nodes.html'\")\nprint(f\"Total nodes: {G_Norway.number_of_nodes()}\")\nprint(f\"Total edges: {G_Norway.number_of_edges()}\")\nprint(f\"Anchor nodes labeled: {len(anchor_nodes_norway)}\")\n\n\n\n\n\nThis code block constructs kinship network graphs for each country in the KINMATRIX dataset and then analyzes how the number of nodes in the family networks vary by age and gender within each country. This analysis helps reveal patterns and differences in family network structure across demographic groups and between countries, providing insight into how kinship connectivity varies by age and gender in different countries.\n\ngraphs = {}\n\nfor country, anc_cou_value in country_map.items():\n    filtered_df = df[df['anc_cou'] == anc_cou_value]\n    G_filtered = nx.Graph()\n    \n    # Add anchor nodes\n    for idx, row in filtered_df.iterrows():\n        G_filtered.add_node(row['anc_id'], \n                            country=row['anc_cou'],\n                            age=row['anc_age'],\n                            gender=row['anc_gnd'])\n    \n    # Add kin nodes and edges\n    for idx, row in filtered_df.iterrows():\n        kin_node = f\"{row['anc_id']}_{row.get('kin_nam', 'unknown')}_{idx}\"\n        G_filtered.add_node(kin_node, relation=row.get('kin_rel1', None))\n        G_filtered.add_edge(row['anc_id'], kin_node, relation=row.get('kin_rel1', None))\n    \n    # Store only the graph\n    graphs[country] = G_filtered\n\ndef analyze_family_degree_by_demographics(G_filtered):\n    degree_by_age = {}\n    degree_by_gender = {}\n\n    # Identify anchor nodes by prerequisite attribute\n    anchor_nodes = [n for n, d in G_filtered.nodes(data=True)\n                    if ('age' in d and 'gender' in d)]\n\n    for anchor_id in anchor_nodes:\n        # Degree of the anchor node (number of kin ties)\n        deg = G_filtered.degree(anchor_id)\n        age = G_filtered.nodes[anchor_id].get('age')\n        gender = G_filtered.nodes[anchor_id].get('gender')\n\n        if age is not None:\n            age_group = age // 10 * 10  # e.g., 27 -&gt; 20s\n            degree_by_age.setdefault(age_group, []).append(deg)\n        if gender is not None:\n            degree_by_gender.setdefault(gender, []).append(deg)\n\n    print(\"Mean Degree by Age Group:\")\n    for age_group, degrees in sorted(degree_by_age.items()):\n        print(f\"  Age {age_group}s: Mean Degree = {np.mean(degrees):.2f}, Count = {len(degrees)}\")\n\n    print(\"\\nMean Degree by Gender:\")\n    for gender, degrees in degree_by_gender.items():\n        print(f\"  Gender {gender}: Mean Degree = {np.mean(degrees):.2f}, Count = {len(degrees)}\")\n\n# Use this updated function for each country's graph:\nfor country in country_map:\n    if country in graphs:\n        print(f\"\\n--- {country} ---\")\n        analyze_family_degree_by_demographics(graphs[country])\n    else:\n        print(f\"No graph available for {country}\")\n\n\n\nInteractive part: Have a look at our findings from sections 1 and 2, based on frameworks taught in class explain a reason for them.\nSome example findings are listed below, you can try to answer these 3, or look for your own and try to link it to a concept from class.\n\nAge does not seem to be a factor in mean degree. In six out of ten countries, anchors in their 20’s have a higher degree than anchors in their 30’s. Why is this, we would expect older anchors to have larger families?\nWomen tend to have a higher mean degree than men in all countries (Poland: Male = 17.97, Female = 20.25). Why is this?\nThe USA has the largest mean degree out of all countries in the dataset by both age and gender. Is it purely because of the large sample size in the US or are there other factors? We might expect the US to have a lower mean degree especially among anchors aged 20-30 as their parents or grandparents are likely to be immigrants disconnected from their families in their original countries."
  },
  {
    "objectID": "docs/SOCI-415/kinmatrix.html#discussion-1",
    "href": "docs/SOCI-415/kinmatrix.html#discussion-1",
    "title": "SOCI 415 Network Analysis - KINMATRIX Dataset",
    "section": "8.1 Discussion",
    "text": "8.1 Discussion\nThe findings in this section are suprising. In small groups discuss these findings, some potential discussion topics are: * Cross-national variation: Poland and Italy—often seen as more traditional and Christian countries—show the highest parental separation rates in this dataset, defying expectations based on cultural stereotypes. * Poland has no difference in parental seperation based on political regions, but the United States does why is that? * How do we interpret the finding that within the US, ‘religious’ families (by self-report) have higher parental separation than non-religious ones? Does anyone have any ideas as to why?"
  },
  {
    "objectID": "docs/intro_to_cnn/intro_to_cnn.html",
    "href": "docs/intro_to_cnn/intro_to_cnn.html",
    "title": "Introduction to Convolutional Neural Networks (CNNs)",
    "section": "",
    "text": "Author: Kaiyan Zhang, PRAXIS UBC Team\nDate: 2025-06\nBefore you begin: Install the dependencies that you don’t have by running the code cell below.\n\n# !pip install opencv-python\n# !pip install numpy\n# !pip install matplotlib\n# !pip install pandas\n# !pip install scikit-learn\n# !pip install seaborn\n# !pip install datasets\n# !pip install torch\n# !pip install tqdm\n\n\n\n1. What are Neural Networks?\nWhat is the first thing that comes to your mind when you hear the word “neural network”? If you are thinking about the human brain and neurons, you are not wrong. In fact, the term “neural network” is inspired by the way how human brain and nervous systems work, where neurons are connected to each other and communicate with each other to process information.\nIn the context of machine learning, a neural network, or more precisely, an artificial neural network (ANN) is defined as “a program, or model, that makes decisions in a manner similar to the human brain, by using processes that mimic the way biological neurons work together to identify phenomena, weigh options and arrive at conclusions”.\nThe definition seems way too formal and scientific, but we can easily translate it into daily language. Think of taking a closed-book multiple choice exam (Oops, gross). Your brain calls on a team of “experts”, one for course facts, another for what you remember about the professor’s hints in class, another for gut instincts, etc. When you read a question, each expert gives you a confidence score. You weight each score by how much you trust that expert, then add them up. The answer with the highest total “trust \\(\\times\\) confidence” wins. After the exam, you see which answers were wrong and adjust those trust weights (trust the right experts more, the wrong ones less), and prepare for the next exam based on this experience. This exactly how a neural network makes decisions and learns via its feedback loop. Neural networks are following a similar thought and learning process as you and me, and this is why they are flexible and powerful, being able to handle complex, abstract tasks and evolve on their own, like an intelligent creature.\nWhile the core idea is not complex, you may want to master some bluffing terms to translate the professional discussions. In the example above, the “experts” you consulted in your mind are called neurons; the key clues you noticed when reading question are called features; your understanding of exam question is called the input layer; your thought process rounds are called hidden layers; your chosen answer is reflected as the output layer; the mind map that connects all the “experts” and input features is architecture; and each exam attempt with the review of feedback is called a training epoch. See, they are really not that deep! You now can also talk about it as an expert.\n\n\n2. An Intuitive Understanding of Convolutional Neural Networks (CNNs)\nNow that we understood what is an artificial neural network, let’s dive into the real topic here: What’s unique about convolutional neural networks (CNNs) and why they are revolutionary to computer vision and image processing?\nLet’s start by discussing the unique point of CNNs. Imagine you are reading a bird guide and trying to learn the characteristics of a night heron and a grey heron so that you can easily distinguish between the two in the field, what would you do? I believe you would naturally try to observe the birds piece by piece: first comparing the features of the juveniles and adults, then noting how they look both in flight and on land. Gradually, your brain forms a complete comparison: the night heron has a shorter beak, a shorter neck, striking red eyes, and dark blue plumage as an adult; while the grey heron has a longer beak, a longer neck, yellow eyes, and wears grey color plumage.\n\n\n\n\nNight heron in a bird guide\n\n\n\n\n\nGrey heron in a bird guide\n\n\n\nA convolutional neural network would read things in the same way, as it doesn’t look at things in a big picture directly (which is usually costly and slow), but would see an image as multiple small patches to study the unique features and construct a detailed field guide of its own. The way how a CNN sees things this way is through convolution: it has a convolutional layer on top of the input layer to learn features piece by piece in its architecture, such that it can process information from an image in a cleverer way. Moreover, CNNs work quite well even when training images are not as tidy and organized as those in a field guide, which makes it efficient in solving real-life problems.\nLet’s recall some basic concepts of convolution and see how they are applied in the CNNs, typically within the convolutional layer. Here, the inputs are images, and they are interpreted by a computer as grids of numbers. The kernels (also called filters) are still the “brushes” you apply on the input image to extract certain features, but in a CNN, there are usually multiple distinct kernels applied at the same time to extract and map different features. After different features are extracted, they will be pooled together with another kernel and produce a summarized output to be passed into the fully-connected layer for classification or other tasks.\nWhile the principles behind the architectures are complicated, many python libraries now offer easy ways to implement these architectures. In a word, with a labeled image dataset, you can also train a CNN classifier yourself. Let’s try out an example together.\n\n\n3. CNN Example: Classifying Handwritten Digits\n\n\n4. (Optional) Build Our Own CNN Classifier: An Example Using CIFAR-10 Dataset\nClassifying is central in the application of CNNs, so let’s try building a classifier using CNN and see how it works with an example. Let’s say, we want to train a model (the “expert”) that identify and distinguish between some daily objects, such as cars, planes, cat, dogs, etc. We first need to find a dataset that contains images of these objects with labels. This is usually hard as we wouldn’t always have clean, labelled datasets of a specific topic. But luckily, we have many datasets for daily objects.\nThe dataset we are using here is CIFAR-10, it is a widely used practice dataset for beginners to image processing that consists of 60000 32 \\(\\times\\) 32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images. Let’s first load the dataset and see what it’s like.\n\nStep 1: Data Preprocessing\n\n# import CIFAR-10 dataset from HuggingFace\nfrom datasets import load_dataset\n\ndataset_train = load_dataset(\n    'cifar10',\n    split='train'# training dataset\n)\n\ndataset_train\n\n\n# check how many labels/number of classes\nnum_classes = len(set(dataset_train['label']))\nnum_classes\n\nWe can also display one of the images to see what it’s like.\n\n# let's view the image (it's very small)\nsample = dataset_train[0]['img']\n\nplt.imshow(sample)\nplt.axis('off')\nplt.show()\n\nCan you see what the image is about? Can you imagine how computers understands it?\nAs most CNNs can only accept images of a fixed size, we will reshape all images to 32 \\(\\times\\) 32 pixels using torchvision.transforms; a pipeline built for image preprocessing. You can think of a pipeline as a series of small programs that together handles a specific task in a sequential order, which in here is resizing the images in the training set.\n\nimport torchvision.transforms as transforms\nfrom tqdm.auto import tqdm\n\n# image size\nimg_size = 32\n\n# preprocess variable, to be used ahead\npreprocess = transforms.Compose([\n    transforms.Resize((img_size,img_size)),\n    transforms.ToTensor()\n])\n\ninputs_train = []\n\nfor record in tqdm(dataset_train):\n    image = record['img']\n    label = record['label']\n\n    # convert from grayscale to RGB\n    if image.mode == 'L':\n        image = image.convert(\"RGB\")\n        \n    # prepocessing\n    input_tensor = preprocess(image)\n    \n    # append to batch list\n    inputs_train.append([input_tensor, label]) \n\nOther than normalizing the general size of the images, we should also normalize the pixel values in the dataset.\n\nmean = [0.4670, 0.4735, 0.4662]\nstd = [0.2496, 0.2489, 0.2521]\n\npreprocess = transforms.Compose([\n    transforms.Normalize(mean=mean, std=std)\n])\n\nfor i in tqdm(range(len(inputs_train))):\n    # prepocessing\n    input_tensor = preprocess(inputs_train[i][0])\n    # replace with normalized tensor\n    inputs_train[i][0] = input_tensor\n\nHere, we load and process the training set that we are using to validate the model quality.\n\n# Loading the dataset\ndataset_val = load_dataset(\n    'cifar10',\n    split='test'  # test set (used as validation set)\n)\n\n# Integrate the preprocessing steps\npreprocess = transforms.Compose([\n    transforms.Resize((img_size,img_size)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=mean, std=std)\n])\n\ninputs_val = []\ni = 0\nfor record in tqdm(dataset_val):\n    image = record['img']\n    label = record['label']\n\n    # convert from grayscale to RBG\n    if image.mode == 'L':\n        image = image.convert(\"RGB\")\n        \n    # prepocessing\n    input_tensor = preprocess(image)\n    inputs_val.append((input_tensor, label)) # append to batch list\n\nWe noticed that the testing and training data are gigantic in size, which would lag our trainings. To avoid the long training time and huge training cost, we often need to split our data into multiple small batches.\nIn CNN training, choosing a batch size of, say, 32 or 64 gives you the best of both worlds: you “study” small, manageable mini-quizzes, get regular feedback to adjust your filter-weights, and keep your compute requirements reasonable, all while learning robustly across the entire image dataset.\n\nimport torch\n\n# Given the amount of data, we set the batch size as 64 to improve the efficiency when running our model\nbatch_size = 64\n\n# We use DataLoader to split both the training and validation dataset into shuffled batches. \n# Shuffle helps prevent model overfitting by ensuring that batches are more representative of the entire dataset.\ndloader_train = torch.utils.data.DataLoader(\n    inputs_train, batch_size=batch_size, shuffle=True\n)\n\ndloader_val = torch.utils.data.DataLoader(\n    inputs_val, batch_size=batch_size, shuffle=False\n)\n\n\n\nStep 2: Training the CNN Classifier\nAfter carefully processing both the training and the test data, we finally came to a stage where we can train our own CNN classifier. The first thing we need to do is to decide which architecture we want to use for the model.\nArchitecture determines the way how a CNN integrate and learn from the features it extracted, and thus largely determines the performance of a model. Throughout the years, there have been several hugely successful CNN architectures, which we won’t be able to discuss in detail. Here, I will only demonstrate the architecture of LeNet-5: It reads images in a sequence that starts with a partial and combines the partials into a comprehensive one. Intuitively, the learning process of this architecture can be thought as learning to write a new character: You learn to write each stroke first, and then follow the structure of the character to put those strokes together into a complete character.\n\n\nimport torch.nn as nn\n\n# creating a CNN class\nclass ConvNeuralNet(nn.Module):\n    #  determine what layers and their order in CNN object \n    def __init__(self, num_classes):\n        super(ConvNeuralNet, self).__init__()\n        self.conv_layer1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, padding=1)\n        self.relu1 = nn.ReLU()\n        self.max_pool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv_layer2 = nn.Conv2d(in_channels=64, out_channels=192, kernel_size=4, padding=1)\n        self.relu2 = nn.ReLU()\n        self.max_pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n\n        self.conv_layer3 = nn.Conv2d(in_channels=192, out_channels=384, kernel_size=3, padding=1)\n        self.relu3 = nn.ReLU()\n        \n        self.conv_layer4 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n        self.relu4 = nn.ReLU()\n\n        self.conv_layer5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n        self.relu5 = nn.ReLU()\n        self.max_pool5 = nn.MaxPool2d(kernel_size=3, stride=2)\n        \n        self.dropout6 = nn.Dropout(p=0.5)\n        self.fc6 = nn.Linear(1024, 512)\n        self.relu6 = nn.ReLU()\n        self.dropout7 = nn.Dropout(p=0.5)\n        self.fc7 = nn.Linear(512, 256)\n        self.relu7 = nn.ReLU()\n        self.fc8 = nn.Linear(256, num_classes)\n    \n    # progresses data across layers    \n    def forward(self, x):\n        out = self.conv_layer1(x)\n        out = self.relu1(out)\n        out = self.max_pool1(out)\n        \n        out = self.conv_layer2(out)\n        out = self.relu2(out)\n        out = self.max_pool2(out)\n\n        out = self.conv_layer3(out)\n        out = self.relu3(out)\n\n        out = self.conv_layer4(out)\n        out = self.relu4(out)\n\n        out = self.conv_layer5(out)\n        out = self.relu5(out)\n        out = self.max_pool5(out)\n        \n        out = out.reshape(out.size(0), -1)\n        \n        out = self.dropout6(out)\n        out = self.fc6(out)\n        out = self.relu6(out)\n\n        out = self.dropout7(out)\n        out = self.fc7(out)\n        out = self.relu7(out)\n\n        out = self.fc8(out)  # final logits\n        return out\n\nAfter designing the network architecture, we initialize it. And if we have access to hardware acceleration (through CUDA or MPS), we move the model to that device to speed up the training.\n\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# set the model to device\nmodel = ConvNeuralNet(num_classes).to(device)\n\nNext, we will set the loss and optimizer function used during the training. These are the factors that determine how much your network learn from the mistakes and adjust the distribution of weights how it trust the “experts”.\nThe loss function is a metric that measures the classification performance. Here the Cross-Entropy Loss function is one of them that is commonly used in neural networks. The optimizer function drives the model to reflect and adjust its weights after each validation, and its parameter learning rate decides how much the model absorb from the lessons. While it seems that a higher learning rate is beneficial, it is actually not as a high learning rate could lead to severe overshooting. That’s why we set the learning rate lr = 0.01 here to prevent overly progressive learning.\n\n# set loss function\nloss_func = nn.CrossEntropyLoss()\n# set learning rate \nlr = 0.01\n# set optimizer as SGD\noptimizer = torch.optim.SGD(\n    model.parameters(), lr=lr\n) \n\nWe will train the model for 25 epochs. To ensure we’re not overfitting to the training set, we pass the validation set through the model for inference only at the end of each epoch. If we see validation set performance suddenly degrade while train set performance improves, we are likely overfitting.\nYou can run the training and fitting loop as follows, but be cautious: This cell will take a long time to run. Alternatively, you can skip 3 cells and load the model we pre-trained directly.\n\ntrain_losses = []\nval_losses = []\nval_accuracies = []\n\nnum_epochs = 25\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    \n    for i, (images, labels) in enumerate(dloader_train):  \n        images = images.to(device)\n        labels = labels.to(device)\n        \n        outputs = model(images)\n        loss = loss_func(outputs, labels)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        \n    avg_train_loss = running_loss / len(dloader_train)\n    train_losses.append(avg_train_loss)\n    \n    with torch.no_grad():\n        model.eval()\n        correct = 0\n        total = 0\n        all_val_loss = []\n        for images, labels in dloader_val:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            total += labels.size(0)\n            predicted = torch.argmax(outputs, dim=1)\n            correct += (predicted == labels).sum().item()\n            all_val_loss.append(loss_func(outputs, labels).item())\n            \n        mean_val_loss = sum(all_val_loss) / len(all_val_loss)\n        mean_val_acc = 100 * (correct / total)\n        \n        val_losses.append(mean_val_loss)\n        val_accuracies.append(mean_val_acc)\n        \n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {mean_val_loss:.4f}, Val Acc: {mean_val_acc:.2f}%')\n\nWe can visualize how training loss, validation loss and validation accuracy evolve over time.\n\nplt.figure(figsize=(8,4))\n\n# Plot Loss\nplt.subplot(1,2,1)\nplt.plot(train_losses, label='Training Loss')\nplt.plot(val_losses, label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss Curve')\nplt.legend()\n\n# Plot Accuracy\nplt.subplot(1,2,2)\nplt.plot(val_accuracies, label='Validation Accuracy', color='green')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy (%)')\nplt.title('Validation Accuracy Curve')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\nThe loss curve and validation accuracy curve show that the training loss and validation loss goes down while the validation accuracy of the model goes up as the training epochs increase. If we add more epochs (costly!), the validation accuracy of the model will be higher, but the model will also be more at risk of overfitting. To prevent which, we often need to regularize the model.\nAfter training for 25 epochs, we see our validation accuracy has passed 70%, we can save the model to file and load it again with the following codes:\n\n# save to file\ntorch.save(model, 'cnn.pt')\n\n\n# load from file and switch to inference mode\nmodel = torch.load('cnn.pt', weights_only=False)\nmodel.eval()\n\n\n\nStep 3: Inference the Classifier\nNow, we can use the trained classifier to predict the labels of the new input. But here, we are just using the test set for validation (which is not recommended).\n\ninput_tensors = []\n\nfor image in dataset_val['img'][:10]:\n    tensor = preprocess(image)\n    input_tensors.append(tensor.to(device))\n\n# stack into a single tensor\ninput_tensors = torch.stack(input_tensors)\ninput_tensors.shape\n\n\n# process through model to get output logits\noutputs = model(input_tensors)\n# calculate predictions\npredicted = torch.argmax(outputs, dim=1)\npredicted\n\n# here are the class names\ndataset_val.features['label'].names\n\n\n# Print out the output label and the true label\nfor i in range(10):\n    example = dataset_val[i]           # get the i-th example as a dict\n    image   = example['img']\n    true_id = example['label']\n    pred_id = predicted[i]\n    \n    true_label = dataset_val.features['label'].names[true_id]\n    pred_label = dataset_val.features['label'].names[pred_id]\n    \n    plt.figure(figsize=(4,4))\n    plt.imshow(image)\n    plt.title(f\"True: {true_label}   |   Pred: {pred_label}\")\n    plt.axis('off')\n    plt.show()\n\nWe can visualize the feature maps at different layers to see how the CNN see images.\n\n# Visualization of features at different layers\nimport torchvision.transforms as T\n\nto_tensor = T.ToTensor()\n\npil_img, _ = dataset_val[0]['img'], dataset_val[0]['label']\n\ninput_tensor = to_tensor(pil_img).unsqueeze(0).to(device)\n\nactivations = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activations[name] = output.detach().cpu()\n    return hook\n\nfor layer_name in ['conv_layer1','conv_layer2','conv_layer3','conv_layer4','conv_layer5']:\n    getattr(model, layer_name).register_forward_hook(get_activation(layer_name))\n\nmodel.eval()\nwith torch.no_grad():\n    _ = model(input_tensor)\n\n# Plot the feature map\nfor name, fmap in activations.items():\n    num_filters = fmap.shape[1]\n    cols = 6\n    rows = min((num_filters + cols - 1) // cols, 4)\n    fig, axes = plt.subplots(rows, cols, figsize=(cols*1.2, rows*1.2))\n    fig.suptitle(f'Feature maps from {name}', fontsize=16)\n    for i in range(rows * cols):\n        r, c = divmod(i, cols)\n        ax = axes[r, c] if rows &gt; 1 else axes[c]\n        if i &lt; num_filters:\n            ax.imshow(fmap[0, i], cmap='viridis')\n            ax.set_title(f'#{i}')\n        ax.axis('off')\n    plt.tight_layout()\n    plt.show()\n\nWe can see that the model made mostly correct predictions, despite the image quality was low that even human may have difficulty to correctly classify. This somewhat shows the advantage of CNNs over humans when confronted with complex, blurry images, but CNNs have more applications than that. They power a host of real-world applications, from enabling your smartphone’s camera to automatically recognize faces and apply portrait effects, to guiding autonomous vehicles by detecting pedestrians, road signs, and lane markings in real time. In healthcare, CNNs help radiologists spot tumors in MRI and CT scans, and dermatologists classify skin lesions from photos. They underpin optical character recognition for digitizing handwritten forms, fuel quality-control systems that spot manufacturing defects on assembly lines, and even drive wildlife monitoring by identifying animals in camera-trap images.\nThis technology is also reshaping some humanities and social science research. For example, in archaeology, CNNs are being used to categorize, complete, and translate broken clay tablets and cuneiform texts; in art history, CNNs are being used to study the pigments and materials used in paintings, as well as the expressions and gestures of the figures in them; and in anthropology, CNNs are being used to distinguish between human races and complex kinships. It is for this reason that we are here to introduce it to you! I hope you enjoyed the class and got something different out of it!\n\n\n\nKey takeaways:\n\nArtificial Neural Networks (ANNs) are programs or models that make decisions in a similar manner to the thought process of a human brain.\nConvolutional Neural Networks (CNNs) differ from other neural networks in the convolutional layer that allows them to understand features from image input in a more efficient way.\nArchitectures are central in neural networks as they determine the ways how a model learn from the input features and thereby determine the model performance.\nMachine Learning and CNNs are fun and practical in the field of humanities and social sciences!\n\n\n\nGlossary\n\n\nAdditional Resources\n\nMLU-EXPLAIN: Neural Networks: A website with straightforward explanation and interactive visualizations of neural networks (with some math and technical terms), including more professional terminologies and advanced concepts that we won’t cover in this notebook. But if you find this notebook to be too light and really hope to learn more, this is a good place to go!\nCNN Explainer: An interesting interactive tutorial that explains how CNN work in a more visual way (but you may also find the explanation a little too technical). Try it out! You can also upload your own images of interest to see how the neural network processes them and classify them. Do you get the same results as you expected? What can you say about it?\n\n\n\nReferences\n\nPinecone. Embedding Methods for Image Search. https://www.pinecone.io/learn/series/image-search\nIBM. What is a neural network? https://www.ibm.com/think/topics/neural-networks\nIBM. What are convolutional neural networks? https://www.ibm.com/think/topics/convolutional-neural-networks\nConvolutional Neural Network From Scratch. https://medium.com/latinxinai/convolutional-neural-network-from-scratch-6b1c856e1c07"
  },
  {
    "objectID": "docs/text_analysis/text_analysis.html",
    "href": "docs/text_analysis/text_analysis.html",
    "title": "Praxis",
    "section": "",
    "text": "The Coding Manual for Qualitative Researchers Qualitative Data: An Introduction to Coding and Analysis https://resources.nu.edu/c.php?g=1007180&p=7392331\n\n\nA interactive NYT article would work nicely here. A lot of their research is qualitative coding voter preferences etc\n\n\n\nhttps://www.mturk.com/ https://www.prolific.com/academic-researchers"
  },
  {
    "objectID": "docs/text_analysis/text_analysis.html#section-1-traditional-qualitative-coding",
    "href": "docs/text_analysis/text_analysis.html#section-1-traditional-qualitative-coding",
    "title": "Praxis",
    "section": "",
    "text": "The Coding Manual for Qualitative Researchers Qualitative Data: An Introduction to Coding and Analysis https://resources.nu.edu/c.php?g=1007180&p=7392331\n\n\nA interactive NYT article would work nicely here. A lot of their research is qualitative coding voter preferences etc\n\n\n\nhttps://www.mturk.com/ https://www.prolific.com/academic-researchers"
  },
  {
    "objectID": "docs/text_analysis/text_analysis.html#section-2-computational-text-analysis",
    "href": "docs/text_analysis/text_analysis.html#section-2-computational-text-analysis",
    "title": "Praxis",
    "section": "Section 2: Computational text analysis",
    "text": "Section 2: Computational text analysis\n\n2.1 Supervised machine learning\nTokenization, Text Normalization, Stop Word Removal, TF-IDF (google it + show off the relevant python/R libraries) For R: - SnowballC - tidytext Python: - scikitlearn\n\n2.1.1 Statistical frameworks\nhttps://smltar.com/ Naive Bayes, Logistic Regression, SVMs - Show off R/python libraries for this\n\n\n2.1.2 Deep learning frameworks\nhttps://www.packtpub.com/en-us/product/getting-started-with-google-bert-9781838821593\n\n\n\n2.2 Unsupervised learning\n\n2.2.1 Topic modelling\n\nLDA Introduction to Topic Modeling and Text Classification, W.J.B. Mattingly\n\n\n\n2.2.2 Clustering\n\nK-means, Hierarchical Text Mining: Classification, Clustering, and Applications, Ashok N. Srivastava\n\n\n\n2.2.3 Embeddings (briefly)\n\nWord2Vec\nSkip-Gram, CBOW Embeddings in Natural Language Processing: Theory and Advances in Vector Representations of Meaning\n\n\n\n\n2.3 Natural Language Processing (brief)\n\nExamples: dependency relationships, object detection, grammatical parsing\n\nNER\n\n\n\n\n2.4 Recent Updates\n\n2.4.1 NLI, zero-shot learning, BERT\n\n\n2.4.2 Classification and discovery using decoder only models\n\n2.4.2.1 Zero shot, few shot, RAG, fine tuning of LLMs (all very briefly! just mention them)\n\n\n2.4.2.2 LLMs using APIs vs. chat-based format (brief)\n\n\n2.4.2.3 Local models vs. sending data to a company (brief)\n\n\n\n\n2.5 AI ethics"
  },
  {
    "objectID": "pages/copyright.html",
    "href": "pages/copyright.html",
    "title": "Copyright Information",
    "section": "",
    "text": "This project uses data from a variety of sources, most available under an open data license. All other material is published under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License..\n\n\n\nOur suggested attribution for this project is:\n\n\n\n\n\n\nNelson, L., Graves, J., and other prAxIs Contributors. 2023. ‘The prAxIs Project: Creating Online Materials for Econometric Teaching’. https://comet.arts.ubc.ca/.\n\n\n\nADD DATA HERE Below as an example\n\nThe 2016 Census Data was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, 2016 Census Public Use Microdata File (PUMF). Individuals File, 2019. This does not constitute an endorsement by Statistics Canada of this product.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product. ​\n\nThe Penn World Table was provided by Feenstra, Robert C., Robert Inklaar and Marcel P. Timmer (2015), “The Next Generation of the Penn World Table” American Economic Review, 105(10), 3150-3182, available for download at www.ggdc.net/pwt\n\nThe Penn World Table Penn World Table 10.0 by Robert C. Feenstra, Robert Inklaar and Marcel P. Timmer is licensed under a Creative Commons Attribution 4.0 International License. This research received support through grants from the National Science Foundation, the Sloan Foundation and the Transatlantic Platform’s Digging into Data program.\n\nThe 2019 Survey of Financial Security was provided under the Statistics Canada Open License (Public).\n\nSource: Statistics Canada, Survey of Financial Security, 2019, 2021. Reproduced and distributed on an “as is” basis with the permission of Statistics Canada.\nAdapted from Statistics Canada, Survey of Financial Security, 2019, 2021. This does not constitute an endorsement by Statistics Canada of this product.\n\nWe also use data from the World Bank’s World Development Indicators and Quarterly Public Sector Debt databases.\n\nBoth of these are licensed under the CC by 4.0 open license.\n\nAll of the citations for the data used in the GEOG 374 notebooks are cited in the specific notebooks.\nThe data used in the projects modules is simulated, as is the salmon data in the prep modules and was created for this project. It falls under the license for the project in general (see above).\n\nThe simulation for the salmon data was based on data from the Pacific Salmon Foundation’s salmon watersheds program"
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html",
    "href": "pages/documentation/writing_self_tests.html",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "",
    "text": "An important part of notebook development is to design them so they give users formative feedback. Formative feedback helps students check if they understand a concept or skill.\nWe prefer to use immediate formative feedback, by integrating tests into the notebooks. These self-tests are run by the students and provide them with instant feedback about whether they have something correct or not.\nThis can be accomplished through the following process:\nIt is also very important to follow best practices when developing these notebooks and tests, since even small mistakes can create a great deal of confusion for users."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#general-framework",
    "href": "pages/documentation/writing_self_tests.html#general-framework",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "1 General Framework",
    "text": "1 General Framework\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nEarly in the notebook, usually in the first executed cell, include a source function call to the test scripts file:\nsource(tests.r)\nThis file should include the tests, as outlined below in Section 3. In this example, they are of the form test().\n\n\n\n\nPython Kernels\nEarly in the notebook, usually in the first executed cell, import the Tests class from the test scripts file:\nfrom .tests.py import *\nThis file should include the tests, as outlined below in Section 4. In this example, they are of the form test(), and are called like Tests.test().\n\n\n\n\n\n1.1 Use in Jupyter Notebooks (.ipynb)\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nIn the notebook, ask the students to fill in the object requested, then call the test function. Add a comment to explain what needs to be changed, if it’s not clear.\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\n\nTry to make the test function is a void call; avoid passing parameters.\nClearly indicate what to change using #comments.\nBe as specific as possible when giving directions.\n\n\n\n\n\nPython Kernels\nIn the notebook, ask the students to fill in the object requested, then call the test function. Add a comment to explain what needs to be changed, if it’s not clear.\nanswer_1 = #fill in the correct value here\n\nTests.test()\n\nTry to make the test function is a void call; avoid passing parameters.\nClearly indicate what to change using #comments.\nBe as specific as possible when giving directions."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#answers-in-.qmd-notebooks",
    "href": "pages/documentation/writing_self_tests.html#answers-in-.qmd-notebooks",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "2 Answers in .qmd notebooks",
    "text": "2 Answers in .qmd notebooks\n\nR KernelsPython Kernels\n\n\n\nR Kernels\nEarly in the notebook, usually in the first executed cell, include a source link to the test scripts file:\nsource(tests.r)\nThis file should include the tests, as outlined below in Section 3. In this example, they are of the form test().\nIn .qmd notebooks, when you write a test include two versions: one with the answers, and one without. Include meta class tags to help tell them apart, and avoid evaluation. The cell should look like:\n#| eval: false\n#| classes: \"question\"\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nfor the question, and like:\n#| eval: false\n#| classes: \"answer\"\n\nanswer_1 &lt;- the_right_answer(stuff)\n\ntest_1()\nfor the answer. This will help debug questions easily.\n\n\n\n\n\n\nTip\n\n\n\nIt’s usually easiest to write the answer first, then debug and test.\n\n\n\n\n\n\nPython Kernels\nEarly in the notebook, usually in the first executed cell, import the Tests class from the test scripts file:\nfrom .tests.py import Tests\nThis file should include the tests, as outlined below in Section 4. In this example, they are of the form test(), and are called like Tests.test().\nIn .qmd notebooks, when you write a test, include two versions: one with the answers, and one without. Include meta class tags to help tell them apart, and avoid evaluation. The cell should look like:\n#| eval: false\n#| classes: \"question\"\n\nanswer_1 = #fill in the correct value here\n\nTests.test_1()\nfor the question, and like:\n#| eval: false\n#| classes: \"answer\"\n\nanswer_1 = the_right_answer(stuff)\n\nTests.test_1()\nfor the answer. This will help debug questions easily.\n\n\n\n\n\n\nTip\n\n\n\nIt’s usually easiest to write the answer first, then debug and test."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#sec-r-tests",
    "href": "pages/documentation/writing_self_tests.html#sec-r-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "3 Writing R Self-Tests",
    "text": "3 Writing R Self-Tests\nSelf-test scripts are R files (.r) which supply the testing functions. They use two libraries:\n\nlibrary(testthat): a test assertion library, which provides functions to check if something is correct and give feedback.\nlibrary(digest): a hash library, which computes and check hash functions.\n\nHere is an example of the first function of a file and the library headers:\nlibrary(testthat)\nlibrary(digest)\n\ntest_1 &lt;- function() {\n  test_that(\"Solution is incorrect\", {\n    expect_equal(digest(answer1), \"dbc09cba9fe2583fb01d63c70e1555a8\")\n  })\n  print(\"Success!\")\n}\nThis creates a function (test1()) that when called in the Jupyter notebook:\n\nFinds the object answer1.\nComputes the hash of it (digest(answer)) and compares it to the string dbc09cba9fe2583fb01d63c70e1555a8 (the correct answer’s hash).\nIf they match, it prints “Success!” otherwise it throws an error.\n\nIn order to develop the test, you can use this template:\n\nCreate a new cell to contain the test. If this a .qmd test, make it the answer version of the test.\nCreate a new function in the script file with a unique name (test_n()) and the answer (answer_n) to test in the testing script.\nCompute digest(answer_n) to get the correct has value.\nAdd it to the expect_equal element in the script.\nIf a .qmd copy the answer, and change it to a question. Then, replace the correct answer with a comment.\n\nNote that you may not want to test the entire object, but rather some particular part of it, such as answer_n$coefs; see Section 3.2 for details.\n\n3.1 Richer Feedback\nThe previous method only tests if an answer exactly matches the correct answer. If there are common errors you may want to give a hint about what is wrong. For example, in a multiple-choice question, answers A and B reflect common misconceptions.\nYou can use tests to give this kind of feedback with a more complex test function. Use the case_when function to give varied responses depending on the answer given by the student. For example:\ntest_1 &lt;- function(answer_1) {\n    ans &lt;- digest(answer_1)\n    case_when(ans == \"dbc09cba9fe2583fb01d63c70e1555a8\" ~ test_that(TRUE),\n             ans == \"dd531643bffc240879f11278d7a360c1\" ~ \n              \"This is a common misconception, remember that...\",\n              TRUE ~ test_that(FALSE))\n}\nYou can adapt this framework for more complex tests, as necessary.\n\n\n\n\n\n\nA Note on Feedback\n\n\n\nIt is important to provide feedback that will guide the student towards the right answer and a greater understanding of the topic at hand. Try not to give feedback along the lines of “That is correct, congratulations!” or “I’m sorry, that is incorrect!.” Feedback should point out the error that students are making and guide them to the correct answer.\n\n\n\n\n3.2 Important Notes\nHere are some common pitfalls and notes about creating tests. The main idea is that hash functions are exact: the objects must be exactly the same. This means you should:\n\nAlways round numbers to 3 or 4 decimal places using the round() function. Do this in the testing function, rather than making students do it.\nNever test objects that include arbitrary elements, such as names or sequences.\nOnly test the simplest object necessary, not the easiest one to test.\n\nFor example, the following objects will return different hashes:\nd1 &lt;- data.frame(age = \"12\")\nd2 &lt;- data.frame(Age = \"12\")\n\ndigest(d1) # == d2da0d698613f4cafa7d6fe5af762294\ndigest(d2) # == cfe4cbf9291d5705b2c61422098db883\nHere are some examples of arbitrary elements that you can miss:\n\nObject or variable names (Age != age)\nRegression models (y ~ x1 + x2 != y ~ x2 + x1)\nFloating point numbers (1.222222222222 != 1.222222222222)\nMethods that us randomization (e.g., Monte Carlo methods)\n\nBottom line: only test mathematical or textual objects, not programming objects unless you are very, very explicit about them."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#sec-py-tests",
    "href": "pages/documentation/writing_self_tests.html#sec-py-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "4 Writing Python Self-Tests",
    "text": "4 Writing Python Self-Tests\nPython self-test scripts are Python files (.py) which supply the testing function in a test class. They use two libraries:\n\nunittest: a test assertion library, which provides functions to check if something is correct and give feedback.\nhashlib: a hash library, which computes and check hash functions, and report the hexdigest of one.\n\nHere is an example of the first function of a file and the library headers:\n\nfrom hashlib import blake2b\nimport unittest import TestCase as t\n\n# Don't change this one\ndef hash(data):\n    h = blake2b(digest_size=20)\n    h.update(data)\n    return h.hexdigest()\n\n\nclass Test():\n\n  def test1():\n    t.assertEqual(hash(answer1), \"dbc09cba9fe2583fb01d63c70e1555a8\")\nSee Section 3.1 and Section 3.2 for guidelines above writing richer tests, and some common mistakes. The issues and advice applies to Python as well."
  },
  {
    "objectID": "pages/documentation/writing_self_tests.html#other-uses-for-tests",
    "href": "pages/documentation/writing_self_tests.html#other-uses-for-tests",
    "title": "Writing Self-Tests for Notebook Development",
    "section": "5 Other Uses for Tests",
    "text": "5 Other Uses for Tests\nYou can also write “hidden” tests for developers; this is recommended when you have a complex example with interdependent parts. Try to make these as hidden as possible from the main notebook; hide them in a supplemental file which is included at runtime."
  },
  {
    "objectID": "pages/index/index_AMNE-376.html",
    "href": "pages/index/index_AMNE-376.html",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "",
    "text": "This section contains material to support UBC’s Greek Art and Architecture (AMNE 376). The visual culture of the ancient Greek world in the second and first millennia BCE, especially from c. 1000 to 30 BCE. Credit will be granted for only one of CLST_V 331, AMNE_V 376 or ARTH_V 331. Equivalency: ARTH_V 331 or CLST_V 331."
  },
  {
    "objectID": "pages/index/index_AMNE-376.html#amne-376-suggested-lesson-plan",
    "href": "pages/index/index_AMNE-376.html#amne-376-suggested-lesson-plan",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "AMNE 376: Suggested Lesson Plan",
    "text": "AMNE 376: Suggested Lesson Plan\n80-minute lecture or 50-minute discussion on computer vision, image embedding, and their applications in Archaeology and Art History.\nAgendas are provided for both lecture and discussion formats.\n\n\nLearning Objectives\nBy the end of this lesson, students will:\n\nBecome familiar with concepts such as computer vision, convolution, convolutional neural network (CNN), and image embeddings.\nUnderstand how computers “see” and distinguish between different images by identifying unique visual elements and quantifying their similarity.\nExplore photographs of Kouroi from Richter’s “Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942)” and create image embeddings for these photographs using pre-trained models.\nLearn how to cluster and classify Kouroi based solely on photographs and critically analyze the advantages and limitations of these techniques and their potential applications in archaeology and art history.\n\n\n\n\nMaterials and Technical Requirements\n\nJupyter notebook (hosted on the prAxIs UBC website)\nDevice with internet access (laptop preferred)\nNo previous coding experience required (familiarity with Python is an asset)\nStudents may pair up (groups of 2 or 3) if device access is limited\nGroup work is encouraged; students will compare findings\n\n\n\n\nPre-lesson Requirements\nInstructor should: - Test run the notebook using Jupyter Open and be familiar with the content. - (Optionally) Instruct students how to run code on Jupyter Open if they are interested in exploring on their own.\nStudents should: - Complete all required readings for “The Archaic World: Temples, Statues, and Colour”. - Browse the text explanations in the static notebook on the prAxIs UBC website. - (If time permits) Browse through the notebooks on convolution and CNN."
  },
  {
    "objectID": "pages/index/index_AMNE-376.html#agenda-for-the-lecture",
    "href": "pages/index/index_AMNE-376.html#agenda-for-the-lecture",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "Agenda for the Lecture",
    "text": "Agenda for the Lecture\nIf this format is chosen, students don’t have to run code themselves, but they may explore after class. Students should have access to image folders and interactive visualization objects.\n\nPre-discussion and brief lecture on Jupyter Notebook, introducing students to the programmatic environment (5–10 min)\nIntroduction section and basics of convolution (10 min)\nDataset exploration, creation of embeddings, PCA visualization and discussion about commonalities among clustered images (20 min)\nIntroduce classification using image embeddings, evaluation of results, group activities (try to classify yourself, strengths and limitations of classifier, etc.) (20 min)\nLead discussion on machine learning applications in archaeology/art history (formalism, relic restoration, the future of archaeology) (15 min)\nConclusions and takeaways (5–10 min)"
  },
  {
    "objectID": "pages/index/index_AMNE-376.html#agenda-for-the-discussion",
    "href": "pages/index/index_AMNE-376.html#agenda-for-the-discussion",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "Agenda for the Discussion",
    "text": "Agenda for the Discussion\nIf the discussion format is chosen, group activities are emphasized and all students should run the notebook code themselves.\n\nBefore discussion: Instructions on how to load/run code on Jupyter Open so students can explore immediately.\nOpen the notebook; execute first two sections and discuss computer vision and convolution (10 min)\nExecute Section 3, 4, 5; discuss observations based on visualized image embeddings, commonalities among Kouros clusters (15 min)\nTry to classify materials/groups of Kouroi based on images, check accuracy (5 min)\nExecute Section 6, discuss classifier quality, metrics, applications, and improvements (15 min)\nClosing discussion (5 min)"
  },
  {
    "objectID": "pages/index/index_AMNE-376.html#activity-materials",
    "href": "pages/index/index_AMNE-376.html#activity-materials",
    "title": "Greek Art and Architecture (AMNE 376)",
    "section": "Activity Materials",
    "text": "Activity Materials\n\nIn-Class Discussion Questions\n\nDescribe the differences and similarities between how computers and humans identify similarity between two images.\nFind photographs of Kouroi that correspond to scatter plot points; discuss patterns among Kouroi that cluster together. What features do you think the embeddings captured?\nDescribe features that characterize a Kouros: what would you focus on to classify their era or material based only on photos?\nDiscuss what different insights the CNN model may provide, and what might be biased by the model itself.\n\n\n\n\nDiscussion Post Questions\n\nList some AI applications in archaeology/art history research. Do you think any can be replaced by human labor?\nBased on your knowledge of computer vision, do you think CNN models can analyze objects more objectively? Why?\nDiscuss whether you would trust the use of AI in the following for studying cultural relics/artifacts: style analysis, sentiment analysis, cultural relic restoration, age identification. Why?"
  },
  {
    "objectID": "pages/index/index_ECON227.html",
    "href": "pages/index/index_ECON227.html",
    "title": "ECON227 - Data in Economics",
    "section": "",
    "text": "This section contains material to support UBC’s Data in Economics: Application-driven introduction to the analysis of economic data. Descriptive analysis, causality, experimental and observational data, hypothesis testing. Restricted to BIE students."
  },
  {
    "objectID": "pages/index/index_ECON227.html#modules",
    "href": "pages/index/index_ECON227.html#modules",
    "title": "ECON227 - Data in Economics",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nECON 227 - How Do Large Language Models Predict?\n\n\nThis notebook aims to explain how large language models (LLMs) work and make predictions through an example of stock price prediction based on historical price data and news…\n\n\n\n4 Aug 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_SOCI280.html",
    "href": "pages/index/index_SOCI280.html",
    "title": "SOCI280 - Data and Society",
    "section": "",
    "text": "110-minute Lesson Plan Overview\n\n\nBy the end of this lesson, students will:\n\nUnderstand and complete sentiment analysis to detect emotional tone (positive, negative, neutral), and understand and complete toxicity analysis to identify harmful or aggressive language (e.g., insults, threats).\nIntroduce concepts in statistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian).\nLearn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than normal political tweets?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\n\nThrough this analysis, we’ll explore various dimensions of AI applications, critically examining how it can better understand and detect the patterns of disinformation when working with large amounts of social data.\n\n\n\n\n\nJupyter notebook (hosted on the prAxIs UBC website)\nDevice with internet access (laptop preferred)\nNo coding experience required (familiarity with Python is an asset)\nStudents may pair up (groups of 2–3) if device access is limited\nGroup work is encouraged; students will compare findings\n\n\n\n\n\n\nStudents have completed the reading up to Section 0 (approx. 5–10 minutes)\nInstructor has loaded and can project the notebook\nStudents reminded to bring a device"
  },
  {
    "objectID": "pages/index/index_SOCI280.html#soci-280-suggested-lesson-plan-ai-text-analysis-and-disinformation",
    "href": "pages/index/index_SOCI280.html#soci-280-suggested-lesson-plan-ai-text-analysis-and-disinformation",
    "title": "SOCI280 - Data and Society",
    "section": "",
    "text": "110-minute Lesson Plan Overview\n\n\nBy the end of this lesson, students will:\n\nUnderstand and complete sentiment analysis to detect emotional tone (positive, negative, neutral), and understand and complete toxicity analysis to identify harmful or aggressive language (e.g., insults, threats).\nIntroduce concepts in statistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian).\nLearn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than normal political tweets?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\n\nThrough this analysis, we’ll explore various dimensions of AI applications, critically examining how it can better understand and detect the patterns of disinformation when working with large amounts of social data.\n\n\n\n\n\nJupyter notebook (hosted on the prAxIs UBC website)\nDevice with internet access (laptop preferred)\nNo coding experience required (familiarity with Python is an asset)\nStudents may pair up (groups of 2–3) if device access is limited\nGroup work is encouraged; students will compare findings\n\n\n\n\n\n\nStudents have completed the reading up to Section 0 (approx. 5–10 minutes)\nInstructor has loaded and can project the notebook\nStudents reminded to bring a device"
  },
  {
    "objectID": "pages/index/index_SOCI280.html#agenda",
    "href": "pages/index/index_SOCI280.html#agenda",
    "title": "SOCI280 - Data and Society",
    "section": "Agenda",
    "text": "Agenda\n\n1. Pre-discussion and Brief Lecture (5–10 minutes)\n\nDiscuss misinformation, disinformation, and propaganda\nEmphasize the role of digital platforms in their spread\nHow researchers detect disinformation\nConsider influence of new technology and personal data\n\nExample:\nhttps://www.engadget.com/ai/researchers-secretly-experimented-on-reddit-users-with-ai-generated-comments-194328026.html\n\n\n\n2. Load the notebook (5 minutes)\n\nStudents open the Jupyter notebook\nPair students without devices\nInstruct all to complete Section 0\n\n\n\n\n3. Section 0 (20 minutes)\n\nWork through code/activities in Section 0\nUpon reaching the Screen Time activity, pause for a brief discussion to compare results\n\n\n\n\n4. Section 1 (15 minutes)\n\n2–3 min. explanation of classification and course connections\nStudents complete Section 1 at their own pace (~15 min.)\n\n\n\n\n5. Sections 3–4 (20 minutes)\n\nPause to discuss Section 1 findings in a group\nStudents complete the remainder of the notebook\n\n\n\n\n6. Takeaways and Activity (5–10 minutes)\n\nTime-dependent; includes:\n\nQuestions about methods\nBrief lecture summarizing key takeaways\nBegin participation activities (discussion posts, worksheets, etc.)"
  },
  {
    "objectID": "pages/index/index_SOCI280.html#activity-materials",
    "href": "pages/index/index_SOCI280.html#activity-materials",
    "title": "SOCI280 - Data and Society",
    "section": "Activity Materials",
    "text": "Activity Materials\n\nDiscussion Post Questions\nRespond in 100–250 words for each:\n\nHow can sentiment analysis be useful in answering research questions? Can you think of any tasks it would be well suited to?\nDo you think the methods in the notebook were useful in understanding disinformation campaigns? What might be some of the limitations to these approaches?\nDiscuss the role AI plays in disinformation, both the detection and analysis of it, and the production of it.\n\n\n\n\nActivity: Identify Disinformation Online\nOn a social media platform of your choice, try to identify a post you believe to be disinformation.\nYou can search for specific topics or wait for something in your feed.\nLink to the content here: _______________________________________________________\n\nWhy do you think this is disinformation?\nWhat data/information are you using from the content and its features to come to this conclusion?\nExplain your reasoning in 150–300 words.\nCompare with the Classifier:\nThink back to the classifier in the notebook.\nWhat data was it using to classify text as disinformation?\nIs that process similar or different from how you identified your post?\nDo you think you are more likely to be correct? If so, why?\nRespond in 200–350 words."
  },
  {
    "objectID": "pages/index/index_SOCI280.html#modules",
    "href": "pages/index/index_SOCI280.html#modules",
    "title": "SOCI280 - Data and Society",
    "section": "Modules",
    "text": "Modules\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "pages/index/index_convolution.html",
    "href": "pages/index/index_convolution.html",
    "title": "Convolution",
    "section": "",
    "text": "The modules in this unit are for the topics with convolution. ADD / CHANGE"
  },
  {
    "objectID": "pages/index/index_convolution.html#modules",
    "href": "pages/index/index_convolution.html#modules",
    "title": "Convolution",
    "section": "Modules",
    "text": "Modules"
  },
  {
    "objectID": "pages/installation/installing_for_development.html",
    "href": "pages/installation/installing_for_development.html",
    "title": "Installing for Development",
    "section": "",
    "text": "So, you want to develop and submit new material to prAxis? Awesome! The official workflow for hacking on COMET uses VSCode. Other IDEs such as RStudio or Jupyter will also work, but we do not provide official instructions.\n\nFirst, you will need to install R using your preferred method for your platform. If you’re not sure what this means, follow the instructions on the R website.\nInstall VSCode using your preferred method for your platform (the linked instructions will be fine for most people). Open VSCode and install the REditorSupport extension, then close VSCode.\nInstall git using your preferred method for your platform (the linked instructions will be fine for most people). If you are developing on the Github version, you may also wish to install GitHub Desktop.\nFinally, install Quarto, and follow the steps to integrate VSCode.\nOpen an R terminal and install renv by runnning install.packages(\"renv\").\nUse git to clone the COMET repository and open it in VSCode. A popup should ask you to install languageserver, click yes.\nWhen Step 6 is complete, restart VSCode. Clone the COMET repository folder and open it. Select the terminal option in the bottom panel and choose “R Terminal” in the dropdown menu next to the plus sign.\nIn the VSCode terminal, run renv::restore(), and type Y to accept installing the packages.\n\nCongratulations, you’re ready to run and hack on all of the COMET notebooks!"
  },
  {
    "objectID": "pages/installation/installing_locally_old.html",
    "href": "pages/installation/installing_locally_old.html",
    "title": "Install and Use COMET",
    "section": "",
    "text": "A Word of Warning\n\n\n\nWe are still fine-tuning this part. It might not work properly. Let us know if you have any issues here (comet.project at ubc dot ca)\nWe have designed COMET to be easy to use and install for a wide range of students. If there’s one thing we know about teaching data and econometrics, it’s that everyone finds their own routine and tools that work best for them. We encourage you to explore and try different things - see what works best for you!\nIf you’re not sure, start with the cloud-based option, then get more sophisticated in the future."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#installing-packages",
    "href": "pages/installation/installing_locally_old.html#installing-packages",
    "title": "Install and Use COMET",
    "section": "Installing Packages",
    "text": "Installing Packages\nYou may at some points need to install some extra packages if you are not working on JupyterOpen. You can do this by opening the server, then clicking on the R Console in the launcher tab.\n\nOnce the console opens, you should see a command line with R version 4.2.1 (2022-06-23 ucrt) or something similar.\nIn the bottom cell window, you should enter:\n\ninstall.packages(c(\"tidyverse\", \"car\", \"stargazer\", \"estimatr\", \"sandwich\"))\nthen hit ctrl-enter to run the command. It should start installing, and may prompt you to select a CRAN mirror (choose any one near you). Be patient: this might take a while!\nYou should only have to do this once for each server you work with."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#create-a-project",
    "href": "pages/installation/installing_locally_old.html#create-a-project",
    "title": "Install and Use COMET",
    "section": "Create a Project",
    "text": "Create a Project\nNext, you’ll want to download the files. Select launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer.\n\nExtract the .zip file to a folder on your computer that you can find easily and give it a name (e.g. comet-project).\nIn RStudio, go to “File &gt; New Project” then select “Existing Directory”. Browse and select the directory from the previous step.\n\nYou’re now ready to go! Use RStudio’s file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .Rmd or .qmd file to open the notebook."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#sec-step1",
    "href": "pages/installation/installing_locally_old.html#sec-step1",
    "title": "Install and Use COMET",
    "section": "Step 1: Install the Environment Manager",
    "text": "Step 1: Install the Environment Manager\nDownload the most recent version of miniconda for your computer operating system from:\nhttps://docs.conda.io/en/latest/miniconda.html\nThe version selection is a little bit different for different operating systems, so click on the appropriate tab below.\n\nWindows 10/11Macintosh\n\n\nIt is important to pay attention to the version you download: different processors will require different versions.\n\nTo check your processor, open the Start menu and search “Processor” then click “View Processor Info”\nUnder “Device Specifications” look at System Type.\n\nif this says 64-bit operating system or something like x64-based processor choose the 64-bit version\nif this says 32-bit operating system choose the 32-bit version.\n\n\n\n\nIt is important to pay attention to the version you download: different processors will require different versions.\n\nTo check your processor, click on the Apple logo in the top-left of your screen, and select “About This Mac”\nLook for the processor information.\n\nif this says something like 3.2 GHz Intel Core i5 or something like x64-based processor choose the macOS Intel x86 64-bit version\nif this says something like Apple M1 choose the macOS Apple M1 64-bit version\n\nWe recommend choosing the .pkg version of the installer.\n\n\n\n\nOnce you have downloaded the installer, run the installer.\n\nMake sure you choose a sensible installation location; you can ignore any warnings about spaces in names.\nCheck the following options, if available:\n\nCreate shortcuts\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.10\nClear the package cache upon completion.\n\nRun the installer, which can take a while."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-2-install-the-environment",
    "href": "pages/installation/installing_locally_old.html#step-2-install-the-environment",
    "title": "Install and Use COMET",
    "section": "Step 2: Install the Environment",
    "text": "Step 2: Install the Environment\nNow that we have our environment manager installed, we need to add in the necessary packages.\n\n\n\n\n\n\nTip\n\n\n\nThis will take a while and requires a stable internet connection; make sure you’re plugged in and not on a bus or something!\n\n\nTo make this easier, we have create an environment file, which contains all of the necessary packages and installation files for miniconda. Download this file and place it in a directory that you can easily find.\n\nYou can find this file here. Right-click, save-as, to download.\n\nRight-click on comet-environment.yml and write down the file path. You will need this in a moment. Next, launch your system’s command prompt:\n\nWindows 10/11Macintosh\n\n\n\nOpen the Start Menu and type in cmd\nRight-click on “Command Prompt” and/or select “Run as Administrator”\nAgree to the warning that pops up, if it does.\n\n\n\n\nClick the Launchpad icon in the Dock, type Terminal in the search field, then click “Terminal”.\n\nIf this doesn’t work, open the Finder, then open the /Applications/Utilities folder, and finally double-click Terminal.\n\n\n\nOnce your command prompt is running, enter the following command:\nconda env create -f \"MYPATH/comet-environment.yml\"\nreplacing \"MYPATH/ with the file path you noted earlier. Hit enter to run it.\nminiconda will run, and install all of the files. This may take some time, so grab a sandwich, and don’t turn-off your computer."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-3-configure-the-irkernel",
    "href": "pages/installation/installing_locally_old.html#step-3-configure-the-irkernel",
    "title": "Install and Use COMET",
    "section": "Step 3: Configure the IRkernel",
    "text": "Step 3: Configure the IRkernel\nThe last major step is to set up the kernel properly. Enter the following into the command prompt and hit enter:\nconda activate comet\nThen, type R. Once R loads, enter the following two commands, hitting enter to run each one:\ninstall.packages('IRkernel')\nIRkernel::installspec()\nThey should complete, and you’re now ready to go. Close the command prompt."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-4-download-the-notebooks",
    "href": "pages/installation/installing_locally_old.html#step-4-download-the-notebooks",
    "title": "Install and Use COMET",
    "section": "Step 4: Download the Notebooks",
    "text": "Step 4: Download the Notebooks\nNext, you’ll want to download the files. Select launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer.\n\nExtract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. Find the file path of the this directory, and copy it down."
  },
  {
    "objectID": "pages/installation/installing_locally_old.html#step-5-using-jupyter-and-creating-a-short-cut",
    "href": "pages/installation/installing_locally_old.html#step-5-using-jupyter-and-creating-a-short-cut",
    "title": "Install and Use COMET",
    "section": "Step 5: Using Jupyter and Creating a Short-Cut",
    "text": "Step 5: Using Jupyter and Creating a Short-Cut\nTest your Jupyter installation by opening a new command prompt, then entering the following two commands:\ncd FILEPATH\nconda activate comet\njupyter lab\nwhere FILEPATH is the directory from Step 4, above.\nYour web-brower should launch, and Jupyter should load. You can now load or create a notebook. Use the file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .ipynb file to start the notebook.\n\n\n\n\n\n\nImportant\n\n\n\nThe command window will stay open, and report the server status. Don’t close this window until you’ve saved your work or your JupyterHub will die and you’ll have to re-do everything.\n\n\nWhenever you want to launch JupyterLab, repeat the two steps above. This can be a little tedious: an alterative is to create a shortcut, which you can do below.\n\nCreating a Shortcut\nThis is different for other operating systems, so choose the version.\n\nWindowsMacintosh\n\n\nOpen notepad from the Start Menu, and then enter:\n@call conda run -n comet --no-capture-output jupyter lab \n@CMD /K\nSave this file as run_comet.bat and place it in your comet-project folder. When you double-click on it, it should immediately launch Jupyter Lab for you in the associated folder.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI haven’t tested this yet! Let me know if it’s busted.\n\n\nClick the Launchpad icon in the Dock, type TextEdit in the search field, then click “TextEdit”.\nLaunch the terminal, and type in nano to launch the (very old-school) nano text editor. In the editor, enter:\n#!/bin/zsh\nconda run -n comet --no-capture-output jupyter lab \nread\nThen save the file by hitting CTRL and X on your keyboard together, and type in run_comet.sh, then hit y to save. The file should now be saved to your computer as run_comet.sh. Make sure you place it in the comet-project folder. There’s one last step: making it executable.\nIn your Terminal then enter:\ncd FILEPATH\nwhere FILEPATH is the location of your run_comet.sh script. Then, enter:\nchmod 755 run_comet.sh\nFinally, to make it run on your computer:\n\nfind the file in your Finder, and right-click, and select “Open with…” and select “Other…”.\ntoggle the dropdown to “All Applications” from “Recommended Applications”\nunder “Utilities” select Terminal\ncheck the “always use this application” box, and hit OK.\n\nYou should now be able to double-click the shortcut to launch Jupyter on your computer!\nAll done! No more command line stuff, hopefully!"
  },
  {
    "objectID": "pages/installation/jupyterhub_setup.html",
    "href": "pages/installation/jupyterhub_setup.html",
    "title": "Accessing COMET using a JupyterHub",
    "section": "",
    "text": "The easiest way to load and use these notebooks is via a JupyterHub. This cloud-based server hosts all of the computational resources needed to run the notebooks. Better still, since it runs in the cloud you don’t need to install anything on your computer, and the performance is not affected by your system resources.\n\nTo launch in a cloud environment, select launch COMET from the top navigation bar, and then choose your hub. For UBC students, we recommend JupyterOpen, which is a UBC-supported hub which we maintain, and has all of the necessary packages pre-installed and robust space and processor support. Note that you must have a Campus-Wide Login (CWL) in order to access these resources.\nIf JupyterOpen is not working, you can also use PIMS Syzygy (Siz-za-gee). There is a UBC-specific version of this server, and several others that use a Google account or other authentication method.\nFinally, if you are part of the Google ecosystem, you can launch them on Google Collab which has a slightly different appearance but it otherwise the same.\n\n\n\n\n\n\n\nWarning\n\n\n\nSyzygy and Google Collab are not officially supported UBC software, and may not be privacy-compliant. You should take responsibility for your own use of these resources if you choose to use them. If Google already has all your data, too bad!\n\n\nOnce you are on the server, use the file navigation on the left to find the notebook you are interested in: we recommend starting with intro_jupyter which is under docs/econ_intro. Open the folder and click on the .ipynb file to start the notebook."
  },
  {
    "objectID": "pages/installation/python_setup.html",
    "href": "pages/installation/python_setup.html",
    "title": "Using Python",
    "section": "",
    "text": "This page explains how to set up Python in order to access and use the COMET notebooks. Credit goes to Professor Daniel Chen and the Mac and Windows installation pages. Additional credit goes to https://www.digitalocean.com/community/tutorials/install-python-windows-10. We will go over how to install Python and how to use it locally through Jupyter, on JupyterHub, and through VSCode."
  },
  {
    "objectID": "pages/installation/python_setup.html#installing-python",
    "href": "pages/installation/python_setup.html#installing-python",
    "title": "Using Python",
    "section": "1. Installing Python",
    "text": "1. Installing Python\nThe first thing we’ll need to do is install Python onto our computer.\n\nWindowsMacOS\n\n\n\nOpen the Miniforge Installation Platform and select the appropriate link. For Windows users, this will be the Windows artifact.\nOnce you’ve downloaded the artifact, you can run the installer and use all of the default options. The install location should look like C:\\Users\\YOUR_USER_NAME\\miniforge3.\nNext, open the Start Menu and search for “Miniforge Prompt”. When this opens you will see a prompt similar to (base) C:\\Users\\your_name. We’ll also check that the Python installation works by running\n\npython --version. This should return Python 3.11.0 or greater. If it does not, confirm that you are in the (base) environment and update the base python with:\nconda install python=3.11.\n\n\n\nYou’ll want to first set your default Terminal shell to Bash as opposed to Zsh. Open the Terminal (see this video for help) and type chsh -s /bin/bash. The close the Terminal and reopen it (this restarts the Terminal). Now, you should see Bash at the top of your Terminal.\nNext, open the Miniforge Installation Platform and select the appropriate link. Check your device type by clicking “About This Mac” on the top left of your screen - if your chip is M1, M2,…, download the arm64 artifact. Otherwise, download the x86_64 artifact. Make sure that this file is in your Downloads folder.\nOpen the terminal and run the following code: bash ${HOME}/Downloads/Miniforge3.sh -b -p \"${HOME}/miniforge3\". You may need to rename the file Miniforge.sh.\nNow you can run source \"${HOME}/miniforge3/etc/profile.d/conda.sh\" conda activate conda init.\nYou should now have access to Python! If the installation was successful, you will see (base) in your terminal before your device name and username. To confirm that conda is working, run\n\nconda --version. You should see something like this: conda 23.5.2. We will also confirm that Python is working. To do so, run python --version. It should return Python 3.11.0. If you do not see Python &gt;3.11, close your terminal and open a new one. Confirm that you are in the (base) environment. Then update the base python with:\nconda install python=3.11."
  },
  {
    "objectID": "pages/installation/python_setup.html#installing-the-python-kernel",
    "href": "pages/installation/python_setup.html#installing-the-python-kernel",
    "title": "Using Python",
    "section": "2. Installing the Python Kernel",
    "text": "2. Installing the Python Kernel\nWe’ll need to install the Python kernel in order to use the Python programming language in Jupyter. To do so, in your terminal, run python. You should see a line with your version of Python and the date that you loaded it.\nThen, run the following lines of code in the terminal:\nconda install ipykernel\nSelect yes if prompted. This will install the Python kernel which will allow us to run Python code!"
  },
  {
    "objectID": "pages/installation/python_setup.html#using-python-locally-through-jupyter",
    "href": "pages/installation/python_setup.html#using-python-locally-through-jupyter",
    "title": "Using Python",
    "section": "3. Using Python Locally through Jupyter",
    "text": "3. Using Python Locally through Jupyter\nThe first option for running Python is locally through Jupyter. Some students may prefer to use the local version of Jupyter that is accessible via browser, rather than through Jupyter desktop. The latter allows for more customizability at the expense of a more intuitive installation and activation process.\n\n3.1. Creating a New Environment\nHere, we will create a new Python environment called “comet”. An environment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nTo do this, in the terminal, enter the following line code:\nconda create -n python_comet_env jupyterlab\nWhat this is doing is creating an environment called python_comet_env using the conda package manager, and we’re asking it to include Jupyter in the environment.\nThen, to enable the environment, run conda activate python_comet_env.\n\n\n\n\n\n\nWarning\n\n\n\nEvery time you want to run a COMET notebook through Jupyter, you will have to run conda activate python_comet_env. Otherwise, your computer will not know how to access Jupyter."
  },
  {
    "objectID": "pages/installation/python_setup.html#opening-jupyter",
    "href": "pages/installation/python_setup.html#opening-jupyter",
    "title": "Using Python",
    "section": "3.2. Opening Jupyter",
    "text": "3.2. Opening Jupyter\nFinally, to open Jupyter, run the following command in your terminal: jupyter lab. This will open up Jupyter as a local copy on your search engine. This must be done after you run conda activate [name of environment]\", otherwise your computer will not know where to find Jupyter.\n\n\n\n\n\n\nWarning\n\n\n\nThis terminal acts as your local Jupyter server. Closing it will shut down your server!"
  },
  {
    "objectID": "pages/installation/python_setup.html#opening-the-comet-modules-locally",
    "href": "pages/installation/python_setup.html#opening-the-comet-modules-locally",
    "title": "Using Python",
    "section": "3.3. Opening the COMET modules Locally",
    "text": "3.3. Opening the COMET modules Locally\nLastly, you’ll want to download the COMET files. In the COMET website, press launch COMET from the top navigation bar, and then click on “launch locally”. This will download a file (main.zip) to you computer. Extract the .zip file to a folder on your computer that you can find easily and rename it from main to comet-project. When you launch Jupyter, you will see all of the files on your computer. Locate the folder where you saved the COMET and start working from there!"
  },
  {
    "objectID": "pages/installation/python_setup.html#using-python-remotely-through-jupyterhub",
    "href": "pages/installation/python_setup.html#using-python-remotely-through-jupyterhub",
    "title": "Using Python",
    "section": "4. Using Python Remotely through JupyterHub",
    "text": "4. Using Python Remotely through JupyterHub\nJupyterHub is a cloud-based server hosts all of the computational resources needed to run the notebooks. Since it runs in the cloud, you don’t need to install anything on your computer, and the performance is not affected by your system resources.\nTo launch in a cloud environment, select launch COMET from the top navigation bar, and then choose your hub. For UBC students, we recommend JupyterOpen, which is a UBC-supported hub which we maintain, and has all of the necessary packages pre-installed and robust space and processor support. Note that you must have a Campus-Wide Login (CWL) in order to access these resources.\nIf JupyterOpen is not working, you can also use PIMS Syzygy (Siz-za-gee). There is a UBC-specific version of this server, and several others that use a Google account or other authentication method.\nFinally, if you are part of the Google ecosystem, you can launch them on Google Collab which has a slightly different appearance but it otherwise the same.\n\n\n\n\n\n\nWarning\n\n\n\nSyzygy and Google Collab are not officially supported UBC software, and may not be privacy-compliant. You should take responsibility for your own use of these resources if you choose to use them. If Google already has all your data, too bad!\n\n\nOnce you are on the server, use the file navigation on the left to find the notebook you are interested in!"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#prerequisites",
    "href": "pages/installation/vscode_setup.html#prerequisites",
    "title": "Using VSCode",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nHave installed VSCode on your device."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#why-use-an-ide",
    "href": "pages/installation/vscode_setup.html#why-use-an-ide",
    "title": "Using VSCode",
    "section": "Why use an IDE?",
    "text": "Why use an IDE?\nAn IDE, also known as a integrated development environment, is a software application that streamlines software development. Using an IDE such as VSCode is often better than locally hosting a jupyter notebook because it allows us to avoid using CLIs, offers code assistance just as syntax highlighting, and has a built-in highly customizable and extensible environment with a vast library of extensions."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#using-comet-with-vscode",
    "href": "pages/installation/vscode_setup.html#using-comet-with-vscode",
    "title": "Using VSCode",
    "section": "Using COMET with VSCode",
    "text": "Using COMET with VSCode"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-r",
    "href": "pages/installation/vscode_setup.html#installing-r",
    "title": "Using VSCode",
    "section": "1. Installing R",
    "text": "1. Installing R\nThe first thing we’ll need to do is install R onto our computer.\n\nWindowsMacOS\n\n\n\nHead to cran.r-project.org for windows and press Download R-4.4.0 for Windows\nSelect Okay &gt; Next and select your destination location.\nSelect the components Main files, 64-bit files and Message translations. Then, press next.\nSelect the toggle No (acccept defaults) and press next.\nSelect both save version number in registry and associate R with .RData files.\nOnce the installation is complete, press Finish.\n\n\n\n\nHead to cran.r-project.org for macOSx and select the release (on the left-hand side) suited for your device type.\nOpen the installer and select continue &gt; continue &gt; continue &gt; agree &gt; install."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-a-r-package-compiler",
    "href": "pages/installation/vscode_setup.html#installing-a-r-package-compiler",
    "title": "Using VSCode",
    "section": "2. Installing a R package Compiler",
    "text": "2. Installing a R package Compiler\nWe’ll need to install a package compiler in order to compile R packages from source.\n\nWindowsMacOS\n\n\nRTools is a program that allows us to compile R packages.\n\nRtools can be installed with link.\n\nIn the Rtools installer, press Next &gt; Next (ensuring that both check boxes are ticked) &gt; Install.\n\n\nFor Mac, you’ll need to install Xcode Command Line Tools instead.\n\nOpen the Mac terminal. To do so, click on the spotlight icon in the menu bar and search for “terminal.”\nInstall Homebrew: Homebrew is a package manager for MacOS which we will use to install Xcode. In the Mac terminal, paste:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis might prompt you to enter your user password, which is the password that you use to sign into your device (Note that for privacy reasons you won’t be able to see the characters that you type). Once you’ve entered your password, press enter. This may take a while to run.\n\nYou should get an output similar to this:\n\n\nNotice the warning. To fix this, we’ll need to add our installation to the system path. In the terminal, run:\n(echo; echo 'eval \"$(/opt/homebrew/bin/brew shellenv)\"') &gt;&gt; ~/.zprofile\nfollowed by:\neval \"$(/opt/homebrew/bin/brew shellenv)\"\nwhich will add the required path to the installation.\n\nVerify installations: to check that Xtools is installed, run $ brew doctor in the termial. This should display the output “Your system is ready to brew.”"
  },
  {
    "objectID": "pages/installation/vscode_setup.html#installing-miniconda",
    "href": "pages/installation/vscode_setup.html#installing-miniconda",
    "title": "Using VSCode",
    "section": "3. Installing MiniConda",
    "text": "3. Installing MiniConda\nAdditionally, we’ll need to do is install miniconda, a python distribution that allows us to simplify package installations. Head to anaconda.com and follow the instructions below, depending on your operating system.\n\nWindowsMacOS\n\n\n\nScroll to Latest Miniconda installer links and select Miniconda3 Windows 64-bit.\nOpen the installer, and select next &gt; I Agree &gt; Just Me (recommended).\nSelect your destination folder of choice and press next.\nSelect the following options:\n\n\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.12\nClear the package cache upon completion\n\n\nLastly, press install.\n\n\n\n\nScroll to Latest Miniconda installer links and select the version compatible with your device.\nOpen the installer and follow the instructions."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#setting-up-our-environment",
    "href": "pages/installation/vscode_setup.html#setting-up-our-environment",
    "title": "Using VSCode",
    "section": "4. Setting up our environment",
    "text": "4. Setting up our environment\nBefore we download the comet modules, we’ll need to set up our environment and install required packages.\n\nIn your computer file system, search for Anaconda Prompt (miniconda3) in your files and open it as an administrator. This should open up a terminal.\nIn the terminal, enter the following line code:\n\nconda create -n comet_env jupyterlab r-essentials git r-tidyverse r-car r-stargazer r-estimatr\nThe first part of this command will create a new python environment called “comet”. An envrionment is an isolated space on your computer where you can work on a given project without the risk of breaking other projects. Essentially, this acts as a safety layer to protect our computer.\nThe second part of this command will add the required r packages for the COMET modules to the environment.\nLastly, to enable the environment, run conda activate comet_env."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#configuring-the-irkernel-and-installing-reditorsupport",
    "href": "pages/installation/vscode_setup.html#configuring-the-irkernel-and-installing-reditorsupport",
    "title": "Using VSCode",
    "section": "5. Configuring the IRKernel and Installing REditorSupport",
    "text": "5. Configuring the IRKernel and Installing REditorSupport\nWe’ll now set up the kernel that will allow us to code in the R programming language in VSCode.\n\nOpen the copy of R 4.4.0 that we installed earlier.\nIn the terminal, paste the following lines of code one at a time:\n\n\ninstall.packages('IRkernel') This will prompt you to select a CRAN mirror. Select the one closest to you (if you are at UBC, that would be Canada (MB)).\n\n\nIRkernel::installspec()\n\n\nWe’ll now need to install the VSCode REditorSupport extension, which will allow us to interact with the R terminal from within VScode.\n\n\n\nOpen R 4.4.0 and paste install.packages(\"languageserver\"). Make sure to select the CRAN mirror closest to you.\nIn VSCode, open the extensions page. You can do so by pressing Ctrl+Shift+X on Windows, or Cmd+Shift+X on MacOS. Alternatively, you can find the extensions panel on the left-hand side of your screen.\n\n\n\n\n\n\n\n\n\n\n\nIn the extensions search, type r and select the first option.\nPress install. You should now be able to access the R terminal directly from the VSCode console. You may need to close and reopen VSCode.\n\n\n\n\nYou can access the R terminal directly though VSCode by right-clicking the arrow next to the + in the terminal and selecting R interactive."
  },
  {
    "objectID": "pages/installation/vscode_setup.html#opening-the-comet-modules",
    "href": "pages/installation/vscode_setup.html#opening-the-comet-modules",
    "title": "Using VSCode",
    "section": "6. Opening the COMET modules",
    "text": "6. Opening the COMET modules\nLastly, we need to actually be able to open and work on the COMET modules in VSCode.\n\nOpen comet.arts.ubc.ca and, in the top right corner, select Launch Comet &gt; Launch Locally. This will download a .zip file to your computer.\nOpen the file in file explorer and extract the zip folder to your prefered destination.\nIn VSCode, select File &gt; Open Folder and select the COMET folder that you just unzipped. This will open the comet modules on your computer!"
  },
  {
    "objectID": "pages/teaching_with_comet.html",
    "href": "pages/teaching_with_comet.html",
    "title": "Teaching with Jupyter and COMET",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\nThis guide is an introduction to how you can use COMET and Jupyter notebooks for teaching. We will mainly discuss Jupyter Notebooks, for reasons explained in Section 2, but most of this advice is easy to adapt to other forms of notebook-based instruction such as R Markdown or Quarto.\nIn this guide, we will discuss:\n\nThe advantages and disadvantages of different notebook-based systems for classroom instruction.\nHow to use COMET-style notebooks in different classroom settings, including an outline of how to plan a lesson.\nHow to develop interactive learning activities to accompany a COMET-style notebooks, including some classroom-tested suggestions.\nAn introduction to developing your own COMET-style notebooks for classroom instruction.\n\n\n\n\n\n\n\nWant the Basics?\n\n\n\nJust looking for a quick overview of how Jupyter notebooks work? Try out getting started introduction to Jupyter notebook, then come back here.\n\n\nAs most of our experience is in post-secondary education, this guide is intended for educators teaching students at the university level. However, it is probably also suitable for upper-year high school students, with sufficient experience1."
  },
  {
    "objectID": "pages/teaching_with_comet.html#disclaimer",
    "href": "pages/teaching_with_comet.html#disclaimer",
    "title": "Teaching with Jupyter and COMET",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\nThis guide is an introduction to how you can use COMET and Jupyter notebooks for teaching. We will mainly discuss Jupyter Notebooks, for reasons explained in Section 2, but most of this advice is easy to adapt to other forms of notebook-based instruction such as R Markdown or Quarto.\nIn this guide, we will discuss:\n\nThe advantages and disadvantages of different notebook-based systems for classroom instruction.\nHow to use COMET-style notebooks in different classroom settings, including an outline of how to plan a lesson.\nHow to develop interactive learning activities to accompany a COMET-style notebooks, including some classroom-tested suggestions.\nAn introduction to developing your own COMET-style notebooks for classroom instruction.\n\n\n\n\n\n\n\nWant the Basics?\n\n\n\nJust looking for a quick overview of how Jupyter notebooks work? Try out getting started introduction to Jupyter notebook, then come back here.\n\n\nAs most of our experience is in post-secondary education, this guide is intended for educators teaching students at the university level. However, it is probably also suitable for upper-year high school students, with sufficient experience1."
  },
  {
    "objectID": "pages/teaching_with_comet.html#sec-why",
    "href": "pages/teaching_with_comet.html#sec-why",
    "title": "Teaching with Jupyter and COMET",
    "section": "2 Why Jupyter Notebooks?",
    "text": "2 Why Jupyter Notebooks?\nWhy are Jupyter Notebooks a valuable tool for teaching? There are two main reasons:\n\nFirst, there are the advantages of Notebooks for teaching.\nSecond, there are the advantages of Jupyter for teaching.\n\nCombining these advantages creates a very valuable tool.\n\n2.1 Why Notebooks?\nA notebook refers to a digital document which combines rich text (including hyperlinks, formatting, and images) with cells that can perform computations. A user is able to change the content of the notebook, such as performing a computation or changing the text.\nNotebooks teach students three important skills, useful for data science and applied social science research:\n\nFirst, they teach students how to perform literate coding. Literate programming dates back to Knuth (1984), and has become extremely popular in sciences that use data. As Kery et al. (2018) explains, combining notes and context with code creates a self-documenting research notebook that addresses many common problems novice (and experienced) researchers face when analyzing data.\nSecond, they encourage replicable and reproducible data analysis. The non-reproducability of empirical results (see Camerer et al. (2018)) has reached crisis-levels in some fields. Because notebooks need to be run from the top-down, they naturally encourage students to make their analyses replicable. The structure of a notebook also encourage transparency when experimenting with analyses. This makes the work more likely to be reproducible.\nThird, they teach industry-relevant skills. Notebooks are extensively used by employers who conduct data science research, or who use data science in their work. Understanding how to write and use notebooks is a valuable skill in itself.\n\nThese properties make notebooks ideal to teach to students. Creating notebooks for classroom instruction turns them from a research tool into a pedagogical tool.\n\n\n2.2 Why Jupyter?\nJupyter is not the only option for notebooks (see Section 2.3). However, it has some advantages for teaching not shared by alternatives:\n\nNo installation necessary: when used through a JupyterHub, Jupyter notebooks do not require students to install any software or have a powerful computer. Even students with just a Chromebook or tablet can use Jupyter notebooks.\n\nThis eliminates many of the most time-consuming and frustrating parts of teaching student data science, including: installing software, troubleshooting package conflicts, issues sharing files and data, and computer problems.\n\nSimple Github integration: through nbgitpuller it is easy to share notebooks directly into a JupyterHub. This means that starting a class using notebooks is as easy as sharing a link with your students.\nLanguage independence: although the Jupyter framework is written in Python, it uses kernels to perform computation. There are dozens of kernels available, including those for popular languages such as R, Julia, Java, C, STATA, and Python itself.\n\nThe biggest strength of Jupyter is its hub-based design. This is also its biggest weakness, since it relies on an internet connection and someone to manage the hub. However, there are many free, well-maintained, hubs online such as:\n\nUBC OpenJupyter\nSyzygy\nGoogle Colab\nGitHub Codespces\n\nYou can also set up your own, or work with non-profits like 2i2c to develop your own hub.\n\n\n2.3 What are the Alternatives?\nJupyter Notebooks are not the only option for teaching using notebooks. In fact, there are significant advantages to other notebook styles, which may be more effective for certain kinds of teaching.\n\nJupyterQuartoR MarkdownObservable\n\n\n\nJupyter Notebooks\nJupyter notebooks are the most widely-used framework for notebook-based content, and are the easiest to use for students. However, they are not always the easiest to develop or maintain. We recommend Jupyter for online and student use.\n\nAdvantages\n\nWidely used, many tutorials and guides online.\nNo software installation needed for users.\nMany public, free, hubs (including Google Collab).\nLarge, open-source community.\n\n\n\nDisadvantages\n\nComplex and difficult local set-up.\nJSON-based file type; hard to maintain.\nLimited display and render options.\n\nLearn more about Jupyter Notebooks.\n\n\n\n\n\nQuarto Notebooks\nQuarto is a strong improvement over R Markdown and supports multiple languages. However, it is still in development and is more complicated. We recommend Quarto for development and offline use2.\n\nAdvantages\n\nInteroperable with R Markdown, but not R specific.\nVery rich output and render options.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\n\n\n\nDisadvantages\n\nNo easy-to-use free hubs available.\nMore complex than comparable notebook formats.\nNew, still in development.\n\nLearn more about Quarto.\n\n\n\n\n\nR Markdown Notebooks\nR Markdown is an excellent alternative to Jupyter for offline-only applications that only use R.\n\nAdvantages\n\nVery widely-used, many tutorials and guides online.\nEasy-to-install locally.\nDefault, enterprise-supported editor (RStudio).\nMany render options for output, rich output.\n\n\n\nDisadvantages\n\nIdiosyncratic syntax.\nNo easy-to-use free hubs available.\nOnly supports R coding, no other languages.\n\nLearn more about R Markdown.\n\n\n\n\n\nObservable Notebooks\nObservable is the newest format on the market, and looks very professional. It is designed for enterprise clients, and is the most complex of the alternatives.\n\nAdvantages\n\nNon-language specific framework.\nExtremely rich output formats.\nStrong dashboarding and interactive support.\nLarge enterprise developer.\n\n\n\nDisadvantages\n\nLarge enterprise developer, no free hubs.\nMost complex of the alternatives.\nNew, still in development.\n\nLearn more about Observable and D3.js."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "href": "pages/teaching_with_comet.html#teaching-with-jupyter-notebooks",
    "title": "Teaching with Jupyter and COMET",
    "section": "3 Teaching with Jupyter Notebooks",
    "text": "3 Teaching with Jupyter Notebooks\nWe have tried several ways of using Jupyter Notebooks when teaching, and found that they fit most teaching styles. We have found them particularly effective in:\n\nLecture.\nFlipped classrooms.\nWorkshops or labs.\n\nYou can find advice on lesson planning in these formats in the tabs below.\n\nLecturesFlipped ClassroomWorkshops or Labs\n\n\n\nDirect Instruction or Lectures\nJupyter notebooks are most effective in lecture when you use them as a demonstration tool which students can follow along with.\n\nThe power of a Jupyter notebook is the interactive nature of the different cells.\nThis allows you to enhance your lecture content by immediately and interactively demonstrating principles by running cells and changing their values.\n\nEssentially, you make can your slides or visualizations interactive by running or editing cells.\n\nYou can either do this directly, by showing the notebook, or you can turn the notebook into a slideshow using either nbcovert or RISE, which create a RevealJS presentation from your notebook.\n\nRevealJS is a powerful HTML-based presentation framework, widely used on the web and in computation.\n\nThere are also powerful libraries for interactive visualization, such as plotly and ggplotly.3\n\nSee Section 4.1 for a guide to creating presentations using Jupyter notebooks.\nWe have found it is usually best to give students the Jupyter notebook of the presentation, as a kind of “hand-out,” while you demonstrate using the presentation display of the notebook. This avoids the problem of having to make sure students have a suitable presentation display tool installed.\n\n\nSuggestions for Teaching\n\nTry demonstrating a cell, then asking students to predict what happen when you make a different change. Then do it!\n\nThis works great with classroom response systems such as iClicker or TopHat.\n\nSpend time thinking about how interacting with the cell can show the concept more effectively than a static visualization.\n\nWe have found this to be particularly useful for dynamics in visualizations, such as showing a change.\n\nSpend time on each interactive part of your presentation, and walk through the changes.\nUse encapsulation by placing code in auxillary files to make the demonstrations easier to follow.\n\nIf students don’t need to know how it works, only what it does, consider re-writing the code to hide the details.\n\n\n\n\n\n\nFlipped Classrooms\nA flipped classroom refers to a teaching model where activities traditional done in the classroom are done at while, while activities done at home are done in the classroom (Akçayır and Akçayır (2018)). “Flipping” the lecture demonstration, outlined above, using Jupyter Notebooks is a natural fit.\nMost flipped classroom experiences tend to use videos (see Akçayır and Akçayır (2018)), and this is quite feasible with Jupyter Notebooks. Record yourself demonstrating the notebook, and have students follow along. We can done this for some of the COMET notebooks (see our project notebooks for example).\nHowever, the interactivity of notebooks makes them ideal for doing as “pre-reading” assignment instead, or in addition, to videos. The active learning created by interacting with the notebook, and completing self-test exercises, makes them more effective than just doing a reading.\nA good flipped-classroom notebook:\n\nIntroduces the topic in a narrative, systematic way, and does not require any significant external references to follow along.\nIncludes regularly-spaced interactive cells, which require students to evaluate and inspect the results.\nHas a series of self tests (see Section 4.2) at regular intervals, to check and reinforce student understanding.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nFor example, you could assign students a pre-reading of classification and clustering before class. In the class, you could then introduce a new dataset and have them explore it using clustering methods.\nMany of our COMET notebooks, especially the intermediate ones are built with this structure, where one “Part” of the notebook can be turned into the classroom part of the flipped classroom.4\n\n\n\n\n\nWorkshops or Labs\nWe have also found it effective to teach using Jupyter notebooks in small group settings, such as workshops. A typical Jupyter workshop in our experience:\n\nDivide the students into groups of about 4-5, and have them physically move so that they are seated next to one another.\n\nThis also works well online, using a feature like Zoom’s breakout rooms.\n\nOnce they are settled, or before moving people into their breakout rooms, introduce the purpose of the workshop. Identify what students are supposed to do, and how they will interact with the Jupyter Notebooks.\nAllow students to work together on the notebooks, while you move around the room discussing with the groups.\n\nIt is often effective to design your notebooks so that they have several identifiable “tasks” or stopping points, where you can bring the workshop back together.\n\nMany of our COMET notebooks, especially the intermediate ones are built with this structure.\nMake sure you build in time for students to introduce themselves to one another, if this is their first time meeting.\n\n\n\nSuggestions for Teaching\nWhen in the classroom, we recommend following-up the notebook with a formative or summative assessment using the same skills, but in a new setting or in a more sophisticated way.\n\nMany of the tools and techniques for flipped classroom instruction work well in a workshop format too. Just make sure there is sufficient support so students can follow the notebooks together.\nEncourage students to work together to troubleshoot problems if they encounter them, so you are not running around too often.\nAn effective strategy is to have the students collaborate on a single “final” version of the notebook together, while experimenting on their own. Nominating one student as the “scribe” is a good way to keep this organized.\n\nAt the end of the workshop, having students hand in their Notebook is an effective way of measuring participation, and encourages participation."
  },
  {
    "objectID": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "href": "pages/teaching_with_comet.html#developing-your-own-notebooks-for-teaching",
    "title": "Teaching with Jupyter and COMET",
    "section": "4 Developing your own Notebooks for Teaching",
    "text": "4 Developing your own Notebooks for Teaching\nIt is easier than you might expect to develop notebooks for teaching. Some of our favourite examples are:\n\nWord Embeddings: an advanced, workshop-style, .ipynb format notebook.\nVisualization: a basic, lecture-style, introduction with time for experimentation.\nExporting Output: a flipped-classroom style book or workshop for STATA output.\n\nPoke around and see some more!\n\n4.1 Presenting and Slideshows\nWhen teaching with Jupyter, effective presentation skills require a little planning. There are several options, depending on what kind of presentation you want to give.\n\n4.1.1 Presenting a Notebook\nPresenting a Notebook as a notebook, such as in a demonstration or workshop, is easy.\n\nIn JupyterLab, the easiest way is use the View menu:\n\nUncheck all of the un-necessary bars, such as the top bar and status bar.\nTurn on “Simple View”, which only shows your activate notebook tab.\nTurn on “Presentation Mode.”\n\n\nThis will create a large-format, interactive, version of your notebook suitable for presenting on an overhead projector or monitor.\n\n\n4.1.2 Presenting a Slideshow\nIf you want to turn your notebook into a slideshow, things are more complicated depending on whether you want it to be interactive or not. However, in general you create a slideshow by designating individual cells are either whole slides, or fragments of slides:\n\nA slide is a single higher-level slide. When the presentation advances from one slide to another, it will “slide” right-to-left.\nA sub-slide is like a lower-level slide. When the presentations advances from to a sub-slide, it will “slide” from up-to-down.\nA fragment is part of a slide. It appears by sliding up, into the slide, keep the previous content visible. This is how you can reveal information or advance content.\n\nYou designate cells as the different part of a presentation by clicking on the gear icon, then selecting the cell. A dropdown menu that says “Slide type” will be visible. Use this to set up your presentation.\nIf you don’t care about interactivity, at this point you can go to “File &gt; Save and Export Notebook as…” then select “Reveal.js Slides.” This will download an .html file with your presentation in it. Learn more about Reveal.js to see how this file works in more detail.\nIf you want to run code in your presentation and edit it as you present, things are more complicated. To make your presentation editable, you need to install a JupyterLab extension called RISE.\n\nRISE is easiest to install on your own computer, not on a JupyterHub unless you have administrator privileges.\nIn the terminal, run pip install jupyterlab_rise then re-launch your server.\nYou can read more about RISE above; it’s still in development so things might change.\n\n\n\n\n4.2 Writing Self-Tests\nWriting self-tests is an important part of providing formative feedback to students. It can be somewhat complicated, but the basic idea is to write cells in your notebooks that look like:\n#an R self test\n\nanswer_1 &lt;- #fill in the correct value here\n\ntest_1()\nThe function test_1() is stored in an auxillary file, and performs a test on the answer. It also gives feedback to the student, such as whether the answer was correct.\nThis requires some set-up, and is slightly different for different languages. To make this easier, we provide a detailed guide for R and Python in our writing self-tests documentation.\n\n\n4.3 Development Tips\nDeveloping notebooks as a one-off is straightforward if you author them in JupyterLab: what you see is what you get. However, if you have a more complex project some planning helps. This includes multiple notebooks, or notebooks you need to collaborate on over time.\n\nWe strongly recommend not developing directly in .ipynb notebooks long term. Draft your initial notebook in .ipynb, then switch to another framework for longer-term development.\n\nThe reason is because editing an .ipynb edits the state of the program, making it easy to accidentally evaluate or delete something.\nIt’s also hard to maintain and doesn’t play nicely with version control systems like git because the documents are very complicated in structure.\n\nOur recommended format is .qmd which can render .ipynb notebooks from the source code. The underlying document is just text, which makes it easy to edit and maintain.\n\nUsing .qmd notebooks is much easier: think of these as the “source code” and the .ipynb as the “output.” This also has an advantage of begin able to create other output formats, like PDFs, websites, or presentations directly from the source code."
  },
  {
    "objectID": "pages/teaching_with_comet.html#teaching-offline",
    "href": "pages/teaching_with_comet.html#teaching-offline",
    "title": "Teaching with Jupyter and COMET",
    "section": "5 Teaching Offline",
    "text": "5 Teaching Offline\nWhile we think that teaching using a JupyterHub is the best option, that may not always be desired or possible. If you want to use COMET notebooks, or similar teaching tools, without a JupyterHub you have two main options:\n\nOption 1: have students install one of the alternative frameworks.\nOption 2: have students install Jupyter locally, on their own computers.\n\nBe prepared to troubleshoot installation issues."
  },
  {
    "objectID": "pages/teaching_with_comet.html#further-reading",
    "href": "pages/teaching_with_comet.html#further-reading",
    "title": "Teaching with Jupyter and COMET",
    "section": "6 Further Reading",
    "text": "6 Further Reading\nYou can see some of our other publications on our dissemination page."
  },
  {
    "objectID": "pages/teaching_with_comet.html#references",
    "href": "pages/teaching_with_comet.html#references",
    "title": "Teaching with Jupyter and COMET",
    "section": "7 References",
    "text": "7 References\n\n\nAkçayır, Gökçe, and Murat Akçayır. 2018. ‘The Flipped Classroom: A Review of Its Advantages and Challenges’. Computers & Education 126: 334–45.\n\n\nCamerer, Colin F, Anna Dreber, Felix Holzmeister, Teck-Hua Ho, Jürgen Huber, Magnus Johannesson, Michael Kirchler, et al. 2018. ‘Evaluating the Replicability of Social Science Experiments in Nature and Science Between 2010 and 2015’. Nature Human Behaviour 2 (9): 637–44.\n\n\nGraves, Jonathan L, Emrul Hasan, and Trish L Varao-Sousa. 2024. ‘Understanding the Hybrid Classroom in Economics: A Case Study’. International Review of Economics Education 45: 100282.\n\n\nKery, Mary Beth, Marissa Radensky, Mahima Arya, Bonnie E John, and Brad A Myers. 2018. ‘The Story in the Notebook: Exploratory Data Science Using a Literate Programming Tool’. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–11.\n\n\nKnuth, Donald Ervin. 1984. ‘Literate Programming’. The Computer Journal 27 (2): 97–111."
  },
  {
    "objectID": "pages/teaching_with_comet.html#footnotes",
    "href": "pages/teaching_with_comet.html#footnotes",
    "title": "Teaching with Jupyter and COMET",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe have taught several of these notebooks to 1st year students during intake events, who were fresh out of high school.↩︎\nWe use Quarto to develop the COMET project.↩︎\nPlotly comes in several flavours for different languages, such as `plotly-r↩︎\nThis was actually our original use for the notebooks, before COMET! See Graves, Hasan, and Varao-Sousa (2024)↩︎"
  },
  {
    "objectID": "pages/using_comet.html",
    "href": "pages/using_comet.html",
    "title": "Using COMET",
    "section": "",
    "text": "Praxis has a sister project named COMET, this page is adopted from COMET.\n\n\nThe COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module.\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\n\n\n\n\nAs we mentioned, all of the Jupyter Notebooks are interactive; to access a notebook, simply launch the appropriate hub from the top menu of this website (where it says, “LAUNCH COMET”). The notebooks can also be downloaded for local use (“LAUNCH COMET &gt; LAUNCH LOCALLY”).\n\nThe interactive element will redirect you to the JupyterHub of your choice, and then synch the modules over.\nThe synching process uses a Jupyter extension called nbgitpuller to perform this operation.\n\n\n\n\n\n\n\nJupyter Troubleshooting Tip\n\n\n\nOccasionally, you can run into problems working with Jupyter in the cloud. If this occurs, you can try (a) deleting all of the COMET modules from your JupyterHub account, then trying to launch again, or (b) downloading the files manually, then uploading them. Option (b) will always work, even if it is a bit time exhuastive.\n\n\n\n\n\nThere are two ways to include a module in Canvas (or another LMS):\n\nEmbed it in a Canvas page or assignment\nInclude it as an external link\n\nThe advantage of option (1) is that you can include other material around the link (such as instructions). The advantage of option (2) is that it is easier.\n\n\nYou can see a visualization below:\n\n\n\nGIF of Embedding COMET into Canvas\n\n\n\nFirst, create the page (or assignment) that you want to embed the page in.\nThen, edit the page, and switch the HTML edit mode\nCopy the following text into the page:\n\n&lt;p&gt;&lt;iframe src=\"PASTE URL HERE\" width=\"800\" height=\"3200\" style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\nReplace the PASTE URL HERE text with the URL of the COMET page you want\nOptionally edit the width and height options to change the size of the embed\nSave the page; you should see the embedded page\n\nYou can now edit this page normally - for instance, by adding other material. You could also just add a URL as hyperlink instead of an embed. It’s up to you!\n\n\n\n\n\n\nCanvas Tip\n\n\n\nYou can find more information about embedding webpages in canvas on this page.\n\n\n\n\n\nYou can also just add a link directly to a Canvas module.\n\nOn the Canvas module, click the (+) to add an item, then select “External URL”.\nEnter the URL of the COMET page and customize the name; this is what will show up on Canvas.\nYou can choose to select the “load in a new tab” buttonif that is your preference.\n\n\n\n\n\nIf you run into issues with a COMET module (say, a bug) you can create an issue to our GitHub directory using the “Report an issue” button at the top right of this page.\nIf you need other support, please contact comet-project at ubc dot ca\n\n\n\n\n\n\nImportant\n\n\n\nIf the issue is about a JupyterHub, and not the notebook specifically, we unfortunately cannot help. Contact your hub maintainer for information.\n\nAt UBC, this is the LTHub"
  },
  {
    "objectID": "pages/using_comet.html#using-comet-for-teaching",
    "href": "pages/using_comet.html#using-comet-for-teaching",
    "title": "Using COMET",
    "section": "",
    "text": "The COMET modules can be used for teaching in a variety of ways. While most of them are designed as a 1-hour (or two 1-hour) labs, they can also be used in a classroom or as a pre-reading/self-study for more advanced students.\n\nMost of the modules include self tests which are designed to be formative in nature. Some also include short writing exercises.\n\nWe would not recommend using these for grades, except if those grades are based on completion.\n\nGenerally, modules are most engaging when someone is “animating” them, such as a TA or Instructor.\n\nCOMET modules can be launched interactively via a JupyterHub. If you are new to working with JupyterHub, you can orient yourself with the structure and interface in our Introduction to Jupyter Module.\n\nUBC offers an in-house JupyterHub called JupyterOpen for which these modules were designed.\nPIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down."
  },
  {
    "objectID": "pages/using_comet.html#interactive-modules",
    "href": "pages/using_comet.html#interactive-modules",
    "title": "Using COMET",
    "section": "",
    "text": "As we mentioned, all of the Jupyter Notebooks are interactive; to access a notebook, simply launch the appropriate hub from the top menu of this website (where it says, “LAUNCH COMET”). The notebooks can also be downloaded for local use (“LAUNCH COMET &gt; LAUNCH LOCALLY”).\n\nThe interactive element will redirect you to the JupyterHub of your choice, and then synch the modules over.\nThe synching process uses a Jupyter extension called nbgitpuller to perform this operation.\n\n\n\n\n\n\n\nJupyter Troubleshooting Tip\n\n\n\nOccasionally, you can run into problems working with Jupyter in the cloud. If this occurs, you can try (a) deleting all of the COMET modules from your JupyterHub account, then trying to launch again, or (b) downloading the files manually, then uploading them. Option (b) will always work, even if it is a bit time exhuastive."
  },
  {
    "objectID": "pages/using_comet.html#using-with-canvas",
    "href": "pages/using_comet.html#using-with-canvas",
    "title": "Using COMET",
    "section": "",
    "text": "There are two ways to include a module in Canvas (or another LMS):\n\nEmbed it in a Canvas page or assignment\nInclude it as an external link\n\nThe advantage of option (1) is that you can include other material around the link (such as instructions). The advantage of option (2) is that it is easier.\n\n\nYou can see a visualization below:\n\n\n\nGIF of Embedding COMET into Canvas\n\n\n\nFirst, create the page (or assignment) that you want to embed the page in.\nThen, edit the page, and switch the HTML edit mode\nCopy the following text into the page:\n\n&lt;p&gt;&lt;iframe src=\"PASTE URL HERE\" width=\"800\" height=\"3200\" style=\"overflow: hidden;\"&gt;&lt;/iframe&gt;&lt;/p&gt;\n\nReplace the PASTE URL HERE text with the URL of the COMET page you want\nOptionally edit the width and height options to change the size of the embed\nSave the page; you should see the embedded page\n\nYou can now edit this page normally - for instance, by adding other material. You could also just add a URL as hyperlink instead of an embed. It’s up to you!\n\n\n\n\n\n\nCanvas Tip\n\n\n\nYou can find more information about embedding webpages in canvas on this page.\n\n\n\n\n\nYou can also just add a link directly to a Canvas module.\n\nOn the Canvas module, click the (+) to add an item, then select “External URL”.\nEnter the URL of the COMET page and customize the name; this is what will show up on Canvas.\nYou can choose to select the “load in a new tab” buttonif that is your preference."
  },
  {
    "objectID": "pages/using_comet.html#problems-and-support",
    "href": "pages/using_comet.html#problems-and-support",
    "title": "Using COMET",
    "section": "",
    "text": "If you run into issues with a COMET module (say, a bug) you can create an issue to our GitHub directory using the “Report an issue” button at the top right of this page.\nIf you need other support, please contact comet-project at ubc dot ca\n\n\n\n\n\n\nImportant\n\n\n\nIf the issue is about a JupyterHub, and not the notebook specifically, we unfortunately cannot help. Contact your hub maintainer for information.\n\nAt UBC, this is the LTHub"
  },
  {
    "objectID": "docs/AMNE-376/development/convnext_embeddings.html",
    "href": "docs/AMNE-376/development/convnext_embeddings.html",
    "title": "Praxis",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport torch\nimport numpy as np\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport timm\nfrom tqdm.notebook import tqdm # Import tqdm for progress bar\n\n\ndf = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')\ndf.head()\n\n\n\n\n\n\n\n\nfilename\npage\ngroup\nera\nmaterial\n\n\n\n\n0\npage188_img01_photo13.jpg\n188\nSOUNION GROUP\n615 - 590 BC\nMarble\n\n\n1\npage202_img01_photo3.jpg\n202\nSOUNION GROUP\n615 - 590 BC\nMarble\n\n\n2\npage202_img01_photo4.jpg\n202\nSOUNION GROUP\n615 - 590 BC\nMarble\n\n\n3\npage205_img01_photo4.jpg\n205\nSOUNION GROUP\n615 - 590 BC\nMarble\n\n\n4\npage211_img01_photo12.jpg\n211\nSOUNION GROUP\n615 - 590 BC\nLead\n\n\n\n\n\n\n\n\nmodel_name = 'convnextv2_tiny' \nmodel = timm.create_model(model_name, pretrained=True)\nmodel.eval()  # set to eval mode\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n\n\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Kaiyan Zhang\\.cache\\huggingface\\hub\\models--timm--convnextv2_tiny.fcmae_ft_in22k_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\nTo support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n  warnings.warn(message)\n\n\nConvNeXt(\n  (stem): Sequential(\n    (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n    (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n  )\n  (stages): Sequential(\n    (0): ConvNeXtStage(\n      (downsample): Identity()\n      (blocks): Sequential(\n        (0): ConvNeXtBlock(\n          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=96, out_features=384, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=384, out_features=96, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (1): ConvNeXtBlock(\n          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=96, out_features=384, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=384, out_features=96, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (2): ConvNeXtBlock(\n          (conv_dw): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=96, out_features=384, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=384, out_features=96, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n      )\n    )\n    (1): ConvNeXtStage(\n      (downsample): Sequential(\n        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n      )\n      (blocks): Sequential(\n        (0): ConvNeXtBlock(\n          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=192, out_features=768, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=768, out_features=192, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (1): ConvNeXtBlock(\n          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=192, out_features=768, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=768, out_features=192, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (2): ConvNeXtBlock(\n          (conv_dw): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=192, out_features=768, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=768, out_features=192, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n      )\n    )\n    (2): ConvNeXtStage(\n      (downsample): Sequential(\n        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n      )\n      (blocks): Sequential(\n        (0): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (1): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (2): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (3): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (4): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (5): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (6): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (7): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (8): ConvNeXtBlock(\n          (conv_dw): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n          (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=384, out_features=1536, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=1536, out_features=384, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n      )\n    )\n    (3): ConvNeXtStage(\n      (downsample): Sequential(\n        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n      )\n      (blocks): Sequential(\n        (0): ConvNeXtBlock(\n          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (1): ConvNeXtBlock(\n          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n        (2): ConvNeXtBlock(\n          (conv_dw): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n          (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (mlp): GlobalResponseNormMlp(\n            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n            (act): GELU()\n            (drop1): Dropout(p=0.0, inplace=False)\n            (grn): GlobalResponseNorm()\n            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n            (drop2): Dropout(p=0.0, inplace=False)\n          )\n          (shortcut): Identity()\n          (drop_path): Identity()\n        )\n      )\n    )\n  )\n  (norm_pre): Identity()\n  (head): NormMlpClassifierHead(\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Identity())\n    (norm): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n    (flatten): Flatten(start_dim=1, end_dim=-1)\n    (pre_logits): Identity()\n    (drop): Dropout(p=0.0, inplace=False)\n    (fc): Linear(in_features=768, out_features=1000, bias=True)\n  )\n)\n\n\n\n# Option A: if model.forward_features exists:\ndef extract_backbone_features(x):\n    # x: tensor (B,3,H,W)\n    feat_map = model.forward_features(x)  # tensor shape: (B, C, H', W')\n    # Usually, to get a single vector per image, apply global pooling:\n    # Many timm models have model.global_pool or model.fc (depending on arch).\n    # For ConvNeXt, there's often model.global_pool (AdaptiveAvgPool).\n    # Check if model.global_pool exists:\n    if hasattr(model, 'global_pool'):\n        # Apply global pool: \n        pooled = model.global_pool(feat_map)  # shape: (B, C, 1, 1)\n        return pooled.view(pooled.size(0), -1)  # shape: (B, C)\n    else:\n        # Fallback: adaptive avg pool:\n        import torch.nn.functional as F\n        pooled = F.adaptive_avg_pool2d(feat_map, 1)  # (B, C, 1, 1)\n        return pooled.view(pooled.size(0), -1)\n\n# Option B: reset classifier to output feature vectors directly\n# This depends on timm model API; many have model.reset_classifier.\ntry:\n    model.reset_classifier(0)  # replace head so output is feature vector\n    # Now model(x) returns features of shape (B, C)\n    def extract_backbone_features(x):\n        return model(x)\nexcept AttributeError:\n    # fallback to forward_features approach above\n    pass\n\n\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\n\n# ImageNet normalization (mean/std) :contentReference[oaicite:4]{index=4}\nimagenet_mean = [0.485, 0.456, 0.406]\nimagenet_std  = [0.229, 0.224, 0.225]\n\n# Decide target input size: many ConvNeXt V2 variants use 224×224 or larger (e.g., 384).\ninput_size = 224  # adjust if your variant expects 384: set 384 accordingly.\n\npreprocess = transforms.Compose([\n    transforms.Resize(int(input_size * 1.14)),  # resize shorter side\n    transforms.CenterCrop(input_size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n])\n\ndef load_and_preprocess(image_path):\n    img = Image.open(image_path).convert('RGB')\n    return preprocess(img)\n\n\nimport torch\nimport numpy as np\n\ndef extract_features_from_folder(folder_path, batch_size=16):\n    # Collect image file paths\n    exts = ('.jpg', '.jpeg', '.png', '.bmp', '.tiff')\n    image_paths = [os.path.join(folder_path, fname) for fname in os.listdir(folder_path)\n                   if fname.lower().endswith(exts)]\n    image_paths.sort()\n    features_list = []\n    filenames = []\n    model.eval()\n    with torch.no_grad():\n        for i in range(0, len(image_paths), batch_size):\n            batch_paths = image_paths[i:i+batch_size]\n            batch_imgs = []\n            for p in batch_paths:\n                try:\n                    tensor = load_and_preprocess(p)\n                    batch_imgs.append(tensor)\n                except Exception as e:\n                    print(f\"Warning: could not process {p}: {e}\")\n            if not batch_imgs:\n                continue\n            batch_tensor = torch.stack(batch_imgs, dim=0).to(device)  # (B,3,H,W)\n            feats = extract_backbone_features(batch_tensor)  # (B, feat_dim)\n            # Convert to CPU numpy\n            feats_np = feats.cpu().numpy()\n            features_list.append(feats_np)\n            filenames.extend([os.path.basename(p) for p in batch_paths])\n    if features_list:\n        features = np.concatenate(features_list, axis=0)  # shape (N, feat_dim)\n    else:\n        features = np.zeros((0,))  # empty\n    return filenames, features\n\n# Example usage:\nfolder = '../data/richter_kouroi_complete_front_only'\nfilenames, features = extract_features_from_folder(folder, batch_size=8)\nprint(f\"Extracted features for {len(filenames)} images; feature dimension: {features.shape[1]}\")\n\nExtracted features for 62 images; feature dimension: 768\n\n\n\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\ndef plot_pca_by_metadata(features, filenames, df, category_col='era', title=None, annotate=False):\n    \"\"\"\n    Plot PCA of `features`, coloring and optionally annotating points by metadata category.\n    \n    Args:\n        features: numpy array of shape (N, D) - feature vectors.\n        filenames: list of length N - filenames corresponding to each feature vector.\n        df: pandas DataFrame containing at least columns ['filename', category_col].\n        category_col: str - column name in df to color by (e.g., 'era' or 'material').\n        title: str or None - plot title.\n        annotate: bool - whether to annotate each point with filename or metadata label.\n    \"\"\"\n    # Ensure DataFrame has the necessary columns\n    if 'filename' not in df.columns or category_col not in df.columns:\n        raise ValueError(f\"DataFrame must contain 'filename' and '{category_col}' columns.\")\n    \n    # Run PCA\n    pca = PCA(n_components=2, random_state=0)\n    feats_2d = pca.fit_transform(features)  # shape (N, 2)\n    \n    # Prepare mapping from filename to category\n    # We'll create a dict for faster lookup\n    # If DataFrame has duplicates for the same filename, last one will be used\n    mapping = dict(zip(df['filename'], df[category_col].astype(str)))\n    \n    # Get unique categories for colormap\n    unique_cats = sorted(df[category_col].astype(str).unique())\n    # Create a consistent color map\n    cmap = plt.get_cmap('tab10')\n    # If more categories than cmap colors, colors will cycle; for many categories, consider a larger colormap\n    era_color_map = {cat: cmap(i % cmap.N) for i, cat in enumerate(unique_cats)}\n    \n    # Build color list and optional labels\n    colors = []\n    annotations = []\n    for fn in filenames:\n        cat = mapping.get(fn, None)\n        if cat is None:\n            # If missing metadata, assign a default color and label\n            colors.append('gray')\n            annotations.append(f\"{fn}\\n(Unknown {category_col})\")\n        else:\n            colors.append(era_color_map[cat])\n            annotations.append(f\"{fn}\\n{cat}\")\n    \n    # Plot\n    plt.figure(figsize=(8, 6))\n    scatter = plt.scatter(feats_2d[:, 0], feats_2d[:, 1], c=colors, s=50, alpha=0.7)\n    \n    # Create legend manually\n    handles = [plt.Line2D([], [], marker=\"o\", ls=\"\", color=era_color_map[cat], label=cat)\n               for cat in unique_cats]\n    plt.legend(handles=handles, title=category_col, bbox_to_anchor=(1.05, 1), loc='upper left')\n    \n    plt.xlabel(\"PC1\")\n    plt.ylabel(\"PC2\")\n    if title is None:\n        title = f\"PCA colored by {category_col}\"\n    plt.title(title)\n    plt.tight_layout()\n    \n    # Annotate points if requested\n    if annotate:\n        for i, text in enumerate(annotations):\n            plt.annotate(text, (feats_2d[i, 0], feats_2d[i, 1]),\n                         textcoords=\"offset points\", xytext=(3, 3),\n                         fontsize=7, color=colors[i])\n    \n    plt.show()\n\n# Example usage (assuming you have `features`, `filenames`, and `df` defined):\nplot_pca_by_metadata(features, filenames, df, category_col='era', title=\"PCA by Era\", annotate=False)\nplot_pca_by_metadata(features, filenames, df, category_col='material', title=\"PCA by Material\", annotate=False)\n\n\n\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import convnext_base\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# Load model and image\nmodel = convnext_base(pretrained=True)\nmodel.eval()\ntarget_layers = [model.features[-1]]\n\n# Load and preprocess image\nimg = Image.open(\"../data/richter_kouroi_filtered_photos/page312_img01_photo4.jpg\").convert(\"RGB\")\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = transform(img).unsqueeze(0)\n\n# GradCAM\nfrom pytorch_grad_cam import GradCAM\ncam = GradCAM(model=model, target_layers=target_layers)\ntargets = [ClassifierOutputTarget(0)]  # Class index to visualize\ngrayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0]\n\n# Overlay CAM on image\nrgb_img = np.array(img.resize((224, 224))) / 255.0\nvisualization = show_cam_on_image(rgb_img, grayscale_cam, use_rgb=True)\n\nplt.imshow(visualization)\nplt.axis(\"off\")\nplt.show()\n\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\nc:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ConvNeXt_Base_Weights.IMAGENET1K_V1`. You can also use `weights=ConvNeXt_Base_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/convnext_base-6075fbad.pth\" to C:\\Users\\Kaiyan Zhang/.cache\\torch\\hub\\checkpoints\\convnext_base-6075fbad.pth\n100%|██████████| 338M/338M [00:09&lt;00:00, 37.6MB/s] \n\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import convnext_base\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# 1. Load and record original size\norig_img = Image.open(\"../data/richter_kouroi_filtered_photos/page312_img01_photo4.jpg\").convert(\"RGB\")\norig_w, orig_h = orig_img.size\nrgb_orig = np.array(orig_img) / 255.0\n\n# 2. Prepare the resized tensor for the model\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = transform(orig_img).unsqueeze(0)\n\n# 3. Load model & CAM setup\nmodel = convnext_base(pretrained=True)\nmodel.eval()\ntarget_layers = [model.features[-1]]\ncam = GradCAM(model=model, target_layers=target_layers)\n\n# 4. Compute the CAM at 224×224\ngrayscale_cam = cam(input_tensor=input_tensor,\n                    targets=[ClassifierOutputTarget(0)])[0]\n\n# 5. Upsample CAM to original size\ngrayscale_cam_orig = cv2.resize(grayscale_cam, (orig_w, orig_h),\n                                interpolation=cv2.INTER_LINEAR)\n\n# 6. Overlay on the original image\nvisualization = show_cam_on_image(rgb_orig,\n                                  grayscale_cam_orig,\n                                  use_rgb=True)\n\n# 7. Display\nplt.figure(figsize=(8,8))\nplt.imshow(visualization)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import convnext_base\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# 1. Load and record original size\norig_img = Image.open(\"../data/richter_kouroi_filtered_photos/page188_img01_photo13.jpg\").convert(\"RGB\")\norig_w, orig_h = orig_img.size\nrgb_orig = np.array(orig_img) / 255.0\n\n# 2. Prepare the resized tensor for the model\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = transform(orig_img).unsqueeze(0)\n\n# 3. Load model & CAM setup\nmodel = convnext_base(pretrained=True)\nmodel.eval()\ntarget_layers = [model.features[-1]]\ncam = GradCAM(model=model, target_layers=target_layers)\n\n# 4. Compute the CAM at 224×224\ngrayscale_cam = cam(input_tensor=input_tensor,\n                    targets=[ClassifierOutputTarget(0)])[0]\n\n# 5. Upsample CAM to original size\ngrayscale_cam_orig = cv2.resize(grayscale_cam, (orig_w, orig_h),\n                                interpolation=cv2.INTER_LINEAR)\n\n# 6. Overlay on the original image\nvisualization = show_cam_on_image(rgb_orig,\n                                  grayscale_cam_orig,\n                                  use_rgb=True)\n\n# 7. Display\nplt.figure(figsize=(8,8))\nplt.imshow(visualization)\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\nimport torch\nimport torchvision.transforms as T\nfrom torchvision.models import convnext_base\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport cv2\nimport numpy as np\n\nfrom pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\n\n# 1. Load sculpture image\norig_img = Image.open(\"sculpture.jpg\").convert(\"RGB\")\norig_w, orig_h = orig_img.size\nrgb_orig = np.array(orig_img) / 255.0\n\n# 2. Detect parts with YOLOv8 (assumes classes: 0=face,1=torso,2=hand)\nfrom ultralytics import YOLO\nyolo = YOLO(\"path/to/your/face_torso_hand_yolov8.pt\")\ndetections = yolo.predict(source=\"sculpture.jpg\")[0]  # first image\n\n# 3. Prepare ConvNeXt + GradCAM\nmodel = convnext_base(pretrained=True)\nmodel.eval()\ncam = GradCAM(model=model, target_layers=[model.features[-1]])\n\n# 4. Preprocess transform\ntransform = T.Compose([\n    T.Resize((224, 224)),\n    T.ToTensor(),\n    T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n])\ninput_tensor = transform(orig_img).unsqueeze(0)\n\n# 5. Compute base CAM once (for class-agnostic) or per class\n#    Here we’ll do per class—map YOLO class to a classifier output index\nclass_to_idx = {0: 0, 1: 1, 2: 2}  # dummy mapping\n\n# Precompute CAM heatmap at 224×224 for each class\ncams = {}\nfor yolo_cls, idx in class_to_idx.items():\n    grayscale_cam = cam(input_tensor=input_tensor,\n                        targets=[ClassifierOutputTarget(idx)])[0]\n    # upsample to original size\n    cams[yolo_cls] = cv2.resize(grayscale_cam, (orig_w, orig_h),\n                                interpolation=cv2.INTER_LINEAR)\n\n# 6. Overlay per-detection\nvis = rgb_orig.copy()\nfor box, score, cls in zip(detections.boxes.xyxy, detections.boxes.conf,\n                           detections.boxes.cls):\n    x1, y1, x2, y2 = map(int, box.cpu().numpy())\n    part_cam = cams[int(cls)]\n\n    # zero-out CAM outside the box\n    mask = np.zeros_like(part_cam)\n    mask[y1:y2, x1:x2] = part_cam[y1:y2, x1:x2]\n    \n    # overlay just that masked CAM\n    vis = show_cam_on_image(vis, mask, use_rgb=True)\n\n    # draw bounding box\n    cv2.rectangle(vis, (x1,y1), (x2,y2), (255,255,255), 2)\n    cv2.putText(vis, f\"{['face','torso','hand'][int(cls)]} {score:.2f}\",\n                (x1,y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (1,1,1), 2)\n\n# 7. Display full-res result\nplt.figure(figsize=(8,8))\nplt.imshow(vis)\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html",
    "href": "docs/SOCI-280/soci_270_bert .html",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "",
    "text": "Module adapted from UBC COMET, prepared originally by Anneke Dresselhuis and Irene Berezin, by Irene Berezin, Anna Kovtunenko, Jalen Faddick and the prAxIs UBC team."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#disinformation-in-the-information-age",
    "href": "docs/SOCI-280/soci_270_bert .html#disinformation-in-the-information-age",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Disinformation in the Information Age",
    "text": "Disinformation in the Information Age\nDisinformation is defined as being deliberately false information, created with the intention to mislead its reader. Disinformation has been weaponized since the early middle ages: for example, in the 19th century, New York-based newspaper The Sun published a series of articles about the discovery of life on the Moon, with the purpose of increasing sales of The Sun. The papers claimed that, using a massive telescope, an english astronomer had discovered vegetation, bipedal beavers, and human-like aliens, dubbed “man-bats”, that were four feet tall, had wings, and could fly (Zielinski, 2015). Whether-or-not the great Moon Hoax lead to The Sun becoming a successfull paper remains uncertain; some accounts claim that the series of papers brought The Sun to international fame, however it’s likely that rumors of The Sun’s hoax increasing the paper’s circulation were exaggerated.\n\n\n&lt;img \n  src=\"soci_270_images/disinformation_example_1.png\" \n  alt=\"An example of disinformation spread on twitter urnign voters to vote via twitter\" \n  style=\"width: 100%; height: auto; display: block;\"&gt;\n\n\n&lt;img \n  src=\"soci_270_images/muller_report_1.png\" \n  alt=\"Middle example\" \n  style=\"width: 100%; height: auto; display: block;\"&gt;\n\n\nThis is an example of a (mostly) harmless disinformation campaign; However, disinformation campaigns can, and have, been used instead to sway public opinion on critical matters. For example, during the Cold War, the KGB orchestrated a widespread disinformation campaign, alledging that HIV/AIDS was a bioweapon engineered by the United States, in an effort to stoke global mistrust of public health authorities and foster anti-americanism (Selvage & Nehring, 2019). A particularly relevant example is political elections: State-sponsored Russian actors have mounted disinformation campaigns in every single U.S. federal election since 2016, at the latest. In 2016, for instance, the Russian state-sponsored Internet Research Agency (IRA) ran hundreds of Facebook and Twitter groups that amplified divisive content, and organized astroturf rallies in key US states, most notably Pennsylvania (Menn and Dave, 2018). The extent to which these coordinated campains influenced the 2016 United States election remains unclear: initial findings by the DOJ suggested that Russia coordinated a sweeping, large scale multi-million dollar online campaign aimed to praise Donald Trump and disparage Hillary Clinton (Muller, 2019). However, multiple studies have found that even under generous assumptions about presuasion rates, the vote-shifts caused by the IRA’s disinformation campaigns were too small to sway the election’s outcome (Allcot & Gentzkow, 2017, Eady et al., 2023).\nWith the rise of digital platforms and generative AI, the scale, speed, and sophistication of disinformation have grown exponentially. From elections and pandemics to social justice movements and international conflicts, false or misleading content is being spread online to manipulate emotions and polarize public opinion. The challenge today is not just the volume of disinformation, but how convincing and targeted it has become. Former U.S. Director of National Intelligence Avril Haines describes how state-sponsored campaigns, like Russia’s Kremlin, now operate using “a vast multimedia influence apparatus,” including bots, cyber-actors, fake news websites, and social media trolls. Large language models (LLMs) can now generate human-like tweets, comments, and articles at scale. Combined with deepfakes, doppelgänger sites, and AI-generated personas, these tools allow bad actors to craft propaganda that appears authentic, emotionally resonant, and difficult to detect.\nIn this notebook, we’ll use machine learning — specifically, pretrained large language models — to study the language of disinformation in a real dataset of English and Russian-language tweets. These tweets include both known propagandist and non-propagandist content."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#learning-outcomes",
    "href": "docs/SOCI-280/soci_270_bert .html#learning-outcomes",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this module, you will be able to better understand disinformation and propaganda techniques using the following computational tools:\n\nSentiment analysis to detect emotional tone (positive, negative, neutral)\nToxicity analysis to identify harmful or aggressive language (e.g., insults, threats)\nStatistical testing to compare patterns between tweet types and languages (e.g., English vs. Russian)\n\nYou’ll learn how to work with pretrained LLMs, interpret model predictions, and use basic statistical methods to answer questions like:\n\nAre propagandist tweets more emotionally charged or toxic than normal political tweets?\nDo they use different rhetorical strategies in different languages?\nCan we identify signals that indicate a tweet is part of a disinformation campaign?\n\nThrough this analysis, we’ll explore various dimmensions of AI applications, critically examining how it can be used to better understand and detect the patterns of disinformation when working with large amounts of social data."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#key-terms-and-concepts",
    "href": "docs/SOCI-280/soci_270_bert .html#key-terms-and-concepts",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Key Terms and Concepts",
    "text": "Key Terms and Concepts\nDisinformation: Disinformation is generally understood as the intentional spreading of false, misleading, or incorrect information, in order to influence the public or create an outcome, usually political.\nPropaganda: Propaganda is similar to disinformation in it’s intent to spread a cause or doctrine, but can differ in how systematically or overtly it is speread. The two concepts are both forms of misinformation, with propaganda generally being employed to promote a cause.\nLarge Language Model (LLM): A Large Language Model is a langauge model trained on a very large set of text data, accessing the features of the text by converting units of text into quanitative representations that can be used for various tasks, such as chatbotting, or in the case of this notebook, Natural Language Processing.\nNatural Language Processing (NLP): Natural Langauge Processing encompasses a wide variety of techniques and practices wtih the goal of enabling computers to understand, interpret, and produce human language. The key NLP techniques in this notebook are sentiment analysis, a technique that analayzes the relative positivity or negativity of langauge, and toxicity analyis which analyzes the relative aggressiveness of language.\nBERT: to enable these analyses we will be using two BERT models. BERT (Bidirectional Encoder Representations from Transformers) is an open-source framework for machine learning and is used for NLP. BERT is well-suited to understanding langauge, rather than generating it, which is what this notebook is concerned with. The specific BERT models we are using are a multi-lingual model, which can analyze tweets in different languages, and a model trained for analyzing tweet sentiments.\nFine Tuning: The multi-lingual model is fine-tuned, or trained specifically on the Russian Disinformation tweet dataset. This is done by inputting a training subset of the data, where the tweets are labeled as Disinformation, into the BERT model to create a model familiar with our data, and well-suited to producing analyses."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#dataset",
    "href": "docs/SOCI-280/soci_270_bert .html#dataset",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "Dataset",
    "text": "Dataset\nOur data for this notebook comes from four sources:\n\n1) The analysis data:\nThe data used from our analysis is a combination of four datasets: 1) The Russian troll tweets dataset, AKA the Internet Research Agency (IRA) tweets dataset is a large, verified collection of social media posts created by the Internet Research agency, the Russian state-sponsored troll farm we talked about earlier. The data was collected and provided by FiveThirtyEight (RIP) an America political news website that focused on providing detailed statistical analyses of the political climate in the United States. Compared to other Russian troll datasets, the IRA dataset is unparalleled in terms of accuracy because the data labels were directly provided by Twitter’s internal security teams, which identified the IRA troll accounts and turned them over to the United States Congress as evidence for the Senate Select Committee on Intelligence’s investigation into foreign election interference. Hence, every single tweet in the dataset is a known, verified Russian disinformation account. 2) The sentiment140 dataset is a massive English language datasets of tweets, created by Standford with the purpose of training sentiment analysis detection models. This dataset serves as our English control group. 3) Likewise, the Rusentimental dataset is a dataset of modern-day Russian tweets, collected by Russian NLP researchers for sentiment analysis purposes. This serves as our Russian control group. 4) Lastly, we added tweets taken from a collection of known Russian disinformation accounts that were active during the start of the Russia-Ukraine War. Researchers selected these tweets based on the accounts’ “association with state-backed media and a history of spreading disinformation and misinformation”.\n\n\n2) The model training data:\nFor our analysis, we trained a tweet propaganda detection model off of the paper “Labeled Datasets for Research on Information Operations”, which provides the same verified collection of disinformation accounts from the IRA, as well as a “control” dataset of legitimate, topically-related accounts, which allowed for a direct, comparative analysis between malicious and organic online posts, within the same conversational political context. To avoid including training data in our in-class analysis, the IRA dataset was randomly split in two, with one half being used for model training and the other half being used for the in-class demo."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#hashtag-analysis",
    "href": "docs/SOCI-280/soci_270_bert .html#hashtag-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "0.1 Hashtag analysis",
    "text": "0.1 Hashtag analysis\nWhat hashtags are being used in tweets from propagandist vs. non-propagandist accounts? For the next part of our data analysis, we’ll find the freqency of the 10 most common hashtags used by the two account types. As you explore the tables below, notice the content and language of the hashtags.\nOur earlier language analysis suggests that most disinformation in this dataset is likely in English. By examining the hashtags used in these tweets, we can better understand the specific words, topics, and narratives that disinformation accounts are trying to amplify.\nTo begin, we’ll extract hashtags from our textual data by finding phrases beginning with the character #, and ignoring hashtags shorter than 10 letters for the sake of making things interesting.\n\ntweets['content'] = tweets['content'].astype(str) # because some of the tweets aren't strings, apparently\ndef extract_hashtags(text):\n    return re.findall(r'#(\\w+)', text.lower())\n\n\ntweets['hashtags'] = tweets['content'].apply(extract_hashtags)\ntweets['hashtags'] = tweets['hashtags'].apply(lambda hashtag_list: [tag for tag in hashtag_list if len(tag) &gt; 10])\n\nprint(tweets[['is_propaganda', 'content', 'hashtags']].head())\n\nNow we can visualize the frequency of the 10 most common hashtags longer than 10 characters used by propagandist accounts over time.\n\n🔎 Engage Critically\n❓ Key Questions\n\nWith reference to the timeline of tweets, and the hashtags below, describe some of the main targets of Russian disinformation.\nGiven what you know about disinformation, what are the intentions of these accounts, and what outcomes are they attempting to create?\nWhat do the hashtags not tell us about the disinformation accounts? Where might our ability to conclude the intentions and outcomes of these accounts be limited by the data we have examined?\nWhat additional data could we collect to better understand this type of disinformation.\n\n\n\npropaganda_tweets = tweets[tweets['is_propaganda'] == 1]\npropaganda_hashtags = propaganda_tweets.explode('hashtags')\ntop_20_propaganda = propaganda_hashtags['hashtags'].value_counts().head(10)\n\ntop_prop_list = top_20_propaganda.index.tolist()\ntop_prop_df = propaganda_hashtags[propaganda_hashtags['hashtags'].isin(top_prop_list)]\ntop_prop_df['date'] = pd.to_datetime(top_prop_df['date'], errors='coerce')\ntop_prop_df.dropna(subset=['date'], inplace=True)\n\nprop_weekly = top_prop_df.groupby('hashtags').resample('W', on='date').size().reset_index(name='count')\n\nfig, ax = plt.subplots(1, 1, figsize=(18, 10))\nfig.suptitle('Weekly spread of Top 10 propaganda hashtags over time', fontsize=20)\n\nsns.lineplot(ax=ax, data=prop_weekly, x='date', y='count', hue='hashtags', palette='viridis')\nax.set_title('Propaganda Hashtags (IRA only)', fontsize=16)\nax.set_xlabel('Date (by week)')\nax.set_ylabel('Weekly Count')\nax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0.)\nax.grid(True, which='both', linestyle='--', linewidth=0.5)\n\nplt.tight_layout(rect=[0, 0, 0.9, 0.96])\nplt.show()\n\nTo explore how these hashtags trend over time, you can use the interactive tool below. Enter one or more hashtags (separated by commas) to visualize their weekly frequency in the dataset. This can help reveal how certain narratives gain momentum or fade in relevance.\n\nall_hashtags_df = tweets.explode('hashtags')\nall_hashtags_df['date'] = pd.to_datetime(all_hashtags_df['date'], errors='coerce')\nall_hashtags_df.dropna(subset=['date', 'hashtags'], inplace=True)\nall_hashtags_df['hashtags'] = all_hashtags_df['hashtags'].str.lower()\n\n\n\nhashtag_input = widgets.Text(value='news,russia,syria',placeholder='Enter hashtags, separated by commas',description='Hashtags:',layout={'width': '50%'})\n\nplot_output = widgets.Output()\n\ndef update_plot(change):\n    hashtags_to_plot = [tag.strip().lower() for tag in change['new'].split(',') if tag.strip()]\n    with plot_output:\n        clear_output(wait=True)\n        if not hashtags_to_plot:\n            print(\"enter at least one hashtag\")\n            return\n        filtered_data = all_hashtags_df[all_hashtags_df['hashtags'].isin(hashtags_to_plot)]\n\n        if filtered_data.empty:\n            print(\"No data found for the specified hashtags\")\n            return\n        weekly_counts = filtered_data.groupby('hashtags').resample('W', on='date').size().reset_index(name='count')\n        fig, ax = plt.subplots(1, 1, figsize=(12, 7))\n        sns.lineplot(data=weekly_counts, x='date', y='count', hue='hashtags', ax=ax)\n        \n        ax.set_title('Weekly Frequency of Selected Hashtags')\n        ax.set_xlabel('Date')\n        ax.set_ylabel('Weekly Mentions')\n        ax.grid(True, linestyle='--', linewidth=0.5)\n        plt.tight_layout()\n        plt.show()\n\nhashtag_input.observe(update_plot, names='value')\n\nprint(\"Enter a comma-separated list of hashtags to see their trends over time.\")\ndisplay(widgets.VBox([hashtag_input, plot_output]))\nupdate_plot({'new': hashtag_input.value})\n\n\n🛑 Stop and Reflect\nNow that we have gone through the dataset and examined a variety of its features, take a few minutes and disucss the following questions with a partner or small group.\n\nWhat are the conclusions we have come to regarding disinformation? How are they influenced or limited by the dataset?\nSo far we have only looked at statistical aspects of the data without using machine learning techniques. Make some predictions on how the machine learning techniques we will use next might change our understanding of online disinformation. How might machine learning, specifically NLP, be used to enrich our understanding of disinformation?\nHas your understanding of online disinformation changed after looking at this data? Write down a few questions you have about online disinformation, the dataset, or the computational methods we have been using."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#can-we-tune-models-to-detect-online-disinformation-campaigns-classifying-current-tweets-with-a-model-finetuned-on-the-russian_disinformation_tweets-dataset",
    "href": "docs/SOCI-280/soci_270_bert .html#can-we-tune-models-to-detect-online-disinformation-campaigns-classifying-current-tweets-with-a-model-finetuned-on-the-russian_disinformation_tweets-dataset",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "1. Can we tune models to detect online disinformation campaigns? Classifying current tweets with a model finetuned on the russian_disinformation_tweets dataset",
    "text": "1. Can we tune models to detect online disinformation campaigns? Classifying current tweets with a model finetuned on the russian_disinformation_tweets dataset\n\nExploring Our Model\nNow that we have examined our data and looked at some of the key features in the Russian Disinformation Dataset, we can start thinking about ways to use machine learning to answer questions, classify features, and make predictions about our dataset. To do any of these tasks we first require a way to interpret the text data and assign numeric qualities to our tweets.\nThe model we are using to do this is a multilingual model which maps sentences and paragraphs into multi-dimensional vector space. In other words, it takes the sentences and paragraphs of our tweets and assigns them a position associated with their meaning. This is done based on the context of the token (the unit of text, like a word or sentence). The model we are using is capable of interpreting multiple languages and is fine-tuned, or specifically trained, on the data we are examining. The code below is going to call upon a pre-built classifier which uses this fine-tuned model to predict whether a tweet is likely Russian propaganda. The two sample tweets are:\n\n“#qanon #trump Hunter Biden is a Ukranian Shill”\n“What great weather we have today”\n\nThe model is going to take these text inputs, represent them in vector space, and then report whether their respective values are similar to those of disinformation tweets.\n\n\nClassification and Its Discontents\nBefore we explore the possibilities of our model to classify, we should first consider some of the main concerns and limitations regarding classification. Classification is an essential element of how machine learning operates. At its core it is the method of finding features that are central to a class and assigning units to that class based on those features. As you may already see, this “in-or-out” framework necessarily flattens some of the richness of human life, in order to effectively create these incredibly useful classes.\n\nExample:\nYou might say a cat and a dog are really easy to classify. Most people know what a dog looks like and that it looks different than a cat. But if all I tell you is that there are two animals that commonly live with humans, that have a tail and paws, and make noise you might have a hard time classifying them, because they share common features.\nIt is important to think deeply about how we are classifying, especially as many datasets are labeled by people, who carry their own understandings of what belongs to each class.\nAny class or classifier will be informed by the balance of abstraction to specificity, and we should always keep this in mind when we are classifying. It is important to be specific enough to ascertain the qualities we are interested in, but not so specific we end up with thousands of classes.\n\nNow that we have explored some of the trickiness of classification as a concept, we can look at how machine learning can help us work through some of these challenges. By using data that is labeled as disinformation our model can be trained to associated certain numerical feautres with disinformation, and when we give it text data that is similar to what it knows to be disinformation, it will classify it as such.\nFor this analysis, we trained a model on a dataset very similar to ours, with the purpose of detecting propaganda tweets. Recall, however, that this model was trained soley on tweets around and before the 2016 presidential election–meaning it has never seen any tweets posted after this point. Given this information, what kinds of tweets do you think the model will struggle with the most? What kinds of propaganda tweets will it excel at detecting? Try using the model below, and see if you can get it to label a string of text as disinformation.\n\nclassifier = pipeline(\"text-classification\",model=\"IreneBerezin/soci-280-model\")\n\nprint(classifier(\"crooked hillary is trying to rig the election! #MAGA!\")) #put your text in here\n\nLet’s now apply the model to a random sample of the dataset we’ve been studying. How well do you think the model will perform?\n\ntweets_sample = tweets.sample(n=2000, random_state=60)\ntweets_inference = tweets_sample[[\"content\", \"is_propaganda\"]].dropna()\ntweets_inference['is_propaganda'] = 1 - tweets_inference['is_propaganda'] # for whatever reasons the labels are switched in the training data\n\nclassifier = pipeline(\"text-classification\",model=\"IreneBerezin/soci-280-model\",device=-1)\npredictions = classifier(tweets_inference['content'].tolist(),batch_size=32,truncation=True)\npredicted_labels = [int(p['label'].split('_')[1]) for p in predictions]\ntrue_labels = tweets_inference['is_propaganda'].tolist()\n\n\n\ndisplay_labels = ['Disinformation', 'Normal Tweet']\n\ncm = confusion_matrix(y_true=true_labels,y_pred=predicted_labels,labels=[0, 1])\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nax.set_title(\"Confusion Matrix\", fontsize=16)\ndisp.plot(ax=ax, cmap='Greys') \nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\nPretty terribly, apparently! Above is what’s called a confusion matrix: it shows the number of labels that were correctly predicted, and incorrectly predicted. We see that, out of a random sample of 2000 tweets:\n\n797 tweets were marked as disinformation, and were, in fact, disinformation\n504 tweets were labelled as normal tweets while they were, in fact, disinformation\n282 tweets were labelled disinformation when they were in fact normal\n417 tweets were correctly labelled as normal tweets\n\nSo, better than blindly guessing (which would put us on average as 500 tweets in each category) but still very bad.\nThis is an example of overfitting: instead of actually learning what constitutes a propagandist tweet, the model simply memorized the specific writing styles of the troll accounts present in the training set. Hence, when it was shown new tweets from different troll accounts in the test set, it failed, as it never learned the general patterns that define an account as part of a coordinated disinformation campaign. The model did the ML equivalent of memorizing in great detail all the solutions to questions in a math textbook instead of actually learning how to solve them."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#what-is-sentiment-analysis",
    "href": "docs/SOCI-280/soci_270_bert .html#what-is-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "1. What is Sentiment Analysis?",
    "text": "1. What is Sentiment Analysis?\n\n“Sentiment analysis is the practice of applying natural language processing and text analysis techniques to identify and extract subjective information from text” (Hussein, 2018).\n\nAs this definition alludes, sentiment analysis is a part of natural language processing (NLP), a field at the intersection of human language and computation. Because humans are complex, emotional beings, the language we use is often shaped by our affective (emotional) dispositions. Sentiment analysis, sometimes referred to as “opinion mining”, is one way researchers can methodologically understand the emotional intentions, typically positive, negative, or neutral sentiments, that lie in textual datasets.\n\n🔎 Engage Critically\nAt the heart of sentiment analysis is the assumption that language reveals interior, affective states, and that these states can be codified and generalized to broader populations. AI scholar Kate Crawford, in her book Atlas of AI, explores how many assumptions found in contemporary sentiment research (i.e., that there are 7 universal emotions) are largely unsubstantiated notions that emerged from mid-20th century research funded by the US Department of Defense. Rather than maintaining that emotions can be universally categorized, her work invites researchers to think about how emotional expression is highly contextualized by social and cultural factors and the distinct subject positions of content makers.\n\n❓ Consider the research question for your sentiment analysis. How might the text you are working with be shaped by the distinct groups that have generated it?\n\n\n❓ Are there steps you can take to educate yourself around the unique language uses of your dataset (for example, directly speaking with someone from that group or learning from a qualified expert on the subject)?\n\nIf you’re interested, you can learn more about data justice in community research in a guide created by UBC’s Office for Regional and International Community Engagement.\n\nThe rise of web 2.0 has produced prolific volumes of user-generated content (UGC) on the internet, particularly as people engage in a variety of social platforms and forums to share opinions, ideas and express themselves. Maybe you are interested in understanding how people feel about a particular political candidate by examining tweets around election time, or you wonder what people think about a particular bus route on reddit. UGC is often unstructured data, meaning that it isn’t organized in a recognizable way.\nStructured data for opinions about a political candidate might look like this:\n\n\n\n\n\n\n\n\nPro\nCon\nNeutral\n\n\n\n\nSupports climate action policies\nNo plan for lowering the cost of living\nUBC Graduate\n\n\nExpand mental health services\n\n\n\n\n\nWhile unstructured data might look like this:\n\nlove that she’s trying to increase mental health services + actually cares abt the climate 👏 but what’s up w rent n grocieries?? i dont wanna go broke out here 😭 a ubc alum too like i thought she’d understand\n\nIn the structured data example above, the reviewer defines which parts of the feedback are positive, negative or neutral. In the unstructured example on the other hand, there are many typos and a given sentence might include a positive and a negative review as well as more nuanced contextual information (i.e. mentioning being a UBC alum when discussing cost of living). While messy, this contextual information often carries valuable insights that can be very useful for researchers.\nThe task of sentiment analysis is to make sense of these kinds of nuanced textual data - often for the purpose of understanding people, predicting human behaviour, or even in some cases, manipulating human behaviour.\nDisinformation campaigns often aim to sway public opinion by influencing the emotional tone of online conversations. Sentiment analysis allows us to detect and understand these patterns by identifying whether large volumes of text express positive, negative, or neutral sentiment.\nOur model is pretrained, meaning it has already learnt from millions of labelled examples how to distinguish different sentiments. Specifically, because the model we’ll be using was trained on English tweets, it’s tuned to the language and syntax common on Twitter/X, and is limited to analyzing English-language text.\nLanguage is complex and always changing.\nIn the English language, for example, the word “present” has multiple meanings which could have positive, negative or neutral connotations. Further, a contemporary sentiment lexicon might code the word “miss” as being associated with negative or sad emotional experiences such as longing; if such a lexicon were applied to a 19th century novel which uses the word “miss” to describe single women, then, it might incorrectly associate negative sentiment where it shouldn’t be. While sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\nNow, we’re ready to get back to our analysis. Below, we’ll load in our model and tokenizer and start playing around with identifying the sentiment of different phrases.\n\nsentiment = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n\nprint(sentiment(\"I hate everyone and everything\"))\nprint(sentiment(\"Life is great!\"))\nprint(sentiment(\"Hello world\"))\n\nLet’s breakdown this output. There are two parts to what the model returns:\n\nLabel → a classification labelling the text as either having positive, negative, or neutral sentiment\nScore → the model’s confidence in it’s classification\n\n\n🔎 Engage Critically\nTry using the interactive tool below to explore how a machine learning model detects sentiment in short texts like tweets. The model classifies each input as positive, neutral, or negative, and assigns a probability score to each label. Type a sentence (like a tweet or short message) into the box below and click “Analyze” to see how the model interprets its emotional tone.\n\n\ntext_input = widgets.Text(\n    value=\"hello world!\",\n    placeholder=\"Type a sentence here\",\n    description=\"Input:\",\n    layout=widgets.Layout(width=\"70%\")\n)\nanalyze_btn = widgets.Button(description=\"Analyze\", button_style=\"primary\")\noutput_area = widgets.Output()\n\ndef on_analyze_clicked(b):\n    with output_area:\n        clear_output(wait=True)\n        scores = sentiment(text_input.value)\n        labels = [item[\"label\"] for item in scores]\n        probs  = [item[\"score\"] for item in scores]\n        fig, ax = plt.subplots(figsize=(6,4))\n        bars = ax.bar(labels, probs)\n        ax.set_ylim(0, 1)\n        ax.set_ylabel(\"Probability\")\n        ax.set_title(\"Sentiment Probability Distribution\")\n        for bar, prob in zip(bars, probs):\n            height = bar.get_height()\n            ax.text(\n                bar.get_x() + bar.get_width() / 2,  \n                height + 0.02,                    \n                f\"{prob:.2f}\",                   \n                ha=\"center\", va=\"bottom\",\n                color=\"red\", fontsize=12)\n        plt.show()\n\nanalyze_btn.on_click(on_analyze_clicked)\n\ndisplay(widgets.VBox([text_input, analyze_btn, output_area]))\n\n\nBatch Sentiment Analysis\nNow, let’s start running sentiment analysis on our dataset. The general steps to run our analysis include:\n\nLoading a pretrained model and tokenizer\nWe load a RoBERTa model that has been fine-tuned for sentiment analysis on tweets, along with its corresponding tokenizer.\nCreating sentiment analysis pipeline\nWe set up a Hugging Face pipeline that handles finer steps in our sentiment analysis, such as tokenization (breaking up text into smaller units, called tokens), batching (processing multiple texts at once for efficiency), and prediction (predicting the overall sentiment).\nRunning batch sentiment analysis on the dataset\nTo efficiently analyze large numbers of tweets, we split the dataset into batches of 1,000 tweets and process them one batch at a time. To store the predictions, we extract the predicted sentiment labels and save them in a column named sentiment.\nPreviewing the results\n\n\ntweets_small = tweets.groupby('source').sample(n=1000, random_state=42)\ntweets_small = tweets_small.sample(frac=1, random_state=42).reset_index(drop=True) #you might want to change this value\ntweets_small[\"sentiment\"] = \"\" \n\n\nbatch_size = 1000\nn = 0\nfor start in range(0, len(tweets_small), batch_size):\n    end = start + batch_size\n    batch_texts = tweets_small[\"content\"].iloc[start:end].tolist()\n    results = sentiment(batch_texts)           \n    labels  = [res[\"label\"] for res in results]\n    tweets_small.loc[start:end-1, \"sentiment\"] = labels \n    n = n+1\n    print(f'batch {n} done')\n\n\nprint(tweets_small[[\"content\", \"sentiment\"]].head())\n\nWe can see the first 5 tweets and their predicted sentiment above.\nNow that we know how to run sentiment analysis to identify the overarching sentiment of a tweet, we are now in good position to ask and investigate whether emotionally charged language is more common in propaganda. Let’s explore this by forming a hypothesis and testing it statistically.\n\n🔎 Engage Critically\nHypothesis: Propagandist tweets (is_propaganda == 1) are more emotionally charged — that is, they are more likely to be classified as Positive or Negative (non-neutral) compared to non-propagandist tweets (is_propaganda == 0).\nWe will test whether the difference in sentiment category frequencies between the two groups is statistically significant.\n\nFirst, let’s examine the sentiment distribution for each group:\n\ndist = pd.crosstab(tweets_small['sentiment'], tweets_small['is_propaganda'], normalize='columns')\ndist.columns = ['Non-propagandist', 'Propagandist']\nprint(dist * 100)\n\nReading the table, we can see that the majority of non-propagandist tweets are either negative (~43%) or neutral (~44%), while the majority of propagandist tweets (~63%) express neutral sentiment.\n\nprint(tweets_small['sentiment'])\n\n\n# We define 'charged' sentiment as Positive or Negative\ntweets_small['charged'] = tweets_small['sentiment'].isin(['positive', 'negative']).astype(int)\n\n# Constructing a  contingency table: rows = propagandist/non propagandist group, columnss = charged vs neutral\ncontingency = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged'])\nprint(\"Contingency table:\\n\", contingency)\n\n# Chi-squared test\nchi2, p, dof, expected = chi2_contingency(contingency)\nprint(f\"p-value = {p:.3e}\")\n\n\n🔎 Engage Critically\nTry interpreting the output. What are our results telling us? Based on the p-value, what can we conclude about our hypothesis?"
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#multilingual-sentiment-analysis",
    "href": "docs/SOCI-280/soci_270_bert .html#multilingual-sentiment-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "3. Multilingual Sentiment Analysis",
    "text": "3. Multilingual Sentiment Analysis\nOur dataset of tweets isn’t entirely in English — many of the tweets are written in Russian. Could this be skewing our results? How is our model actually handling Russian-language tweets compared to English ones?\n\n🔎 Engage Critically\nRecall our discussion on sentiment lexicons in Section 1:\n\nWhile sentiment analysis can be a useful tool, it demands ongoing criticality and reflexivity from a researcher (you!). Throughout your analysis, be sure to continually ask yourself whether a particular sentiment lexicon is appropriate for your project.\n\n❓ How might the use of a monolingual sentiment model introduce bias into our results? Are non-English tweets being misclassified as neutral, negative, or positive when they shouldn’t be?\n\nWith this in mind, let’s explore below. We’ll use the Unicode values of Cyrillic characters to identify Russian-language tweets, and run sentiment analysis seperately on Russian and and English tweets.\n\ntweets_lang = tweets_small.copy()\ntweets_small['language'] = tweets_small['language'].str.lower().str[:2]\n\nen_tweets = tweets_lang[tweets_lang['language'] == 'en']\nru_tweets  = tweets_lang[tweets_lang['language'] == 'ru']\neng_dist = pd.crosstab(en_tweets['sentiment'], en_tweets['is_propaganda'], normalize='columns') * 100\nru_dist  = pd.crosstab(ru_tweets['sentiment'],  ru_tweets['is_propaganda'],  normalize='columns') * 100\n\n# Renaming columns for clarity\ncol_map = {0: 'Non-propagandist', 1: 'Propagandist'}\n\neng_dist = eng_dist.rename(columns=lambda c: col_map.get(c, str(c)))\nru_dist  = ru_dist.rename(columns=lambda c: col_map.get(c, str(c)))\n\nprint(\"English Tweets Sentiment (%):\\n\", eng_dist, \"\\n\")\nprint(\"Russian Tweets Sentiment (%):\\n\", ru_dist)\n\n\n🔎 Engage Critically\nTake a moment to interpret the results before continuing. What do they tell us about the performance of our model on Russian-language tweets? Why do you think that is?\n\nFrom the table above, we can see that our model is performing very poorly on Russian-language tweets, as nearly all of the Russian tweets are being marked as neutral regardless of if they are propagandist or not. This means that the pretrained model we were using before is not an appropriate choice based on the characteristics of our data, namely that a significant portion of the tweets are written in Russian, a language the model was not trained to make reliable predictions on.\nLet’s try re-running our sentiment analysis using a different model. This time, we’ll use a model trained on 198 million tweets that were not filtered by language. As a result, the training data reflects the most commonly used languages on the platform at the time of collection, with Russian conveniently ranking as the 11th most frequent.\nWe’ll follow the same steps for batch sentiment analysis that we did in Section 2:\n\nsentiment_multi = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual\")\n\nbatch_size = 1000\nfor start in range(0, len(tweets_small), batch_size):\n    end = start + batch_size\n    batch_texts = tweets_small[\"content\"].iloc[start:end].tolist()\n    results = sentiment_multi(batch_texts)           \n    labels  = [res[\"label\"] for res in results]\n    tweets_small.loc[start:end-1, \"sentiment\"] = labels \n\nprint(tweets_small[[\"content\", \"sentiment\"]].head())\n\nTo make the results easier to visualize, let’s create a table that shows the percentage distribution of sentiment labels (positive, neutral, negative) within propagandist and non-propagandist tweets.\n\nmulti_dist = pd.crosstab(tweets_small['sentiment'], tweets['is_propaganda'], normalize='columns') * 100\nmulti_dist.columns = ['Non-propagandist', 'Propagandist']\nprint(\"Multilingual Model Sentiment (%):\\n\", multi_dist)\n\nThe sentiment distribution between propagandist and non-propagandist tweets is quite similar when using the multilingual model. Both groups are predominantly neutral (around 50%), with roughly equal proportions of negative and positive sentiment.\nNow, let’s run a statistical test to see if there’s a meaningful difference in sentiment between propagandist and non-propagandist tweets. Specifically, we want to know:\n\nAre propagandist tweets more likely to be emotionally charged (positive or negative) than neutral, compared to non-propagandist tweets?\n\nTo answer this, we’ll use a chi-squared test, which helps us check whether the differences we see in the data are likely due to chance or if they’re statistically significant.\n\ntweets_small['charged_multi'] = tweets_small['sentiment'].isin(['positive','negative']).astype(int)\ncontingency_multi = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged_multi'])\nchi2_multi, p_multi, *_ = chi2_contingency(contingency_multi)\nprint(f\"Chi-squared p-value with multilingual model: {p_multi:.3e}\")\n\n\n🔎 Engage Critically\nTake a moment to interpret the results before continuing. What does our p-value tell us about our inital research question above?\n\nOur p-value (0.00003727) is much smaller than the common significance level of 0.05, indicating that the difference in how emotionally charged tweets are distributed between propagandist and non-propagandist groups is very unlikely to be due to random chance.\nThis means there is strong evidence that propagandist tweets are more likely to be emotionally charged compared to non-propagandist tweets, according to the multilingual model’s sentiment analysis."
  },
  {
    "objectID": "docs/SOCI-280/soci_270_bert .html#introduction-to-toxicity-analysis",
    "href": "docs/SOCI-280/soci_270_bert .html#introduction-to-toxicity-analysis",
    "title": "Introduction to Sentiment Analysis: Identifying and Mapping Disinformation Campaigns using NLP",
    "section": "4. Introduction to Toxicity Analysis",
    "text": "4. Introduction to Toxicity Analysis\nWarning: this section contains examples of potentially offensive or profane text\nToxicity analysis is another type of classification task that uses machine learning to detect whether a piece of text contains toxic speech. Jigsaw, a Google subsidary and leader in technological solutions for threats to civil society, uses the following definition for “toxic speech” proposed by Dixon et al. (2018):\n\n”[R]ude, disrespectful, or unreasonable language that is likely to make someone leave a discussion”\n\n\n🔎 Engage Critically\nThis definition is widely considered by the NLP community to be ill-defined and vague. Why do you think? What issues could potentially arise from this definition, and how could they impact (for example) a comment flagging tool that gives warnings to social media users whose comments meet this definition of toxic speech?\n\nA core issue defined by Berezin, Farahbakhsh, and Crespi (2023) is that the definition “gives no quantitative measure of the toxicity and operates with highly subjective cultural terms”, yet still remains widely used by researchers and developers in the field. We’ll explore some of the ways this definition is influencing toxicity analysis briefly below.\n\n4.1 Positive profanity\n\n\n\nFuck dude, nurses are the shit (Mauboussin, 2022)\n\n\nConsider the Reddit post above. Is the comment an example of toxic speech? Probably not, right?\nNow imagine you are Perspective API, Google’s AI toxicity moderation tool, with your scope of “toxic speech” limited solely to the definition of ”rude, disrespectful, or unreasonable language that is likely to make someone leave a discussion”. Because of your architecture, you are limited in the way you can understand a message in context. You process the comment and immediately detect two profanities that meet your requirement for being rude language, and assign it a subsequent toxicity score:\n\n\n\nFuck dude, nurses are the shit with Toxicity Score (98.62%) (Mauboussin, 2022)\n\n\nThis is where, in the NLP community, there has been a growing discussion to ensure toxicity analysis tools, especially detectors used in online discussion and social media platforms, are more robust than simply being ‘profanity detectors’. They must be able to interpret a word in context.\n\n\n4.2 In-group language\nConsider in-group words used by distinct communities. Many of these words, once used as derogatory slurs against a group of people (such as Black or LGBTQ+ folk), have now largely been reclaimed and are prevalent in the lexicons of individuals identifying within these communities, no longer considered offensive when used by the in-group. However, if human annotators label textual data that ML models then are trained on, biases can permeate the models and lead to the classification of non-toxic, in-group language as harmful or offensive. Notably, African-American Vernacular English (AAVE) has been found to be flagged as toxic due to linguistic bias. XX frames how the challenge impacts toxicity detectors well:\n\n🔎 Engage Critically\nHow do you think this challenge can impact toxicity detectors? Resende et al. (2024) underscore this tension, noting that:\n\n…[S]uch a fine line between causal speaking and offensive discourse is problematic from a human and computational perspective. In that case, these interpretations are confounding to automatic content moderation tools. In other words, toxicity/sentiment analysis tools are usually developed using manual rules or supervised ML techniques that employ human-labeled data to extract patterns. The disparate treatment embodied by machine learning models usually replicates discrimination patterns historically practiced by humans when interacting with processes in the real world. Due to biases in this process, a lack of context leads both rule-based and machine learning-based models to a concerning scenario where minorities do not receive equal treatment.  Resende et al., 2024, p. 2\n\n\nResende et al. (2024) also conducted a comparison analysis of toxicity models, including Google’s Perspective API and Detoxify (the model we’ll be using for our own analysis soon).\n\n\n\nComparing the Toxicity Scoring Models (Resende et al., 2024)\n\n\nThis bias shown in this model’s performance can come from many factors in its structure, from data provenance and annotation to model architecture and processing, to a combination of many.\n\n🔎 Engage Critically\nIf you could, what questions would you want to ask the people who build these models?\n\n\n\nToxicity analysis using Detoxify\nThe model we’ll be using is called Detoxify (you can read more about it here). It was trained on large datasets of online comments across seven languages, including English and Russian. Detoxify provides an overall toxicity score for each text and can also detect five specific subtypes of toxicity: identity_attack, insult, obscene, sexual_explicit, and threat.\nIn the context of our dataset, propagandist tweets often aim to provoke strong emotions, spread hate, or stir conflict. Running toxicity analysis can help us investigate questions like:\n\nAre propagandist tweets more toxic than non-propagandist ones?\nWhat types of toxic language are most common?\nAre there patterns in how toxicity is used to influence or manipulate public discourse?\n\nToxicity analysis gives us another lens to understand how language and emotion are used in disinformation campaigns. Let’s begin by importing the necessary libraries and tools:\n\nfrom detoxify import Detoxify\n\nmodel = Detoxify('original', device='cpu')\n\nBefore we throw this model at our dataset, let’s take a look at what ‘toxic’ really means.\nWe’ll be repeating the same hypothesis test that we performed using our sentiment analysis models, this time trying to answer:\n\nAre tweets deemed toxic more likely to originate from propagandists relative to non-propagandist tweets?\n\nHere, we’ll define a tweet toxic if it meets or exceeds a toxicity theshold of 0.5.\n\ntexts = tweets_small['content'].tolist()\nresults = model.predict(texts)\ntoxicity_df = pd.DataFrame(results)\ntweets_small = tweets_small.join(toxicity_df)\n\n\nthreshold = 0.5\ntweets_small['charged'] = (tweets_small['toxicity'] &gt; threshold).astype(int)\n\ncontingency = pd.crosstab(tweets_small['is_propaganda'], tweets_small['charged'])\nprint(\"Contingency table:\\n\", contingency)\n\nchi2, p, dof, expected = chi2_contingency(contingency)\nprint(f\"Chi-squared statistic: {chi2:.2f}\")\nprint(f\"p-value: {p:.3e}\")\n\nprint(\"\\nSample of results:\")\nprint(tweets_small[['content', 'is_propaganda', 'toxicity', 'charged']].head())\n\nFollowing the logic from the sentiment analysis results, what do these results tell us about our hypothesis? How do you think the results would change if we used a different threshold to define toxicity?"
  }
]