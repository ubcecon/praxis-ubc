<!DOCTYPE html>
<html lang="en"><head>
<script src="amne376_presentation_files/libs/clipboard/clipboard.min.js"></script>
<script src="amne376_presentation_files/libs/quarto-html/tabby.min.js"></script>
<script src="amne376_presentation_files/libs/quarto-html/popper.min.js"></script>
<script src="amne376_presentation_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="amne376_presentation_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="amne376_presentation_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="amne376_presentation_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.40">

  <meta name="author" content="Kaiyan Zhang, Krishaant Pathman, Irene Berezin">
  <title>AMNE 376</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="amne376_presentation_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="amne376_presentation_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="amne376_presentation_files/libs/revealjs/dist/theme/quarto-06273e61f1f04043d27f5a4ba8912ce7.css">
  <link href="amne376_presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="amne376_presentation_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="amne376_presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="amne376_presentation_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">AMNE 376</h1>
  <p class="subtitle">Computational Studies of Greek Art and Architecture: A Machine Learning Approach</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Kaiyan Zhang, Krishaant Pathman, Irene Berezin 
</div>
</div>
</div>

</section>
<section id="introduction" class="slide level2">
<h2>Introduction</h2>
<p>In exploring Ancient Greek art and architecture, we often rely on established interpretations rooted in classical scholarship. However, <strong>contemporary methodologies</strong> in archaeology and art history invite us to reconsider these works through fresh perspectives. As anthropologist Clifford Geertz noted</p>
<blockquote>
<p><em>“Cultural analysis is not about finding universal truths, but about appreciating the diversity and complexity of human experience.”</em></p>
</blockquote>
<p>New approaches encourage us to delve into the nuanced contexts and varied meanings that classical artworks may embody.</p>
<p>By integrating modern analytical techniques, we can uncover layers of significance in Greek art that traditional methods might overlook. This course will employ such <strong>interdisciplinary strategies</strong> to deepen our understanding of ancient works, revealing the dynamic interplay between form, function, and cultural expression in classical antiquity.</p>
</section>
<section id="notebook-1-introduction-to-cnn-and-image-embedding" class="slide level2">
<h2>Notebook 1: Introduction to CNN and Image Embedding</h2>
<p><strong>Learning Outcomes</strong>: Through this class, students will</p>
<ul>
<li>understand how computers process visual input</li>
<li>understand convolutional neural networks (CNNs) and how they extract abstract features from images</li>
<li>understand the basic forms of image embedding through visual representation and their applications</li>
<li>practice basic image embedding techniques using a dataset of Greek artworks</li>
<li>discuss the differences between how computers understand art and how humans understand art, and whether this cognitive model is useful in art history research.</li>
</ul>
</section>
<section id="class-structure" class="slide level2">
<h2>Class Structure</h2>
<ul>
<li><strong>Dataset</strong>: We will provide students with an image dataset with Greek artworks of different categories, downloaded from <a href="https://harvardartmuseums.org/collections">Harvard Art Museum Collections</a>.</li>
<li>Provided computational resources permit, we will provide students with code templates and let them run the code themselves. If time permits, they can also try to embed other images they are interested in.</li>
<li>We will begin by introducing the basic image loading and the traditional image representations through <strong>color histograms</strong> to demonstrate the basic concepts of image features to students.</li>
<li>For image embedding task, the notebook will use <strong><a href="https://huggingface.co/facebook/dinov2-base">DINO v2</a></strong> base patch model, and focus solely on its applications instead of its principles, the key ideas will all be presented through visualizations (scatterplots, histograms and the image itself).</li>
<li>We will refer to ideas from the book <em>Computational Formalism: Art History and Machine Learning</em> by Amanda Wasielewski to demonstrate some interesting applications of machine learning in visual art studies.</li>
<li>We will briefly introduce the key applications here and elaborate on them in subsequent notes, including <strong>similarity analysis</strong>, <strong>cluster analysis</strong>, and <strong>classification analysis</strong>.</li>
</ul>
</section>
<section id="key-features" class="slide level2">
<h2>Key Features</h2>
<ul>
<li>Basic, Practical and Generalizable Concepts in Multi-fields</li>
</ul>
<div class="columns">
<div class="column" style="width:35%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/color_histogram_example.png"></p>
<figcaption>Color Histogram</figcaption>
</figure>
</div>
</div><div class="column" style="width:65%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="images/CNN_example.jpg"></p>
<figcaption>Principles of CNN</figcaption>
</figure>
</div>
</div></div>
<ul>
<li>Connecting to Cutting-edge Research</li>
</ul>
<div style="font-size: 80%;">
<p>&nbsp;&nbsp;&nbsp;&nbsp; Wasielewski, A. (2023). Computational Formalism: Art History and Machine Learning. The MIT Press. <a href="https://doi.org/10.7551/mitpress/14268.001.0001" class="uri">https://doi.org/10.7551/mitpress/14268.001.0001</a></p>
</div>
<div style="font-size: 80%;">
<p>&nbsp;&nbsp;&nbsp;&nbsp; Wang, J. Z., &amp; Adams, R. B., Jr.&nbsp;(Eds.). (2024). Modeling visual aesthetics, emotion, and artistic style. Springer. <a href="https://doi.org/10.1007/978-3-031-50269-9" class="uri">https://doi.org/10.1007/978-3-031-50269-9</a></p>
</div>
<!-- ![](image.png){.absolute top=0 left=0 height="230" width="200"} -->
</section>
<section id="notebook-2-introduction-to-computational-formalism" class="slide level2">
<h2>Notebook 2: Introduction to Computational Formalism</h2>
<p><strong>Learning Outcomes</strong>: Through this class, students will</p>
<ul>
<li>understand the concept of <strong>formalism</strong> in the context of art studies</li>
<li>understand the basic techniques of <strong>computational formalism</strong> and describe how it differ from “traditional” formalism</li>
<li>explore an example of Greek artifacts, applying the methodology and interpret the result</li>
<li>critically analyze the pros and cons of formalism and computational formalism, and explain how to integrate these techniques with archaeology.</li>
</ul>
</section>
<section id="class-structure-1" class="slide level2">
<h2>Class Structure</h2>
<ul>
<li><strong>Dataset:</strong> We will provide students with a dataset of images of ancient Greek sculptures from the <a href="https://harvardartmuseums.org/collections">Harvard Art Museums</a>, covering different periods (from the 18th century BCE to the 4th century BCE), accompanied by metadata containing relevant information of these sculptures for students to explore.</li>
<li>We will first create the <strong>image embedding</strong> of this dataset using <strong><a href="https://huggingface.co/facebook/dinov2-base">DINO v2</a></strong> base patch model, and cluster the images to see if any interesting patterns are observed. Here we would also introduce the concept of <strong>Bag of Visual Words</strong> that we create based on clustering.</li>
<li>We will then show how we can discover the <strong>principal component</strong> of the embeddings representing the century or genre of sculptures through image classification.</li>
<li>Then based on the created embeddings, we will demonstrate how we can summarize the extracted <strong>Bag of Visual Words</strong> into human-readable language using large language models.</li>
</ul>
</section>
<section id="key-features-1" class="slide level2">
<h2>Key Features</h2>
<ul>
<li>Continuation of the Previous Topic with a Practical Example</li>
</ul>
<div class="columns">
<div class="column" style="width:25%;">
<p><img data-src="images/greek_sculpture_example1.jpg"></p>
</div><div class="column" style="width:45%;">
<p><img data-src="images/greek_sculpture_example2.jpg"></p>
</div></div>
<div style="font-size: 60%;">
<p>Source: Harvard Art Museum</p>
</div>
<ul>
<li>Supplement to the Course’s Original Context-based Classical Art Research Methods.</li>
</ul>
<div style="font-size: 80%;">
<blockquote>
<p>Drawing from the analysis of machine learning in art history described in this book, one might begin to reimagine “digital” art history outside of the technology-specific paradigm in which it has been placed.</p>
</blockquote>
<pre><code>                                                            -- Computational Formalism: Art History and Machine Learning</code></pre>
</div>
</section>
<section id="notebook-3-introduction-to-ai-assisted-restoration-of-archaeological-objects" class="slide level2">
<h2>Notebook 3: Introduction to AI Assisted Restoration of Archaeological Objects</h2>
<p><strong>Learning Outcomes</strong>: Through this class, students will</p>
<ul>
<li>understand the principles of AI restoration and reconstruction of images (image generating and inpainting)</li>
<li>understand how we train and verify the accuracy of AI restoration models</li>
<li>understand how we can extend the knowledge of 2D image processing to 3D</li>
<li>explore a dataset of vessel and coin fragments, and apply a pre-trained generative AI model to reconstruct the objects if computational resources permit</li>
<li>discuss the underlying risk and bias in using AI for archaeological restoration</li>
</ul>
</section>
<section id="class-structure-2" class="slide level2">
<h2>Class Structure</h2>
<ul>
<li><strong>Dataset:</strong> We will provide students with an image dataset of Greek vessels and coin fragments, as well as complete objects that have been intentionally obscured for verification purposes.</li>
<li>We will introduce the concept of <strong>image inpainting</strong> and <strong>generative models</strong> (such as diffusion models or GANs) for reconstructing missing parts of archaeological objects.</li>
<li>Students will observe how a <strong>pre-trained generative AI model</strong> can reconstruct missing sections of an artifact from its fragment image, and compare the AI-generated reconstruction with the original complete object.</li>
<li>If computational resources allow, students can experiment with reconstructing different fragments or adjusting model parameters to see how the results change.</li>
<li>We will also discuss the limitations and ethical considerations of AI-based reconstructions in archaeology, including accuracy, authenticity, and interpretive risks.</li>
</ul>
</section>
<section id="key-features-2" class="slide level2">
<h2>Key Features</h2>
<ul>
<li>A More Visually Vivid and Interesting Case</li>
</ul>
<div class="column" style="width:70%;">
<p><img data-src="images/ai_reconstruction_example.jpg"></p>
</div><div style="font-size: 60%;">
<p>Source: Stoean <em>et al</em> (2024)</p>
</div>
<ul>
<li>Introduction to a Future Trend in Archaeological Restoration</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><img data-src="images/ai_assist_gif_example.gif"></p>
</div><div class="column" style="width:50%;">
<p><img data-src="images/ai_assist_gif_example2.gif"></p>
</div></div>
<ul>
<li>A Practical Approach to the Application of Knowledge Accumulation and Archaeological Ethics</li>
</ul>
</section>
<section id="reference" class="slide level2">
<h2>Reference</h2>
<ul>
<li><a href="https://www.pinecone.io/learn/series/image-search/">Pinecone: Embedding Methods for Image Search</a></li>
<li>Wasielewski, A. (2023). Computational Formalism: Art History and Machine Learning. The MIT Press. <a href="https://doi.org/10.7551/mitpress/14268.001.0001" class="uri">https://doi.org/10.7551/mitpress/14268.001.0001</a></li>
<li>Wang, J. Z., &amp; Adams, R. B., Jr.&nbsp;(Eds.). (2024). Modeling Visual Aesthetics, Emotion, and Artistic Style. Cham, Switzerland: Springer Nature. <a href="https://doi.org/10.1007/978-3-031-50269-9" class="uri">https://doi.org/10.1007/978-3-031-50269-9</a></li>
<li>Stoean, R., Bacanin, N., Stoean, C., &amp; Ionescu, L. (2024). Bridging the past and present: AI-driven 3D restoration of degraded artefacts for museum digital display. Journal of Cultural Heritage, 69, 18-26. <a href="https://doi.org/10.1016/j.culher.2024.07.008" class="uri">https://doi.org/10.1016/j.culher.2024.07.008</a></li>
<li>Cardarelli, L. (2024). From fragments to digital wholeness: An AI generative approach to reconstructing archaeological vessels. Journal of Cultural Heritage, 70, 250-258. <a href="https://doi.org/10.1016/j.culher.2024.09.012" class="uri">https://doi.org/10.1016/j.culher.2024.09.012</a></li>
</ul>

</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="amne376_presentation_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="amne376_presentation_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="amne376_presentation_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="amne376_presentation_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="amne376_presentation_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="amne376_presentation_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="amne376_presentation_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="amne376_presentation_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="amne376_presentation_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="amne376_presentation_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>