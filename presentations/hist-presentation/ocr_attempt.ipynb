{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ee6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz                                # PyMuPDF\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    AutoModelForImageTextToText\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ddc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_images(pdf_path: str) -> list[Image.Image]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    images = []\n",
    "    for page in doc:\n",
    "        pix = page.get_pixmap()\n",
    "        mode = \"RGBA\" if pix.alpha else \"RGB\"\n",
    "        img = Image.frombytes(mode, [pix.width, pix.height], pix.samples)\n",
    "        images.append(img)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c8cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nanonets_model(model_path: str):\n",
    "    model = AutoModelForImageTextToText.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=\"auto\",\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"flash_attention_2\"\n",
    "    )\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    return model, tokenizer, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6167718b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_image_with_nanonets(\n",
    "    image: Image.Image,\n",
    "    model: AutoModelForImageTextToText,\n",
    "    processor: AutoProcessor,\n",
    "    max_new_tokens: int = 4096\n",
    ") -> str:\n",
    "    prompt = (\n",
    "        \"Extract the text from the above document as if you were reading it naturally. \"\n",
    "        \"Return the tables in html format. Return the equations in LaTeX representation. \"\n",
    "        \"If there is an image in the document and image caption is not present, add a small description of the image inside the <img></img> tag; \"\n",
    "        \"otherwise, add the image caption inside <img></img>. Watermarks should be wrapped in brackets. \"\n",
    "        \"Ex: <watermark>OFFICIAL COPY</watermark>. Page numbers should be wrapped in brackets. \"\n",
    "        \"Ex: <page_number>14</page_number> or <page_number>9/22</page_number>. Prefer using ☐ and ☑ for check boxes.\"\n",
    "    )\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "        ]},\n",
    "    ]\n",
    "    text = processor.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=[image],\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=False\n",
    "    )\n",
    "    generated = processor.batch_decode(\n",
    "        output_ids[:, inputs.input_ids.shape[-1]:],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    return generated[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b55044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_judge(text: str) -> str:\n",
    "    match = re.search(r\"Judge[:\\s]+([A-Za-z .,\\-]+)\", text)\n",
    "    return match.group(1).strip() if match else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff3bb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_pdfs_to_dataframe(pdf_paths: list[str], model_path: str) -> pd.DataFrame:\n",
    "    model, tokenizer, processor = load_nanonets_model(model_path)\n",
    "    records = []\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        # convert each page to image\n",
    "        pages = convert_pdf_to_images(pdf_path)\n",
    "        page_texts = [ocr_image_with_nanonets(img, model, processor) for img in pages]\n",
    "        combined_text = \"\\n\".join(page_texts)\n",
    "\n",
    "        case_name  = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        judge_name = extract_judge(combined_text)\n",
    "\n",
    "        records.append({\n",
    "            \"case_name\":  case_name,\n",
    "            \"judge_name\": judge_name,\n",
    "            \"text\":       combined_text\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(records, columns=[\"case_name\", \"judge_name\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db4f531",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\h'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\h'\n",
      "C:\\Users\\Irene\\AppData\\Local\\Temp\\ipykernel_4972\\1527349647.py:2: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  \"presentations\\hist-presentation\\e5yrggr.pdf\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de19da0b49864e0b900ba13d318ebce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Irene\\miniconda3\\envs\\tlef-ai-312\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Irene\\.cache\\huggingface\\hub\\models--nanonets--Nanonets-OCR-s. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d388bbf6d3415ba9ebb93960506ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee3471e3c354c5fa1068106cef36493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706232a6dee74477a527d33fc63559fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Irene\\AppData\\Local\\Temp\\ipykernel_4972\\1527349647.py:2: SyntaxWarning: invalid escape sequence '\\h'\n",
      "  \"presentations\\hist-presentation\\e5yrggr.pdf\"\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m pdf_list = [\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpresentations\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mhist-presentation\u001b[39m\u001b[33m\\\u001b[39m\u001b[33me5yrggr.pdf\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m ]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m df = \u001b[43mocr_pdfs_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnanonets/Nanonets-OCR-s\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m df.head()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mocr_pdfs_to_dataframe\u001b[39m\u001b[34m(pdf_paths, model_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mocr_pdfs_to_dataframe\u001b[39m(pdf_paths: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], model_path: \u001b[38;5;28mstr\u001b[39m) -> pd.DataFrame:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     model, tokenizer, processor = \u001b[43mload_nanonets_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     records = []\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m pdf_path \u001b[38;5;129;01min\u001b[39;00m pdf_paths:\n\u001b[32m      6\u001b[39m         \u001b[38;5;66;03m# convert each page to image\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mload_nanonets_model\u001b[39m\u001b[34m(model_path)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_nanonets_model\u001b[39m(model_path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     model = \u001b[43mAutoModelForImageTextToText\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     model.eval()\n\u001b[32m      9\u001b[39m     tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Irene\\miniconda3\\envs\\tlef-ai-312\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Irene\\miniconda3\\envs\\tlef-ai-312\\Lib\\site-packages\\transformers\\modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Irene\\miniconda3\\envs\\tlef-ai-312\\Lib\\site-packages\\transformers\\modeling_utils.py:4499\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4497\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m4499\u001b[39m     config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4501\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4506\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4507\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m   4508\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(config, *model_args, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Irene\\miniconda3\\envs\\tlef-ai-312\\Lib\\site-packages\\transformers\\modeling_utils.py:2183\u001b[39m, in \u001b[36mPreTrainedModel._autoset_attn_implementation\u001b[39m\u001b[34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[39m\n\u001b[32m   2180\u001b[39m     config._attn_implementation = \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2183\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2189\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflex_attention\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2191\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._check_and_enable_flex_attn(config, hard_check_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Irene\\miniconda3\\envs\\tlef-ai-312\\Lib\\site-packages\\transformers\\modeling_utils.py:2325\u001b[39m, in \u001b[36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[39m\u001b[34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[39m\n\u001b[32m   2323\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[32m   2324\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2327\u001b[39m flash_attention_version = version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mflash_attn\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   2328\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.version.cuda:\n",
      "\u001b[31mImportError\u001b[39m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "pdf_list = [\n",
    "    \"presentations\\hist-presentation\\e5yrggr.pdf\"\n",
    "]\n",
    "df = ocr_pdfs_to_dataframe(pdf_list, \"nanonets/Nanonets-OCR-s\")\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "praxis_ai_gpu",
   "language": "python",
   "name": "praxis_ai_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
