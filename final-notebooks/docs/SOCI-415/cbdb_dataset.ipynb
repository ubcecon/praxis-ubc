{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: SOCI 415 Network Analysis - CBDB Dataset\n",
        "author: prAxIs UBC Team <br> _Alex Ronczewski_\n",
        "date: '2025-08-24'\n",
        "description: Using Network Analysis to Analyze The China Biographical Dataset\n",
        "categories:\n",
        "  - Python\n",
        "  - network analysis\n",
        "format:\n",
        "  html: default\n",
        "  ipynb:\n",
        "    jupyter:\n",
        "      kernelspec:\n",
        "        display_name: Python\n",
        "        language: python3\n",
        "        name: python3\n",
        "---\n",
        "\n",
        "# 0.0 Prerequisites \n",
        "\n",
        "* SOCI 415 Network Analysis Intro Notebook\n",
        "* Kinmatrix Dataset Notebook\n",
        "\n",
        "The China Biographical Database is a freely accessible relational database with biographical information about approximately 641,568 individuals as of August 2024, currently mainly from the 7th through 19th centuries. It was developed as a collaborative project between scholars at Harvard University, Academia Sinica, and Peking University. The data is useful for statistical, social network, and spatial analysis as well as serving as a biographical reference for Chinese History. \n",
        "\n",
        "This dataset has many variables and is far more complex than the KINMATRIX Dataset we have used before, the dataset is far larger and contains more traditional networks as opposed to KINMATRIX's ego networks. If you wish to read more about the structure of the dataset you can follow the link [Structure of CBDB](https://projects.iq.harvard.edu/cbdb/structure-cbdb). \n",
        "\n",
        "# 1.0 Data Loading and Intro\n",
        "\n",
        "As with all of our notebooks so far we will begin with loading all of our libraries. This library list is similar to the one used in the KINMATRIX data analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Libraries listed below\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import networkx as nx #Our Python network analysis library\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.cm as cm\n",
        "import random\n",
        "from community import community_louvain\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from pyvis.network import Network\n",
        "from scipy import stats\n",
        "import seaborn as sns\n",
        "from collections import OrderedDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we have all of the libraries loaded in for our analysis we can load our data. The dataset is a .db file meaning it is a database. The CBDB data is extensive and has lots of variables within it, in order to access them we have to choose a table from our .db file. A table is a structured collection of data organized in rows and columns, similar to a spreadsheet. Each table contains records (rows), and every record has fields (columns). \n",
        "\n",
        "Display all of the tables in the dataset .db file. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "db_path = r'datasets\\latest.db' #path\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# List all tables\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "tables = cursor.fetchall()\n",
        "\n",
        "print(\"Tables in database:\", tables)\n",
        "\n",
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output of the cell above is a list of all of the tables in the dataset and we can see that the list is extensive. The one we are most intested in is called KIN_DATA. It contains the necessary information to construct our network. We will now load the table. The output of this cell will be all of the variables within the table 'KIN_DATA.'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Connect to the database\n",
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "df = pd.read_sql_query(\"SELECT * FROM KIN_DATA\", conn)\n",
        "\n",
        "# Show the first few rows\n",
        "print(df.head())\n",
        "\n",
        "conn.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from the output that the dataset contains both English and Chinese data so we will have to be careful to select the English language variables. We have now loaded the correct data and just like with the KINMATRIX Dataset we will build a NetworkX Graph and print the number of nodes and edges. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create an empty graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges with kinship type as edge attribute\n",
        "for _, row in df.iterrows():\n",
        "    person = row['c_personid']\n",
        "    kin = row['c_kin_id']\n",
        "    kin_type = row['c_kin_code']\n",
        "    G.add_edge(person, kin, kinship=kin_type)\n",
        "\n",
        "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
        "print(f\"Number of edges: {G.number_of_edges()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have over 278,000 nodes and a similar amount of edges. This is a far bigger dataset than we have previously used. The is a very extensive dataset where one of these networks is larger than all of the networks for the KINMATRIX Dataset. \n",
        "\n",
        "We already ran into computational problem when analyzing the KINMATRIX Dataset so we will have to be careful. We can not visualize the full network and all complex calculations like centrality will take a long time. \n",
        "\n",
        "# 2.0 Louvain and Community Clusters\n",
        "\n",
        "The first thing we will do for our analysis is look for are smaller sub-groups. This is analysis we could not do for the KINMATRIX Dataset as it was an ego-centric network. This is very popular technique in network analysis as it helps uncover hidden group structure in very expansive and complex networks. \n",
        "\n",
        "In our case large networks (like the CBDB) can be overwhelming and computationally intensive to study. Unlike the KINMATRIX dataset where the data was naturally divided into countries and provinces there are far less clear subgroups in this dataset. By identifying clusters, sociologists can summarize, visualize, and understand the major subgroups and their relationships, making the network more interpretable. To preform this community analysis we will need to introduce some more terminology.    \n",
        "\n",
        "**Cohesive subgroups** in network analysis refer to clusters of nodes within a network that are more *densely connected to each other than to the rest of the network*. These subgroups indicate areas of high interaction or strong relationships within the larger network. Identifying cohesive subgroups helps in understanding the structure and dynamics of the network, such as how information or influence flows within and between these groups. The process of finding cohesive subgroups within networks is called **cohesive group analysis**.\n",
        "\n",
        "A **clique** is a subset of nodes within a graph where every node is directly connected to every other node in the subset. This means that in a clique, all possible edges between the nodes are present, making it a maximally connected subgraph. All nodes are that are by themselves are inherently a clique (a 1-clique)\n",
        "\n",
        "Let's see what this look like on an example network before moving towards our real data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Create our network\n",
        "G_cliques_example = nx.Graph()\n",
        "edges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\n",
        "G_cliques_example.add_edges_from(edges_list)\n",
        "pos = nx.spring_layout(G_cliques_example, seed=1000)\n",
        "\n",
        "#Find our cliques and print where they are\n",
        "cliques = [x for x in nx.find_cliques(G_cliques_example)]\n",
        "print(cliques)\n",
        "\n",
        "#Draw our graph      \n",
        "nx.draw(G_cliques_example,pos=pos, with_labels=True, edgecolors=\"black\", node_color = \"bisque\", node_size=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are three cliques in this network: $(4, 0, 1, 2, 3)$, $(4, 5)$, and $(6, 5)$:\n",
        "\n",
        "We can colour them green to be more easily identifiable. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Set the figure size (24,6) not (8,6) because we have 3 graphs to show\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 6))\n",
        "fig.set_facecolor('lightblue')\n",
        "\n",
        "#Create our network\n",
        "G_cliques_example = nx.Graph()\n",
        "edges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\n",
        "G_cliques_example.add_edges_from(edges_list)\n",
        "pos = nx.spring_layout(G_cliques_example, seed=1000)\n",
        "\n",
        "#Find our cliques and print them\n",
        "cliques = [x for x in nx.find_cliques(G_cliques_example)]\n",
        "print(cliques)\n",
        "\n",
        "#Colour our cliques\n",
        "clique_1 = [\"mediumseagreen\", \"mediumseagreen\", \"mediumseagreen\", \"mediumseagreen\", \"mediumseagreen\", \"bisque\", \"bisque\"]\n",
        "clique_2 = [\"bisque\", \"bisque\", \"bisque\",  \"bisque\", \"mediumseagreen\", \"mediumseagreen\", \"bisque\"]\n",
        "clique_3 = [\"bisque\", \"bisque\", \"bisque\", \"bisque\",  \"bisque\", \"mediumseagreen\", \"mediumseagreen\"]\n",
        "\n",
        "#Draw our cliques       \n",
        "nx.draw(G_cliques_example, ax=axes[0], pos=pos, with_labels=True, edgecolors=\"black\", node_color = clique_1, node_size=800)\n",
        "nx.draw(G_cliques_example, ax=axes[1], pos=pos, with_labels=True, edgecolors=\"black\", node_color = clique_2, node_size=800)\n",
        "nx.draw(G_cliques_example, ax=axes[2], pos=pos, with_labels=True, edgecolors=\"black\", node_color = clique_3, node_size=800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nodes can also be in multiple cliques. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Create our example graph\n",
        "G_cliques_example = nx.Graph()\n",
        "edges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\n",
        "G_cliques_example.add_edges_from(edges_list)\n",
        "pos = nx.spring_layout(G_cliques_example, seed=1000)\n",
        "\n",
        "#Find the cliques\n",
        "cliques = [x for x in nx.find_cliques(G_cliques_example)]\n",
        "print(cliques)\n",
        "\n",
        "#For loop\n",
        "node_counts = {}\n",
        "for clique in cliques: #for each clique in the list of cliques...\n",
        "    for node in clique: # for each node in each clique...\n",
        "        if node in node_counts: #checks whether the current node already exists as a key in the node_counts dictionary\n",
        "            node_counts[node] += 1 #if it is in the dictionary, increase it's value by 1\n",
        "        else:\n",
        "            node_counts[node] = 1 #if it isn't, dont change\n",
        "\n",
        "#Colour our nodes\n",
        "colors = []\n",
        "for node in G_cliques_example.nodes():\n",
        "    if node_counts[node] == 1:\n",
        "        colors.append(\"lightgreen\")\n",
        "    elif node_counts[node] == 2:\n",
        "        colors.append(\"forestgreen\")\n",
        "    elif node_counts[node] == 3:\n",
        "        colors.append(\"orange\")\n",
        "    else:\n",
        "        colors.append(\"red\")\n",
        "\n",
        "#Draw our network           \n",
        "nx.draw(G_cliques_example,pos=pos, with_labels=True, edgecolors=\"black\", node_color = colors, node_size=800)\n",
        "patch_green = mpatches.Patch(color='lightgreen', label='node in one clique') \n",
        "patch_forest = mpatches.Patch(color='forestgreen', label='node in two cliques') \n",
        "plt.legend(handles=[patch_green, patch_forest])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Network-level analysis: Clusters and clustering coefficients\n",
        "\n",
        "A **cluster** (also known as a community) is a set of nodes in a graph that are densely connected to each other but sparsely connected to nodes in other clusters. For example, in a social network, a cluster might represent a group of people who frequently interact with each other but have fewer interactions with people outside the group. **Community detection** is the process of finding such communities within nodes. \n",
        "\n",
        "Before diving into community detection, we first need to understand modularity. **Modulaity** is a numerical measure for the community structure of a graph: it compares the density of edges within the communities of a network to the density of edges between communities. A positive modularity value suggests a strong community structure, while values closer to zero or negative indicate that the divisions are no better than random.\n",
        "\n",
        "The Louvain algorithm is a community detection method in networks that aims to optimize modularity. By optimizing modularity, the Louvain algorithm effectively uncovers natural divisions in the network where connections are dense within clusters and sparse between them, thus identifying meaningful community structures.\n",
        "\n",
        "First, each node is assigned to its own community, and nodes are then iteratively moved to neighboring communities if it increases the modularity. In the second phase, the algorithm creates a new network where each community from the first phase is treated as a single node, and the process is repeated. This hierarchical approach continues until no further modularity improvements can be made, resulting in a final set of communities that maximize modularity. \n",
        "\n",
        "Let's first try running the Louvain Algorithm on a random graph to demonstrate how it works before running it on our real data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Set the seed so it is reproducable\n",
        "random.seed(1)\n",
        "\n",
        "n = 20  # number of nodes\n",
        "d = 3   # degree of each node\n",
        "\n",
        "# Generate the random regular graph\n",
        "rr_graph = nx.random_regular_graph(d, n)\n",
        "\n",
        "partition = community_louvain.best_partition(rr_graph)\n",
        "pos = nx.spring_layout(rr_graph, seed=42)\n",
        "num_communities = max(partition.values()) + 1\n",
        "cmap = cm.get_cmap('viridis', num_communities)\n",
        "nx.draw_networkx_nodes(\n",
        "    rr_graph, pos, node_size=40, cmap=cmap, node_color=list(partition.values())\n",
        ")\n",
        "nx.draw_networkx_edges(rr_graph, pos, alpha=0.5)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see by the node coloring that by optimizing modularity the Louvain Algorithm has found smaller subgroups within our random network. Now we can try it on our real data. \n",
        "\n",
        "##  2.2 Louvain Run on Real Data\n",
        "\n",
        "As with before we will set a random seed so that the analysis is reporducable. We will also print out average community size along with the size of the largest and smallest communities. We will also print the top 10 largest communities and their sizes. \n",
        "\n",
        "We will also be working with the largest connected component for this analysis. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Set a seed for reproducability of our results\n",
        "np.random.seed(1)\n",
        "\n",
        "# Load kinship data into DataFrame\n",
        "conn = sqlite3.connect(db_path)\n",
        "df = pd.read_sql_query(\"SELECT c_personid, c_kin_id, c_kin_code FROM KIN_DATA\", conn)\n",
        "conn.close()\n",
        "\n",
        "# Build the Graph\n",
        "G = nx.Graph()\n",
        "for _, row in df.iterrows():\n",
        "    person = row['c_personid']\n",
        "    kin = row['c_kin_id']\n",
        "    kin_type = row['c_kin_code']\n",
        "    G.add_edge(person, kin, kinship=kin_type)\n",
        "\n",
        "# Work with the largest connected component\n",
        "largest_cc = max(nx.connected_components(G), key=len)\n",
        "G_sub = G.subgraph(largest_cc).copy()\n",
        "print(f\"Largest connected component nodes: {G_sub.number_of_nodes()}\")\n",
        "\n",
        "# Run Louvain Community Detection\n",
        "print(\"Running Louvain algorithm...\")\n",
        "partition = community_louvain.best_partition(G_sub)\n",
        "\n",
        "# Community Analysis Output\n",
        "num_communities = len(set(partition.values()))\n",
        "print(f\"In our data Louvain has detected {num_communities} communities.\")\n",
        "\n",
        "# Count community sizes\n",
        "community_sizes = Counter(partition.values())\n",
        "print(f\"\\nMetrics about Community size:\")\n",
        "print(f\"Average community size: {np.mean(list(community_sizes.values())):.1f}\")\n",
        "print(f\"Largest community: {max(community_sizes.values())} people\")\n",
        "print(f\"Smallest community: {min(community_sizes.values())} people\")\n",
        "\n",
        "#Show top 10 largest communities\n",
        "print(f\"\\nLargest Communities:\")\n",
        "for i, (comm_id, size) in enumerate(community_sizes.most_common(10)):\n",
        "    print(f\"Community {comm_id}: {size:,} people\")\n",
        "\n",
        "# Calculate modularity\n",
        "modularity = community_louvain.modularity(partition, G_sub)\n",
        "print(f\"\\nModularity Score: {modularity:.4f}\")\n",
        "print(\"(Higher modularity indicates stronger community structure)\")\n",
        "\n",
        "#Distributon Histogram of Community Size\n",
        "fig, ax = plt.subplots(figsize=(15, 12))\n",
        "fig.suptitle('CBDB Kinship Network Community Analysis', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Community size histogram\n",
        "ax.hist(list(community_sizes.values()), bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "ax.set_xlabel('Community Size (number of people)')\n",
        "ax.set_ylabel('Number of Communities')\n",
        "ax.set_title('Distribution of Community Sizes')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the visualization we can see most communities are around 200 - 400 nodes in size. \n",
        "\n",
        "Lets look at two of these communities in more detail, we will look at one average sized one and one large one. \n",
        "\n",
        "The two we will use are: \n",
        "\n",
        "* Community 20: 703 nodes with 940 edges\n",
        "* Community 125: 308 nodes with 392 edges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "def visualize_cbdb_community(G_sub, partition, community_id, max_nodes=1000):\n",
        "    community_nodes = [str(node) for node, comm_id in partition.items() if comm_id == community_id]\n",
        "    \n",
        "    print(f\"Community {community_id}: {len(community_nodes)} people\")\n",
        "    \n",
        "    # Create subgraph with string node IDs\n",
        "    subgraph = G_sub.subgraph([int(node) for node in community_nodes])\n",
        "    print(f\"Showing {len(community_nodes)} people, {subgraph.number_of_edges()} relationships\")\n",
        "    \n",
        "    net = Network(height=\"700px\", width=\"100%\", bgcolor=\"#ffffff\", notebook=True)\n",
        "    \n",
        "    for node in community_nodes:\n",
        "        degree = subgraph.degree[int(node)] \n",
        "        size = max(15, min(35, 15 + degree))\n",
        "        net.add_node(str(node), \n",
        "                     label=str(node), \n",
        "                     size=size, \n",
        "                     color=\"#3498db\",\n",
        "                     title=f\"Person {node}\\nConnections: {degree}\")\n",
        "    \n",
        "    for u, v, data in subgraph.edges(data=True):\n",
        "        net.add_edge(str(u), str(v), color=\"#cccccc\", title=f\"Kinship: {data.get('kinship','family')}\")\n",
        "    \n",
        "    filename = f\"community_{community_id}.html\"\n",
        "    net.show(filename)\n",
        "    print(f\"Interactive network: {filename}\")\n",
        "\n",
        "#Run the function\n",
        "visualize_cbdb_community(G_sub, partition, 20)\n",
        "visualize_cbdb_community(G_sub, partition, 125)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will again use a PyVis visualization, just like with the KINMATRIX Visualizations we can zoom and pan around and hover on the nodes. This time you can also drag the nodes and the surrounding nodes will move like bacteria under a microscope.  \n",
        "\n",
        "## 2.3 Macro Stats and Dynasties\n",
        "\n",
        "To get a better understanding of our communities we will look at some summary statistics. Let's briefly define what these macro statistics will be: \n",
        "\n",
        "* Diameter: The longest shortest path between any two nodes in the network or component. This is a way to measure how \"wide\" our network is. \n",
        "* Average Path Lenght: The average number of steps along the shortest paths for all possible pairs of nodes in the graph\n",
        "* Average Clustering: The likelihood that any two neighbors of a node are also connected to each other. Higher values mean people in the community tend to form \"groups\".\n",
        "\n",
        "We will also look at degree centrality, density and number of nodes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "def macro_stats(G_sub, partition, community_id):\n",
        "    nodes = [n for n, c in partition.items() if c == community_id]\n",
        "    subg = G_sub.subgraph(nodes)\n",
        "    stats = {}\n",
        "\n",
        "    if nx.is_connected(subg):\n",
        "        stats['diameter'] = nx.diameter(subg)\n",
        "        stats['avg_path_length'] = nx.average_shortest_path_length(subg)\n",
        "    else:\n",
        "        # statistics of largest connected component only\n",
        "        lcc = subg.subgraph(max(nx.connected_components(subg), key=len))\n",
        "        stats['diameter'] = nx.diameter(lcc)\n",
        "        stats['avg_path_length'] = nx.average_shortest_path_length(lcc)\n",
        "\n",
        "    degrees = dict(subg.degree())\n",
        "    max_degree = max(degrees.values())\n",
        "    n = len(degrees)\n",
        "    if n > 1:\n",
        "        degree_centralization = sum(max_degree - d for d in degrees.values()) / ((n-1)*(n-2))\n",
        "    else:\n",
        "        degree_centralization = 0\n",
        "    stats['degree_centralization'] = degree_centralization\n",
        "    stats['avg_clustering'] = nx.average_clustering(subg)\n",
        "    stats['density'] = nx.density(subg)\n",
        "    stats['n_nodes'] = n\n",
        "    return stats\n",
        "\n",
        "#Print:\n",
        "stats_20 = macro_stats(G_sub, partition, 20)\n",
        "stats_125 = macro_stats(G_sub, partition, 125)\n",
        "print(\"Community 50 macro stats:\", stats_20)\n",
        "print(\"Community 125 macro stats:\", stats_125)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now have more quantifiable metrics for our visualizations. Let's contunue our analysis with finding which dynasties these communities are primarily from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "# Community 125\n",
        "personids_125 = [n for n, c in partition.items() if c == 125]\n",
        "conn = sqlite3.connect(db_path)\n",
        "ids_tuple_125 = tuple(personids_125)\n",
        "query_125 = f'''\n",
        "    SELECT c_personid, c_dy FROM BIOG_MAIN\n",
        "    WHERE c_personid IN {ids_tuple_125}\n",
        "'''\n",
        "df_dyn_125 = pd.read_sql_query(query_125, conn)\n",
        "\n",
        "# Community 20\n",
        "personids_20 = [n for n, c in partition.items() if c == 20]\n",
        "ids_tuple_20 = tuple(personids_20)\n",
        "query_20 = f'''\n",
        "    SELECT c_personid, c_dy FROM BIOG_MAIN\n",
        "    WHERE c_personid IN {ids_tuple_20}\n",
        "'''\n",
        "df_dyn_20 = pd.read_sql_query(query_20, conn)\n",
        "conn.close()\n",
        "\n",
        "# Show the most common dynasty codes\n",
        "print(\"Community 125 dominant dynasties:\")\n",
        "print(df_dyn_125['c_dy'].value_counts().head(3))\n",
        "\n",
        "print(\"Community 20 dominant dynasties:\")\n",
        "print(df_dyn_20['c_dy'].value_counts().head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that Community 125 is primarily from Dynasty 6, and Community 50 is primarily from Dynasty 15, but this does not tell us much we need to translate 6 and 15 into real dynasty names. These values represent names of real dynasties, but without the key this output does not mean anything.  \n",
        "\n",
        "Lets now map these values to real dynasty names. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "conn = sqlite3.connect(db_path)\n",
        "dynasty_mapping = pd.read_sql_query('SELECT c_dy, c_dynasty FROM DYNASTIES', conn)\n",
        "conn.close()\n",
        "\n",
        "def get_dynasty_breakdown(df_dyn, dynasty_mapping):\n",
        "    df_merged = df_dyn.merge(dynasty_mapping, on='c_dy', how='left')\n",
        "    return df_merged['c_dynasty'].value_counts()  \n",
        "\n",
        "print(\"Community 125 top 3 dynasties:\")\n",
        "print(get_dynasty_breakdown(df_dyn_125, dynasty_mapping).head(3))\n",
        "\n",
        "print(\"Community 20 top 3 dynasties:\")\n",
        "print(get_dynasty_breakdown(df_dyn_20, dynasty_mapping).head(3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we know that Community 125 is primarily made up of individuals from the Tang Dynasty and Community 50 is primarily made up of individuals from the Song Dynasty. \n",
        "\n",
        "## 2.4 Coloring Nodes\n",
        "\n",
        "Like with the KINMATRIX Dataset we will color our Pyvis visualizations by a variable of interest. In this case we will color Community 125 by gender where male is purple and women are yellow. There are no unknown gender variables in either of these communities so we can just have two colors in our color map.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "# Get all node and person id's\n",
        "all_personids = list(set([n for n in partition.keys()]))\n",
        "\n",
        "conn = sqlite3.connect(db_path)\n",
        "query = f'''\n",
        "    SELECT c_personid, c_female FROM BIOG_MAIN\n",
        "    WHERE c_personid IN ({','.join(str(pid) for pid in all_personids)})\n",
        "'''\n",
        "df_gender = pd.read_sql_query(query, conn)\n",
        "conn.close()\n",
        "\n",
        "# Function to map female/male from number 1/0\n",
        "def sex_label(val):\n",
        "    return 'female' if val == 1 else ('male' if val == 0 else 'unknown')\n",
        "\n",
        "gender_dict = {row['c_personid']: sex_label(row['c_female']) for _, row in df_gender.iterrows()}\n",
        "\n",
        "def visualize_cbdb_community_gender(G_sub, partition, community_id, gender_dict, max_nodes=1000):\n",
        "    community_nodes = [str(node) for node, comm_id in partition.items() if comm_id == community_id]\n",
        "    print(f\"Community {community_id}: {len(community_nodes)} people\")\n",
        "    if len(community_nodes) > max_nodes:\n",
        "        print(f\"Sampling {max_nodes} nodes for performance...\")\n",
        "        subgraph_full = G_sub.subgraph([int(node) for node in community_nodes])\n",
        "        degrees = dict(subgraph_full.degree())\n",
        "        sorted_nodes = sorted(community_nodes, key=lambda x: degrees.get(int(x), 0), reverse=True)\n",
        "        community_nodes = sorted_nodes[:max_nodes//2] + random.sample(sorted_nodes[max_nodes//2:], max_nodes//2)\n",
        "\n",
        "    subgraph = G_sub.subgraph([int(node) for node in community_nodes])\n",
        "    print(f\"Showing {len(community_nodes)} people, {subgraph.number_of_edges()} relationships\")\n",
        "    net = Network(height=\"700px\", width=\"100%\", bgcolor=\"#ffffff\", notebook=True)\n",
        "\n",
        "    color_map = {'male': '#800080',   # purple\n",
        "             'female': '#FFFF00'}  # yellow \n",
        "    for node in community_nodes:\n",
        "        gender = gender_dict.get(int(node), 'unknown')\n",
        "        degree = subgraph.degree[int(node)]\n",
        "        size = max(15, min(35, 15 + degree))\n",
        "        net.add_node(str(node), \n",
        "                     label=str(node),\n",
        "                     size=size,\n",
        "                     color=color_map.get(gender, '#bdbdbd'),\n",
        "                     title=f\"Person {node}\\nConnections: {degree}\\nGender: {gender}\")\n",
        "\n",
        "    for u, v, data in subgraph.edges(data=True):\n",
        "        net.add_edge(str(u), str(v), color=\"#cccccc\", title=f\"Kinship: {data.get('kinship','family')}\")\n",
        "    filename = f\"community_{community_id}_gender.html\"\n",
        "    net.show(filename)\n",
        "    print(f\"Interactive network: {filename}\")\n",
        "\n",
        "visualize_cbdb_community_gender(G_sub, partition, 125, gender_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that most of the nodes are male especially the most central nodes, we will come back to this later, but think about why that is? Continuing on with the notion of centrality lets color the nodes in Community 20 with a color gradient where the nodes with the lowest degree centrality will be blue and the node with the highest will be red. This will serve as an intuitive method to visualize degree centrality.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(1)\n",
        "\n",
        "def visualize_cbdb_community_degree(G_sub, partition, community_id, max_nodes=1000):\n",
        "    community_nodes = [str(node) for node, comm_id in partition.items() if comm_id == community_id]\n",
        "    print(f\"Community {community_id}: {len(community_nodes)} people\")\n",
        "\n",
        "    subgraph = G_sub.subgraph([int(node) for node in community_nodes])\n",
        "    print(f\"Showing {len(community_nodes)} people, {subgraph.number_of_edges()} relationships\")\n",
        "    net = Network(height=\"700px\", width=\"100%\", bgcolor=\"#ffffff\", notebook=True)\n",
        "\n",
        "    degrees = dict(subgraph.degree())\n",
        "    deg_values = [degrees[int(node)] for node in community_nodes]\n",
        "    min_deg, max_deg = min(deg_values), max(deg_values)\n",
        "    norm = plt.Normalize(min_deg, max_deg)\n",
        "    cmap = plt.get_cmap('coolwarm') \n",
        "\n",
        "    for node in community_nodes:\n",
        "        degree = degrees[int(node)]\n",
        "        size = max(15, min(35, 15 + degree))\n",
        "        rgb_vals = cmap(norm(degree))[:3]\n",
        "        hex_color = '#%02x%02x%02x' % tuple(int(x*255) for x in rgb_vals)\n",
        "        net.add_node(str(node),\n",
        "                     label=str(node),\n",
        "                     size=size,\n",
        "                     color=hex_color,\n",
        "                     title=f\"Person {node}\\nConnections: {degree}\")\n",
        "\n",
        "    for u, v, data in subgraph.edges(data=True):\n",
        "        net.add_edge(str(u), str(v), color=\"#cccccc\", title=f\"Kinship: {data.get('kinship','family')}\")\n",
        "    filename = f\"community_{community_id}_degree.html\"\n",
        "    net.show(filename)\n",
        "    print(f\"Interactive network: {filename}\")\n",
        "    print(f\"Color gradient: low degree (blue) to high degree (red).\")\n",
        "\n",
        "visualize_cbdb_community_degree(G_sub, partition, 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5 Visualization Discussion\n",
        "\n",
        "In small groups of 3-4 look at the visualizations from this dataset and compare them to our pyVis visualizations from the KINMATRIX Dataset. Try to be specific and use the terminology introduced in the introductory notebook. How are they similar and how are they different? \n",
        "\n",
        "# 3.0 Degree Centrality for Important Family Members\n",
        "\n",
        "We can see from our community visualization that some members are more central and connect different parts of the family network. In family analysis these would be considered important connectors. In order to examine them further we will use the measures of centrality we introduced in the first notebook. Centrality measures are quantitative metrics that identify the most important or influential nodes within a network, we will do this to try and identify key historical figures and try to reveal hidden connections.  \n",
        "\n",
        "**Note:**\n",
        "The cells below take a very long time to run, so I will leave it all commented and just have the output pasted below.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#print(f\"Analyzing network with {G_sub.number_of_nodes():,} nodes and {G_sub.number_of_edges():,} edges\")\n",
        "#degree_centrality = nx.degree_centrality(G_sub)\n",
        "#betweenness_centrality = nx.betweenness_centrality(G_sub, k=1000)  \n",
        "#closeness_centrality = nx.closeness_centrality(G_sub)\n",
        "#eigenvector_centrality = nx.eigenvector_centrality(G_sub, max_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create Centrality DataFrame and Display Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#Create a comprehensive centrality dataframe\n",
        "#centrality_df = pd.DataFrame({\n",
        "#    'person_id': list(G_sub.nodes()),\n",
        "#    'degree': [G_sub.degree(node) for node in G_sub.nodes()],\n",
        "#    'degree_centrality': [degree_centrality[node] for node in G_sub.nodes()],\n",
        "#    'betweenness_centrality': [betweenness_centrality[node] for node in G_sub.nodes()],\n",
        "#    'closeness_centrality': [closeness_centrality[node] for node in G_sub.nodes#    'eigenvector_centrality': [eigenvector_centrality[node] for node in G_sub.nodes()]\n",
        "#})\n",
        "#print(\"Centrality Statistics:\")\n",
        "#print(centrality_df.describe())\n",
        "#print(f\"\\nDataFrame shape: {centrality_df.shape}\")\n",
        "#print(f\"Columns: {list(centrality_df.columns)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output from above cell:**\n",
        "\n",
        "Centrality Statistics:\n",
        "           person_id        degree  degree_centrality  betweenness_centrality  \n",
        "count   52992.000000  52992.000000       52992.000000            52992.000000   \n",
        "mean   115894.558952      2.463353           0.000046                0.000341   \n",
        "std    117167.480510      2.806804           0.000053                0.002023   \n",
        "min         4.000000      1.000000           0.000019                0.000000   \n",
        "25%     21419.750000      1.000000           0.000019                0.000000   \n",
        "50%    119799.500000      2.000000           0.000038                0.000000   \n",
        "75%    178417.250000      3.000000           0.000057                0.000075   \n",
        "max    691363.000000    147.000000           0.002774                0.161482   \n",
        "\n",
        "       closeness_centrality  eigenvector_centrality  \n",
        "count          52992.000000            5.299200e+04  \n",
        "mean               0.054518            2.787520e-04  \n",
        "std                0.010117            4.335138e-03  \n",
        "min                0.024212            1.972009e-17  \n",
        "25%                0.047437            3.783786e-12  \n",
        "50%                0.054617            3.964649e-10  \n",
        "75%                0.061708            4.877353e-08  \n",
        "max                0.085383            5.227996e-01  \n",
        "\n",
        "\n",
        "Now we can look at the most central nodes in the dataset to try and find important individuals in the data. \n",
        "\n",
        "In order to find these key individuals we will use our three centrality measures. \n",
        "\n",
        "* Degree Centrality: Measures how many direct connections a node (person) has. It's the family member with the most immediate kinship ties. \n",
        "* Betweeness Centrality: Captures how often a node lies on the shortest path between other nodes. It identifies family members who act as bridges, connecting separate branches or generations.\n",
        "* Eigenvector Centality:  Reflects not just the number of connections, but also the quality-being connected to other well-connected family members. High eigenvector centrality means the person is part of the core, influential family group for instance an emperor or very high ranking official. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#print(\"Degree Centrality (Most Connected Family Members):\")\n",
        "#top_degree = centrality_df.nlargest(10, 'degree')\n",
        "#for idx, row in top_degree.iterrows():\n",
        "#    print(f\"Person {row['person_id']:>8}: {row['degree']:>3} connections (centrality: {row['degree_centrality']:.4f})\")\n",
        "\n",
        "#print(\"Betweeness Centrality (Best Family Bridges):\")\n",
        "#top_betweenness = centrality_df.nlargest(10, 'betweenness_centrality')\n",
        "#for idx, row in top_betweenness.iterrows():\n",
        "#    print(f\"Person {row['person_id']:>8}: {row['betweenness_centrality']:.4f} (degree: {row['degree']:>3})\")\n",
        "\n",
        "#print(\"Eigenvector Centality (Most Influential Family Connections):\")\n",
        "#top_eigenvector = centrality_df.nlargest(10, 'eigenvector_centrality')\n",
        "#for idx, row in top_eigenvector.iterrows():\n",
        "#    print(f\"Person {row['person_id']:>8}: {row['eigenvector_centrality']:.4f} (degree: {row['degree']:>3})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The cells above has a long run time (30mins so)\n",
        "\n",
        "Output:\n",
        "**Degree Centrality (Most Connected Family Members):**\n",
        "Person   3211.0: 147.0 connections (centrality: 0.0028)\n",
        "Person  19244.0: 95.0 connections (centrality: 0.0018)\n",
        "Person   9008.0: 80.0 connections (centrality: 0.0015)\n",
        "Person   9002.0: 69.0 connections (centrality: 0.0013)\n",
        "Person  10502.0: 63.0 connections (centrality: 0.0012)\n",
        "Person  13060.0: 61.0 connections (centrality: 0.0012)\n",
        "Person   3214.0: 56.0 connections (centrality: 0.0011)\n",
        "Person  19246.0: 56.0 connections (centrality: 0.0011)\n",
        "Person   9001.0: 53.0 connections (centrality: 0.0010)\n",
        "Person    705.0: 52.0 connections (centrality: 0.0010)\n",
        "\n",
        "**Betweeness Centrality (Best Family Bridges):**\n",
        "Person  13059.0: 0.1588 (degree: 49.0)\n",
        "Person  13626.0: 0.1217 (degree: 14.0)\n",
        "Person  13060.0: 0.1147 (degree: 61.0)\n",
        "Person   8075.0: 0.1137 (degree: 29.0)\n",
        "Person  17267.0: 0.1092 (degree: 4.0)\n",
        "Person    705.0: 0.0717 (degree: 52.0)\n",
        "Person  95047.0: 0.0713 (degree: 10.0)\n",
        "Person    810.0: 0.0685 (degree: 7.0)\n",
        "Person  19244.0: 0.0609 (degree: 95.0)\n",
        "Person   3211.0: 0.0604 (degree: 147.0)\n",
        "\n",
        "**Eigenvector Centality (Most Influential Family Connections):**\n",
        "Person   3211.0: 0.5228 (degree: 147.0)\n",
        "Person   9002.0: 0.2929 (degree: 69.0)\n",
        "Person   9001.0: 0.2602 (degree: 53.0)\n",
        "Person   3198.0: 0.2168 (degree: 28.0)\n",
        "Person  10495.0: 0.1676 (degree: 23.0)\n",
        "Person   9003.0: 0.1582 (degree: 35.0)\n",
        "Person  10530.0: 0.1571 (degree: 15.0)\n",
        "Person  10509.0: 0.1546 (degree: 11.0)\n",
        "Person  15975.0: 0.1437 (degree: 11.0)\n",
        "Person  16805.0: 0.1421 (degree: 19.0)\n",
        "\n",
        "**Analysis**\n",
        "\n",
        "Lets look at our three types of centrality from before and use them to order our most important individuals. \n",
        "\n",
        "* Degree Centrality: These are the family members with the most direct kinship ties.\n",
        "* Betweeness Centrality: These family members connect different branches/generations\n",
        "* Eigenvector Centality: These are connected to other highly connected family members\n",
        "\n",
        "Below are three graphs plotting our output for most central individuals. \n",
        "\n",
        "![Degree Centrality](media/degree_centrality.png)\n",
        "\n",
        "![Betweenness Centrality](media/betweenness_centrality.png)\n",
        "\n",
        "![Eigenvector Centrality](media/eigenvector_centrality.png)\n",
        "\n",
        "Person 3211 emerges as the most connected individual with 147 kinship ties, nearly 50% more than the second-most connected person (Person 19244 with 95 connections). This level of connectivity suggests Person 3211 likely represents either a major family patriarch who lived an exceptionally long life, accumulated multiple marriages and offspring, or potentially a family line that was consolidated under a single record. Person 3211 dominates this measure with a score of 0.5228, more than double the second-highest individual (Person 9002 with 0.2929). This indicates that Person 3211 is not only highly connected but also connected to other highly connected families, representing the apex of elite Chinese society.\n",
        "\n",
        "Even more striking is Person 17267, who achieves a betweenness centrality of 0.1092 with only 4 direct connections. This individual represents what network analysts call a \"critical bridge\" - someone whose position in the network gives them disproportionate influence over information flow and family interactions. In historical Chinese context, such individuals likely played crucial roles connecting large families like an emperor or emperor's wife.\n",
        "\n",
        "# 3.1 Discussion on Centrality \n",
        "\n",
        "Using our definitions of the three centrality measures we are using:\n",
        "\n",
        "* Degree Centrality: Measures how many direct connections a node (person) has. It's the family member with the most immediate kinship ties. \n",
        "* Betweeness Centrality: Captures how often a node lies on the shortest path between other nodes. It identifies family members who act as bridges, connecting separate branches or generations.\n",
        "* Eigenvector Centality: Reflects not just the number of connections, but also the quality-being connected to other well-connected family members. High eigenvector centrality means the person is part of the core, influential family group for instance an emperor or very high ranking official. \n",
        "\n",
        "Discuss in small groups of 3-4 people how these centrality measures exist in your own family and friendship networks. Think about how all three of these are present in your daily lives. For example do you have a friend who is really popular (high degree centrality), do you have a family member who acts as a link between two large families (high betweeness centrality). Try to link these definitions to real people.  \n",
        "\n",
        "# 3.2 Looking at these key individuals\n",
        "\n",
        "Looking at our top ten list of key individuals we will examine further the most standout ones are: \n",
        "\n",
        "* Person 3211\n",
        "* Person 17267\n",
        "\n",
        "And our most prominent bridges are below, these nodes have high betweenness despite having a lower degree, meaning they’re crucial “gatekeepers” between groups.\n",
        "\n",
        "* Person 13059\n",
        "* Person 13626 \n",
        "* Person 8075\n",
        "\n",
        "Let's find who these people are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Key people from the previous network analysis\n",
        "key_people = [3211, 17267, 13059, 13626, 8075]\n",
        "\n",
        "# Combined query for biography info + native place\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    bm.c_personid,\n",
        "    bm.c_name_chn,\n",
        "    bm.c_surname_chn,\n",
        "    bm.c_name,\n",
        "    bm.c_surname,\n",
        "    bm.c_birthyear,\n",
        "    bm.c_deathyear,\n",
        "    bm.c_dy,         -- dynasty code\n",
        "    bm.c_female,\n",
        "    a.c_name AS NativePlace_CHN\n",
        "FROM BIOG_MAIN bm\n",
        "LEFT JOIN BIOG_ADDR_DATA bad\n",
        "    ON bm.c_personid = bad.c_personid\n",
        "    AND bad.c_addr_type = 1\n",
        "LEFT JOIN ADDRESSES a\n",
        "    ON bad.c_addr_id = a.c_addr_id\n",
        "WHERE bm.c_personid IN ({\",\".join(map(str, key_people))})\n",
        "\"\"\"\n",
        "\n",
        "df = pd.read_sql_query(query, conn)\n",
        "\n",
        "# Combine name fields into one df\n",
        "df[\"FullName_CHN\"] = df[\"c_surname_chn\"].fillna('') + df[\"c_name_chn\"].fillna('')\n",
        "df[\"FullName_ENG\"] = (df[\"c_surname\"].fillna('') + \" \" + df[\"c_name\"].fillna('')).str.strip()\n",
        "df[\"Gender\"] = df[\"c_female\"].map({0: \"Male\", 1: \"Female\"})\n",
        "\n",
        "# Keep relevant columns and remove duplicates\n",
        "final_df = df[[\n",
        "    \"c_personid\", \"FullName_CHN\", \"FullName_ENG\",\n",
        "    \"c_birthyear\", \"c_deathyear\", \"c_dy\", \"Gender\", \"NativePlace_CHN\"\n",
        "]].drop_duplicates()\n",
        "\n",
        "# Display nicely using the display command instead of print\n",
        "display(final_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have the information on our most key people. The people with the highest degree and eigenvector centrality were Zhao Tingmei and Li Zhao. \n",
        "\n",
        "[Zhao Tingmei](https://en.wikipedia.org/wiki/Zhao_Tingmei) formally known as Prince Fudao, was an imperial prince of the Song dynasty. On the Wikipedia page it states that he had 15 offsprings so it makes sense that he is so integrated into the network. \n",
        "\n",
        "The identity of Li Zhao is less clear, but he is most likely [King Li of Zhou](https://en.wikipedia.org/wiki/King_Li_of_Zhou).\n",
        "\n",
        "As for our most prominent bridges they are:\n",
        "\n",
        "* [Li Fang](https://en.wikipedia.org/wiki/Li_Fang_(Song_dynasty))\n",
        "* [Li Yuan(Emperor Gaozu of Tang)](https://en.wikipedia.org/wiki/Emperor_Gaozu_of_Tang)\n",
        "* Li Yuanyi - I can not find information on him \n",
        "\n",
        "This is extermenly interesting as we can link all of these nodes to real people from ancient Chinese history. If we are interested in where they are from we can also find that using the dataset. \n",
        "\n",
        "For instance Zhao Tingmei and Li Fang are from Kaifeng. Kaifeng is a city in central China’s Henan province, just south of the Yellow River. The city was the Northern Song Dynasty capital from the 10th to 12th centuries.\n",
        "\n",
        "Li Yuan (Emperor Gaozu of Tang) and Li Yuanyi are from Chang'an. Chang'an was a city in China, located near the modern city of Xi'an, which served as the capital of several Chinese dynasties from 202 BCE to 907 CE. \n",
        "\n",
        "Finally Li Zhao is from Raoyang, compared to the other two Raoyang is less historically significant region. Raoyang County is a county in the southeast of the Hebei province.\n",
        "\n",
        "# 4.0 Women more as Bridges\n",
        "\n",
        "Intuitively from the KINMATRIX dataset we would image that women could act as bridges in these large family networks. An example of this would be a strategic marriage, but is this the case? We will look at the data and try to understand if that is the case. \n",
        "\n",
        "This cell explores the BIOG_MAIN table structure to find the gender field and loads a sample to understand the data structure. In order to identify if women act as bridges we need the gender information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def explore_biog_main():\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # Explore table structure\n",
        "    cursor.execute(\"PRAGMA table_info(BIOG_MAIN)\")\n",
        "    columns = cursor.fetchall()\n",
        "    print(\"BIOG_MAIN table columns:\")\n",
        "    for col in columns:\n",
        "        print(f\"  {col[1]} ({col[1]})\")\n",
        "    \n",
        "    # Test for common gender field names\n",
        "    gender_fields = ['c_female', 'c_sex', 'c_gender', 'female', 'sex', 'gender']\n",
        "    gender_field = None\n",
        "    \n",
        "    for field in gender_fields:\n",
        "        try:\n",
        "            test_query = f\"SELECT {field} FROM BIOG_MAIN LIMIT 5\"\n",
        "            cursor.execute(test_query)\n",
        "            results = cursor.fetchall()\n",
        "            print(f\"\\nFound field '{field}' with sample values: {[r[0] for r in results]}\")\n",
        "            gender_field = field\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "        \n",
        "    conn.close()\n",
        "    return gender_field\n",
        "\n",
        "# Run the function\n",
        "gender_field = explore_biog_main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The variable we are going to use is **c_female,** it is an indicator variable which takes the values of 0 or 1; where 1 means the node is female. \n",
        "\n",
        "We will now load the data from before, but with the gender variable and rebuild the network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def load_network_data():\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    \n",
        "    # Load kinship relationships\n",
        "    kin_df = pd.read_sql_query(\"SELECT c_personid, c_kin_id, c_kin_code FROM KIN_DATA\", conn)\n",
        "    print(f\"Loaded {len(kin_df):,} kinship relationships\")\n",
        "    \n",
        "    # Build network graph\n",
        "    G = nx.Graph()\n",
        "    for _, row in kin_df.iterrows():\n",
        "        person = row['c_personid']\n",
        "        kin = row['c_kin_id']\n",
        "        kin_type = row['c_kin_code']\n",
        "        G.add_edge(person, kin, kinship=kin_type)\n",
        "    \n",
        "    print(f\"Full network: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
        "    \n",
        "    # Work with largest connected component\n",
        "    largest_cc = max(nx.connected_components(G), key=len)\n",
        "    G_sub = G.subgraph(largest_cc).copy()\n",
        "    print(f\"Largest component: {G_sub.number_of_nodes():,} nodes, {G_sub.number_of_edges():,} edges\")\n",
        "    \n",
        "    conn.close()\n",
        "    return G_sub, kin_df\n",
        "\n",
        "# Load the network\n",
        "G_sub, kin_df = load_network_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because finding centrality is too computationally intensive the cells will be commented out just like before with their output pasted below. Just like with the men we will find the Betweenness, Closeness and Eigenvector centrality. \n",
        "\n",
        "Instead of using the whole data we will also be using the largest component of the data which is made up of 52,992 nodes, and 65,269 edges (still very large). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#def create_centrality_dataframe(G_sub, degree_centrality, betweenness_centrality, \n",
        "#                               closeness_centrality, eigenvector_centrality):\n",
        "\n",
        "    # Create comprehensive centrality dataframe using your existing variables\n",
        "#    analysis_df = pd.DataFrame({\n",
        "#        'person_id': list(G_sub.nodes()),\n",
        "#        'degree': [G_sub.degree(node) for node in G_sub.nodes()],\n",
        "#        'degree_centrality': [degree_centrality[node] for node in G_sub.nodes()],\n",
        "#        'betweenness_centrality': [betweenness_centrality[node] for node in G_sub.nodes()],\n",
        "#        'closeness_centrality': [closeness_centrality[node] for node in G_sub.nodes()],\n",
        "#       'eigenvector_centrality': [eigenvector_centrality[node] for node in G_sub.nodes()]\n",
        "#    })\n",
        "    \n",
        "#    print(f\"Centrality analysis complete for {len(analysis_df):,} individuals\")\n",
        "#    print(f\"Average degree: {analysis_df['degree'].mean():.2f}\")\n",
        "#    print(f\"Average betweenness centrality: {analysis_df['betweenness_centrality'].mean():.6f}\")\n",
        "#    print(f\"Average closeness centrality: {analysis_df['closeness_centrality'].mean():.6f}\")\n",
        "#    print(f\"Average eigenvector centrality: {analysis_df['eigenvector_centrality'].mean():.6f}\")\n",
        "    \n",
        "#    return analysis_df\n",
        "\n",
        "# Use your existing centrality calculations (no recalculation needed!)\n",
        "#centrality_df = create_centrality_dataframe(G_sub, degree_centrality, betweenness_centrality, closeness_centrality, eigenvector_centrality)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Centrality analysis complete for 52,992 individuals:\n",
        "\n",
        "* Average degree: 2.46\n",
        "* Average betweenness centrality: 0.000342\n",
        "* Average closeness centrality: 0.054518\n",
        "* Average eigenvector centrality: 0.000279\n",
        "\n",
        "The next step is to load gender data from the database and add our gender variable:\n",
        "\n",
        "* Gender field = c_female\n",
        "* Birth year field = c_birthyear\n",
        "* Death year field = c_deathyear\n",
        "* Index year field = c_index_year\n",
        "\n",
        "We still need centrality so this cell will also be commented out: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#}#def load_and_merge_gender_data(centrality_df, gender_field='c_female'):\n",
        "#    \"\"\"Load gender & year data from BIOG_MAIN and merge with centrality_df\"\"\"\n",
        "#    conn = sqlite3.connect(db_path)\n",
        "\n",
        "#    biog_query = f\"\"\"\n",
        "#        SELECT \n",
        "#            c_personid, \n",
        "#            c_name_chn, \n",
        "#            c_name,\n",
        "#            {gender_field} AS gender,\n",
        "#            c_birthyear,\n",
        "#            c_deathyear,\n",
        "#            c_index_year\n",
        "#        FROM BIOG_MAIN\n",
        "#        WHERE {gender_field} IS NOT NULL\n",
        "#    \"\"\"\n",
        "#    biog_df = pd.read_sql_query(biog_query, conn)\n",
        "#    conn.close()\n",
        "#    print(f\"Loaded {len(biog_df):,} individuals with gender info\")\n",
        "#    merged_df = centrality_df.merge(\n",
        "#        biog_df[['c_personid', 'gender', 'c_birthyear', 'c_deathyear', 'c_index_year']],\n",
        "#        left_on='person_id',\n",
        "#        right_on='c_personid',\n",
        "#        how='left'\n",
        "#    )\n",
        "    # Gender distribution\n",
        "#    print(\"\\nGender distribution (0=male, 1=female):\")\n",
        "#    print(merged_df['gender'].value_counts(dropna=False))\n",
        "\n",
        "#    coverage = merged_df['gender'].notna().sum() / len(merged_df) * 100\n",
        "#    print(f\"Gender data coverage: {coverage:.1f}% of network nodes\")\n",
        "\n",
        "#    return merged_df\n",
        "\n",
        "#final_df = load_and_merge_gender_data(centrality_df, gender_field='c_female')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Output:**\n",
        "Gender distribution (0=male, 1=female):\n",
        "\n",
        "* 0    48869\n",
        "* 1     4123\n",
        "\n",
        "Name: count, dtype: int64\n",
        "Gender data coverage: 100.0% of network nodes\n",
        "\n",
        "We can see from the output that the majority of nodes are male which is something which we saw with the Pyvis visualization. This gender inconsistency makes sense as for historical records they are more likely to record men espeically for ones going back to the 7th century. In the largest component we are using only 7.8% of the nodes are female. This might not end up being an issue as it is possible for women to still be stronger bridges except for a few outlier men. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#final_df['gender_label'] = final_df['gender'].map({0: 'Male', 1: 'Female'})\n",
        "#bridge_thresh = 0.001 #Define a 'bridge' threshold\n",
        "\n",
        "#female_bridges = final_df[(final_df['gender_label'] == 'Female') & \n",
        "#                          (final_df['betweenness_centrality'] > bridge_thresh)]\n",
        "#male_bridges = final_df[(final_df['gender_label'] == 'Male') & \n",
        "#                        (final_df['betweenness_centrality'] > bridge_thresh)]\n",
        "\n",
        "#n_female = (final_df['gender_label'] == 'Female').sum()\n",
        "#n_male = (final_df['gender_label'] == 'Male').sum()\n",
        "\n",
        "#prop_female_bridges = len(female_bridges) / n_female * 100 if n_female else 0\n",
        "#prop_male_bridges = len(male_bridges) / n_male * 100 if n_male else 0\n",
        "\n",
        "#print(f\"Females with betweenness > {bridge_thresh}: {len(female_bridges)} / {n_female} ({prop_female_bridges:.2f}%)\")\n",
        "#print(f\"Males with betweenness > {bridge_thresh}: {len(male_bridges)} / {n_male} ({prop_male_bridges:.2f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "\n",
        "* Females with betweenness > 0.001: 101 / 4123 (2.45%)\n",
        "* Males with betweenness > 0.001: 4452 / 48869 (9.11%)\n",
        "\n",
        "We can see that 9.11% percent of men pass our treshold of 0.001 while only 2.45% of women, so most of the main 'bridges' in our dataset are men. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#final_df['gender_label'] = final_df['gender'].map({0: 'Male', 1: 'Female'})\n",
        "\n",
        "#bridge_thresh = 0.01 # Define a 'bridge' threshold \n",
        "\n",
        "#female_bridges = final_df[(final_df['gender_label'] == 'Female') & \n",
        "#                          (final_df['betweenness_centrality'] > bridge_thresh)]\n",
        "#male_bridges = final_df[(final_df['gender_label'] == 'Male') & \n",
        "#                        (final_df['betweenness_centrality'] > bridge_thresh)]\n",
        "\n",
        "#n_female = (final_df['gender_label'] == 'Female').sum()\n",
        "#n_male = (final_df['gender_label'] == 'Male').sum()\n",
        "\n",
        "#prop_female_bridges = len(female_bridges) / n_female * 100 if n_female else 0\n",
        "#prop_male_bridges = len(male_bridges) / n_male * 100 if n_male else 0\n",
        "\n",
        "#print(f\"Females with betweenness > {bridge_thresh}: {len(female_bridges)} / {n_female} ({prop_female_bridges:.2f}%)\")\n",
        "#print(f\"Males with betweenness > {bridge_thresh}: {len(male_bridges)} / {n_male} ({prop_male_bridges:.2f}%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output:\n",
        "* Females with betweenness > 0.01: 5 / 4123 (0.12%)\n",
        "* Males with betweenness > 0.01: 205 / 48869 (0.42%)\n",
        "\n",
        "If we up our treshhold we can see that this does not change with 0.42% men compared to 0.12% women passing our treshhold of 0.01. This means that women in our dataset do not act as bridges more than men. Still let's look at the most prominent female bridges and like with the men link them to real historical figures. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#top_female_bridges = final_df[final_df['gender_label'] == 'Female'].nlargest(10, 'betweenness_centrality')\n",
        "#print(\"Top 10 Female Bridges in the Network:\")\n",
        "#for i, row in top_female_bridges.iterrows():\n",
        "#    print(f\"Person {row['person_id']}: Betweenness={row['betweenness_centrality']:.3f}, Degree={row['degree']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Top 10 Female Bridges in the Network:\n",
        "\n",
        "* Person 141303: Betweenness=0.037, Degree=12\n",
        "* Person 93663: Betweenness=0.029, Degree=28\n",
        "* Person 17702: Betweenness=0.015, Degree=11\n",
        "* Person 142641: Betweenness=0.011, Degree=6\n",
        "* Person 140204: Betweenness=0.010, Degree=5\n",
        "* Person 4217: Betweenness=0.009, Degree=12\n",
        "* Person 141789: Betweenness=0.009, Degree=12\n",
        "* Person 194260: Betweenness=0.009, Degree=8\n",
        "* Person 134070: Betweenness=0.009, Degree=3\n",
        "* Person 141996: Betweenness=0.008, Degree=15\n",
        "\n",
        "As with the best male connectors let's find the identies of the top 3 female bridges:\n",
        "\n",
        "* Person 141303: Betweenness=0.037, Degree=12\n",
        "* Person 93663: Betweenness=0.029, Degree=28\n",
        "* Person 17702: Betweenness=0.015, Degree=11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conn = sqlite3.connect(db_path)\n",
        "\n",
        "# Key people\n",
        "key_women = [141303, 93663, 17702]\n",
        "\n",
        "# Combined query for biography + native place\n",
        "query = f\"\"\"\n",
        "SELECT \n",
        "    bm.c_personid,\n",
        "    bm.c_name_chn,\n",
        "    bm.c_surname_chn,\n",
        "    bm.c_name,\n",
        "    bm.c_surname,\n",
        "    bm.c_birthyear,\n",
        "    bm.c_deathyear,\n",
        "    bm.c_dy,         -- dynasty code\n",
        "    bm.c_female,\n",
        "    a.c_name AS NativePlace_CHN\n",
        "FROM BIOG_MAIN bm\n",
        "LEFT JOIN BIOG_ADDR_DATA bad\n",
        "    ON bm.c_personid = bad.c_personid\n",
        "    AND bad.c_addr_type = 1\n",
        "LEFT JOIN ADDRESSES a\n",
        "    ON bad.c_addr_id = a.c_addr_id\n",
        "WHERE bm.c_personid IN ({\",\".join(map(str, key_women))})\n",
        "\"\"\"\n",
        "\n",
        "# Read into dataframe\n",
        "df = pd.read_sql_query(query, conn)\n",
        "\n",
        "# Combine name fields\n",
        "df[\"FullName_CHN\"] = df[\"c_surname_chn\"].fillna('') + df[\"c_name_chn\"].fillna('')\n",
        "df[\"FullName_ENG\"] = (df[\"c_surname\"].fillna('') + \" \" + df[\"c_name\"].fillna('')).str.strip()\n",
        "df[\"Gender\"] = df[\"c_female\"].map({0: \"Male\", 1: \"Female\"})\n",
        "\n",
        "# Keep relevant columns and remove duplicates\n",
        "final_df = df[[\n",
        "    \"c_personid\", \"FullName_CHN\", \"FullName_ENG\",\n",
        "    \"c_birthyear\", \"c_deathyear\", \"c_dy\", \"Gender\", \"NativePlace_CHN\"\n",
        "]].drop_duplicates()\n",
        "\n",
        "# Display nicely\n",
        "display(final_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The top women 'bridges' in our dataset are:\n",
        "\n",
        "* Wu Shi: who is the wife of [Emperor Gaozong of Song](https://en.wikipedia.org/wiki/Emperor_Gaozong_of_Song). It says in the data that she is from Qiantang which is a river near Shanghai. \n",
        "* [Wu Zhao(Wu Zetian)](https://en.wikipedia.org/wiki/Wu_Zetian) who was empress of China from 660 to 705, ruling first through others and later in her own right. She ruled as empress through her husband Emperor Gaozong and later as empress dowager through her sons Emperors Zhongzong and Ruizong, from 660 to 690. She is from Wenshui which is  \"a county in the west-central part of Shanxi Province, China.\"\" \n",
        "* Cui Sui is most likely daugher of [Cui Ting Shi](https://en.wikipedia.org/wiki/Cui_Shi). She is from Qinghe which is \"located in the south of Hebei province, China, bordering Shandong province to the east.\"\n",
        "\n",
        "# 5.0 Networks over time and Across Dynasties\n",
        "\n",
        "Next we will look at the dynasties over time and try to determine if there are quantifiable differences between the networks for each of these dynasties.\n",
        "\n",
        "To determine the dynasty we will use the `c_index_year` variable and if that does not exist as a substitute we will use `c_birthyear`. We have repeated this analysis with the `c_dynasty` variable the findings were nearly identical, but the code was more complicated so we have settled on this version.\n",
        "\n",
        "Load more data and define dynasties. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conn = sqlite3.connect(db_path)\n",
        "biog_df = pd.read_sql_query(\"\"\"\n",
        "    SELECT c_personid, c_birthyear, c_deathyear, c_index_year\n",
        "    FROM BIOG_MAIN\"\"\", conn)\n",
        "\n",
        "def assign_dynasty(row):\n",
        "    year = row['c_index_year']\n",
        "    if year is None:\n",
        "        year = row['c_birthyear'] \n",
        "    if year is None:\n",
        "        return 'Unknown'\n",
        "    if 0 <= year <= 618: return 'Pre-Tang'\n",
        "    if 618 <= year <= 907: return 'Tang'\n",
        "    if 907 < year < 960: return 'Five Dynasties/10 Kingdoms'\n",
        "    if 960 <= year <= 1279: return 'Song'\n",
        "    if 1279 <= year <= 1368: return 'Yuan'\n",
        "    if 1368 <= year <= 1644: return 'Ming'\n",
        "    if 1644 <= year <= 1912: return 'Qing'\n",
        "    if 1912 <= year <= 2025: return 'Modern'\n",
        "    return 'Other'\n",
        "\n",
        "biog_df['dynasty'] = biog_df.apply(assign_dynasty, axis=1)\n",
        "conn.close()\n",
        "\n",
        "#uncomment if we want to see sorted by ammount\n",
        "#print(biog_df['dynasty'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#order from before just cleanly defined\n",
        "dynasty_order = [\n",
        "    'Pre-Tang',\n",
        "    'Tang',\n",
        "    'Five Dynasties/10 Kingdoms',\n",
        "    'Song',\n",
        "    'Yuan',\n",
        "    'Ming',\n",
        "    'Qing',\n",
        "    'Modern',\n",
        "    'Other'   \n",
        "    ]\n",
        "\n",
        "biog_df['dynasty'] = pd.Categorical(\n",
        "    biog_df['dynasty'],\n",
        "    categories=dynasty_order,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "#Print in order\n",
        "#print(biog_df['dynasty'].value_counts(sort=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we will filter by dynasties and create NetworkX objects for each dynasty. We will be able to see how many nodes and edges each dynasty has. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define custom dynasty order\n",
        "dynasty_order = [\n",
        "    'Pre-Tang',\n",
        "    'Tang',\n",
        "    'Five Dynasties/10 Kingdoms',\n",
        "    'Song',\n",
        "    'Yuan',\n",
        "    'Ming',\n",
        "    'Qing',\n",
        "    'Modern',\n",
        "    'Other'\n",
        "]\n",
        "\n",
        "# Load kinship relationships\n",
        "conn = sqlite3.connect(db_path)\n",
        "kin_df = pd.read_sql_query(\"SELECT c_personid, c_kin_id FROM KIN_DATA\", conn)\n",
        "conn.close()\n",
        "\n",
        "# Merge dynasty info for person and kin\n",
        "kin_df = kin_df.merge(\n",
        "    biog_df[['c_personid', 'dynasty']],\n",
        "    left_on='c_personid', right_on='c_personid',\n",
        "    how='left'\n",
        ")\n",
        "kin_df = kin_df.merge(\n",
        "    biog_df[['c_personid', 'dynasty']],\n",
        "    left_on='c_kin_id', right_on='c_personid',\n",
        "    how='left',\n",
        "    suffixes=('_person', '_kin')\n",
        ")\n",
        "\n",
        "# Ensure dynasty columns are ordered categorical\n",
        "kin_df['dynasty_person'] = pd.Categorical(\n",
        "    kin_df['dynasty_person'],\n",
        "    categories=dynasty_order,\n",
        "    ordered=True\n",
        ")\n",
        "kin_df['dynasty_kin'] = pd.Categorical(\n",
        "    kin_df['dynasty_kin'],\n",
        "    categories=dynasty_order,\n",
        "    ordered=True\n",
        ")\n",
        "\n",
        "def build_dynasty_networks(kin_df):\n",
        "    dynasty_graphs = {}\n",
        "    for dyn in dynasty_order:\n",
        "        if dyn in kin_df['dynasty_person'].values:\n",
        "            sub_edges = kin_df[kin_df['dynasty_person'] == dyn]\n",
        "            G = nx.Graph()\n",
        "            G.add_edges_from(zip(sub_edges['c_personid_person'], sub_edges['c_kin_id']))\n",
        "            dynasty_graphs[dyn] = G\n",
        "            print(f\"Dynasty {dyn}: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "    return dynasty_graphs\n",
        "\n",
        "dynasty_graphs = build_dynasty_networks(kin_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output is great, we can see that most of the data has a corresponding dynasty. There is very little modern as the scope of the dataset is from the 7th through the 19th century so this makes sense. We can ignore other, and modern for this analysis.   \n",
        "\n",
        "Now we will calculate Degree Centrality, Density, Clustering Coefficient, and Modularity for our dynasties. This will serve as the first step towards trying to find differences or similarities between these dynasties. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# List of included dynasties in order\n",
        "ordered_dynasties = [\"Pre-Tang\", \"Tang\", \"Five Dynasties/10 Kingdoms\", \"Song\", \"Yuan\", \"Ming\", \"Qing\"]\n",
        "\n",
        "def analyze_dynasty_metrics_selected(dynasty_graphs, ordered_dynasties):\n",
        "    results = {}\n",
        "    for dyn_name in ordered_dynasties:\n",
        "        G = dynasty_graphs.get(dyn_name)\n",
        "        if G is None:\n",
        "            continue\n",
        "        # Degree centrality\n",
        "        deg_cent = nx.degree_centrality(G)\n",
        "        avg_deg_cent = sum(deg_cent.values()) / len(deg_cent)\n",
        "\n",
        "        # Average degree per node\n",
        "        avg_degree = (2 * G.number_of_edges()) / G.number_of_nodes()\n",
        "\n",
        "        # Graph density\n",
        "        density = nx.density(G)\n",
        "\n",
        "        # Clustering coefficient (average)\n",
        "        avg_clust = nx.average_clustering(G)\n",
        "\n",
        "        # Community detection: modularity\n",
        "        communities = list(nx.community.greedy_modularity_communities(G))\n",
        "        modularity = nx.community.modularity(G, communities)\n",
        "\n",
        "        results[dyn_name] = {\n",
        "            \"avg_degree_centrality\": avg_deg_cent,\n",
        "            \"avg_degree\": avg_degree,\n",
        "            \"density\": density,\n",
        "            \"avg_clustering_coefficient\": avg_clust,\n",
        "            \"modularity\": modularity\n",
        "        }\n",
        "        print(f\"{dyn_name}: Avg Degree Centrality={avg_deg_cent:.4f}, Avg Degree={avg_degree:.2f}, \"\n",
        "              f\"Density={density:.6f}, Avg Clust Coeff={avg_clust:.4f}, Modularity={modularity:.4f}\")\n",
        "    return results\n",
        "\n",
        "#Run the function\n",
        "dynasty_metrics_selected = analyze_dynasty_metrics_selected(dynasty_graphs, ordered_dynasties)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From findings we will see that Degree Centrality and Density are very small these are expected because of the huge size of the network. We also have very high modularity near 1, as expected in a family network there are strong community structures. Song has the highest Average Degree for each node at 2.38 which is far more than the other dynasties. Let's continue with our analysis. \n",
        "\n",
        "Degree Distribution Shape Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def analyze_degree_distributions(dynasty_graphs, ordered_dynasties):\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    for i, dyn_name in enumerate(ordered_dynasties):\n",
        "        G = dynasty_graphs.get(dyn_name)\n",
        "        if G is None:\n",
        "            continue\n",
        "            \n",
        "        degrees = [d for n, d in G.degree()]\n",
        "        \n",
        "        # Plot distribution\n",
        "        plt.subplot(3, 3, i+1)\n",
        "        plt.hist(degrees, bins=50, alpha=0.7, color=f'C{i}')\n",
        "        plt.title(f'{dyn_name} Degree Distribution')\n",
        "        plt.xlabel('Degree')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.yscale('log')  # Log scale to see heavy tails\n",
        "        \n",
        "        # Calculate distribution statistics\n",
        "        mean_deg = np.mean(degrees)\n",
        "        median_deg = np.median(degrees)\n",
        "        max_deg = max(degrees)\n",
        "        std_deg = np.std(degrees)\n",
        "        skewness = stats.skew(degrees)\n",
        "        \n",
        "        print(f\"{dyn_name}: Mean={mean_deg:.2f} \"\n",
        "              f\"Std={std_deg:.2f}, Skewness={skewness:.2f}\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Run analysis\n",
        "analyze_degree_distributions(dynasty_graphs, ordered_dynasties)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our visualization is a histogram of the frequency of high degree nodes. All of them are similar except for the Ming Dynasty.\n",
        "\n",
        "There are many potential reasons for this, but one that stands out is the Ming Dynasties unique approach to scholar network formation and bureaucratic recruitment, which created a more centralized and hierarchical system compared to other Chinese dynasties. This very selective system created a different social network structure than previous dynasties. The Ming examination system produced a small core of connected scholars and officials who formed the adminstrative portion of the government. Unlike earlier dynasties where multiple pathways to office existed, the Ming system funneled nearly all political advancement through this single, narrow channel. This single channel was a series of exams which 2-3 million applicants would attempt per year with only about a thousand passing. This process lead to a social network which was more evenly connected than previous or future dynasties.  \n",
        "\n",
        "We will continue our analysis with looking at how many components each dynasty has and their fragmentation and average size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def analyze_connected_components(dynasty_graphs, ordered_dynasties):\n",
        "    results = {}\n",
        "    \n",
        "    for dyn_name in ordered_dynasties:\n",
        "        G = dynasty_graphs.get(dyn_name)\n",
        "        if G is None:\n",
        "            continue\n",
        "            \n",
        "        # Get connected components\n",
        "        components = list(nx.connected_components(G))\n",
        "        num_components = len(components)\n",
        "        component_sizes = [len(c) for c in components]\n",
        "        \n",
        "        # Calculate metrics\n",
        "        largest_component_size = max(component_sizes) if component_sizes else 0\n",
        "        fragmentation_index = 1 - (largest_component_size / G.number_of_nodes())\n",
        "        avg_component_size = np.mean(component_sizes) if component_sizes else 0\n",
        "        \n",
        "        results[dyn_name] = {\n",
        "            \"num_components\": num_components,\n",
        "            \"largest_component_size\": largest_component_size,\n",
        "            \"fragmentation_index\": fragmentation_index,\n",
        "            \"avg_component_size\": avg_component_size,\n",
        "            \"component_sizes\": component_sizes\n",
        "        }\n",
        "        \n",
        "        print(f\"{dyn_name}: Components={num_components}, \"\n",
        "              f\"Largest={largest_component_size}, \"\n",
        "              f\"Fragmentation={fragmentation_index:.4f}, \"\n",
        "              f\"Avg Size={avg_component_size:.2f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run analysis\n",
        "component_results = analyze_connected_components(dynasty_graphs, ordered_dynasties)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected the Pre-Tang dynasty is quite disconnected with many smaller clusters. The Song dynasty has the lowest fragmentation and the largest component meaning most of the network for this dynasty is one large component. The Yuan dynasty is also very fragmented which is suprising as it covered a large political period. The Ming dynasty has the most components by far but a minuscule largest connected cluster. In general dynasties before Yuan have quite low fragmentation compared to ones after Yuan. \n",
        "\n",
        "We will contunue with temporal patterning analysis to see if within these dynasties there were large changes throughout the years. \n",
        "\n",
        "We will use the *'c_index_year'* variable to see if density, clustering, or average degree changes over time change inside a dynasty."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def analyze_temporal_patterns(kin_df, biog_df, ordered_dynasties):\n",
        "    # Merge to get information for each edge\n",
        "    kin_temporal = kin_df.merge(biog_df[['c_personid', 'c_index_year', 'dynasty']], \n",
        "                               left_on='c_personid_person', right_on='c_personid', how='left')\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for dyn_name in ordered_dynasties:\n",
        "        dyn_data = kin_temporal[kin_temporal['dynasty_person'] == dyn_name].copy()\n",
        "        \n",
        "        # Group by time periods (for us 50-year bins)\n",
        "        if dyn_data['c_index_year'].notna().sum() > 0:\n",
        "            min_year = int(dyn_data['c_index_year'].min())\n",
        "            max_year = int(dyn_data['c_index_year'].max())\n",
        "            bins = range(min_year, max_year + 50, 50)\n",
        "            dyn_data['time_bin'] = pd.cut(dyn_data['c_index_year'], bins=bins)\n",
        "            \n",
        "            # Calculate metrics per time period\n",
        "            temporal_metrics = []\n",
        "            for time_bin in dyn_data['time_bin'].cat.categories:\n",
        "                bin_data = dyn_data[dyn_data['time_bin'] == time_bin]\n",
        "                if len(bin_data) > 10:  # Only analyze bins with sufficient data\n",
        "                    # Create subgraph for this time period\n",
        "                    edges = zip(bin_data['c_personid_person'], bin_data['c_kin_id'])\n",
        "                    temp_G = nx.Graph()\n",
        "                    temp_G.add_edges_from(edges)\n",
        "                    \n",
        "                    if temp_G.number_of_nodes() > 0:\n",
        "                        avg_deg = (2 * temp_G.number_of_edges()) / temp_G.number_of_nodes()\n",
        "                        density = nx.density(temp_G)\n",
        "                        clustering = nx.average_clustering(temp_G)\n",
        "                        \n",
        "                        temporal_metrics.append({\n",
        "                            'time_period': str(time_bin),\n",
        "                            'avg_degree': avg_deg,\n",
        "                            'density': density,\n",
        "                            'clustering': clustering,\n",
        "                            'nodes': temp_G.number_of_nodes(),\n",
        "                            'edges': temp_G.number_of_edges()\n",
        "                        })\n",
        "            \n",
        "            results[dyn_name] = temporal_metrics\n",
        "            \n",
        "            # Print summary\n",
        "            print(f\"\\n{dyn_name} Temporal Analysis ({min_year}-{max_year}):\")\n",
        "            for metric in temporal_metrics[:20]:  # Show first 20 time periods (which is all)\n",
        "                print(f\"  {metric['time_period']}: Avg Degree={metric['avg_degree']:.2f}, \"\n",
        "                      f\"Density={metric['density']:.6f}, Clustering={metric['clustering']:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run temporal analysis\n",
        "temporal_results = analyze_temporal_patterns(kin_df, biog_df, ordered_dynasties)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The findings here are quite interesting there seems to be quite a bit of variation inside of the dynasties. \n",
        "\n",
        "* Pre-Tang: The clustering increases through the years which makes sense.\n",
        "* Tang: The average degre is quite stable with a slight spike in the middle. Clustering gradually increases from very low (0.0210) to peak at mid-period (0.0667), then drops similar to average degree. \n",
        "* Five Dynasties/10 Kingdoms Temporal Analysis: Large drop in Average Degree, with a very large decrease in clustering from (0.0755 to 0.0106)\n",
        "* Song: Late Song becomes more clustered and somewhat denser compared to early Song.\n",
        "* Yuan: Large drop in average degree (1.94 to 1.59) and large decrease in clustering (0.1219 to 0.0221)\n",
        "* Ming: Very stable compared to other dynasties\n",
        "* Qing: Early Qing is more cohesive than late Qing except for a late density spike that is not accompanied by clustering. \n",
        "\n",
        "# 6.0 Citations\n",
        "\n",
        "Al-Taie, M. Z., & Kadry, S. (2017). *Python for graph and network analysis*. Springer. https://doi.org/10.1007/978-3-319-53004-8\n",
        "\n",
        "Cartwright, Mark. “The Civil Service Examinations of Imperial China.” World History Encyclopedia, https://www.worldhistory.org#organization, 15 Aug. 2025, www.worldhistory.org/article/1335/the-civil-service-examinations-of-imperial-china/. \n",
        "\n",
        "“Chang’an.” Wikipedia, Wikimedia Foundation, 10 Aug. 2025, en.wikipedia.org/wiki/Chang%27an. \n",
        "\n",
        "“China Biographical Database Project (CBDB).” Home, Harvard, 2024, projects.iq.harvard.edu/cbdb/home. \n",
        "\n",
        "“The China Biographical Database User’s Guide.” The China Biographical Database, Harvard, 26 July 2024, projects.iq.harvard.edu/sites/projects.iq.harvard.edu/files/cbdb/files/cbdb_users_guide.pdf. \n",
        "\n",
        "“Cui Shi.” Wikipedia, Wikimedia Foundation, 18 July 2025, en.wikipedia.org/wiki/Cui_Shi. \n",
        "“Dynasties of China.” Wikipedia, Wikimedia Foundation, 27 July 2025, en.wikipedia.org/wiki/Dynasties_of_China. \n",
        "\n",
        "“Emperor Gaozong of Song.” Wikipedia, Wikimedia Foundation, 18 July 2025, en.wikipedia.org/wiki/Emperor_Gaozong_of_Song. \n",
        "\n",
        "“Emperor Gaozu of Tang.” Wikipedia, Wikimedia Foundation, 2 Aug. 2025, en.wikipedia.org/wiki/Emperor_Gaozu_of_Tang. \n",
        "\n",
        "“Kaifeng.” Wikipedia, Wikimedia Foundation, 11 Aug. 2025, en.wikipedia.org/wiki/Kaifeng.\n",
        "\n",
        "“King Li of Zhou.” Wikipedia, Wikimedia Foundation, 20 July 2025, en.wikipedia.org/wiki/King_Li_of_Zhou. \n",
        "\n",
        "“Li Fang (Song Dynasty).” Wikipedia, Wikimedia Foundation, 27 May 2025, en.wikipedia.org/wiki/Li_Fang_(Song_dynasty). \n",
        "\n",
        "“Ming Dynasty.” Encyclopedia Britannica, Encyclopedia Britannica, inc., 31 July 2025, www.britannica.com/topic/Ming-dynasty-Chinese-history. \n",
        "\n",
        "“Qiantang River.” Wikipedia, Wikimedia Foundation, 23 July 2025, en.wikipedia.org/wiki/Qiantang_River. \n",
        "\n",
        "“Qinghe County, Hebei.” Wikipedia, Wikimedia Foundation, 2 Aug. 2025, en.wikipedia.org/wiki/Qinghe_County,_Hebei. \n",
        "\n",
        "“Raoyang County.” Wikipedia, Wikimedia Foundation, 28 Feb. 2025, en.wikipedia.org/wiki/Raoyang_County.\n",
        "\n",
        "“Song Dynasty.” Encyclopedia Britannica, Encyclopædia Britannica, inc., 17 July 2025, www.britannica.com/topic/Song-dynasty. \n",
        "\n",
        "“Tang Dynasty.” Encyclopedia Britannica, Encyclopædia Britannica, inc., 19 July 2025, www.britannica.com/topic/Tang-dynasty. \n",
        "\n",
        "“Wenshui County.” Wikipedia, Wikimedia Foundation, 12 Aug. 2024, en.wikipedia.org/wiki/Wenshui_County. \n",
        "\n",
        "“Wu Zetian.” Wikipedia, Wikimedia Foundation, 10 Aug. 2025, en.wikipedia.org/wiki/Wu_Zetian. \n",
        "\n",
        "“Zhao Tingmei.” Wikipedia, Wikimedia Foundation, 10 Feb. 2025, en.wikipedia.org/wiki/Zhao_Tingmei. "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}