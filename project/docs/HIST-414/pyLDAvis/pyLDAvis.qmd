---
title: Using pyLDAvis for Analysis
jupyter: python3
description: Building on embeddings, this notebook uses Latent Dirichlet Allocation to uncover themes in legal judgments and create interactive visualizations that explore the multiple dimensions of legal reasoning.
---

Legal judgments are complex documents that draw upon many facets of legal reasoning, including the interpretation of applicable law, evaluation of evidence presented in court, and consideration of prior precedents. This makes them difficult to systemically analyze as they often defy single membership classification. Rice (2019) introduces Latent Dirichlet Allocation as a method of unsupervised structured topic modelling (STM), allowing for underlying themes in legal texts to be uncovered via a computational approach and capture the proportionate attention given to multiple legal dimensions within each judgement. Acting as a continuation of Notebook 1, the goal of this notebook is to create interactive visualizations in order to explore and understand these relationships. 


```{python}
#Import packages 
import pandas as pd
import json
import pathlib
import re, string
import multiprocessing
import requests

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

from gensim import corpora
from gensim.utils import simple_preprocess
from gensim.models.phrases import Phrases, Phraser
from gensim.corpora import Dictionary
from gensim.models.ldamulticore import LdaMulticore
from gensim.models import CoherenceModel
from gensim.models import LdaMulticore

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import multiprocessing
```

Explanation of LDA from notebook 2 or if this is attached to notebook 2 then this is unnecessary 

### 1. Load the data

The data we will be using is the Supreme Court of Canada Bulk Decisions Dataset which contains the full text of Supreme Court of Canada decisions.

```{python}
#Get the data from Github (Same step as in notebook 2 just done differently can pick best one)

start_year = 1877  # First year of data sought (1877+)
end_year = 2024  # Last year of data sought (2024 -)

# Load data directly from GitHub repository
base_url = 'https://raw.githubusercontent.com/Refugee-Law-Lab/scc_bulk_data/master/DATA/YEARLY/'
results = []

# load data (all years, json files)
print(f"Loading data from {start_year} to {end_year}...")
for year in range(start_year, end_year+1):
    try:
        url = base_url + f'{year}.json'
        response = requests.get(url)
        if response.status_code == 200:
            results.extend(response.json())
            print(f"Loaded {year} ✓")
    except Exception as e:
        print(f"Error loading {year}: {e}")

# Convert to dataframe
df = pd.DataFrame(results)
print(f"\nTotal records loaded: {len(df)}")
df.head()
```

```{python}
# download extra datasets to clean our supreme court data
nltk.download('stopwords') #list of stop words english and french
nltk.download('wordnet') #wordnet dataset for lemmatization 
nltk.download('omw-1.4') #multilingual support for French mostly

# Initialize lemmatizer and stopwords
LEMMA = WordNetLemmatizer()
en_stop = set(stopwords.words('english'))
fr_stop = set(stopwords.words('french'))
custom_stop = {'from', 'subject', 're', 'edu', 'thereof', 'herein',
               'hereby', 'aforesaid', 'whereas', 'shall', 'henceforth',
               'article', 'section', 'v'}
stop_words = en_stop | fr_stop | custom_stop #make a custom set of stop words
```

### 2. Main cleaning Stage  
This is the same as in notebook 2, but in one cell for clarity as we are repeating analysis. More concretely we are cleaning the data with these steps:
1. Lowercases all text 
2. Removes newlines/extra spaces 
3. Remove punctuation 
4. Tokenizes with Gensim's simple_preprocess 
5. Filters stopwords (See above cell stopword set)
6. Applies WordNet lemmatization (See above lemmatization dataset)

Next we make a fuinction that finds Bigrams (which are consecutive written units in this case words). After that we make a function to construct a document-term matrix. Finally we define a function to train the model. 

```{python}
#Step 2
#Clean the text
def preprocess_texts(df, text_col="unofficial_text", stop_words=stop_words, lemmatize=True):
    """
    Tokenize, remove stopwords/punctuation, and optionally lemmatize the texts 
    Returns list of token lists
    """
    texts = df[text_col].astype(str).tolist()
    lemmatizer = WordNetLemmatizer()
    processed = []
    
    for doc in texts:
        tokens = []
        text = doc.lower().replace("\n", " ")
        text = re.sub(r"\s+", " ", text)
        text = text.translate(str.maketrans('', '', string.punctuation))
        
        for token in simple_preprocess(text, deacc=True, min_len=3):
            if token not in stop_words:
                if lemmatize:
                    token = lemmatizer.lemmatize(token)
                tokens.append(token)
        processed.append(tokens)
    return processed

#Find bigrams Function
def make_bigrams(tokenized_texts, min_count=10, threshold=50):
    """
    Fit a bigram phraser and apply to tokenized texts
    """
    phrases = Phrases(tokenized_texts, min_count=min_count, threshold=threshold)
    bigram_mod = Phraser(phrases)
    return [bigram_mod[doc] for doc in tokenized_texts]

#document-term matrix
def create_dictionary_corpus(texts, no_below=15, no_above=0.5):
    """
    Build Gensim dictionary and corpus from tokenized texts
    """
    id2word = corpora.Dictionary(texts)
    id2word.filter_extremes(no_below=no_below, no_above=no_above)
    corpus = [id2word.doc2bow(text) for text in texts]
    return id2word, corpus

#Training Function
def train_lda(corpus, id2word, num_topics=25, passes=10, iterations=50):
    """
    Train LDA model
    """
    workers = max(1, multiprocessing.cpu_count()-1)
    lda_model = LdaMulticore(
        corpus=corpus,
        id2word=id2word,
        num_topics=num_topics,
        random_state=1975,
        chunksize=20000,
        passes=passes,
        iterations=iterations,
        eval_every=None,
        workers=workers,
        alpha='asymmetric',
        eta='auto',
        per_word_topics=False
    )
    return lda_model
```

```{python}
print("\nDataframe columns:")
print(df.columns.tolist())
print("\nFirst row sample:")
print(df.iloc[0].to_dict())
```

```{python}
#Step 3 - Here we run our functions from above
#Might be computationally intensive (16m 7s)

# check text collumns
print("Available columns in dataframe:")
print(df.columns.tolist())

# Use the appropriate text column (in this case unofficial_text)
text_column = 'unofficial_text' 

# Preprocess texts
print("Preprocessing texts...")
tokenized_texts = preprocess_texts(df, text_col=text_column)

# Create bigrams
print("Creating bigrams...")
bigrams_list = make_bigrams(tokenized_texts)

# Create dictionary and corpus
print("Creating dictionary and corpus...")
id2word, corpus = create_dictionary_corpus(bigrams_list)

# Train LDA model - 
print("Training LDA model...")
lda = train_lda(corpus, id2word, num_topics=25)

print("LDA model training complete!")
print(f"Number of topics: {lda.num_topics}")
print(f"Corpus size: {len(corpus)}")
print(f"Dictionary size: {len(id2word)}")
```

### 3. Optimal Number of Topics
In order to have a optimized model we will evaluate the amount of topics to use in the analysis. We will do this by evaluating model quality across a range of topic counts. The metric we will use is called coherence score, which measures how interpretable and semantically meaningful the topics are. Higher scores mean that the topics will make more sense to humans. 

Why do we do this as opposed to just training the model? Picking too few topics can make the themes too broad and uninformative, while too many topics can make them fragmented and incoherent. Here we have an empirical metric we can loook at to determine the best count. 

```{python}
#Compute coherence values 
def compute_coherence_values(corpus, id2word, texts, start=5, limit=30, step=5):
    coherences = []
    model_list = []
    for num in range(start, limit+1, step):
        model = train_lda(corpus, id2word, num_topics=num)
        model_list.append(model)
        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=id2word, coherence='c_v')
        coherences.append(coherencemodel.get_coherence())
    return model_list, coherences
```

```{python}
#Find the optimal amount of topics using function from above
#Computatiponally intensive (48m 28s)
model_list, coherences = compute_coherence_values(corpus, id2word, bigrams_list, start=5, limit=30, step=5)

import matplotlib.pyplot as plt

x = range(5, 31, 5)
plt.plot(x, coherences)
plt.xlabel("Number of Topics")
plt.ylabel("Coherence Score (c_v)")
plt.title("Coherence Score vs Number of Topics")
plt.show()

# Select the model with the highest coherence
optimal_index = coherences.index(max(coherences))
optimal_model = model_list[optimal_index]
optimal_num_topics = x[optimal_index]
print(f"Optimal number of topics: {optimal_num_topics} with coherence score: {coherences[optimal_index]}")
```

We are looking the amount of topics with the highest coherence score which we can determine with the graph above. Our code looks for models from 5, 10, 15, 20, 25, and 30 topics and plots them. In this case it is 15, as it has the highest coherence score. 

### 4. pyLDAvis visualizaiton

Now we will make our interactive visualizaion of our topic model using the package pyLDAvis. This package is made specifically for LDA (Latent Dirichlet Allocation). 

"pyLDAvis is designed to help users interpret the topics in a topic model that has been fit to a corpus of text data. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization." [Read More about pyLDAvis](https://pypi.org/project/pyLDAvis/). 

```{python}
#We can delete this cell for the final 
# Install pyLDAvis if not already installed
!pip install pyldavis

# Import pyLDAvis
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
```

```{python}
#Basic visualization without optimal topic count (Lower down we have one with optimal count this one can be deleted)
def create_interactive_topic_visualization(lda_model, corpus, id2word, save_html=False, filename='lda_visualization.html'):
    """
    Create an interactive pyLDAvis visualization for the LDA model
    Parameters:
    - lda_model: Trained Gensim LDA model
    - corpus: Document-term matrix
    - id2word: Gensim dictionary
    """
    
    # Enable pyLDAvis for notebook display
    pyLDAvis.enable_notebook()
    
    # Prepare the visualization data
    vis_data = gensimvis.prepare(lda_model, corpus, id2word, sort_topics=False)
    
    # Display the visualization
    if save_html:
        # Save as standalone HTML file
        pyLDAvis.save_html(vis_data, filename)
        print(f"Interactive visualization saved as '{filename}'")
        print("You can open this file in any web browser to view the interactive visualization.")
    
    # Return the visualization object for display in notebook
    return vis_data

# Create and display the interactive visualization
interactive_vis = create_interactive_topic_visualization(
    lda_model=lda, 
    corpus=corpus, 
    id2word=id2word, 
    save_html=True,
    filename='supreme_court_topics_basic.html'
)

# Print
interactive_vis
```

```{python}
# We extract topics for Lambda Values 
def extract_topic_terms_by_lambda(vis_data, lambda_value=0.6, num_terms=10):
    """
    Extract the most relevant terms for each topic at a specific lambda value
    
    Parameters:
    - vis_data: Prepared pyLDAvis data
    - lambda_value: Relevance metric (0 = most distinctive, 1 = most probable)
    - num_terms: Number of terms to extract per topic
    """
    all_topics = {}
    num_topics = len(vis_data.topic_coordinates)
    
    # Calculate relevance for each term in our topics
    for i in range(1, num_topics + 1):  
        topic_name = f'Topic{i}'
        topic_data = vis_data.topic_info[vis_data.topic_info.Category == topic_name].copy()
        
        # Calculate relevance score
        topic_data['relevance'] = (lambda_value * topic_data['logprob'] + 
                                  (1 - lambda_value) * topic_data['loglift'])
        
        # Get top terms by relevance
        top_terms = topic_data.sort_values(by='relevance', ascending=False).Term[:num_terms].values
        all_topics[f'Topic {i}'] = top_terms
    
    # Create a DataFrame
    topics_df = pd.DataFrame(all_topics).T
    topics_df.columns = [f'Term_{i+1}' for i in range(num_terms)]
    
    return topics_df

# Extract topic terms for different lambda values
print("Top terms by topic (Lambda = 0.1 - most distinctive):")
distinctive_terms = extract_topic_terms_by_lambda(interactive_vis, lambda_value=0.1)
print(distinctive_terms)

print("\nTop terms by topic (Lambda = 0.6 - balanced relevance):")
balanced_terms = extract_topic_terms_by_lambda(interactive_vis, lambda_value=0.6)
print(balanced_terms)

print("\nTop terms by topic (Lambda = 1.0 - most probable):")
probable_terms = extract_topic_terms_by_lambda(interactive_vis, lambda_value=1.0)
print(probable_terms)
```

```{python}
#Second attempt at visualization with optimal model 

def create_interactive_topic_visualization(lda_model, corpus, id2word, save_html=False, filename='lda_visualization.html'):
    """
    Create an interactive pyLDAvis visualization for the LDA model
    
    Parameters:
    - lda_model: Trained Gensim LDA model
    - corpus: Document-term matrix
    - id2word: Gensim dictionary
    - save_html: Whether to save as standalone HTML file
    - filename: Name for the HTML file if saving
    """
    
    # Enable pyLDAvis for notebook display (if using Jupyter)
    pyLDAvis.enable_notebook()
    
    # Prepare the visualization data
    print("Preparing visualization data...")
    vis_data = gensimvis.prepare(lda_model, corpus, id2word, sort_topics=False)
    
    # Display the visualization
    if save_html:
        # Save as standalone HTML file
        pyLDAvis.save_html(vis_data, filename)
        print(f"Interactive visualization saved as '{filename}'")
        print("You can open this file in any web browser to view the interactive visualization.")
    
    # Return the visualization object for display in notebook
    return vis_data

# Use your visualization function with the optimal model
interactive_vis = create_interactive_topic_visualization(
    lda_model=optimal_model,
    corpus=corpus,
    id2word=id2word,
    save_html=True,
    filename='supreme_court_topics_optimal.html'
)
interactive_vis
```

## References

bmabey , and marksusol . “Pyldavis.” PyPI, 23 Apr. 2023, pypi.org/project/pyLDAvis/. 

Mattingly, W.J.B. “Introduction to Topic Modeling and Text Classification.” 1. Introduction to Topic Modeling - Introduction to Topic Modeling and Text Classification, Feb. 2021, topic-modeling.pythonhumanities.com/01_01_introduction_to_topic_modeling.html. 

“Supreme Court of Canada Bulk Decisions Dataset.” Refugee Law Lab - Refugee Law Lab, 27 June 2024, refugeelab.ca/bulk-data/scc/. 

Tran, Khuyen. “Pyldavis: Topic Modeling Exploration Tool.” Neptune.Ai, 20 May 2025, neptune.ai/blog/pyldavis-topic-modeling-exploration-tool. 

“What Is Pyldavis Library in Python?” HowDev, how.dev/answers/what-is-pyldavis-library-in-python. Accessed 9 June 2025. 


