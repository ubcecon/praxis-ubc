---
title: Text Embeddings for Regina V Wing Chong (1885)
jupyter: python3
description: An introduction to text and sentence embeddings, similarity analysis, stance detection, and topic modeling in the 1885 legal case Regina v. Wing Chong. 
---

```{python}
# Data Wrangling
import os
import numpy as np
import pandas
from nltk.tokenize import word_tokenize
```

```{python}
with open('data/Regina_V_Wing_Chong.txt', encoding='utf-8') as f:
    full_text = f.read()
print(full_text)
```

### BERT Word Embeddings

```{python}
import re

def clean_text(text):
    
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    
    return text.strip()

text_cleaned = clean_text(full_text)
print(text_cleaned[:500])  # Print the first 500 characters of the cleaned text
```

```{python}
# Load pre-trained BERT tokenizer and model
from transformers import BertTokenizer, BertModel
import torch

tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')
bert_model = BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')
```

```{python}
# Create the word embeddings
# Tokenize the cleaned text into words
tokens = word_tokenize(text_cleaned)

token_frequencies = {}

for token in tokens:
    token_frequencies[token] = token_frequencies.get(token, 0) + 1
```

```{python}
sorted_tokens = sorted(token_frequencies.items(), key=lambda x: x[1], reverse=True)

# Example: print top 10 most frequent tokens
print("Most frequent tokens:")
for token, freq in sorted_tokens[:20]:
    print(f"{token}: {freq}")
```

```{python}
import re
# Build ethnicity vocabulary
ethnicities = [
    "chinese", "japanese", "black", "white", "yellow", "chinamans", "hong kong",
    "canada", "american", "americans", "european", "china", "chinaman", "britain",
    "canadian", "latino", "mongolian", "asian", "indian", "india", "english",
    "british", "america", "columbia", "ontario", "australia", "australian",
    "germans", "german", "chinamen", "italian", "italy", "french", "france"
]

pattern = re.compile(r"\b(" + "|".join(map(re.escape, ethnicities)) + r")\b", flags=re.IGNORECASE)

# Mask in any string
def mask_ethnicity(tokens):
    masked = []
    for tok in tokens:
        masked.append(pattern.sub("[MASK]", tok))
        
    return masked
```

```{python}
example_word = ["chinaman", "chinese women"]

mask_ethnicity(example_word)
```

```{python}
tokens = mask_ethnicity(tokens)

# Get unique words to avoid redundant computation
unique_tokens = list(set(tokens))


# Include the word "chinese" as our target
unique_tokens.append("chinese")

# Print the shape of unique tokens
print(f'There are {len(unique_tokens)} unique tokens in this corpus.')
```

```{python}
# Prepare a dictionary to store word embeddings
bert_word_embeddings = {}

# For each word, get its BERT embedding by feeding it as a single-token input
for word in unique_tokens:
    word_inputs = tokenizer(word, return_tensors='pt', truncation=True, max_length=10)
    with torch.no_grad():
        word_outputs = bert_model(**word_inputs)
        # Use the [CLS] token embedding as the word embedding
        word_embedding = word_outputs.last_hidden_state[:, 0, :].squeeze().numpy()
        bert_word_embeddings[word] = word_embedding
    
```

```{python}
# Print embedding for the word of interest 'chinese'

print(f"BERT embedding for 'chinese':\n{bert_word_embeddings.get('chinese')}")
```

```{python}
# Compute cosine similarity between all words with Chinese in the model
from scipy.spatial.distance import cosine

similarity_scores = {}

for other_word in bert_word_embeddings.keys():
    if other_word != "chinese":
        similarity = 1 - cosine(bert_word_embeddings["chinese"], bert_word_embeddings[other_word])
        similarity_scores[other_word] = similarity

# Sort by cosine similarity
sorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)

# Print the top 10 most similar words
print("Top 10 most similar words to 'chinese':")
for word, score in sorted_similarity[:10]:
    print(f"{word}: {score:.4f}")
```

```{python}
similarity_scores = {}

for other_word in bert_word_embeddings.keys():
    if other_word != "commerce":
        similarity = 1 - cosine(bert_word_embeddings["commerce"], bert_word_embeddings[other_word])
        similarity_scores[other_word] = similarity

# Sort by cosine similarity
sorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)

# Print the top 10 most similar words
print("Top 10 most similar words to 'commerce':")
for word, score in sorted_similarity[:10]:
    print(f"{word}: {score:.4f}")
```

```{python}
emd = np.array(bert_word_embeddings.get('chinese')) - np.array(bert_word_embeddings.get('alien'))

similarity_scores = {}

for other_word in bert_word_embeddings.keys():
    similarity = 1 - cosine(emd, bert_word_embeddings[other_word])
    similarity_scores[other_word] = similarity

# Sort by cosine similarity
sorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)

# Print the top 10 most similar words
print("Top 10 most similar words to 'chinese - alien':")
for word, score in sorted_similarity[:10]:
    print(f"{word}: {score:.4f}")
```

```{python}
# Generate a 2D PCA for visualiaztion
from sklearn.decomposition import PCA
pca = PCA(n_components=2)

word_embeddings = np.array(list(bert_word_embeddings.values()))
pca_results = pca.fit_transform(word_embeddings)
```

```{python}
import plotly.express as px
df_pca = pandas.DataFrame(pca_results, columns = ['x', 'y'])
df_pca['word'] = list(bert_word_embeddings.keys())
# Highlight the word 'chinese' in the plot
df_pca['highlight'] = df_pca['word'].apply(lambda x: 'chinese' if x == 'chinese' else '')

fig = px.scatter(
    df_pca,
    x='x',
    y='y',
    title=' Visualization of 2D PCA of the legal-BERT Word Embeddings',
    color='highlight',                        
    hover_data=['word'], 
    text= 'highlight'
)

fig.show()
```

```{python}
# Generate a t-SNE plot for visualization
from sklearn.manifold import TSNE
tsne = TSNE(n_components=2, random_state=42)

tsne_results = tsne.fit_transform(word_embeddings)
```

```{python}
# Create a DataFrame for visualization
df_tsne = pandas.DataFrame(tsne_results, columns=['x', 'y'])
df_tsne['word'] = list(bert_word_embeddings.keys())
# Highlight the word 'chinese' in the plot
df_tsne['highlight'] = df_tsne['word'].apply(lambda x: 'chinese' if x == 'chinese' else '')

fig = px.scatter(
    df_tsne,
    x='x',
    y='y',
    title='t-SNE Visualization of legal-BERT Word Embeddings',
    color='highlight',                        
    hover_data=['word'], 
    text= 'highlight'
)

fig.show()
```

### Sentence Embeddings

```{python}
from pathlib import Path

# Read the txt file as lines
lines = Path("data/Regina_V_Wing_Chong.txt").read_text(encoding="utf-8").splitlines()

# Extract line 67 as the target
target = lines[91]
print("Line 91:", target)
```

```{python}
paragraphs = [p.strip() for p in full_text.split("\n\n") if p.strip()]

for paragraph in paragraphs[:5]:
    print(paragraph)
```

```{python}
from sentence_transformers import SentenceTransformer

# Import the sentence transformer model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Calculate embeddings by calling model.encode()
paragraph_embeddings = model.encode(paragraphs, convert_to_tensor=True)
print(paragraph_embeddings.shape)
```

```{python}
# We also want to encode the target separately
target_embedding = model.encode(target, convert_to_tensor=True)
```

```{python}
import torch
from torch.nn.functional import cosine_similarity

# Calculate the cosine similarity
sims = cosine_similarity(target_embedding.unsqueeze(0), paragraph_embeddings)

k = min(10, sims.shape[0])

topk = torch.topk(sims, k=k-1)

top_paragraphs = []

for score, idx in zip(topk.values, topk.indices):
    top_paragraphs.append(paragraphs[idx])
    print(f"{score:.4f}\t{paragraphs[idx]}")
```

```{python}
import spacy

# Tokenize the text into sentences
nlp = spacy.load("en_core_web_sm")
doc = nlp(full_text)
sentences = [sent.text.strip() for sent in doc.sents]

print(sentences)
```

```{python}
sentence_embeddings = model.encode(sentences, convert_to_tensor=True)
print(sentence_embeddings.shape)
```

```{python}
# Calculate the cosine similarity
sims = cosine_similarity(target_embedding.unsqueeze(0), sentence_embeddings)

k = min(10, sims.shape[0])

topk = torch.topk(sims, k=k-1)

for score, idx in zip(topk.values, topk.indices):
    print(f"{score:.4f}\t{sentences[idx]}")
```

We apply a trained model to mask key words related to ethnicity and nationality identities.

```{python}
from transformers import pipeline
import numpy

ner = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english", grouped_entities=True)

def mask_ethnicity_hf(text):
    entities = ner(text)
    spans_to_mask = [e for e in entities if e["entity_group"] == "MISC" or e["entity_group"] == "ORG" or e["entity_group"] == "PER" or e["entity_group"] == "LOC" or e["entity_group"] == "NORP"]
    # typically nationality is in MISC or NORP depending on the model
    masked = text
    for ent in sorted(spans_to_mask, key=lambda e: e["start"], reverse=True):
        masked = masked[:ent["start"]] + "[MASK]" + masked[ent["end"]:]
    return masked
```

```{python}
def mask_ethnicity(texts):
    masked_list = []
    for sent in texts:
        sent = mask_ethnicity_hf(sent)
        masked_list.append(sent)
        
    return masked_list
```

```{python}
# Example output applying this pre-trained model
example_text = """And when this happens, and when we allow freedom ring, when we let it ring from every village and every hamlet, 
from every state and every city, we will be able to speed up that day when all of God's children, Black men and white men, 
Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual: Free at last. 
Free at last. Thank God almighty, we are free at last."""

masked_example = mask_ethnicity_hf(example_text)

print(masked_example)
```

```{python}
masked_paragraphs = mask_ethnicity(paragraphs)

masked_paragraphs[40]
```

```{python}
# Calculate embeddings by calling model.encode()
masked_paragraph_embeddings = model.encode(masked_paragraphs, convert_to_tensor=True)
print(masked_paragraph_embeddings.shape)
```

```{python}
# Calculate the cosine similarity
sims = cosine_similarity(target_embedding.unsqueeze(0), masked_paragraph_embeddings)

k = min(10, sims.shape[0])

topk = torch.topk(sims, k=k-1)

top_masked_paragraphs = []

for score, idx in zip(topk.values, topk.indices):
    top_masked_paragraphs.append(masked_paragraphs[idx])
    print(f"{score:.4f}\t{masked_paragraphs[idx]}")
```

### Natural Language Inference

```{python}
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# Choose a strong NLI model
model_name = "lexlms/legal-roberta-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model     = AutoModelForSequenceClassification.from_pretrained(model_name)

# Create an NLI pipeline
nli = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    device=-1,                   
    return_all_scores=True        
)

# Define the premise
premise = "Chinese immigrants should enjoy equal rights and legal protections."

results = []
for sent in top_masked_paragraphs:
    inputs = tokenizer.encode_plus(premise, sent, return_tensors="pt", truncation=True)
    out = model(**inputs).logits.softmax(dim=-1).tolist()[0]
    label_idx = out.index(max(out))
    label = ["FAVOR", "NEUTRAL", "AGAINST"][label_idx]
    results.append((sent, label, dict(zip(["favor","neutral","against"], out))))

# Print stance results
for sent, label, probs in results:
    print(f"{label.lower():>12}  {probs[label.lower()]:.2f}  ->  {sent}")
```

```{python}
from transformers import pipeline
import pandas as pd

# Load the MNLI‑based zero‑shot classifier
classifier = pipeline(
    "zero-shot-classification",
    model="facebook/bart-large-mnli",
    device=-1
)

# Use the NLI labels as your “candidate labels”
candidate_labels = ["entailment", "neutral", "contradiction"]

records = []
for para in paragraphs:
    out = classifier(
        sequences=para,
        candidate_labels=candidate_labels,
        hypothesis_template="Given the context that the texts for classification are from a legal ruling in 1885, this paragraph is {} of the premise 'Chinese immigrants should enjoy equal rights and legal protections'."
    )
    # out['labels'] is sorted by score descending
    scores = dict(zip(out["labels"], out["scores"]))
    pred = out["labels"][0]

    records.append({
        "paragraph": para,
        "entailment":   scores.get("entailment", 0.0),
        "neutral":      scores.get("neutral",    0.0),
        "contradiction":scores.get("contradiction", 0.0),
        "predicted":    pred
    })

#  Build a DataFrame
df_nli = pd.DataFrame(records)

# Inspect the first few rows
print(df_nli.head())
```

```{python}
df_nli.shape
```

```{python}
counts = df_nli['predicted'].value_counts()

proportions = df_nli['predicted'].value_counts(normalize=True)

result = pd.DataFrame({
    'count': counts,
    'proportion': proportions
})

print(result)
```

### Topic Modelling Through BERTopic

```{python}
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer
from sklearn.feature_extraction.text import CountVectorizer


ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)
vectorizer = CountVectorizer(stop_words="english", ngram_range=(1,2), max_df=0.85, min_df=2)

topic_model = BERTopic(
    vectorizer_model=vectorizer, 
    ctfidf_model= ctfidf_model
)

topics, probs = topic_model.fit_transform(paragraphs)

df_topic = topic_model.get_topic_info()
print(df_topic)
```

```{python}
for para in df_topic["Representative_Docs"][1]:
    print(para)
```

```{python}
rep_list = []

for list in df_topic["Representation"]:
    rep_list.extend(list)
    
print(rep_list)
```

```{python}
topic_labels = topic_model.generate_topic_labels(
    nr_words=3,       
    separator=" ",     
    topic_prefix=False 
)

topic_list = []

for label in topic_labels:
    phrases = label.split(" ")
    topic_list.extend(phrases)
    
print(topic_list)
```

```{python}
from umap import UMAP
import pandas as pd
import plotly.express as px

# Get your topic-term embeddings
embeddings = topic_model.c_tf_idf_.toarray()

# 1) Build a UMAP reducer with random init
umap_model = UMAP(
    n_neighbors=15,
    n_components=2,
    metric="cosine",
    init="random",
    random_state=42
)
reduced_embeddings = umap_model.fit_transform(embeddings)

# 2) Build a DataFrame and scatter
df = pd.DataFrame(reduced_embeddings, columns=["x", "y"])
df["topic"] = topic_model.get_topic_info()["Topic"].values

fig = px.scatter(
    df,
    x="x",
    y="y",
    text="topic",
    title="Topic visualization"
)
fig.show()
```

```{python}
from sentence_transformers import SentenceTransformer

embedding_model = SentenceTransformer("nlpaueb/legal-bert-base-uncased")

```

```{python}
from bertopic import BERTopic
from bertopic.vectorizers import ClassTfidfTransformer

topic_model = BERTopic(embedding_model=embedding_model,
                       vectorizer_model= vectorizer,
                       ctfidf_model= ctfidf_model)

topics, probs = topic_model.fit_transform(masked_paragraphs)

df_topic = topic_model.get_topic_info()
print(df_topic)
```

```{python}
for para in df_topic['Representative_Docs']:
    print(para)
```

```{python}
topic_labels = topic_model.generate_topic_labels(
    nr_words=5,       
    separator=" ",     
    topic_prefix=False 
)

topic_labels
```

```{python}
from umap import UMAP
import pandas as pd
import plotly.express as px

# Get your topic-term embeddings
embeddings = topic_model.c_tf_idf_.toarray()

# 1) Build a UMAP reducer with random init
umap_model = UMAP(
    n_neighbors=15,
    n_components=2,
    metric="cosine",
    init="random",
    random_state=42
)
reduced_embeddings = umap_model.fit_transform(embeddings)

# 2) Build a DataFrame and scatter
df = pd.DataFrame(reduced_embeddings, columns=["x", "y"])
df["topic"] = topic_model.get_topic_info()["Topic"].values

fig = px.scatter(
    df,
    x="x",
    y="y",
    text="topic",
    title="Topic visualization"
)
fig.show()
```


