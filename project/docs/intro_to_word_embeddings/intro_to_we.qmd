---
title: Introduction to Word Embeddings
jupyter: python3
---

Module adapted from UBC COMET, prepared originally by Laura Nelson and COMET team members Jonathan Graves, Angela Chen and Anneke Dresselhuis, by Anna Kovtunenko and the prAxIs UBC team. 

## Learning Objectives
- Use critical and reflexive thinking to gain a deeper understanding of how the inherent social and cultural biases of language are reproduced and mapped into language computation models

## Vocabulary and Key Concepts
- Machine learning (ML)
- Natural language processing (NLP)
- vector
- dimensionality

## What are Word Embeddings?
Word embeddings are one way that humans can represent language in a way that is legible to a machine. More specifically, they are an NLP approach and type of word-vector encoding that use vectors to store textual data in a multi-dimensional space. Storing this data in multiple dimensions allows for the inclusion of valuable semantic meaning of words and relationships between words.

For example, imagine we are trying to answer the question “How many times does the word ‘candidate’ show up in tweets during a defined electoral period?”. A basic approach to this problem is using a simple word frequency analysis to count how many times ‘candidate’ appears in our set of tweets, but this method overlooks nuanced information about how the word may be used. To understand what kind of language, biases or attitudes contextualize the term, “candidate” in discourse, we would need to use a method like word embedding to encode meaning into our understanding of how people have talked about candidates over time. Instead of describing our text as a series of word counts, we would treat our text like coordinates in space, where similar words and concepts are closer to each other, and words that are different from each other are further away.

## A Brief Review of Vectors
A mathematical object that has both a magnitude and a direction. You can think of it as an arrow pointing from one location to another in space. This arrow represents both a size (how long it is) and a direction (the way it’s pointing).

## Making a word embedding
So, how do word embeddings work? To make a word embedding, an input word gets compressed into a dense vector.

The magic and mystery of the word embedding process is that often the vectors produced during the model embed qualities of a word or phrase that are not interpretable by humans. However, for our purposes, having the text in vector format is all we need. With this format, we can perform tests like cosine similarity (which we will discuss later) and other kinds of operations. Such operations can reveal many different kinds of relationships between words, as we’ll examine a bit later.

## Word2vec
So, how can we actually generate word embeddings? This is where Word2Vec comes in: Word2vec is a machine learning model designed to represent words as vectors that capture semantic relationships. It generates **low-dimensional word embeddings** by learning from word contexts in a large corpus, allowing words with similar meanings to have vectors close together. Rather than being a pre-trained model with pre-existing knowledge, Word2Vec is an algorithmic tool that learns patterns and relationships between words from the specific text it is given. The series of algorithms inside of the word2vec model try to describe and acquire parameters for a given word in terms of the text that appear immediately to the right and left in actual sentences. Essentially, it learns how to predict text.

Without going too deep into the algorithm, suffice it to say that it involves a two-step process:

1. First, the input word gets compressed into a dense vector, as seen in the simplified diagram, “Creating a Word Embedding,” above.
2. Second, the vector gets decoded into the set of context words. Keywords that appear within similar contexts will have similar vector representations in between steps.

Word2vec can operate using one of two model architectures: Continuous Bag of Words (CBOW) and Skip-Gram. The two variants can be distinguished partly by their input and output during training.

- Continuous Bag of Words (CBOW) → **CBOW** takes the context words (for example, “Call”,“Ishmael”) as a single input and tries to predict the word of interest (“me”).
- Skip-Gram → **Skip-Gram** does the opposite, taking a word of interest as its input (for example, “me”) and tries to learn how to predict its context words (“Call”,“Ishmael”).

In general, CBOW is is faster and does well with frequent words, while Skip-Gram potentially represents rare words better.

Since the word embedding is a vector, we are able perform tests like cosine similarity (which we’ll learn more about in a bit!) and other kinds of operations. Those operations can reveal many different kinds of relationships between words, as we shall see.

## Bias and Language Models
You might already be piecing together that the encoding of meaning in word embeddings is entirely shaped by the patterns of language use captured in the training data. That is, what is included in a word embedding directly reflects the complex social and cultural biases of everyday human language - in fact, exploring how these biases function and change over time (as we will do later) is one of the most interesting ways to use word embeddings in social research.

#### It is simply impossible to have a bias-free language model (LM). <br>

In LMs, bias is not a bug or a glitch, rather, it is an essential feature that is baked into the fundamental structure. For example, LMs are not outside of learning and absorbing the pejorative dimensions of language which in turn, can result in reproducing harmful correlations of meaning for words about race, class or gender (among others). When unchecked, these harms can be “amplified in downstream applications of word embeddings” ([Arseniev-Koehler & Foster, 2020, p. 1](https://osf.io/preprints/socarxiv/b8kud/)).

Just like any other computational model, it is important to critically engage with the source and context of the training data. One way that [Schiffers, Kern and Hienert](https://arxiv.org/abs/2302.06174v1) suggest doing this is by using domain specific models (2023). Working with models that understand the nuances of your particular topic or field can better account for “specialized vocabulary and semantic relationships” that can help make applications of word embeddings more effective.

## Preparing for our analysis 
### Word2vec Features

**Here are a few features of the word2vec tool that we can use to customize our analysis:**

- `size`: Number of dimensions for word embedding model
- `window`: Number of context words to observe in each direction
- `min_count`: Minimum frequency for words included in model
- `sg` (Skip-Gram): ‘0’ indicates CBOW model; ‘1’ indicates Skip-Gram
- `alpha`: Learning rate (initial); prevents model from over-correcting, enables finer tuning
- `iterations`: Number of passes through dataset
- `batch size`: Number of words to sample from data during each pass

*Note: the script uses default value for each argument.*

**Some limitations of the word2vec Model**

- Within word2vec, common articles or conjunctions, called **stop words** such as “the” and “and,” may not provide very rich contextual information for a given word, and may need additional subsampling or to be combined into a word phrase (Anwla, 2019).
- word2vec isn’t always the best at handling out-of-vocabulary words well (Chandran, 2021).

Let’s begin our analysis!

## Example #1: Liberty, Freedom, and Taxation


```{python}
#| vscode: {languageId: plaintext}
# !pip install matplotlib
# !pip install pandas

%pylab inline
matplotlib.style.use('ggplot')
```

### Create a Document-Term Matrix (DTM) with a Few Pseudo-Texts

To start off, we’re going to create a mini dataframe called a **Document-Term Matrix (DTM)**. A **DTM** is a matrix (or in our case, a tidyverse dataframe) that represents the frequency of terms (words) appearing in a collection of documents. Our DTM is based on the use of the words "liberty,” “freedom” and “taxation” found in three different texts: A, B and C.

```{python}
#| vscode: {languageId: plaintext}
# dataframes!
import pandas

# Construct dataframe with three novels each containing three words
columns = ['liberty','freedom','taxation']
indices = ['Text A', 'Text B', 'Text C']
dtm = [[50,60,60],[90,10,10], [20,70,70]]
dtm_df = pandas.DataFrame(dtm, columns = columns, index = indices)

# Show dataframe
dtm_df
```

### Visualize
We’ll start by graphing all three axes using the plotly library:

```{python}
#| vscode: {languageId: plaintext}
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(dtm_df['sausage'], dtm_df['bacon'], dtm_df['eggs'], color='blue', s=50)

ax.set_xlabel('Liberty')
ax.set_ylabel('Freedom')
ax.set_zlabel('Taxation')

for novel, row in dtm_df.iterrows():
    ax.text(row['sausage'], row['bacon'], row['eggs'], novel, fontsize=9, ha='right')

plt.show()
```

```{python}
#| vscode: {languageId: plaintext}
Now, let’s take a look at just two axes, liberty and taxation.
```

```{python}
#| vscode: {languageId: plaintext}
# Plot our points
scatter(dtm_df['taxation'], dtm_df['liberty'])

# Make the graph look good
xlim([0,100]), ylim([0,100])
xlabel('taxation'), ylabel('liberty')
```

### Vectors
At a glance, a couple of points are lying closer to one another. We used the word frequencies of just two of the three words (liberty and taxation) in order to plot our texts in a two-dimensional plane. The term frequency “summaries” of Text A & Text C are pretty similar to one another: they both share a major concern with “taxation”, whereas Novel B seems to focus primarily on “liberty”.

This raises a question: how can we turn our intuition that the spatial distance presented here expresses topical similarity into something we can measure and analyze?

## Cosine Similarity
The most common measurement of distance between points is their Cosine Similarity. Cosine similarity can operate on textual data that contain word vectors and allows us to identify how similar documents are to each other, for example. Cosine Similarity thus helps us understand how much content overlap a set of documents have with one another.

Using our example above, we can see that the angle from (0,0) between Novel C and Novel A (orange triangle) is smaller than between Novel A and Novel B (navy triangle) or between Novel C and Novel B (both triangles together).

Because this similarity measurement uses the cosine of the angle between vectors, the magnitude is not a matter of concern (this feature is really helpful for text vectors that can often be really long!). Instead, the output of cosine similarity yields a value between 0 and 1 (we don’t have to work with something confusing like 18º!) that can be easily interpreted and compared - and thus we can also avoid the troubles associated with other dimensional distance measures such as [Euclidean Distance](https://en.wikipedia.org/wiki/Euclidean_distance).

To better understand, imagine you’re standing in the middle of a room. If you point one arm at "liberty" and the other at "freedom," your arms are nearly aligned, meaning those words are similar and the cosine angle is close to 0. If one arm points to "liberty" and the other to "taxation," they form a wide angle with a value close to 1, meaning they're not very similar.

