[
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook_II.html",
    "href": "docs/7_Network_Analysis/network_analysis_notebook_II.html",
    "title": "Networks and NetworkX II",
    "section": "",
    "text": "This notebook continues in demonstrating more advanced concepts in network analysis pertaining to archeology and provides a hands-on tutorial to using the NetworkX Python library for these applications."
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook_II.html#path-analysis",
    "href": "docs/7_Network_Analysis/network_analysis_notebook_II.html#path-analysis",
    "title": "Networks and NetworkX II",
    "section": "4 Path Analysis",
    "text": "4 Path Analysis\nWhat is path analysis? In this context it is finding the shortest, average or longest path between two or more nodes.\n\n4.1 Path Analysis on a small graph\nWe can use path analysis to find shortest path from a point or average path lenght. We are using new notation in our nx.Graph function we are adding a lenght between points which is shown as ('B', 'D', LENGHT) this lenght will be shown in the graph and has to be taken into account for finding the shortest path.\nThe other new important command fron NetworkX we are using is nx.shortest_path which takes a start and end point then finds the lenght and shortest path between them. We will look for the shortest path from node 9 to node 5. Then using the command nx.draw_networkx_edges we will highlight the shortest path in blue.\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport random\n\n# Set a random seed for reproducibility\nrandom.seed(3)\n\n# Create a random regular graph\nn = 12  # number of nodes\nd = 3   # degree of each node\nG = nx.random_regular_graph(d, n)\n\n# Assign random weights to edges\nfor (u, v) in G.edges():\n    G[u][v]['weight'] = random.randint(1, 10)\n\ndef find_shortest_path(G, start, end):\n    try:\n        path = nx.shortest_path(G, start, end, weight='weight')\n        length = nx.shortest_path_length(G, start, end, weight='weight')\n        return path, length\n    except nx.NetworkXNoPath:\n        return None, None\n\ndef main():\n    # Create a random regular graph\n    n = 10  # number of nodes (increased to at least 10)\n    d = 3   # degree of each node\n    G = nx.random_regular_graph(d, n)\n\n    # Assign random weights to edges\n    for (u, v) in G.edges():\n        G[u][v]['weight'] = random.randint(1, 10)\n\n    # Find shortest path from node 4 to node 9\n    start, end = 9, 5\n    path, length = find_shortest_path(G, start, end)\n    if path:\n        print(f\"Shortest path from {start} to {end}: {' -&gt; '.join(map(str, path))}\")\n        print(f\"Path length: {length}\")\n  \n    # Visualize the graph\n    pos = nx.spring_layout(G, seed=42)\n    \n    # Draw the graph\n    nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=16, font_weight='bold')\n    \n    # Draw edge labels\n    edge_labels = nx.get_edge_attributes(G, 'weight')\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n    \n    # Highlight the shortest path in blue\n    if path:\n        path_edges = list(zip(path, path[1:]))\n        nx.draw_networkx_edges(G, pos, edgelist=path_edges, edge_color='b', width=2)\n    \n    plt.title(\"Random Regular Graph with Shortest Path\")\n    plt.axis('off')\n    plt.show()\n\nif __name__ == \"__main__\":\n    main()\n\n\n\n4.2 Path Analysis on our data\nLet’s say we are intested in finding the shortest path from Veloukovo in mainland Greece to Delos (an island in the Aegean). If we were to just look at the graph it would be difficult to find. The code below will find the shortest path.\nOnce again we will use nx.shortest_path this can also be easily edited to find the shortest path from any node name. It can be done by editing the variables start and end.\n\n# Find shortest path from Veloukovo to Delos\nStart = \"Veloukovo\"\nEnd = \"Delos\"\n\n# use a full path\nG_full = nx.from_pandas_edgelist(df_edges, source=\"SOURCE_LOCATION\", target=\"TARGET_LOCATION\")\n\nG_sub = G_full.subgraph(filtered_nodes)\n\nshortest_path = nx.shortest_path(G_full, source=Start, target=End)\n\n# Calculate the path length\npath_length = nx.shortest_path_length(G_full, source=Start, target=End)\n\n# Print the shortest path and length\nprint(f\"Shortest path from {Start} to {End}:\")\nprint(\" -&gt; \".join(shortest_path))\nprint(f\"Path length: {path_length}\")\n\nIf we want to visualy demonstate this distance we can plot it on the map with a blue line. The code below will do this and using the command nx.draw_networkx_edges we can draw a blue line to demonstate the optimal path.\n\n#Create a full map\nG_full = nx.from_pandas_edgelist(df_edges, source=\"SOURCE_LOCATION\", target=\"TARGET_LOCATION\")\n\nG_sub = G_full.subgraph(filtered_nodes)\n\n#Find the positions of Nodes\npositions = {loc: (gdf_pos_filtered.loc[loc].geometry.x, gdf_pos_filtered.loc[loc].geometry.y) for loc in gdf_pos_filtered.index}\n\n# Plot the graph\nfig, ax = plt.subplots(figsize=(8, 8))\n\n# Plot filtered nodes and edges\nnx.draw_networkx_edges(G_sub, pos=positions, ax=ax, alpha=0.1)\nnx.draw_networkx_nodes(G_sub, pos=positions, ax=ax, node_size=20, node_color=\"red\", alpha=0.6)\n\n# Highlight the shortest path (only the part within the filtered area)\npath_edges = list(zip(shortest_path, shortest_path[1:]))\npath_edges_filtered = [edge for edge in path_edges if edge[0] in filtered_nodes and edge[1] in filtered_nodes]\nnx.draw_networkx_edges(G_sub, pos=positions, ax=ax, edgelist=path_edges_filtered, edge_color='blue', width=2)\n\n# Highlight Veloukovo and Delos if they're in the filtered area\nhighlight_nodes = [node for node in [\"Veloukovo\", \"Delos\"] if node in filtered_nodes]\nnx.draw_networkx_nodes(G_sub, pos=positions, ax=ax, nodelist=highlight_nodes, \n                       node_color=\"yellow\", node_size=100, edgecolors=\"black\")\n\n# Add labels for Veloukovo and Delos if they're in the filtered area\nlabels = {node: node for node in highlight_nodes}\nnx.draw_networkx_labels(G_sub, pos=positions, ax=ax, labels=labels, \n                        font_size=10, font_weight=\"bold\")\nax.axis('off')\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\nax.set_xlim(min_x, max_x)\nax.set_ylim(min_y, max_y)\n\nplt.tight_layout()\nplt.show()\n\nOur dataset is very interconnected with most nodes having over 20 edges, but for a less interconnected network this analysis could prove very useful to find the shortest path."
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook_II.html#diffusion",
    "href": "docs/7_Network_Analysis/network_analysis_notebook_II.html#diffusion",
    "title": "Networks and NetworkX II",
    "section": "5 Diffusion",
    "text": "5 Diffusion\nAnother application of network analysis is to do with the diffusion of innovation, ideas or a disease through a network. We will demonstrate it below with a gif. The code is complicated, but the goal for it is to act as an interesting demonstarting of what is possible with Network Analysis.\n\n#Import specific libraries to create a gif\nimport matplotlib.animation as animation\nfrom IPython.display import HTML, display\nimport base64\n\n# Create a function to simulate the diffusion\ndef simulate_diffusion(G, initial_infected, steps, infection_probability):\n    infected = set(initial_infected)\n    infection_times = {node: 0 for node in initial_infected}\n    \n    for step in range(1, steps + 1):\n        new_infected = set()\n        for node in infected:\n            for neighbor in G.neighbors(node):\n                if neighbor not in infected and neighbor not in new_infected:\n                    if random.random() &lt; infection_probability:\n                        new_infected.add(neighbor)\n                        infection_times[neighbor] = step\n        infected.update(new_infected)\n        yield infected, infection_times\n\ndef update(frame):\n    infected, infection_times = frame\n    colors = ['red' if node in infected else 'blue' for node in G.nodes()]\n    ax.clear()\n    nx.draw(G, pos, node_color=colors, with_labels=True, ax=ax)\n    ax.set_title(f\"Time step: {max(infection_times.values())}\")\n    return ax\n\n# Create a graph\nG = nx.karate_club_graph()\npos = nx.spring_layout(G)\n\n# Set up the initial infected nodes and parameters\ninitial_infected = [0]\ninfection_probability = 0.15\nsteps = 30\n\n# Set up the figure and axis\nfig, ax = plt.subplots(figsize=(10, 8))\n\n# Create the animation\nani = animation.FuncAnimation(\n    fig, \n    update, \n    frames=simulate_diffusion(G, initial_infected, steps, infection_probability),\n    interval=750, \n    repeat=False,\n    save_count=steps\n)\n\n# Save the animation as GIF\nani.save('diffusion_process.gif', writer='pillow')\nprint(\"Animation saved as 'diffusion_process.gif'\")\n\n# Display the saved GIF in the notebook\nwith open(\"diffusion_process.gif\", \"rb\") as file:\n    gif_data = file.read()\n    encoded_gif = base64.b64encode(gif_data).decode('utf-8')\n    display(HTML(f'&lt;img src=\"data:image/gif;base64,{encoded_gif}\" width=\"600\"&gt;'))\n\nplt.close(fig)\n\nAbove is a demonstration of diffusion across a graph, you can customize the infection rate, which nodes start infected and the shape of the graph itself."
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook_II.html#interactive-network-exploration",
    "href": "docs/7_Network_Analysis/network_analysis_notebook_II.html#interactive-network-exploration",
    "title": "Networks and NetworkX II",
    "section": "6. Interactive Network Exploration",
    "text": "6. Interactive Network Exploration\nUsing Python and Network Analysis you can construct interesting and interactive graphical displays. These range from zoom sliders, hoverable names and OTHERS\n\n6.1 Interactive Zoom Slider\nHere we will have interactive bars on the top of the screen demonstrating X and Y ranges which can be changed and the graph will zoom in accordingly. This is very helpful as when zoomed the graph ignores edges which run off the page making ledgibility much easier.\n\n# Define bounding box for filtered view\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\n\n\ndef plot_network(x_range, y_range):\n    # Filter GeoDataFrame and create subgraph\n    gdf_pos_filtered = gdf_pos.cx[x_range[0]:x_range[1], y_range[0]:y_range[1]]\n    filtered_nodes = gdf_pos_filtered.index.tolist()\n    G_sub = G.subgraph(filtered_nodes)\n\n\n    # Prepare positions for the filtered subgraph\n    positions_filtered = {loc: (gdf_pos_filtered.loc[loc].geometry.x, gdf_pos_filtered.loc[loc].geometry.y) for loc in gdf_pos_filtered.index}\n\n\n    # Plot the filtered subgraph on a real-world map\n    fig, ax = plt.subplots(figsize=(12, 12))\n    gdf_pos_filtered.plot(ax=ax, alpha=0)\n    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n\n    nx.draw_networkx_edges(G_sub, pos=positions_filtered, ax=ax, alpha=0.5)\n    nx.draw_networkx_nodes(G_sub, pos=positions_filtered, ax=ax, node_size=60, node_color=\"red\", edgecolors=\"black\", alpha=0.8)\n\n\n    plt.title(f\"Network Overlay on Real-World Map (Nodes: {len(G_sub.nodes)}, Edges: {len(G_sub.edges)})\")\n    plt.axis('off')\n    plt.show()\n\n\n# Create interactive sliders\nx_slider = FloatRangeSlider(\n    value=[min_x + 100000, max_x - 100000],\n    min=min_x,\n    max=max_x,\n    step=(max_x - min_x) / 100,\n    description='X Range:',\n    layout=Layout(width='500px')\n)\n\ny_slider = FloatRangeSlider(\n    value=[min_y + 100000, max_y - 100000],\n    min=min_y,\n    max=max_y,\n    step=(max_y - min_y) / 100,\n    description='Y Range:',\n    layout=Layout(width='500px')\n)\n\n# Display interactive widget\ninteract(plot_network, x_range=x_slider, y_range=y_slider)"
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook_II.html#citations",
    "href": "docs/7_Network_Analysis/network_analysis_notebook_II.html#citations",
    "title": "Networks and NetworkX II",
    "section": "7. Citations",
    "text": "7. Citations\nAl-Taie, M. Z., & Kadry, S. (2017). Python for graph and network analysis. In Advanced information and knowledge processing. https://doi.org/10.1007/978-3-319-53004-8\nAshkan et al. “Animate Graph Diffusion with NetworkX.” Stack Overflow, 1 Dec. 1960, stackoverflow.com/questions/31815454/animate-graph-diffusion-with-networkx.\nGeorgiev, Petko. “NetworkX: Network Analysis with Python.” Cambridge, Feb. 2015, www.cl.cam.ac.uk/teaching/1415/L109/l109-tutorial_2015.pdf.\nMcKinney, Trenton. “Introduction to Network Analysis in Python.” Trenton McKinney Github, 22 May 2020, trenton3983.github.io/posts/intro-network-analysis/.\n“Networkx/Networkx: Network Analysis in Python.” GitHub, github.com/networkx/networkx. Accessed 26 Sept. 2024."
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook_II.html#very-interactive-display---works-well",
    "href": "docs/7_Network_Analysis/network_analysis_notebook_II.html#very-interactive-display---works-well",
    "title": "Networks and NetworkX II",
    "section": "6.2 Very interactive display - works well",
    "text": "6.2 Very interactive display - works well\n\nfrom shapely.geometry import box\nimport geopandas as gpd\nimport plotly.graph_objects as go\n\n# Assuming gdf_pos is already defined and converted to EPSG:4326\n\n# Define bounding box for filtered view\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\n\n# Create a box geometry\nbbox_geometry = box(min_x, min_y, max_x, max_y)\n\n# Create a GeoDataFrame with the box geometry\nbbox = gpd.GeoDataFrame(geometry=[bbox_geometry], crs='EPSG:3857')\nbbox = bbox.to_crs('EPSG:4326')\nmin_lon, min_lat, max_lon, max_lat = bbox.total_bounds\n\n# Filter GeoDataFrame and create subgraph\ngdf_pos_filtered = gdf_pos.cx[min_lon:max_lon, min_lat:max_lat]\nfiltered_nodes = gdf_pos_filtered.index.tolist()\nG_sub = G.subgraph(filtered_nodes)\n\n# Create edge traces\nedge_traces = []\nfor edge in G_sub.edges():\n    x0, y0 = gdf_pos_filtered.loc[edge[0], 'geometry'].x, gdf_pos_filtered.loc[edge[0], 'geometry'].y\n    x1, y1 = gdf_pos_filtered.loc[edge[1], 'geometry'].x, gdf_pos_filtered.loc[edge[1], 'geometry'].y\n    edge_trace = go.Scattermapbox(\n        lon=[x0, x1],\n        lat=[y0, y1],\n        mode='lines',\n        line=dict(width=1, color='rgba(136, 136, 136, 0.5)'),\n        hoverinfo='none'\n    )\n    edge_traces.append(edge_trace)\n\n# Create node trace\nnode_trace = go.Scattermapbox(\n    lon=gdf_pos_filtered.geometry.x,\n    lat=gdf_pos_filtered.geometry.y,\n    mode='markers',\n    marker=dict(\n        size=10,\n        color='red',\n        opacity=0.8\n    ),\n    text=[f\"Node: {node}\" for node in G_sub.nodes()],\n    hoverinfo='text'\n)\n\n# Create the figure\nfig = go.Figure(data=edge_traces + [node_trace])\n\n# Update layout to use mapbox\nfig.update_layout(\n    mapbox=dict(\n        style=\"open-street-map\",\n        center=dict(lon=gdf_pos_filtered.geometry.x.mean(), lat=gdf_pos_filtered.geometry.y.mean()),\n        zoom=7\n    ),\n    showlegend=False,\n    margin={\"l\":0,\"r\":0,\"t\":0,\"b\":0}\n)\n\nfig.show()"
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html",
    "title": "Networks and NetworkX",
    "section": "",
    "text": "This notebook introduces key concepts in network analysis pertaining to archeology and provides a hands-on tutorial to using the NetworkX Python library."
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html#prerequisites",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html#prerequisites",
    "title": "Networks and NetworkX",
    "section": "0. Prerequisites",
    "text": "0. Prerequisites\n\nRead through and follow the instructions in the pre-reading.\n\nHave Python and Jupyter set up on your device.\nHave created the networks environment.\n\nIf you have not done this, see the pre-reading."
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html#what-is-network-analysis",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html#what-is-network-analysis",
    "title": "Networks and NetworkX",
    "section": "1. What is Network analysis?",
    "text": "1. What is Network analysis?\nNetwork Analysis is a set of techniques used to study the structure and dynamics of networks. Networks are collections of objects/locations/entities (called nodes) connected by relationships (called edges). Network analysis has applications in many fields, including sociology, biology, economics, computer science, and more.\n\n\n\nRegional networks of ceramic similarity across time in the greater Arizona/New Mexico area of the US (From Mills et al. 2013, Fig. 2)\n\n\nNetwork science in archeology is traditional network science used in an archeological context. According to Brighmans and Peeples (2023), archeological network science falls within the discipline of archeology and is not a discipline of its own: “[N]etwork science applied to archaeological research is a subset of archaeological research: it does not happen in isolation, it is not immune to the limitations of archaeological data nor does it replace archaeological theory.”\n\n\n\nArcheological network research process. (Brighmans and Peeples, 2023)\n\n\n\n1.1 Key terms\n\n\n\n\nAn example of a graph with nodes and edges.\n\n\n\n&lt;p&gt;&lt;b&gt;Node:&lt;/b&gt; A node is a representation of an individual entity or actor in a network. In different contexts, nodes can be people, organizations, cities, or any other unit of analysis.&lt;/p&gt;\n&lt;p&gt;&lt;b&gt;Edge:&lt;/b&gt;An edge represents the relationship or connection between two nodes. Edges can be directed (having a specific direction from one node to another) or undirected (no direction, implying a mutual relationship)&lt;/p&gt;\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\n&lt;p&gt;&lt;b&gt;Degree:&lt;/b&gt;  The degree of a node is the number of edges connected to it. In directed networks, this can be further divided into in-degree (number of incoming edges) and out-degree (number of outgoing edges).&lt;/p&gt;\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\n&lt;p&gt;The network above is an example of a &lt;b&gt;Undirected graph&lt;/b&gt;, a graph with no direction. This means that if there is a connection between examples node A and node B, it is bidirectional— A is connected to B, and B is connected to A.&lt;/p&gt;\n&lt;p&gt;The example to the left is a &lt;b&gt;directed graph&lt;/b&gt;: the edges between nodes have a specific direction. This means that if there is an edge from node A to node B, it does not imply there is an edge from B to A unless explicitly stated.&lt;/p&gt;\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\n&lt;p&gt;&lt;b&gt;Density:&lt;/b&gt;  Density is a measure that indicates how closely connected the nodes in a network are. Specifically, it refers to the ratio of the number of actual edges in the network to the maximum possible number of edges between nodes.&lt;/p&gt;\n\n\n\n\n\n\nAn example of a network with nodes colored by degree.\n\n\n\n&lt;p&gt;&lt;b&gt;Centrality:&lt;/b&gt; Centrality measures the importance, influence, or prominence of nodes (entities) within a network. In other words, the centrality of a node tells us how \"important\" a node is to the aggregate network. There are many different kinds of centrality, but the four most well-known ones are degree, betweenness, closeness, and eigenvector centrality. This notebook will primarily focus on the first three. \n\n\n\n\n1.2 NetworkX\nNetworkX is a Python library that is used for the creation, manipulation, and visualization of complex networks. It provides tools to work with both undirected and directed networks, perform network-related calculations, and visualize the results.\n\n1.2.1 Importing NetworkX\nWe can import NetworkX using the import command. At the same time, we’ll also import the matplotlib.pyplot library, for plotting graphs. Additionally, we’ll import pandas for basic data wrangling, and numpy for math. The as command allows us to use networkx commands without needing to type out networkx each time. Additionally, we’ll import the community_louvain package for the louvain clustering algorithm. Along with some other libraries for our code to function.\n\nimport matplotlib.pyplot as plt #allows us to call the matplotlib.pyplot library as 'plt'\nimport matplotlib.patches as mpatches #imports mpatches matplotlib subpackage \nimport networkx as nx #allows us to call the networkx library as 'nx'\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport contextily as ctx\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\nimport community as community_louvain\nimport random\nimport contextily as ctx\n\n\n\n1.2.2 Creating simple networks using NetworkX\nWe’ll start by creating a simple graph:\n\nG = nx.Graph() #creates an empty network graph\n\nnodes = (1, 2, 3, 4, 5, 6) #our nodes, labeled 1,2,3,4,5,6.\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\n        #the connections between our nodes are stored in an array, containing pairs of numbers called tuples.\nG.add_edges_from(edges) #the `add_edges_from()` command adds edges to the network\nG.add_nodes_from(nodes) #the `add_nodes_from()` command adds nodes to the network\n\nnx.draw(G, with_labels = True) #renders the graph in the notebook\n        #the `with_labels = True` argument specifies that we want labels on the nodes.\n\nLet’s create a directed graph using nx.DiGraph(). We’ll also set our node positions using a seed: this will ensure that each time the nodes are rendered they hold the same position on the graph. You can set the seed to any number.\n\nG = nx.DiGraph() #creates an empty directed graph object\nnodes = (1, 2, 3, 4, 5, 6) \nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nG.add_edges_from(edges) \nG.add_nodes_from(nodes) \n\nposition = nx.spring_layout(G, seed=100)\n\nnx.draw(G, pos = position, with_labels = True) # `pos` argument assigns a position to each node\n\n\n\n\n1.3 Creating Random Graphs\nInstead of creating a graph with predetermined positions of nodes and edges we can also generate a random graph with a set amount of nodes and edges. Below you can change the amount of nodes and edges by changing n and d which correspond to the number of nodes and the degree (number of edges) that each node has.\nThe first most basic command we will use is the nx.random_regular_graph command. Which generates a random regular graph.\n\n# Set a seed for reproducibility\nrandom.seed(42)\n\n# Parameters\nn = 20  # number of nodes\nd = 3   # degree of each node\n\n# Generate the random regular graph\nrr_graph = nx.random_regular_graph(d, n)\n\n# Visualize the graph, you can change the size, color, font and node size. \nplt.figure(figsize=(8, 6)) \nnx.draw(rr_graph, with_labels=True, node_color='lightgreen', node_size=500, font_size=10, font_weight='bold')\nplt.title(\"Random Regular Graph\")\nplt.show()\n\n# Print some basic information about the graph\nprint(f\"Number of nodes: {rr_graph.number_of_nodes()}\")\nprint(f\"Number of edges: {rr_graph.number_of_edges()}\")\nprint(f\"Degree of each node: {d}\")\n\nAnother option is using the Erdős-Rényi model which can be accessed using the nx.erdos_renyi_graph(n, p) command. This command has two inputs n and p. N is the number of nodes and p is the probability of edge creation to each node.\n\n# Set a seed for reproducibility\nrandom.seed(43)\n\n# Parameters\nn = 20  # number of nodes\np = 0.2  # probability of edge creation\n\n# Generate the Erdős-Rényi random graph\ner_graph = nx.erdos_renyi_graph(n, p)\n\n# Visualize the graph\nplt.figure(figsize=(8, 6))\nnx.draw(er_graph, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_weight='bold')\nplt.title(\"Erdős-Rényi Random Graph\")\nplt.show()\n\n# Print some basic information about the graph\nprint(f\"Number of nodes: {er_graph.number_of_nodes()}\")\nprint(f\"Number of edges: {er_graph.number_of_edges()}\")\nprint(f\"Average degree: {sum(dict(er_graph.degree()).values()) / n:.2f}\")\n\nThere are more commands in NetworkX to generate random graphs, but the two above demonstrate two common methods of random graph generations. The first being a set number of nodes and edges and the second being a set number of nodes and a probability of edge creation between them."
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html#degrees-density-and-weights",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html#degrees-density-and-weights",
    "title": "Networks and NetworkX",
    "section": "2. Degrees, Density and Weights",
    "text": "2. Degrees, Density and Weights\n\n2.1 Degrees\nThe degree of a node is the number of edges that are connected to a node. The degree of a node \\(N\\) is denoted as \\(deg(N)\\). The maximum degree of a network \\(G\\) is denoted by \\(\\Delta(G)\\) and is the degree of the node with the highest degree in the network. Conversely, the minimum degree is denoted as \\(\\delta(G)\\).\n\nIf a node on a graph with \\(n\\) nodes has degree \\(n-1\\) it is called a dominating vertex. Not every graph has a dominating vertex.\n\nWe can see the degree of each node by running dict(G.degree()). This create a dictionary of key-value pairs for our network, where each key is the name of the node and the value is it’s respective degree.\n\ndegrees = dict(G.degree())\n\nIf we want to see the degree of node \\(n\\), we can do so by running print(degrees[n]). For instance:\n\nprint(degrees[1])\n\nLet’s color the nodes of our graph based on their degree. We’ll create a function called get_node_colors which takes in the degree dictionary of each node and returns a color. We’ll then create a for-loop that iterates over each nodes in the list of nodes, gets the color of each node using the get_node_colors function we defined earlier, and appends it to an empty list called color_map.\n\ndegrees = dict(G.degree())\nnodes = list(G.nodes())\n\ndef get_node_colors(degree):\n    if degree in [1, 2]:\n        return 'blue'\n    elif degree in [3, 4]:\n        return 'green'\n    elif degree in [5, 6]:\n        return 'yellow'\n    else:\n        return 'red' \n\ncolor_map = [] #`color_map` is an empty list\n\nfor node in nodes:\n  color = get_node_colors(degrees[node]) # get color of current node using node_colors according to degree of node\n  color_map.append(color) # appends color of each node to color_map for each node in nodes\n\nprint(degrees)\nprint(nodes)\nprint(color_map)\n\nThe \\(n\\)-th entry in color_map corresponds to the \\(n\\)-th node in nodes. For instance, color_map[0] returns the color of the first node (1).\n\ncolor_map[0]\n\nWe can now color the nodes of our graph, using the color map we defined above. The node_color argument takes in an array or list of colors that it uses to color each node.\n\nG = nx.DiGraph() # creates an empty directed graph object\nnodes = (1, 2, 3, 4, 5, 6) \nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nG.add_edges_from(edges) \nG.add_nodes_from(nodes) \n\nposition = nx.spring_layout(G, seed=100)\n\nnx.draw(G, pos = position, node_color=color_map, with_labels=True) \n    # node_color argument colors the nodes based on a given list or array of colors, \n    # with the first color corresponding to the first node, second to the second node, etc.\n\nLet’s also add a legend to our graph, which gives information about the meaning of each color. We’ll do this using the mpatches subpackage we imported earlier.\n\nblue_patch = mpatches.Patch(color='blue', label='1-2 edges') \ngreen_patch = mpatches.Patch(color='green', label='3-4 edges')\nyellow_patch = mpatches.Patch(color='yellow', label='5-6 edges')\nplt.legend(handles=[blue_patch, green_patch, yellow_patch]) #adds legend to the plot\n\nnx.draw(G, pos = position, node_color=color_map, with_labels=True)\n\n\n\n2.2 Density\nDensity is defined as:\n\\[\n\\text{Density} = \\frac{\\text{Number of Possible Edges}}{\\text{Number of Actual Edges}}\n​\\]\nIn an undirected graph, the total number of edges is \\(\\frac{V\\times(V-1)}{2}\\), where V is the total number of nodes. In a directed graph, the total number of edges is \\(V\\times(V-1)\\), because a connection between point A and point B can either be from point A to point B, or to point A from point B (hence multiplying by 2).\n\nNote that self-loops (edges from and to the same node) are counted in the total number of edges but not in the maximum number of edges so graphs can have a density greater than 1.\n\nThe formula for undirected graph density is:\n\\[\n\\frac{2E}{V(V-1)}\n\\]\nAnd for directed graphs, it is:\n\\[\n\\frac{E}{V(V-1)}\n\\]\nWhere \\(E\\) is the number of edges in our graph and \\(V\\) is the number of nodes.\nWe can calculate the density of our graph:\n\nnx.density(G)\n\n\n\n2.3 Weights\nOften times, you may end up working with weighted graphs: for instance, these weights could correspond to popularity of roads in road networks, or the number of artifacts in common between sites.\nWe’ll standardize our weights to be between 1 and 2 (as otherwise the results are messy). We’ll do this using a for-loop, like we did with the degrees.\n\nG_weights = nx.DiGraph() #creating a new graph object called G_weights\nnodes = [1, 2, 3, 4, 5, 6]\nedges = [(1, 2), (2, 3), (3, 1), (1,5), (3,5), (4, 5), (4, 6), (6, 1), (6, 3), (6,4), (4, 3), (5, 5), (3, 5)]\nweights = [100, 50, 75, 50, 60, 100, 100, 75, 40, 50, 50, 100, 100] #add list of weights\nG_weights.add_edges_from(edges) \nG_weights.add_nodes_from(nodes) \n\nadjusted_weights = []\nfor weight in weights:\n    adjusted_weight = 1+ (max(weights)-weight)/(max(weights)-min(weights)) #standardizes weights to be between 1 and 2\n    adjusted_weights.append(adjusted_weight)\n\nposition = nx.spring_layout(G, seed=100)\n\nprint(adjusted_weights)\nnx.draw(G_weights, pos = position, width = adjusted_weights, with_labels = True) \n    # width argument take in a list or array of numbers corresponding to weights\n\nThis is great, but the results aren’t very clear. Let’s add a color gradient to the edges to represent different weights.\n\nnorm = plt.Normalize(min(weights), max(weights), clip=False) \n    #`plot.normalizes` normalizes the weights such that they are evenly distributed across the gradient spectrum\nedge_colors = plt.cm.Greys(norm(weights)) \n    # norm(weights) normalizes the weights \n    # plot.cm.greys() assigns the weights to color values\n    # edge_colors is a multidimensional array of RGBA color values corresponding to each edge\n\nfig, ax = plt.subplots() #explicitly specifying figure and axes in order to create a color bar\n\nnx.draw(G_weights, pos=position, edge_color=edge_colors, width=adjusted_weights, with_labels=True, ax=ax) \n    #ax = ax argument needed for color bar\n\n# Adding color bar\nsm = plt.cm.ScalarMappable(cmap=\"Greys\", norm=norm) # creates a scalarmappable object which acts \n                                                    # as a bridge between the numerical weight values and color map\nplt.colorbar(sm, ax=ax) #plotting color bar\n\n\n\n3. Adjacency matrices\nAn Adjacency matrix is a method of representing graphs in matrix form. In an adjacency matrix, the rows and columns correspond to the vertices (or nodes) of the graph. The entries of the matrix indicate whether pairs of vertices are adjacent or not in the graph. Normally, a value of 1 is assigned to entries where an edge is present, and 0 is assigned to entries where an edge is not. For a weighed graph, the weight of the edge is represented as a numerical value for entries where an edge is present.\nWe can convert our simple graph to an adjacency matrix:\n\nnx.to_pandas_adjacency(G)\n\nIf we want to use our weighted graph, we can use the following code:\n\n# len(edges) returns the total number of entries in the list of edges.\n# range(len(edges)): This generates a sequence of numbers from 0 to n-1 where n is len(edges), \n    #so the for-loop will run n times with i taking each value in that range, one at a time.\n\nfor i in range(len(edges)):\n    edge = edges[i] # retrieves the edge at position i in the list of edges\n    weight = weights[i] # retrieves the weight at position i in the list of weights\n    G_weights.add_edge(edge[0], edge[1], weight=weight) # adds an edge with a weight to the graph \n    \nnx.to_pandas_adjacency(G_weights, nodelist=nodes, weight='weight') #converts to pandas adjacency matrix with the weights in place\n\nWe can visualize our matrix using the code below. Note that instead of using nx.to_pandas_adjacency we use nx.to_numpy_array: this allows us to store the matrix in the form of an array.\n\nadj_matrix = nx.to_numpy_array(G_weights, nodelist=nodes, weight='weight')\n\n\nplt.figure(figsize=(8, 8)) #displays data as an image on a 2d raster; in our case, a numpy array\n\nplt.imshow(adj_matrix, cmap='gray_r')\n\nfor i in range(adj_matrix.shape[0]): #loops through each row of the matrix\n    for j in range(adj_matrix.shape[1]): #for each row, loops through each column of the matrix\n        plt.text(j, i, int(adj_matrix[i, j]),\n                 ha='center', va='center', color='red', size=30) #prints the value at that position in the matrix on the graph\n\nplt.title('Adjacency Matrix Visualization')\nplt.xlabel('Node Index')\nplt.ylabel('Node Index')"
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html#working-with-archeological-datasets",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html#working-with-archeological-datasets",
    "title": "Networks and NetworkX",
    "section": "4. Working with archeological datasets",
    "text": "4. Working with archeological datasets\nThe ICRATES database of tablewares in the Roman East is a dataset of red slip tablewares between the late Hellenistic and late Roman periods in the east Mediterranean (Bes, 2015). The dataset is in fact comprised of two datasets: the first is a dataset of nodes with their respected connections, as well as weights for each edge and other information. The second dataset contains geographical coordinates for each node in the network. We will use these coordinates to add a geospatial component to our analysis, by placing each node in it’s respective geographical coordinates and adding a map overlay. We begin by demonstrating how to import and visualize the dataset in python, and then introduce various centrality and connectivity measures.\nImportantly, the nodes in the first dataset are categorized by time period. We will initially focus on a singular time period, but will showcase how to graph multiple time frames towards the end of the notebook.\n\n4.1 Importing datasets in Python\nWe can import datasets in python using pandas’ read_ function. In our case, both network and location datasets are stored in excel files. We can thus use the read_excel function.\n\ndf_edges = pd.read_excel(\"vistorian_network.xls\") #loading in dataset\n\nG_directed = nx.from_pandas_edgelist(df_edges, source=\"SOURCE_LOCATION\", target=\"TARGET_LOCATION\", create_using=nx.DiGraph()) #creating a graph object\nG = nx.from_pandas_edgelist(df_edges, source=\"SOURCE_LOCATION\", target=\"TARGET_LOCATION\")\n\n\n\n4.2 Adding a geospacial component to our graph\nWe could just plot this network right away, but this wouldn’t convey good information, as the nodes in our dataset represent real places. Instead, let’s bind each node to it’s respective geographical location using the geopandas library.\n\ndf_pos = pd.read_excel(\"vistorian_locations.xls\", index_col=0)\n # loading in coordinates of nodes; index_col specifies that the first column (with index 0) in the Excel sheet should be used as the row labels for the DataFrame\n\ngdf_pos = gpd.GeoDataFrame(df_pos, geometry=gpd.points_from_xy(df_pos['LONGITUDE'], df_pos['LATITUDE']), crs='EPSG:4326') \n # creates a geodataframe, a special dataframe from the geopandas library for storing geographic datapoints.\n    # crs='EPSG:4326' argument defines the Coordinate Reference System for the geometric data: in our case, the location dataset uses the WGS84 coordinate system\n\ngdf_pos = gdf_pos.to_crs('EPSG:3857') #reprojecting to web mercator for the world map overlay to accurately show up on our graph\n\n\ngdf_pos #returns a GeoDataframe of all the nodes and their respective positions\n\n\ngdf_pos.loc['Abdera'] \n# the loc accessor in pandas (and by extension, GeoPandas, which extends pandas) is used to access a group of rows and columns by labels. This will be useful for our next step.\n\nGreat! we now have a GeoDataFrame of each node in our graph, with their respective position. However, networkX’s requires a dictionary of node-location pairs for properly mapping each node. Let’s create a new dictionary using the dataframe above.\n\npositions = {} #empty dictionary which will contain the positions of each node\nfor location in gdf_pos.index: # iterates over each index in the gdf_pos GeoDataFrame. The index in gdf_pos is the names of all our nodes.\n    x = gdf_pos.loc[location].geometry.x  # For each location, this line accesses the `geometry` column to retrieve the x-coordinate (latitude).\n    y = gdf_pos.loc[location].geometry.y  # Same thing but for y-coordinate (longitude)\n    positions[location] = (x, y)  # Adding to the dictionary with the location as the key\n\nNow we have a dictionary containing each node, and it’s geographic position. We can now plot our graph!\n\nfig, ax = plt.subplots(figsize=(9, 9))# Creating subplots so that we can overlay the network and the map\ngdf_pos.plot(ax=ax, alpha=0)\n\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)# Adding the basemap: we are using CartoDB for this\n\n# Drawing the network edges and nodes, as usual\nnx.draw_networkx_edges(G, pos=positions, ax=ax, alpha=0.05)\nnx.draw_networkx_nodes(G, pos=positions, ax=ax, node_size=20, node_color=\"red\", edgecolors=\"black\", alpha=0.8)\n\nWe can focus on a specific geographical area by specifying longitudonal and latitudonal limits and filtering the locations dataset for locations that fall within those limits. From now on, let’s focus our analysis on the area surrounding the Aegean Sea.\n\ngdf_pos = gpd.GeoDataFrame(df_pos, geometry=gpd.points_from_xy(df_pos['LONGITUDE'], df_pos['LATITUDE']), crs='EPSG:4326')\ngdf_pos = gdf_pos.to_crs('EPSG:3857')\n\n# We create a \"bonding box\" which contains the outer limits of our positions.\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\n\ngdf_pos_filtered = gdf_pos.cx[min_x:max_x, min_y:max_y] # Filtering the GeoDataFrame to include only points within the bounding box\nfiltered_nodes = gdf_pos_filtered.index.tolist()\n\nG_sub = G.subgraph(filtered_nodes) # Create a subgraph from the original graph G using only the filtered nodes\n\n# Prepare positions for the filtered subgraph\npositions_filtered = {}\nfor loc in gdf_pos_filtered.index:\n    x = gdf_pos_filtered.loc[loc].geometry.x\n    y = gdf_pos_filtered.loc[loc].geometry.y\n    positions_filtered[loc] = (x, y)\n\n# Plotting the filtered subgraph\nfig, ax = plt.subplots(figsize=(9, 9))\ngdf_pos_filtered.plot(ax=ax, alpha=0)\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\nnx.draw_networkx_edges(G_sub, pos=positions_filtered, ax=ax, alpha=0.1)\nnx.draw_networkx_nodes(G_sub, pos=positions_filtered, ax=ax, node_size=60, node_color=\"red\", edgecolors=\"black\", alpha=0.8)\n\nplt.show()\n\n\n\n4.3 Working with weighed graphs\nIf we print the df_edges dataframe, we see that it contains WEIGHT and TYPE columns, that correspond to each edge in the network. The TYPE category corresponds to the type of pottery linking two sites, and the WEIGHT cateogry corresponds to the relative importance of each edge.\n\nprint(df_edges)\n\nprint(max(df_edges['WEIGHT']))\nprint(min(df_edges['WEIGHT']))\n\nprint(df_edges['TYPE'].unique())\n\nWe can see that the WEIGHT cateory has values that range from 0 to 17, and the TYPE categories has two pottery types: ESB, ESC, as well as undefined pottery types (NaN). Let’s change the width of each edge in the network to reflect it’s weight, and the color of each edge to reflect it’s type.\nThis is made slightly difficult as we are working with a subgraph and the TYPE and WIDTH categories contain entries for the full graph.\n\ndf_edges\n\n\nG_sub_weighted = G.subgraph(filtered_nodes)\n\n# creating a new column called `edge_tuple` which contains tuples of the source and target locations.\n# We such a column for filtering the weight and type categories to only contain edges within G_sub_weighted and not G.\nedge_tuples = []\nfor index, row in df_edges.iterrows(): #Iterating over the rows of df_edges using iterrows() \n    edge_tuple = (row['SOURCE_LOCATION'], row['TARGET_LOCATION'])  # Creating a tuple from the 'SOURCE_LOCATION`` and 'TARGET_LOCATION' columns\n    edge_tuples.append(edge_tuple)  # Appending each tuple to the list\ndf_edges['edge_tuple'] = edge_tuples #  appending edge_tuples to df_edges \n\nsubgraph_edges = list(G_sub_weighted.edges()) #listing all the edges in the subgraph\nfiltered_edges = df_edges[df_edges['edge_tuple'].isin(subgraph_edges)] #filtering df_edges to only contain edges in the subgraph\nfiltered_edges.reset_index(drop=True, inplace=True) #the index gets all sliced up when filtering rows, this makes it normal again\n\nedge_colors = [] #create an empty set which contains the colors of the artifacts ESB, ESC\nfor type in filtered_edges['TYPE']: \n    if type == 'ESB':\n        edge_colors.append('maroon')\n    elif type == 'ESC':\n        edge_colors.append('navy')\n    else:\n        edge_colors.append('plum') \n\n# Set the weights of the edges\nweights = filtered_edges['WEIGHT']\nadjusted_weights = []\nfor weight in weights:\n    adjusted_weight = 0.1 + ((max(weights)- min(weights))/20)  \n    adjusted_weights.append(adjusted_weight)\n\nfig, ax = plt.subplots(figsize=(9, 9))\ngdf_pos_filtered.plot(ax=ax, alpha=0)\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n#Plot the network on the graph\nnx.draw_networkx_nodes(G_sub_weighted, pos=positions_filtered, ax=ax, node_size=60, node_color=\"red\", edgecolors=\"black\", alpha=0.8)\nnx.draw_networkx_edges(G_sub_weighted, pos=positions_filtered, ax=ax, width=adjusted_weights, edge_color=edge_colors, alpha=0.3)\n\n#Show our graph\npatch_maroon = mpatches.Patch(color='maroon', label='ESB') \npatch_navy = mpatches.Patch(color='navy', label='ESC') \nplt.legend(handles=[patch_maroon, patch_navy])\nplt.show()\n\n#Print information about the types of edges\nprint(adjusted_weights)\nprint(filtered_edges)\n\nThis works, but the weightings are hard to see. One alternative is to filter the subgraph by weight, and graph the different weights side-by-side:\n\n# creating a function that creates a list of colors, then apply the function to each subset of edges filtered by weight\ndef get_edge_colors(filtered_edges):\n    edge_colors = []\n    for type in filtered_edges['TYPE']: \n        if type == 'ESB':\n            edge_colors.append('maroon')\n        elif type == 'ESC':\n            edge_colors.append('navy')\n        else:\n            edge_colors.append('green')\n    return edge_colors\n\n# Filtering the edges by weight\nfiltered_edges_w1 = filtered_edges[filtered_edges['WEIGHT'] == 1]\nfiltered_edges_w2 = filtered_edges[filtered_edges['WEIGHT'] == 2]\nfiltered_edges_w3 = filtered_edges[filtered_edges['WEIGHT'] == 3]\n\n# applying the get_edge_colors function on each subset of edges\nedge_colors_w1 = get_edge_colors(filtered_edges_w1)\nedge_colors_w2 = get_edge_colors(filtered_edges_w2)\nedge_colors_w3 = get_edge_colors(filtered_edges_w3)\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 6)) #creating a plot; 1, 3 creates 3 subplots spaced horizontally\n\n# Graph 1: weight = 1\naxes[0].set_title('Weight 1') #axes[n] specifies which plot to plot the graph in: axes[0] is plot 1.\n\ngdf_pos_filtered.plot(ax=axes[0], alpha=0)\nctx.add_basemap(axes[0], source=ctx.providers.CartoDB.Positron)\n\nnx.draw_networkx_nodes(G_sub_weighted, pos=positions_filtered, ax=axes[0], node_size=60, node_color=\"red\", edgecolors=\"black\", alpha=0.8)\nnx.draw_networkx_edges(G_sub_weighted, pos=positions_filtered, ax=axes[0], edgelist=list(filtered_edges_w1['edge_tuple']),\n                       edge_color=edge_colors_w1, alpha=0.4) #`list(filtered_edges_w1['edge_tuple'])` extracts the edge_tuple column and converts the entries into a list\n\n# Graph 2: weight = 2\naxes[1].set_title('Weight 2')\ngdf_pos_filtered.plot(ax=axes[1], alpha=0)\nctx.add_basemap(axes[1], source=ctx.providers.CartoDB.Positron)\nnx.draw_networkx_nodes(G_sub_weighted, pos=positions_filtered, ax=axes[1], node_size=60, node_color=\"red\", edgecolors=\"black\", alpha=0.8)\nnx.draw_networkx_edges(G_sub_weighted, pos=positions_filtered, ax=axes[1], edgelist=list(filtered_edges_w2['edge_tuple']),\n                    edge_color=edge_colors_w2, alpha=0.4)\n\n# Graph 3: weight = 3\naxes[2].set_title('Weight 3')\ngdf_pos_filtered.plot(ax=axes[2], alpha=0)\nctx.add_basemap(axes[2], source=ctx.providers.CartoDB.Positron)\nnx.draw_networkx_nodes(G_sub_weighted, pos=positions_filtered, ax=axes[2], node_size=60, node_color=\"red\", edgecolors=\"black\", alpha=0.8)\nnx.draw_networkx_edges(G_sub_weighted, pos=positions_filtered, ax=axes[2], edgelist=list(filtered_edges_w3['edge_tuple']),\n                       edge_color=edge_colors_w3, alpha=0.4)\n\npatch_maroon = mpatches.Patch(color='maroon', label='ESB') \npatch_navy = mpatches.Patch(color='navy', label='ESC') \naxes[0].legend(handles=[patch_maroon, patch_navy], loc='upper left')\naxes[1].legend(handles=[patch_maroon, patch_navy], loc='upper left')\naxes[2].legend(handles=[patch_maroon, patch_navy], loc='upper left')\nplt.show()"
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html#measures-of-centrality",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html#measures-of-centrality",
    "title": "Networks and NetworkX",
    "section": "5. Measures of Centrality",
    "text": "5. Measures of Centrality\nCentrality is defined as the set of metrics used to determine the importance or influence of a particular node within a network. It helps to identify which nodes hold strategic significance in terms of connectivity, information flow, or influence over other nodes. Various centrality metrics, such as degree, betweenness, and eigenvector centrality, provide different perspectives on the role each node plays within the network’s overall structure.\n\n5.1 Network Distance and Eccentricity\nBefore talking about centrality, we first need to talk a bit about distance. Distance, also known as Geodesic distance, is defined as the number of edges traversed by the shortest path between two nodes.\n\nThe distance between a node and itself is 0.\nThe distance between a node and a node for which no shortest path exists (such as a node that is disconnected from other nodes) is \\(\\infty\\).\nThe distance between a node and it’s neighbor is 1.\n\nA node’s eccentricity is the maximum distance from said node to all other nodes in the graph. For instance, in the following network, the eccentricity of node \\(A\\) is 2, but the eccentricity of node \\(B\\) is 1.\n\nnodes = (\"A\",\"B\", \"C\")\nedges = [(\"A\",\"B\"), (\"B\", \"C\")]\n\nG_example = nx.Graph()\nG_example.add_edges_from(edges)\nG_example.add_nodes_from(nodes)\n\ncolor_map = [\"salmon\", \"lightblue\", \"salmon\"]\n\n\nred_patch = mpatches.Patch(color='salmon', label='eccentricity = 1') \nblue_patch = mpatches.Patch(color='lightblue', label='eccentricity = 2') \nplt.legend(handles=[blue_patch, red_patch])\n\nnx.draw(G_example, node_color=color_map, with_labels=True)\n\nIf we color the nodes of our Aegean sea pottery network by eccentricity, we see an interesting result:\n\n#Bring in our data to be used\ngdf_pos = gpd.GeoDataFrame(df_pos, geometry=gpd.points_from_xy(df_pos['LONGITUDE'], df_pos['LATITUDE']), crs='EPSG:4326')\ngdf_pos = gdf_pos.to_crs('EPSG:3857')\n\n#Set a bounding box on our data\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\n\n#Use the bounding box on our data\ngdf_pos_filtered = gdf_pos.cx[min_x:max_x, min_y:max_y] \nfiltered_nodes = gdf_pos_filtered.index.tolist()\n\nG_sub_eccentricity = G.subgraph(filtered_nodes) \n\neccentricities = nx.eccentricity(G_sub_eccentricity) #calculating eccentricities\neccentricities_array = np.array(list(eccentricities.values()))\n\n#Set a colour map for different values of eccentricity in our network\ncolor_map = []\nfor eccentricity in eccentricities_array:\n    if eccentricity == 1:\n        color_map.append(\"palegreen\")\n    elif eccentricity == 2:\n        color_map.append(\"slateblue\")\n    elif eccenticity == 3:\n        color_map.append(\"orange\")\n    else:\n        color_map.append(\"grey\")\n\n#Filter oyur graph\npositions_filtered = {}\nfor loc in gdf_pos_filtered.index:\n    x = gdf_pos_filtered.loc[loc].geometry.x\n    y = gdf_pos_filtered.loc[loc].geometry.y\n    positions_filtered[loc] = (x, y)\n\nfig, ax = plt.subplots(figsize=(9, 9))\ngdf_pos_filtered.plot(ax=ax, alpha=0)\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n#Draw our graph with different colours for eccentricity\nnx.draw_networkx_edges(G_sub_eccentricity, pos=positions_filtered, ax=ax, alpha=0.1)\nnx.draw_networkx_nodes(G_sub_eccentricity, pos=positions_filtered, node_color=color_map, ax=ax, node_size=60, edgecolors=\"black\", alpha=0.8)\n\npatch = mpatches.Patch(color='slateblue', label='eccentricity = 2') \nplt.legend(handles=[patch])\n\n#show the graph\nplt.show()\n\nAll nodes are at most 2 edges away from every other other node!\n\n\n5.2 Degree Centrality\nDegree centrality is simple: Recall that the degree of a node is the number of nodes directly connected to it. In degree centrality, the more adjacent nodes, the more important the network is considered to be. Degree centrality is used primarily in social networks, where nodes with higher degrees are commonly major channels of information. A high degree means a node has many direct ties with other nodes, and has better access to resources within the network.\nNote that the networkX nx.degree_centrality() function normalizes each node’s degree by dividing by the maximum possible degree in the network. Therefore for graphs without self-loops (such as the one we are working with) the degree centrality is always \\(\\leq 1\\). For educational purposes, we un-normalize the degree values, but this is not common practice.\nWe can calculate the degree centrality of all our nodes in our network:\n\n#Bring in our dataset\ngdf_pos = gpd.GeoDataFrame(df_pos, geometry=gpd.points_from_xy(df_pos['LONGITUDE'], df_pos['LATITUDE']), crs='EPSG:4326')\ngdf_pos = gdf_pos.to_crs('EPSG:3857')\n\n#Set a bounding box to limit the scope of the graph\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\n\n#Filter our graph using the bounding box \ngdf_pos_filtered = gdf_pos.cx[min_x:max_x, min_y:max_y] \nfiltered_nodes = gdf_pos_filtered.index.tolist()\n\nG_sub = G.subgraph(filtered_nodes) \n\n#Find the centrality values \ncentrality = nx.degree_centrality(G_sub)\ncentrality_values = np.array(list(centrality.values()))\nn = len(centrality_values)\noriginal_degrees = centrality_values * (n - 1)\n\n#Colour our map based on the degree centrality \ncolor_map=[]\nfor degree in original_degrees:\n    if degree == max(original_degrees):\n        color_map.append(\"indianred\")\n    else:\n        normalized_value = degree / max(original_degrees)  # Normalize the degree to the range [0, 1]\n        color_map.append(plt.cm.winter(normalized_value))\n\ncmap = plt.cm.winter\n\n#Filter our graph \npositions_filtered = {}\nfor loc in gdf_pos_filtered.index:\n    x = gdf_pos_filtered.loc[loc].geometry.x\n    y = gdf_pos_filtered.loc[loc].geometry.y\n    positions_filtered[loc] = (x, y)\n\n\nfig, ax = plt.subplots(figsize=(9, 9))\ngdf_pos_filtered.plot(ax=ax, alpha=0)\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n\nedges= nx.draw_networkx_edges(G_sub, pos=positions_filtered, ax=ax, alpha=0.1)\nnodes = nx.draw_networkx_nodes(G_sub, pos=positions_filtered, node_color=color_map, cmap=cmap, ax=ax, node_size=60, edgecolors=\"black\", alpha=1)\n\n\n# Adding the colorbar using the inset_axes function\naxins = inset_axes(ax,\n                   width=\"5%\", \n                   height=\"30%\",  \n                   loc='lower left', \n                   bbox_to_anchor=(0.03, 0.03, 1, 1), \n                   bbox_transform=ax.transAxes, \n                   borderpad=0.3)\n\n# Creating colorbar\nnorm = plt.Normalize(vmin=min(original_degrees), vmax=max(original_degrees))\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array(original_degrees)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\n\npatch = mpatches.Patch(color='indianred', label='maximum degree (degree = 36)') \nplt.legend(handles=[patch], loc='center left', bbox_to_anchor=(10, 3))\nplt.show()\n\n#Print some information about our graph\nprint(\"minimum degree:\", min(original_degrees))\nprint(\"maximum degree:\", max(original_degrees))\nprint(\"difference:\", max(original_degrees)-min(original_degrees))\n\n\n\n5.3 Closeness Centrality\nCloseness centrality is a measure of how close a node is to all other nodes in the network. It can be computed as the “sum of the geodesic distances of a node to all other nodes in the network”. A node is important if it is close to all other nodes in the network. One flaw of closeness centrality is that while it is a useful indicator of node importance in small networks, it produces little variation in large networks with many edges. This is particularly evident in our example network:\n\n#Import a library so the code works\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\n\n#Bring in our data\ngdf_pos = gpd.GeoDataFrame(df_pos, geometry=gpd.points_from_xy(df_pos['LONGITUDE'], df_pos['LATITUDE']), crs='EPSG:4326')\ngdf_pos = gdf_pos.to_crs('EPSG:3857')\n\n#Bounding Box\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\n\n#Filter our graph using bounding box\ngdf_pos_filtered = gdf_pos.cx[min_x:max_x, min_y:max_y]\nfiltered_nodes = gdf_pos_filtered.index.tolist()\n\nG_sub = G.subgraph(filtered_nodes) \n\n# Find centrality \ncentrality = nx.closeness_centrality(G_sub)\ncentrality_values = np.array(list(centrality.values()))\n\n#Set a colour for different values of centrality \ncolor_map=[]\nfor degree in centrality_values:\n    if degree == max(centrality_values):\n        color_map.append(\"indianred\")\n    else:\n        normalized_value = degree / max(centrality_values)\n        color_map.append(plt.cm.winter(normalized_value))\n\ncmap = plt.cm.winter\n\npositions_filtered = {}\nfor loc in gdf_pos_filtered.index:\n    x = gdf_pos_filtered.loc[loc].geometry.x\n    y = gdf_pos_filtered.loc[loc].geometry.y\n    positions_filtered[loc] = (x, y)\n\n\nfig, ax = plt.subplots(figsize=(9, 9))\n\ngdf_pos_filtered.plot(ax=ax, alpha=0)\n\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n#Draw our edges\nedges= nx.draw_networkx_edges(G_sub, pos=positions_filtered, ax=ax, alpha=0.1)\nnodes = nx.draw_networkx_nodes(G_sub, pos=positions_filtered, node_color=color_map, cmap=cmap, ax=ax, node_size=60, edgecolors=\"black\", alpha=1)\n\naxins = inset_axes(ax,\n                   width=\"5%\", \n                   height=\"30%\", \n                   loc='lower left',\n                   bbox_to_anchor=(0.03, 0.03, 1, 1), \n                   bbox_transform=ax.transAxes, \n                   borderpad=0.3)\n\nnorm = plt.Normalize(vmin=min(original_degrees), vmax=max(original_degrees))\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array(original_degrees)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\n\npatch = mpatches.Patch(color='indianred', label='maximum closeness (closeness = 39.093)') \nplt.legend(handles=[patch], loc='center left', bbox_to_anchor=(7, 3))\nplt.show()\n\n\n#Print information about our graph\nprint(\"minumim closeness:\", min(original_degrees))\nprint(\"maximum closeness:\", max(original_degrees))\nprint(\"difference:\", max(original_degrees)-min(original_degrees))\n\n\n\n5.4 Betweenness Centrality\nBetweenness Centrality is a measure of the importance of a node based on how well it serves as a bridge between nodes in a network. The mathematical representation of the betweeness centrality of a node is the number of times each node has to pass through that node to reach every other node in a network. Nodes with high betweenness thus serve as “bridges” within a network.\nThe Betweenness centrality of a given node \\(i\\) is calculated as: \\(b(i)=\\sum_{j,k}\\frac{g_{jik}}{g_{jk}}\\), where \\(g_{jik}\\) is the number of paths from node \\(j\\) to node \\(k\\) passing through node \\(i\\), and \\(g_{jk}\\) is the number of paths from node \\(g\\) to node \\(j\\) (including paths passing through node \\(i\\)).\nNote that, for undirected graphs, two adjacent nodes can only have one path between them (ie, between two adjacent nodes \\(A\\) and \\(B\\), if \\(A\\rightarrow B\\) is a path, then \\(B \\rightarrow A\\) is not).\nConsider the graph below:\n\n#Define our network\nG_betweenness_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_betweenness_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_betweenness_example, seed=1000)\n\n#Draw our graph\nnx.draw(G_betweenness_example,pos=pos, with_labels=True, edgecolors=\"black\", node_color=\"bisque\", node_size=800)\n\nNode \\(4\\) serves as a bridge between nodes 5 and 6 to the rest of the nodes in the network. For a path to be drawn between nodes 6 or 5 to nodes 0,1,2,3, the path must go through node 4. Let’s calculate the betweenness centrality of this network, and label nodes by centrality:\n\ncentrality\n\n\n#Define our network\nG_betweenness_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_betweenness_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_betweenness_example, seed=1000)\n\n#Find the centrality values for our nodes\ncentrality = nx.betweenness_centrality(G_betweenness_example, normalized=False)\ncentrality_values = np.array(list(centrality.values()))\ncmap=\"BuPu\"\n\n#Put labels on our network\nlabels = {}\nfor node in G_betweenness_example.nodes():\n    labels[node] = centrality_values[node]\n\n\n#Draw our graph using `nx.draw`\nnx.draw(G_betweenness_example,pos=pos, node_color=centrality_values, edgecolors=\"black\", cmap=cmap, node_size=800)\nnx.draw_networkx_labels(G_betweenness_example, pos, labels=labels, font_color=\"orangered\")\n\nWe can see that node 4 does indeed have the highest betweenness centrality. The values of 0 for nodes 0, 1, 2, 3 and 6 indicate that each node can reach every other node without passing through those nodes. The value of 5.0 for node 5 indicates that five nodes must pass through node 5 in order to reach another node. In other words, \\(\\frac{g_{jik}}{g_{jk}}=1\\) for 5 pairs of nodes in the network.\nColoring the nodes in our archeology dataset by betweenness centrality, we get:\n\n#Bring in our data\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\ngdf_pos = gpd.GeoDataFrame(df_pos, geometry=gpd.points_from_xy(df_pos['LONGITUDE'], df_pos['LATITUDE']), crs='EPSG:4326')\ngdf_pos = gdf_pos.to_crs('EPSG:3857')\n\n#Bounding box\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\n\n#Filter graph using bounding box\ngdf_pos_filtered = gdf_pos.cx[min_x:max_x, min_y:max_y] \nfiltered_nodes = gdf_pos_filtered.index.tolist()\n\nG_sub = G.subgraph(filtered_nodes) \n\n#Find centrality values and place them into an array.\ncentrality = nx.betweenness_centrality(G_sub)\ncentrality_values = np.array(list(centrality.values()))\nn = len(centrality_values)\noriginal_degrees = centrality_values * (n - 1)\n\n#Colour our values based on centrality\ncolor_map=[]\nfor degree in original_degrees:\n    if degree == max(original_degrees):\n        color_map.append(\"indianred\")\n    else:\n        normalized_value = degree / max(original_degrees) \n        color_map.append(plt.cm.winter(normalized_value))\n\ncmap = plt.cm.winter\n\n#Filter our new network\npositions_filtered = {}\nfor loc in gdf_pos_filtered.index:\n    x = gdf_pos_filtered.loc[loc].geometry.x\n    y = gdf_pos_filtered.loc[loc].geometry.y\n    positions_filtered[loc] = (x, y)\n\nfig, ax = plt.subplots(figsize=(9, 9))\n\n\ngdf_pos_filtered.plot(ax=ax, alpha=0)\n\n\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n# Create our edges and nodes\nedges= nx.draw_networkx_edges(G_sub, pos=positions_filtered, ax=ax, alpha=0.1)\nnodes = nx.draw_networkx_nodes(G_sub, pos=positions_filtered, node_color=color_map, cmap=cmap, ax=ax, node_size=60, edgecolors=\"black\", alpha=1)\n\n\naxins = inset_axes(ax,\n                   width=\"5%\", \n                   height=\"30%\",  \n                   loc='lower left', \n                   bbox_to_anchor=(0.03, 0.03, 1, 1), \n                   bbox_transform=ax.transAxes,  \n                   borderpad=0.3)\n\nnorm = plt.Normalize(vmin=min(original_degrees), vmax=max(original_degrees))\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array(original_degrees)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\n\n#Plot our graph\npatch = mpatches.Patch(color='indianred', label='maximum betweenness (betweenness = 2.621)') \nplt.legend(handles=[patch], loc='center left', bbox_to_anchor=(7, 3))\nplt.show()\n\n\n\n5.5 Eigenvector centrality\nEigenvector centrality is a measure of the influence of a node in a network by considering not just how many connections it has (as we did with degree centrality), but also the importance of those connections: A node with high eigenvector centrality is connected to many nodes that themselves have high centrality, making it more influential in spreading information or resources. Unlike simpler measures like degree centrality, which only counts connections, eigenvector centrality looks at the overall structure of the network. It helps identify key players in a network who might not have the most connections but are well-connected to other important nodes.\nThe term “eigenvector” in comes from linear algebra: an eigenvector is a special kind of vector taken from a network’s adjacency matrix. Each element in this eigenvector represents a node’s centrality in the network.\n\n#Bring in data\nfrom mpl_toolkits.axes_grid1.inset_locator import inset_axes\ngdf_pos = gpd.GeoDataFrame(df_pos, geometry=gpd.points_from_xy(df_pos['LONGITUDE'], df_pos['LATITUDE']), crs='EPSG:4326')\ngdf_pos = gdf_pos.to_crs('EPSG:3857')\n\n#Bounding Box\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\n\n#Filter using bounding box\ngdf_pos_filtered = gdf_pos.cx[min_x:max_x, min_y:max_y]\nfiltered_nodes = gdf_pos_filtered.index.tolist()\n\nG_sub = G.subgraph(filtered_nodes) \n\n#Find the eigenvector centrality in our graph\ncentrality = nx.eigenvector_centrality(G_sub)\ncentrality_values = np.array(list(centrality.values()))\n\n# Colour our nodes\ncolor_map=[]\nfor degree in centrality_values:\n    if degree == max(centrality_values):\n        color_map.append(\"indianred\")\n    else:\n        normalized_value = degree / max(centrality_values) \n        color_map.append(plt.cm.winter(normalized_value))\n\ncmap = plt.cm.winter\n\n#Filter the graph\npositions_filtered = {}\nfor loc in gdf_pos_filtered.index:\n    x = gdf_pos_filtered.loc[loc].geometry.x\n    y = gdf_pos_filtered.loc[loc].geometry.y\n    positions_filtered[loc] = (x, y)\n\n\nfig, ax = plt.subplots(figsize=(9, 9))\n\ngdf_pos_filtered.plot(ax=ax, alpha=0)\n\n\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n#Draw our edges and nodes\nedges= nx.draw_networkx_edges(G_sub, pos=positions_filtered, ax=ax, alpha=0.1)\nnodes = nx.draw_networkx_nodes(G_sub, pos=positions_filtered, node_color=color_map, cmap=cmap, ax=ax, node_size=60, edgecolors=\"black\", alpha=1)\n\n\naxins = inset_axes(ax,\n                   width=\"5%\",  \n                   height=\"30%\",  \n                   loc='lower left', \n                   bbox_to_anchor=(0.03, 0.03, 1, 1),  \n                   bbox_transform=ax.transAxes, \n                   borderpad=0.3)\n\nnorm = plt.Normalize(vmin=min(original_degrees), vmax=max(original_degrees))\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array(original_degrees)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\n\n#Plot our graph \npatch = mpatches.Patch(color='indianred', label='maximum betweenness (betweenness = 2.621)') \nplt.legend(handles=[patch], loc='center left', bbox_to_anchor=(7, 3))\nplt.show()\n\n\n\n5.6 Directed graphs and Putting it all together\nLastly, let’s clean up our centrality graphs by putting them together into one graph. We’ll also use a directed version of our graph, to see how that changes the results.\n\n#Bring in our data\ndf_edges = pd.read_excel(\"vistorian_network.xls\")\ndf_pos = pd.read_excel(\"vistorian_locations.xls\", index_col=0)\n\n#Define the graph\nG_directed = nx.from_pandas_edgelist(df_edges, source=\"SOURCE_LOCATION\", target=\"TARGET_LOCATION\", create_using=nx.DiGraph())\ngdf_pos = gpd.GeoDataFrame(df_pos, geometry=gpd.points_from_xy(df_pos['LONGITUDE'], df_pos['LATITUDE']), crs='EPSG:4326')\ngdf_pos = gdf_pos.to_crs('EPSG:3857')\n\n#Create a bounding box and filter the graph using it\nmin_x, min_y = 1858948, 4055442\nmax_x, max_y = 3336323, 5175704\ngdf_pos_filtered = gdf_pos.cx[min_x:max_x, min_y:max_y]\nfiltered_nodes = gdf_pos_filtered.index.tolist()\n\n#Plot the now directed points\nG_filtered = G_directed.subgraph(filtered_nodes)\npositions_filtered = {loc: (gdf_pos_filtered.loc[loc].geometry.x, gdf_pos_filtered.loc[loc].geometry.y) for loc in gdf_pos_filtered.index}\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\nfig.set_facecolor('lightblue')\nfor ax in axes.flatten():\n    gdf_pos_filtered.plot(ax=ax, alpha=0)\n    ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n#The codes below are from the above sections (more notes can be found there on what each code block does)\n\n# Plotting Degree Centrality\ndegree_centrality = nx.degree_centrality(G_filtered)\ndegree_values = np.array(list(degree_centrality.values()))\nn = len(degree_values)\noriginal_degrees = degree_values * (n - 1)\ncolor_map=[]\n\nfor degree in original_degrees:\n    if degree == max(original_degrees):\n        color_map.append(\"indianred\")\n    else:\n        normalized_degree = degree / max(original_degrees)  \n        color_map.append(plt.cm.winter(normalized_degree))\n\nnx.draw_networkx_edges(G_filtered, pos=positions_filtered, ax=axes[0, 0], alpha=0.1)\nnx.draw_networkx_nodes(G_filtered, \n                       pos=positions_filtered, \n                       node_color=color_map, \n                       cmap=plt.cm.winter, \n                       ax=axes[0, 0], \n                       node_size=60, \n                       edgecolors=\"black\", \n                       alpha=1)\n\naxes[0, 0].set_title(\"Degree Centrality\")\naxes[0, 0].set_facecolor(\"white\")\n\n# Add colorbar for Degree Centrality\naxins = inset_axes(axes[0, 0], width=\"5%\", height=\"30%\", loc='lower left', \n                   bbox_to_anchor=(0.03, 0.03, 1, 1), \n                   bbox_transform=axes[0, 0].transAxes, \n                   borderpad=0.3)\nsm = plt.cm.ScalarMappable(cmap=plt.cm.winter, norm=plt.Normalize(vmin=min(original_degrees), vmax=max(original_degrees )))\nsm.set_array(original_degrees)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\npatch = mpatches.Patch(color='indianred', label=f'max degree ({max(original_degrees):.3f})')\naxes[0, 0].legend(handles=[patch], loc='upper left')\n\n###\n\n\n# Plot Closeness Centrality\ncloseness_centrality = nx.closeness_centrality(G_filtered)\ncloseness_values = np.array(list(closeness_centrality.values()))\n\ncolor_map_closeness=[]\nfor degree in closeness_values:\n    if degree == max(closeness_values):\n        color_map_closeness.append(\"indianred\")\n    else:\n        normalized_degree_closeness = degree / max(closeness_values) \n        color_map_closeness.append(plt.cm.winter(normalized_degree_closeness))\n\nnx.draw_networkx_edges(G_filtered, pos=positions_filtered, ax=axes[1, 0], alpha=0.1)\nnx.draw_networkx_nodes(G_filtered, pos=positions_filtered, node_color=color_map_closeness, cmap=plt.cm.winter, ax=axes[1, 0], node_size=60, edgecolors=\"black\", alpha=1)\naxes[1, 0].set_title(\"Closeness Centrality\")\naxes[1, 0].set_facecolor(\"white\")\n\n# Add colorbar for Closeness Centrality\naxins = inset_axes(axes[1, 0], width=\"5%\", height=\"30%\", loc='lower left', \n                   bbox_to_anchor=(0.03, 0.03, 1, 1), \n                   bbox_transform=axes[1, 0].transAxes, \n                   borderpad=0.3)\nsm = plt.cm.ScalarMappable(cmap=plt.cm.winter, norm=plt.Normalize(vmin=min(closeness_values), vmax=max(closeness_values)))\nsm.set_array(closeness_values)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\npatch = mpatches.Patch(color='indianred', label=f'max closeness ({max(closeness_values):.3f})')\naxes[1, 0].legend(handles=[patch], loc='upper left')\n\n# Plot Betweenness Centrality\nbetweenness_centrality = nx.betweenness_centrality(G_filtered)\nbetweenness_values = np.array(list(betweenness_centrality.values()))\ncolor_map_betweenness=[]\nfor degree in betweenness_values:\n    if degree == max(betweenness_values):\n        color_map_betweenness.append(\"indianred\")\n    else:\n        normalized_degree_betweenness = degree / max(betweenness_values)  # Normalize the degree to the range [0, 1]\n        color_map_betweenness.append(plt.cm.winter(normalized_degree_betweenness))\nnx.draw_networkx_edges(G_filtered, pos=positions_filtered, ax=axes[0, 1], alpha=0.1)\nnx.draw_networkx_nodes(G_filtered, pos=positions_filtered, node_color=color_map_betweenness, cmap=plt.cm.winter, ax=axes[0, 1], node_size=60, edgecolors=\"black\", alpha=1)\naxes[0, 1].set_title(\"Betweenness Centrality\")\naxes[0, 1].set_facecolor(\"white\")\n\n# Add colorbar for Betweenness Centrality\naxins = inset_axes(axes[0, 1], width=\"5%\", height=\"30%\", loc='lower left', bbox_to_anchor=(0.03, 0.03, 1, 1), bbox_transform=axes[0, 1].transAxes, borderpad=0.3)\nsm = plt.cm.ScalarMappable(cmap=plt.cm.winter, norm=plt.Normalize(vmin=min(betweenness_values), vmax=max(betweenness_values)))\nsm.set_array(betweenness_values)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\npatch = mpatches.Patch(color='indianred', label=f'max betweenness ({max(betweenness_values):.3f})')\naxes[0, 1].legend(handles=[patch], loc='upper left')\n\n# Plot Eigenvector Centrality\neigenvector_centrality = nx.eigenvector_centrality(G_filtered, max_iter=1000)\neigenvector_values = np.array(list(eigenvector_centrality.values()))\n\ncolor_map_eigenvector=[]\nfor degree in eigenvector_values:\n    if degree == max(eigenvector_values):\n        color_map_eigenvector.append(\"indianred\")\n    else:\n        normalized_degree_eigenvector = degree / max(eigenvector_values)  # Normalize the degree to the range [0, 1]\n        color_map_eigenvector.append(plt.cm.winter(normalized_degree_eigenvector))\nnx.draw_networkx_edges(G_filtered, pos=positions_filtered, ax=axes[1, 1], alpha=0.1)\nnx.draw_networkx_nodes(G_filtered, pos=positions_filtered, node_color=color_map_eigenvector, cmap=plt.cm.winter, ax=axes[1, 1], node_size=60, edgecolors=\"black\", alpha=1)\naxes[1, 1].set_title(\"Eigenvector Centrality\")\naxes[1, 1].set_facecolor(\"white\")\n\n# Add colorbar for Eigenvector Centrality\naxins = inset_axes(axes[1, 1], width=\"5%\", height=\"30%\", loc='lower left', bbox_to_anchor=(0.03, 0.03, 1, 1), bbox_transform=axes[1, 1].transAxes, borderpad=0.3)\nsm = plt.cm.ScalarMappable(cmap=plt.cm.winter, norm=plt.Normalize(vmin=min(eigenvector_values), vmax=max(eigenvector_values)))\nsm.set_array(eigenvector_values)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\npatch = mpatches.Patch(color='indianred', label=f'max eigenvector centrality ({max(eigenvector_values):.3f})')\naxes[0, 1].legend(handles=[patch], loc='upper left')"
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html#group-level-analysis-finding-subgroups-within-networks",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html#group-level-analysis-finding-subgroups-within-networks",
    "title": "Networks and NetworkX",
    "section": "6. Group-level analysis: finding subgroups within networks",
    "text": "6. Group-level analysis: finding subgroups within networks\nCohesive subgroups in network analysis refer to clusters of nodes within a network that are more densely connected to each other than to the rest of the network. These subgroups indicate areas of high interaction or strong relationships within the larger network. Identifying cohesive subgroups helps in understanding the structure and dynamics of the network, such as how information or influence flows within and between these groups. The process of finding cohesive subgroups within networks is called cohesive group analysis.\n\nA clique is a subset of nodes within a graph where every node is directly connected to every other node in the subset. This means that in a clique, all possible edges between the nodes are present, making it a maximally connected subgraph.\n\nAll nodes are that are by themselves are inherently a clique (a 1-clique)\n\nIn undirected graphs, a \\(k\\)-core is a subgraph in which every node is connected to at least \\(k\\) other nodes within the subgraph. The \\(k\\)-core is obtained by iteratively removing all nodes with a degree less than \\(k\\) until no more nodes can be removed.\n\n\n#Create our network\nG_cliques_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_cliques_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_cliques_example, seed=1000)\n\n#Find our cliques and print where they are\ncliques = [x for x in nx.find_cliques(G_cliques_example)]\nprint(cliques)\n\n#Draw our graph      \nnx.draw(G_cliques_example,pos=pos, with_labels=True, edgecolors=\"black\", node_color = \"bisque\", cmap=cmap, node_size=800)\n\nWe can see that there are three cliques in this network: \\((4, 0, 1, 2, 3)\\), \\((4, 5)\\), and \\((6, 5)\\):\n\n#Set the figure size (24,6) not (8,6) because we have 3 graphs to show\nfig, axes = plt.subplots(1, 3, figsize=(24, 6))\nfig.set_facecolor('lightblue')\n\n#Create our network\nG_cliques_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_cliques_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_cliques_example, seed=1000)\n\n#Find our cliques and print them\ncliques = [x for x in nx.find_cliques(G_cliques_example)]\nprint(cliques)\n\n#Colour our cliques\nclique_1 = [\"mediumseagreen\", \"mediumseagreen\", \"mediumseagreen\", \"mediumseagreen\", \"mediumseagreen\", \"bisque\", \"bisque\"]\nclique_2 = [\"bisque\", \"bisque\", \"bisque\",  \"bisque\", \"mediumseagreen\", \"mediumseagreen\", \"bisque\"]\nclique_3 = [\"bisque\", \"bisque\", \"bisque\", \"bisque\",  \"bisque\", \"mediumseagreen\", \"mediumseagreen\"]\n\n#Draw our cliques       \nnx.draw(G_cliques_example, ax=axes[0], pos=pos, with_labels=True, edgecolors=\"black\", node_color = clique_1, cmap=cmap, node_size=800)\nnx.draw(G_cliques_example, ax=axes[1], pos=pos, with_labels=True, edgecolors=\"black\", node_color = clique_2, cmap=cmap, node_size=800)\nnx.draw(G_cliques_example, ax=axes[2], pos=pos, with_labels=True, edgecolors=\"black\", node_color = clique_3, cmap=cmap, node_size=800)\n\n\n#Create our example graph\nG_cliques_example = nx.Graph()\nedges_list = [(0,1),(0,2),(0,3),(0,4),(1,2),(2,3),(3,4),(1,4),(2,4),(1,3),(4,5),(5,6)]\nG_cliques_example.add_edges_from(edges_list)\npos = nx.spring_layout(G_cliques_example, seed=1000)\n\n#Find the cliques\ncliques = [x for x in nx.find_cliques(G_cliques_example)]\nprint(cliques)\n\n#For loop\nnode_counts = {}\nfor clique in cliques: #for each clique in the list of cliques...\n    for node in clique: # for each node in each clique...\n        if node in node_counts: #checks whether the current node already exists as a key in the node_counts dictionary\n            node_counts[node] += 1 #if it is in the dictionary, increase it's value by 1\n        else:\n            node_counts[node] = 1 #if it isn't, dont change\n\n#Colour our nodes\ncolors = []\nfor node in G_cliques_example.nodes():\n    if node_counts[node] == 1:\n        colors.append(\"lightgreen\")\n    elif node_counts[node] == 2:\n        colors.append(\"forestgreen\")\n    elif node_counts[node] == 3:\n        colors.append(\"orange\")\n    else:\n        colors.append(\"red\")\n\n#Draw our network           \nnx.draw(G_cliques_example,pos=pos, with_labels=True, edgecolors=\"black\", node_color = colors, cmap=cmap, node_size=800)\npatch_green = mpatches.Patch(color='lightgreen', label='node in one clique') \npatch_forest = mpatches.Patch(color='forestgreen', label='node in two cliques') \nplt.legend(handles=[patch_green, patch_forest])\nplt.show()\n\nLet’s repeat the same process as the example above on our archeology dataset:\n\nG_sub_cliques = G.subgraph(filtered_nodes) \n\n#Set the plot size\nfig, ax = plt.subplots(figsize=(9, 9))\ngdf_pos_filtered.plot(ax=ax, alpha=0)\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\n#Find our cliques\ncliques = [x for x in nx.find_cliques(G_sub_cliques)]\nprint(cliques)\n\nnode_counts = {} #empty dictionary of key-value pairs where keys are nodes and values are the frequency that they show up in a clique\nfor node in G_sub_cliques.nodes():\n    node_counts[node] = 0 # makes dictionary values start at 0 \n\nfor clique in cliques: #for each clique in the list of cliques...\n    for node in clique: # for each node in each clique...\n        node_counts[node] += 1 #if it is in the dictionary, increase it's value by 1\n\n#Set the colours  for different cliques\ncolors = []\nmax_values = max(node_counts.values()) \nmin_values = min(node_counts.values())\nfor node in G_sub_cliques.nodes():\n    count = node_counts.get(node, 0)\n    if count == max_values:\n        colors.append(\"salmon\")\n    elif count == min_values:\n        colors.append(\"forestgreen\")\n    else:\n        normalized_value = count / max_values\n        colors.append(plt.cm.PuBu(normalized_value))\n\n#Draw our network\nnx.draw_networkx_edges(G_sub_cliques, pos=positions_filtered, ax=ax, alpha=0.1)\nnx.draw_networkx_nodes(G_sub_cliques, pos=positions_filtered, ax=ax, node_size=60, node_color=colors, edgecolors=\"black\", alpha=0.8)\n\n#Make a legend in the corner of the graph with labels\npatch_forestgreen = mpatches.Patch(color='forestgreen', label=f\"Minimum number of cliques: {min_values}\") \npatch_salmon = mpatches.Patch(color='salmon', label=f\"Maximum number of cliques: {max_values}\") \nplt.legend(handles=[patch_salmon, patch_forestgreen])\n\naxins = inset_axes(ax,\n                   width=\"5%\",  \n                   height=\"30%\",  \n                   loc='lower left',  \n                   bbox_to_anchor=(0.03, 0.03, 1, 1), \n                   bbox_transform=ax.transAxes,  \n                   borderpad=0.3)\n\nnode_counts_values = np.array(list(node_counts.values()))\nnorm = plt.Normalize(vmin=min_values, vmax=max_values)\nsm = plt.cm.ScalarMappable(cmap=\"PuBu\", norm=norm)\nsm.set_array(node_counts_values)\nplt.colorbar(sm, cax=axins, orientation=\"vertical\")\n\n#Plot our graph\nplt.show()"
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html#network-level-analysis-clusters-and-clustering-coefficients",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html#network-level-analysis-clusters-and-clustering-coefficients",
    "title": "Networks and NetworkX",
    "section": "7. Network-level analysis: Clusters and clustering coefficients",
    "text": "7. Network-level analysis: Clusters and clustering coefficients\nA cluster (also known as a community) is a set of nodes in a graph that are densely connected to each other but sparsely connected to nodes in other clusters. For example, in a social network, a cluster might represent a group of people who frequently interact with each other but have fewer interactions with people outside the group. Community detection is the process of finding such communities within nodes.\nBefore diving into community detection, we first need to understand modularity. Modulaity is a numerical measure for the community structure of a graph: it compares the density of edges within the communities of a network to the density of edges between communities. Mathematically, modularity compares the observed density of edges within a community to the expected density of edges if the connections were random. A positive modularity value suggests a strong community structure, while values closer to zero or negative indicate that the divisions are no better than random.\n\n9.1 Louvain Algorithm\nThe Louvain algorithm is a community detection method in networks that aims to optimize modularity. By optimizing modularity, the Louvain algorithm effectively uncovers natural divisions in the network where connections are dense within clusters and sparse between them, thus identifying meaningful community structures.\nFirst, each node is assigned to its own community, and nodes are then iteratively moved to neighboring communities if it increases the modularity. In the second phase, the algorithm creates a new network where each community from the first phase is treated as a single node, and the process is repeated. This hierarchical approach continues until no further modularity improvements can be made, resulting in a final set of communities that maximize modularity.\n\nG_sub_louvain = G.subgraph(filtered_nodes) \n\n#Set our figure size\nfig, ax = plt.subplots(figsize=(9, 9))\ngdf_pos_filtered.plot(ax=ax, alpha=0)\nctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron)\n\npartition = community_louvain.best_partition(G_sub_louvain) #applying louvain\n\n#Colour map of possible colors with values associated with it\ncolor_map = {\n    0: \"coral\",\n    1: \"cornflowerblue\",\n    2: \"limegreen\",\n    3: \"crimson\",\n    4: \"purple\"}\n\n#Set node colors\nnode_colors = []\nfor node in G_sub_louvain.nodes():\n    cluster_id = partition[node]  # Get the cluster ID for the node\n    color = color_map[cluster_id]  # Get the corresponding color from the color_map\n    node_colors.append(color)\n\n#Draw our network\nnx.draw_networkx_edges(G_sub_louvain, pos=positions_filtered, ax=ax, alpha=0.1)\nnx.draw_networkx_nodes(G_sub_louvain, pos=positions_filtered, \n                       ax=ax, node_size=60, node_color=node_colors,edgecolors=\"black\", alpha=0.8)\nprint(partition)\n\n#Make a legend in the corner with color values and show our graph \npatch_coral = mpatches.Patch(color='coral', label=\"Community 0\") \npatch_cornflower = mpatches.Patch(color='cornflowerblue', label=\"Community 1\") \npatch_limegreen = mpatches.Patch(color='limegreen', label=\"Community 2\") \npatch_crimson = mpatches.Patch(color='crimson', label=\"Community 4\") \nplt.legend(handles=[patch_coral, patch_cornflower, patch_limegreen, patch_crimson])\nplt.show()\n\n\nclusters = {}\n# Iterating through the partition dictionary\nfor node, cluster in partition.items():\n    if cluster not in clusters:     # If the cluster is not already in the dictionary, add it\n        clusters[cluster] = []\n    clusters[cluster].append(node)    \nprint(\"community_0:\", clusters[0]) #Prints the names of nodes in each community 0,1,2 \nprint(\"community_1:\", clusters[1])\nprint(\"community_2:\", clusters[2])\n\n\n\n7.2 Clustering coefficients\nThe last thing we’ll cover is clustering coefficients. The clustering coefficient of a graph is the measure of how closely nodes in a graph tend to cluster together. It gives an indication of the degree to which nodes in a graph tend to form clusters. Mathematically, the clustering coefficient is the fraction of a node’s neighbors (nodes connected to that node via an edge), that are also neighbors with each other.\nThere are two primary types of clustering coefficients: local coefficients and global coefficients. A local coefficient is a measure of how many of the node’s neighbors are also neighbors of each other, relative to the total number of possible connections between those neighbors. Mathematically, it is expressed as: \\[C(v)=\\frac{2 \\cdot\\; T(v)}{k_{v}​×(k_{v}​−1)}​\\]\nWhere v is a node in a graph, \\(T(v)\\) is the actual number of edges between the neighbors of v, and \\(k_{v}​×(k_{v}​−1)\\) is the maximum total number of possible connections between the neighbors of v.\nThe global coefficient is the average of the local coefficients of all the nodes in a graph. Both coefficients range between 0 and 1.\nWe can calculate the global and local coefficients for our network:\n\nclustering_coefficients = nx.clustering(G_sub_louvain)\nprint(clustering_coefficients)\n\nThe code above gives the clustering coefficients for every node in the graph. If we want the global coefficient, we can use:\n\naverage_clustering_coefficient = nx.average_clustering(G_sub_louvain)\nprint(average_clustering_coefficient)\n\nOur high clustering coefficient indicates that the nodes in this network tend to create tightly knit groups with a high density of connections."
  },
  {
    "objectID": "docs/7_Network_Analysis/network_analysis_notebook.html#citations",
    "href": "docs/7_Network_Analysis/network_analysis_notebook.html#citations",
    "title": "Networks and NetworkX",
    "section": "8. Citations",
    "text": "8. Citations\nAl-Taie, M. Z., & Kadry, S. (2017). Python for graph and network analysis. In Advanced information and knowledge processing. https://doi.org/10.1007/978-3-319-53004-8\nBes, P., 2015. Once upon a Time in the East. The Chronological and Geographical Distribution of Terra Sigillata and Red Slip Ware in the Roman East. Roman and Late Antique Mediterranean Pottery 6. Archaeopress, Oxford.\nBrughmans, T., & Peeples, M. A. (2023). Network Science in Archaeology. https://doi.org/10.1017/9781009170659\nMills, B. J., Clark, J. J., Peeples, M. A., Haas, W. R., Roberts, J. M., Hill, J. B., Huntley, D. L., Borck, L., Breiger, R. L., Clauset, A., & Shackley, M. S. (2013). Transformation of social networks in the late pre-Hispanic US Southwest. Proceedings of the National Academy of Sciences, 110(15), 5785–5790. https://doi.org/10.1073/pnas.1219966110\nSocial and economic networks. (n.d.). QuantEcon DataScience. https://datascience.quantecon.org/applications/networks.html\nSayama, Hiroki. “15.6: Generating Random Graphs.” Mathematics LibreTexts, Libretexts, 30 Apr. 2024, math.libretexts.org/Bookshelves/Scientific_Computing_Simulations_and_Modeling/Introduction_to_the_Modeling_and_Analysis_of_Complex_Systems_(Sayama)/15%3A_Basics_of_Networks/15.06%3A_Generating_Random_Graphs."
  },
  {
    "objectID": "docs/4_Advanced/advanced_network_analysis/intro_to_python_network_analysis.html",
    "href": "docs/4_Advanced/advanced_network_analysis/intro_to_python_network_analysis.html",
    "title": "Network analysis workshop pre-reading",
    "section": "",
    "text": "Python is a high-level programming language known for its readability and versatility, designed by Guido van Rossum and first released in 1991. A “high level” programming language is one that provides a strong degree of abstraction from the hardware and low-level operations of a computer: high-level programming languages are known for their versatility and ease of use, and are the preferred choice for tasks such as data analysis. Examples include R, Java, and Python.\n\n\n\nInstead of installing python directly, we’ll be installing it using anaconda, a package management system for R and Python. A package management system is a collection of software tools that automate and simplify installing and managing libraries, collections of pre-written code that can be used to perform common tasks without having to write code from scratch. For instance, suppose we wanted to graph the function \\(y=x^{2}\\) in python. Doing so would require installing and importing the numpy and matplotlib libraries. If we wanted to graph the same function in R, we would download and import the ggplot2 library.\n\n\nWe’ll be installing a smaller version of anaconda called miniconda.\n\nPress here to install on windows\nPress here to find your installation version for Mac.\n\n\n⚠️ For Macs: It is important to pay attention to the version you download: different processors will require different versions. - To check your processor, click on the Apple logo in the top-left of your screen, and select “About This Mac” - Look for the processor information. - if this says something like 3.2 GHz Intel Core i5 or something like x64-based processor choose the macOS Intel x86 64-bit version - if this says something like Apple M1 choose the macOS Apple M1 64-bit version - We recommend choosing the .pkg version of the installer.\n\nOnce you have downloaded the installer, run the installer.\n\nMake sure you choose a sensible installation location; you can ignore any warnings about spaces in names.\n\nCheck the following options, if available:\n\nCreate shortcuts\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.10\nClear the package cache upon completion.\n\nRun the installer, which can take a while.\n\n\n\n\n\nOnce you’ve installed minconda and followed the installation steps in the installer, open a miniconda command line window.\n\nOn Windows, press the search tab and search Anaconda prompt. Right-click on the result and press run as administrator.\nOn Mac, click the Launchpad icon in the Dock, type Terminal in the search field, then click “Terminal”.\n\nIf this doesn’t work, open the Finder, then open the /Applications/Utilities folder, and finally double-click Terminal.\n\n\nOnce your command prompt is running, enter the following command:\nconda create -n networks jupyter\nThis will create a new environment, a self-contained directory that will contain everything needed to run a specific project. This is particularly important as it prevents package conflicts from arising when working on multiple projects. For instance, if one project requires python 3.8, and another requies python 4.11, we can create one environment which uses python 3.8, and another which uses python 4.11.\nThe conda create -n networks jupyter command creates a new environment called networks which contains the jupyter library, a library for creating and running jupyter notebooks, interactive notebooks that contain code, visualizations and text.\nOnce the code above has finished running, enter:\nconda activate networks to activate the environment.\n\n\n\nif the steps above were done correctly, on windows, you should see something like this.\n\n\n\n\n\n\nWe can now install the packages used in the network_analysis workshop. To do so, run the following commands in your console:\npip install networkx matplotlib pandas numpy geopandas contextily community\nThis will install all the packages required for the notebook.\n\n\n\nTBA when we find out where the notebooks will be\n\n\n\nFor the network analysis workshop, we’ll be using the ICRATES database of tablewares in the Roman East. Please follow the link here and download the excel files called Network dataset and Locations dataset under “Network visualisation with The Vistorian”."
  },
  {
    "objectID": "docs/4_Advanced/advanced_network_analysis/intro_to_python_network_analysis.html#setting-up-python-and-accessing-the-network-analysis-notebook",
    "href": "docs/4_Advanced/advanced_network_analysis/intro_to_python_network_analysis.html#setting-up-python-and-accessing-the-network-analysis-notebook",
    "title": "Network analysis workshop pre-reading",
    "section": "",
    "text": "Python is a high-level programming language known for its readability and versatility, designed by Guido van Rossum and first released in 1991. A “high level” programming language is one that provides a strong degree of abstraction from the hardware and low-level operations of a computer: high-level programming languages are known for their versatility and ease of use, and are the preferred choice for tasks such as data analysis. Examples include R, Java, and Python.\n\n\n\nInstead of installing python directly, we’ll be installing it using anaconda, a package management system for R and Python. A package management system is a collection of software tools that automate and simplify installing and managing libraries, collections of pre-written code that can be used to perform common tasks without having to write code from scratch. For instance, suppose we wanted to graph the function \\(y=x^{2}\\) in python. Doing so would require installing and importing the numpy and matplotlib libraries. If we wanted to graph the same function in R, we would download and import the ggplot2 library.\n\n\nWe’ll be installing a smaller version of anaconda called miniconda.\n\nPress here to install on windows\nPress here to find your installation version for Mac.\n\n\n⚠️ For Macs: It is important to pay attention to the version you download: different processors will require different versions. - To check your processor, click on the Apple logo in the top-left of your screen, and select “About This Mac” - Look for the processor information. - if this says something like 3.2 GHz Intel Core i5 or something like x64-based processor choose the macOS Intel x86 64-bit version - if this says something like Apple M1 choose the macOS Apple M1 64-bit version - We recommend choosing the .pkg version of the installer.\n\nOnce you have downloaded the installer, run the installer.\n\nMake sure you choose a sensible installation location; you can ignore any warnings about spaces in names.\n\nCheck the following options, if available:\n\nCreate shortcuts\nAdd Miniconda3 to my PATH environment variable\nRegister Miniconda3 as my default Python 3.10\nClear the package cache upon completion.\n\nRun the installer, which can take a while.\n\n\n\n\n\nOnce you’ve installed minconda and followed the installation steps in the installer, open a miniconda command line window.\n\nOn Windows, press the search tab and search Anaconda prompt. Right-click on the result and press run as administrator.\nOn Mac, click the Launchpad icon in the Dock, type Terminal in the search field, then click “Terminal”.\n\nIf this doesn’t work, open the Finder, then open the /Applications/Utilities folder, and finally double-click Terminal.\n\n\nOnce your command prompt is running, enter the following command:\nconda create -n networks jupyter\nThis will create a new environment, a self-contained directory that will contain everything needed to run a specific project. This is particularly important as it prevents package conflicts from arising when working on multiple projects. For instance, if one project requires python 3.8, and another requies python 4.11, we can create one environment which uses python 3.8, and another which uses python 4.11.\nThe conda create -n networks jupyter command creates a new environment called networks which contains the jupyter library, a library for creating and running jupyter notebooks, interactive notebooks that contain code, visualizations and text.\nOnce the code above has finished running, enter:\nconda activate networks to activate the environment.\n\n\n\nif the steps above were done correctly, on windows, you should see something like this.\n\n\n\n\n\n\nWe can now install the packages used in the network_analysis workshop. To do so, run the following commands in your console:\npip install networkx matplotlib pandas numpy geopandas contextily community\nThis will install all the packages required for the notebook.\n\n\n\nTBA when we find out where the notebooks will be\n\n\n\nFor the network analysis workshop, we’ll be using the ICRATES database of tablewares in the Roman East. Please follow the link here and download the excel files called Network dataset and Locations dataset under “Network visualisation with The Vistorian”."
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html",
    "href": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html",
    "title": "01.1 - Common Stata_Kernel Error",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer.\nHave followed the instructions in 01 - Jupyter and Stata."
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#prerequisites",
    "href": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#prerequisites",
    "title": "01.1 - Common Stata_Kernel Error",
    "section": "",
    "text": "Have installed Stata onto your personal computer.\nHave installed Anaconda Navigator onto your personal computer.\nHave followed the instructions in 01 - Jupyter and Stata."
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#outcome",
    "href": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#outcome",
    "title": "01.1 - Common Stata_Kernel Error",
    "section": "Outcome",
    "text": "Outcome\nThe only outcome of this notebook is to fix the error which occurs when you run\n  python -m stata_kernel.install\nThe correct output is Installing Jupyter kernel spec, if this is the output you see then you do not have an issue and can move on to the next notebook, but if what pops up is Cannot import kernel Installing Jupyter kernel spec then you have an error and stata_kernel will not work. More information about this error can be found on this Github Post. This is a common issue which arises from the main Stata application/exe being installed in a place where stata_kernel does not expect it.\nThis notebook will be a step by step guide for both Mac and Windows to solve this issue. First we will cover Mac (All sections starting with 1.X) and then in the lower section we will cover Windows (Sections starting with 2.X). The steps are very similar, but because of OS differences some steps will be different."
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#mac",
    "href": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#mac",
    "title": "01.1 - Common Stata_Kernel Error",
    "section": "1.0 Mac",
    "text": "1.0 Mac\nBelow we will show steps for Mac if you have a Windows computer scroll down to section 2.0\n\n1.1 Browing all files on Mac\nIn order to follow this guide you must be able to browse all of your files.\n\nClick anywhere on the desktop to ensure Finder is active in the menu bar.\nGo to the top menu bar and click on “Finder” &gt; “Settings” (or “Preferences” in older macOS versions)\nIn the Finder Settings window, select the “General” tab.\nUnder the “Show these items on the desktop” section, check the box next to “Hard disks”\nClose the Settings window.\nYou may also need to restart your computer for the change to take place\n\n\n\n1.2 Find the path to your Stata app\n\nOpen your hard drive icon on your desktop\n\n\n\n\nMacintosh HD logo on Desktop\n\n\n\nNavigate to our Stata installation\nWrite down to path for the Stata Installation it should look similar to /Applications/Stata/StataSE.app/Contents/MacOS/StataSE\nWrite this path down and save it for later you will need it. Also make sure it is case sensitive as macOS is case sensitive.\n\n\n\n1.3 Show hidden files and folders on Mac\n\nOpen Terminal from the Applications &gt; Utilities folder. (Or Command + Space and type in Terminal).\nType in the following command\n\ndefaults write com.apple.Finder AppleShowAllFiles YES\n\nNow restart you computer for the change to take place\n\nYou will now be able to see all folders including the hidden ones like your OS and other things Mac does not by default show the user. Most importantly the /usr/local/opt folder where by default the anaconda3 instalation is.\nMore information about this step can be found in this Article.\n\nNote: Being able to see all files on Mac is needed for this fix, but does have some risks. If you are an inexperienced user then exposing hidden folders system-wide could potentially lead to accidental modifications of important system files if you’re not careful.\n\n\nIf you wish to disable seeing hidden files after the fix is completed it is simple. After you reach the end of the Mac section and see that stata_kernel now works. Type in the following command into your termianal\n\ndefaults write com.apple.Finder AppleShowAllFiles NO\nAfter you type in the command restart your computer, you now will have returned to not being able to see all files.\n\n\n1.4 Navigate to your Anaconda3 Installation\n\nThere are all kinds of commands which can be ran in the terminal (Command + Space) to find the path for the Anaconda installation some examples are\n\nwhich conda\nconda info --base\nwhereis conda\nThey will show where your Anaconda installation is\n\nOnce you have the path follow it by clicking on Macintosh HD on your desktop and going to it. The path will look similar to /opt/anaconda3/envs/stata_r_env/lib/python3.x/site-packages/stata_kernel. The x in python3 will be the version number of Python.\nOnce inside of the stata_kernel folder open config.py to do this you will need a code editor of some kind. Some free options are Sublime Text, VSCode, Brackets just choose one which is simple and then open the file.\n\nOnce the file is open replace the value of the variable stata_path with your stata path which you wrote down earlier.\n\nThe code which you are looking for is on line 84\nstata_path = r'PASTE YOUR PATH TO STATA FROM EARLIER BETWEEN THE APOSTROPHE’\n        if not stata_path:\n            self.raise_config_error('stata_path')\nAfter editing it make sure that - r’ ahead of the apostrophe is intact and don’t change anything else\n\nSave and close the file.\n\n\n\n1.4.1 An alternate approach to finding your Anaconda3 Installation (and the config.py file)\nIf this did not work you could also search for config.py in spotlight search (Command + Space). This approach can also work, but will depend on how many applications you have installed on your computer as many applications will have a config.py file. If you do this make sure that it is the correct one for stata_kernel by looking at the red square at the bottom of finder and seeing if it is in the correct path. The path should look like /opt/anaconda3/envs/stata_r_env/lib/python3.x/site-packages/stata_kernel. Just without the slashes and with nice folder icons.\n\n\n\nFinder File Path on Mac\n\n\n\n\n1.5 Find your .stata_kernel.conf file and edit it\n\nYou should be able to find this file using spotlight search (Command + Space), you will again need a code editor to open the file.\nOnce inside, change the variable stata_path to be equal to the path to the main Stata installation which you saved earlier\nLine 2 of the file will look like\n\nstata_path = YOUR PATH HERE\nThis time do not include ’r and apostrophes. Except for the stata_path do not change anything else.\n\n\n1.6 Seeing if it worked.\n\nOpen Terminal (Command + Space and type in Terminal).\nType in\n\nconda activate stata_r_env \n\n\n\nEnvironments in Terminal\n\n\nYou will see on the left that you are now in the environment called (stata_r_env) this means it worked. The difference is the terminal will look different as it a white Mac terminal.\n\nOnce that is done type in\n\npython -m stata_kernel.install\nand the output message should be. Installing Jupyter kernel spec. This means it worked and you are done.\n\nIf the output is Cannot import kernel, Installing Jupyter kernel spec. Then something went wrong in the process. Look back throught this notebook or look at the Github post referenced earlier.\nIf you want to open a local jupyter notebook use the command\n\njupyter notebook"
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#windows",
    "href": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#windows",
    "title": "01.1 - Common Stata_Kernel Error",
    "section": "2.0 Windows",
    "text": "2.0 Windows\nBelow is the section for a Windows computer.\n\n2.1 Finding your path to the main Stata installation\n\nIn the file explorer find your path to the .exe file for Stata. The path should look like C:\\Program Files\\Stata18\\Stata-SE64.exe. Write it down and save it for later.\n\nImportat: Do not type Stata into your windows search and right click on it and select open file location this will not be the correct path.\nThe path will most likely be the folder Program Files under your default drive.\n\n\n2.2 Navigate to your Anaconda3 Installation\n\nOpen your file explorer and look for your Anaconda3 installation\nIf you are unsure where to look you can run the command below in the anaconda prompt (this can be opened by clicking the windows key and typing in anaconda prompt)\n\nconda info --base\nwhich will have as an output where the Anaconda3 installation is.\n\nInside of the Anaconda path find the path to the stata_kernel, the path should look like D:\\Users\\USERNAME\\anaconda3\\envs\\stata\\Lib\\site-packages\\stata_kernel\nInside open the config.py file, you can do this using notepad. You do not need any new programs to open this file.\n\n\n\n2.3 How to open a .py file using notepad\n\nRight-click on the config.py file.\nSelect “Open with” from the context menu.\nChoose “Notepad” from the list of available programs.\nIf Notepad isn’t listed as an option: Click on “Choose another app” or “More apps”.\nScroll down and click on “Notepad” if you see it, or select “Look for another app on this PC”. Navigate to C:and select “notepad.exe”.\n\n\n\n2.4 Editing the file itself\n\nOnce the file is open replace the value of the variable stata_path with your stata path which you wrote down earlier. It should look similar to C:\\Program Files\\Stata18\\Stata-SE64.exe\n\nThe code which you are looking for is on line 84\nstata_path = r'PASTE YOUR PATH TO STATA FROM EARLIER BETWEEN THE APOSTROPHE’\n        if not stata_path:\n            self.raise_config_error('stata_path')\nAfter editing it make sure that - r’ ahead of the apostrophe is intact and don’t change anything else\n\nSave and close the file.\n\n\n\n2.5 Finding and editing the .stata_kernel.conf\n\nUnder your users directory look for the .stata_kernel.conf, you can also search for it using the search function within file explorer.\nOpen the file using notepad the same way as with the config.py file. If you do not remember how to do this you can follow the instructuons in Section 2.3\nOnce inside, change the variable stata_path to be equal to the path to the main Stata installation which you saved earlier Line 2 of the file will look like\n\nstata_path = YOUR PATH HERE\nThis time do not include ’r and apostrophes. Except for the stata_path do not change anything else. 4. Save and close the file\n\n\n2.6 Seeing if it worked.\n\nOpen Anaconda Prompt.\nType in\n\nconda activate stata_r_env \n\n\n\nEnvironments in Anaconda Prompt\n\n\nYou will see on the left that you are now in the environment called (stata_r_env) this means it worked.\n\nOnce that is done type in\n\npython -m stata_kernel.install\nand the output message should be. Installing Jupyter kernel spec. This means it worked and you are done.\n\nIf the output is Cannot import kernel, Installing Jupyter kernel spec. Then something went wrong in the process.\nIf you want to open a local jupyter notebook use the command\n\njupyter notebook"
  },
  {
    "objectID": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#conclusion",
    "href": "docs/5_Research/econ490-stata/01_1_Stata_Kernel_Error.html#conclusion",
    "title": "01.1 - Common Stata_Kernel Error",
    "section": "3. Conclusion",
    "text": "3. Conclusion\nThis should solve the common issue of having the Cannot import kernel Installing Jupyter kernel spec error message when trying to install stata_kernel."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html",
    "href": "pages/installation/running_comet_locally.html",
    "title": "Running Comet Locally",
    "section": "",
    "text": "Run the COMET website locally using Git and Docker."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#outcome",
    "href": "pages/installation/running_comet_locally.html#outcome",
    "title": "Running Comet Locally",
    "section": "",
    "text": "Run the COMET website locally using Git and Docker."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#preface",
    "href": "pages/installation/running_comet_locally.html#preface",
    "title": "Running Comet Locally",
    "section": "1.0 Preface",
    "text": "1.0 Preface\nThis notebook will be split into two parts. All sections beginning with a one will be for Windows, and all sections with a two will be for Mac. We will begin with Windows."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#cloning-the-github-repository-windows",
    "href": "pages/installation/running_comet_locally.html#cloning-the-github-repository-windows",
    "title": "Running Comet Locally",
    "section": "1.1 Cloning the Github Repository (Windows)",
    "text": "1.1 Cloning the Github Repository (Windows)\nOn Windows, first visit the offical Git Website and click download for Windows. After installation, follow the instructions in the install wizard, you can use the default settings (unless you have some other preferences). Once this process is finished, you have two options. 1. you can clone the repository using the Command Line, or 2. you can clone it using a GUI Git Client like Github Desktop. This is personal preference, if you are less comfortable using the terminal you can use Github Desktop. For the next steps you only need to follow either 1.1.1 or 1.1.2, as they accomplish the same task of cloning the repository."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#cloning-the-repository-using-command-line",
    "href": "pages/installation/running_comet_locally.html#cloning-the-repository-using-command-line",
    "title": "Running Comet Locally",
    "section": "1.1.1 Cloning the Repository Using Command Line",
    "text": "1.1.1 Cloning the Repository Using Command Line\nOpen the application Git Bash using Windows Search. If you wish to clone it to a specific location, navigate to that directory using the command cd \"your\\path\\to\\it\" if not it will install in the default location (the GitHub Folder). Next, inside Git Bash run the command:\ngit clone https://github.com/ubcecon/comet-open.git\nRunning this command will prompt you for your sign-in information for your GitHub account, follow the steps in the terminal. If you do not have an account, create one as it is needed for this notebook.\nAfter this process if completed, you will have a new directory named comet-open inside your GitHub Folder."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#cloning-the-repository-using-github-desktop",
    "href": "pages/installation/running_comet_locally.html#cloning-the-repository-using-github-desktop",
    "title": "Running Comet Locally",
    "section": "1.1.2 Cloning the Repository Using GitHub Desktop",
    "text": "1.1.2 Cloning the Repository Using GitHub Desktop\nIf you have not already, install GitHub Desktop, by clicking download and following the instructions in the install wizard. Once this is done, open GitHub Desktop and sign in using your GitHub Account.\nAt the top, click on File -&gt; Clone Repository and in the URL tab, enter https://github.com/ubcecon/comet-open.git, then next choose the local path and click clone."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#enable-virtualization-and-install-wsl2",
    "href": "pages/installation/running_comet_locally.html#enable-virtualization-and-install-wsl2",
    "title": "Running Comet Locally",
    "section": "1.2 Enable Virtualization and Install WSL2",
    "text": "1.2 Enable Virtualization and Install WSL2\nTo run COMET locally, you will need to activate virtualization and install Windows Subsystem for Linux 2 (WSL2). To do this, you will need to restart your computer and enter your BIOS. This can be done by pressing F2, Delete, or another key during the restart process. The correct key will flash on screen. Once inside the BIOS look for a setting related to virtualization, which may be called “Intel Virtualization Technology,” “AMD-V,” or similar, depending on your motherboard manufacturer. Once you find the setting, enable it, save changes, and exit BIOS.\nTo install WSL 2, open PowerShell as Admin by right-clicking on it and selecting Run as Administrator. Then run the command below, during the install process, you will be asked to restart your computer.\nwsl --install\nVirtualization is necessary for this process, but it can have some negative impacts on your computer. None of these are major, but they’re still something to keep in mind. There can be some performance impacts, system slowdowns, and issues with software or application compatibility (Licencing issues, Kernel Level Anti-Cheats for certain videogames, or simply software vendors stating that they do not want their apps running in VM’s).\nIf any of this is an issue, you can always disable Virtualization by going back to your BIOS and disabling the feature in the same way you enabled it."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#installing-docker",
    "href": "pages/installation/running_comet_locally.html#installing-docker",
    "title": "Running Comet Locally",
    "section": "1.3 Installing Docker",
    "text": "1.3 Installing Docker\nGo to the Docker Website and click download for Windows AMD64, then follow the instructions in the installer. During the installation process, ensure that the “Use WSL 2 instead of Hyper-V” option is selected."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#building-comet-locally",
    "href": "pages/installation/running_comet_locally.html#building-comet-locally",
    "title": "Running Comet Locally",
    "section": "1.4 Building COMET Locally",
    "text": "1.4 Building COMET Locally\nOpen an Admin PowerShell Terminal (right-clicking on it and pressing Run as Administrator) and navigate to your path to the GitHub Comet-Open folder.\nIt will look similar to C:\\Users\\your\\path\\to\\GitHub\\comet-open. Once you find this path, write it down and then in the PowerShell type in.\ncd \"C:\\Users\\your\\path\\to\\GitHub\\comet-open\"\nYou can then check if you are on the main branch of comet-open by running the following command inside of the command directory from before. If you are not already on the main branch, then this command will swap you back to it.\ngit checkout main\nNext, run the command:\ndocker build -t comet-local:latest -f C:\\Users\\your\\path\\to\\GitHub\\comet-open\\meta\\building\\.dockerfile .\nThis will build the image for COMET locally. This process will take some time, once it is completed, you will have a local image of the COMET website called comet-local with the tag latest. At this point, you are almost done.\nNext, run the command\ndocker run -d -p 8080:80 comet-local:latest\nThis will run the image inside of a container which can then be viewed with the URL http://localhost:8080. Open a browser and type in this URL, you will see the COMET Website displayed locally with all of the notebooks rendered."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#mac",
    "href": "pages/installation/running_comet_locally.html#mac",
    "title": "Running Comet Locally",
    "section": "2.0 Mac",
    "text": "2.0 Mac\nThis process is more difficult on Mac this is because if you have a Mac with a Silicon chip (M Series Chip) it uses ARM64 Architecture as opposed to AMD64 like Windows. Docker works far better on AMD64 and most image creation on Mac is emulating AMD64 from ARM64 which takes longer and is far more unstable when doing intensive tasks (like rendering hundreds of notebooks with Quarto). This has been tested working on a Macbook M3 Pro with 18gb of RAM running MacOS Sequoia 15.3 and the build took ~50 minutes to complete."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#cloning-the-github-repository-mac",
    "href": "pages/installation/running_comet_locally.html#cloning-the-github-repository-mac",
    "title": "Running Comet Locally",
    "section": "2.1 Cloning the Github Repository (Mac)",
    "text": "2.1 Cloning the Github Repository (Mac)\nOn Mac, Git can be preinstalled. If it is not yet installed, then we will be following the instructions on the Official Git Website. First, we will install Homebrew. To do this, open a terminal (press Command + Space and search Terminal) and paste in the command below.\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nThis command will install Homebrew. It may prompt you for your password - you can enter it in the terminal. Once this process finishes, type in the command below into the Terminal and Homebrew will install the Git package.\nbrew install git\nOnce this process is finished you have two options.\n\nYou can clone the repository using the Command Line.\nYou can clone the repository using a GUI Git Client, like GitHub Desktop.\n\nThis is a matter of personal preference. If you are less comfortable using the terminal, you can use GitHub Desktop. You only need to follow either 2.1.1 or 2.1.2, as they accomplish the same task of cloning the repository."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#cloning-the-repository-using-terminal",
    "href": "pages/installation/running_comet_locally.html#cloning-the-repository-using-terminal",
    "title": "Running Comet Locally",
    "section": "2.1.1 Cloning the Repository Using Terminal",
    "text": "2.1.1 Cloning the Repository Using Terminal\nOpen your Terminal as we did before, and then navigate to the directory where you want to clone the repository using the cd command. For example:\ncd ~/Documents\nchanges the directory to your Documents folder.\nClone the repository by running the command:\ngit clone https://github.com/ubcecon/comet-open.git"
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#cloning-the-repository-using-github-desktop-1",
    "href": "pages/installation/running_comet_locally.html#cloning-the-repository-using-github-desktop-1",
    "title": "Running Comet Locally",
    "section": "2.1.2 Cloning the Repository Using GitHub Desktop",
    "text": "2.1.2 Cloning the Repository Using GitHub Desktop\nInstall GitHub Desktop by pressing download and following the installation instructions. Once this is done, open GitHub Desktop and sign in using your GitHub Account.\nAt the top, click on File -&gt; Clone Repository and in the URL tab enter https://github.com/ubcecon/comet-open.git. Next choose the local path and press clone."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#installing-docker-1",
    "href": "pages/installation/running_comet_locally.html#installing-docker-1",
    "title": "Running Comet Locally",
    "section": "2.2 Installing Docker",
    "text": "2.2 Installing Docker\nGo to the Docker Website. Once there, press Download Docker Desktop and select Download for Mac for the correct chip archetype (Intel or Apple Silicon). If you are unsure which chip your Mac has, you can check under About This Mac. Once downloaded, open the .dmg file and drag the Docker icon to your Applications folder. Then open Docker Desktop from your Applications folder. To complete the installation of Docker Desktop, select Use recommended settings, then type your password and then wait for Docker to start. You’ll see the whale icon in your menu bar when it’s ready."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#configuring-docker",
    "href": "pages/installation/running_comet_locally.html#configuring-docker",
    "title": "Running Comet Locally",
    "section": "2.2 Configuring Docker",
    "text": "2.2 Configuring Docker\nIn order to build the image we have to change some setting in the Docker app. First open the Docker application and navigate to settings (Gear Icon). Under General: Disable “Use Rosetta for x86/amd64 emulation.” Once that is done go to resources for the successful build I had 12GB of Ram, 8 cores of CPU and 4GB of Swap Space allocated, I recommend this, but it might differ from computer to computer."
  },
  {
    "objectID": "pages/installation/running_comet_locally.html#build-comet-locally",
    "href": "pages/installation/running_comet_locally.html#build-comet-locally",
    "title": "Running Comet Locally",
    "section": "2.3 Build COMET Locally",
    "text": "2.3 Build COMET Locally\nNext, navigate to the folder where you cloned the comet-open directory and copy the path to the comet-open folder. You can do this by right-clicking on the folder and clicking copy. Then, open your Terminal again (Command + Space and search for Terminal), and change the directory using cd to the path to the folder. It should look something like this:\ncd /Users/Path/to/your/comet-open\nYou can then check if you are on the main branch of comet-open by running the following command.\ngit branch\nThis will list out which branches are available, and there will be a * next to the branch that you are on. If you are not already on the main branch, this command will swap you back to it:\ngit switch main\nNext, run the following command, but be sure to change the path to YOURS!!! And made sure to set YOUR platform - this should be arm64 or amd64 depending on your Mac.\ndocker buildx build --platform linux/arm64 -f ./meta/building/.dockerfile -t comet-main:local --load .\nThis will build the image for COMET locally. This process will take a long time (~50 minutes), so be patient! Once it is completed, you will have a local image of the COMET website called comet-local with the tag latest. At this point you are almost done\nNext run the command\ndocker run -d -p 8080:80 comet-local:latest\nThis will run the image inside of a container which can then be viewed with the URL http://localhost:8080. Open a browser and type in this URL, you will see the COMET website displayed locally with all of the notebooks rendered."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "Spring 2025 Update\n\n\n\nWe have recently updated and fixed several notebooks and other issues on the website.\n\nIf you have an old version of the GitHub project on your Jupyter account, you should delete it and load the new version.\nLinks to the website may not be working currently.\n\n(Last Update: 2024-01-03)\n\n\n\n\n\nCOMET (Creating Online Materials for Econometric Teaching) is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2022 that seeks to provide fundamental econometrics learning resources for students and teachers alike.\nBased at UBC’s Vancouver School of Economics, our team consists of faculty and students (at all levels) working in partnership to develop hands-on learning modules that explore the real-world applications of economic theory using statistics.\n\n\n\nVancouver School of Economics\n\n\n\n\n\n\n\n\n\n\nThese modules cover topics from beginner to advanced applications of statistical methods in economics. Topics include:\n\nDescriptive statistics\nProbability\nEstimation\nHypothesis testing\nAnalysis of Variance\nSimple and multiple regression\nTime series analysis\nSimultaneous equation estimation\nDifference in Differences\n\nModules can be accessed on the left-hand side of this page. They are written as JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself, complete the exercises and get automatic feedback throughout your learning. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n\nThese modules, focused on economic questions, models, and data, use the interactive notebook software, JupyterNotebooks to synthesize theoretical learning, practice, and analysis into a single learning experience that is appropriate for either laboratory or flipped classroom instruction.\nOur work adapts proven pedagogical strategies to integrate practical learning throughout economics curriculum in order to provide students with the opportunity to improve their technical data skills.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating COMET resources into your instruction, check out our Using COMET for Teaching page.\n\n\n\n\n\n\n\n\nThis project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teachings’. https://comet.arts.ubc.ca/.\n\n\n\n\n\nHowever, some notebooks have an additional suggested attribution (especially those in GEOG 374). Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page.\n\n\n\n\nCOMET is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with COMET!\n\nSome partners from other fields have made substantial contributions to this project. We call these people part of  COMET+. These materials are particularly relevant for interdisciplinary students in economics or other fields who need to combine insights from econometrics with other statistical approaches.\n\n\n\n\nUBC’s Department of Geography: Hewitt Lab - Nina Hewitt, Mike Jerowsky, and Kevin Pierce\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe COMET Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  },
  {
    "objectID": "index.html#about-comet",
    "href": "index.html#about-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "COMET (Creating Online Materials for Econometric Teaching) is a Large Teaching and Learning Enhancement Fund (TLEF) project started at the University of British Columbia in 2022 that seeks to provide fundamental econometrics learning resources for students and teachers alike.\nBased at UBC’s Vancouver School of Economics, our team consists of faculty and students (at all levels) working in partnership to develop hands-on learning modules that explore the real-world applications of economic theory using statistics.\n\n\n\nVancouver School of Economics"
  },
  {
    "objectID": "index.html#getting-started-with-comet",
    "href": "index.html#getting-started-with-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "These modules cover topics from beginner to advanced applications of statistical methods in economics. Topics include:\n\nDescriptive statistics\nProbability\nEstimation\nHypothesis testing\nAnalysis of Variance\nSimple and multiple regression\nTime series analysis\nSimultaneous equation estimation\nDifference in Differences\n\nModules can be accessed on the left-hand side of this page. They are written as JupyterNotebooks and can be viewed in their .html form via a browser, or by downloading them in .ipynb form and launching them in a JupyterHub. If you are affiliated with UBC, you can do this directly from the website via UBC’s in-house JupyterHub called JupyterOpen for which these modules were designed. PIMS offers a non-UBC specific JupyterHub called Syzygy if you are at another institution or JupyterOpen goes down.\nLaunching the notebooks in a JupyterHub will allow you to run the code for yourself, complete the exercises and get automatic feedback throughout your learning. Please let us know if you have any problems - you can submit an issue to our GitHub directory if you find something that you think could be improved. Happy learning!\n\n\n\n\n\n\nThese modules, focused on economic questions, models, and data, use the interactive notebook software, JupyterNotebooks to synthesize theoretical learning, practice, and analysis into a single learning experience that is appropriate for either laboratory or flipped classroom instruction.\nOur work adapts proven pedagogical strategies to integrate practical learning throughout economics curriculum in order to provide students with the opportunity to improve their technical data skills.\nWe are driven to champion accessibility in our creation of these modules by publishing an open-source library of educational resources for broad instructional needs with the hope of lowering software costs and hardware requirements for students and learning institutions.\nWe welcome any feedback on how our project might be more accessible. This can be done by submitting an issue to our GitHub directory.\nFor more information on integrating COMET resources into your instruction, check out our Using COMET for Teaching page."
  },
  {
    "objectID": "index.html#citing-comet",
    "href": "index.html#citing-comet",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "This project is open-source with a mixture of licenses for the data. Our notebooks are all licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n\n\nCC-BY-SA NC.\nThis means that in general you can use and adapt this project for your own teaching or learning provided that you:\n\nProvide attribution (see our suggestion below).\n\nYou only use this material for non-commercial purposes (i.e. you cannot make money off it)\nIf you produce derivative materials they must share the CC-BY-SA NC license\n\nOur suggested attribution is:\n\n\n\n\n\n\n\n\nAdshade, M., Graves, J., Hasan, H., and other COMET Contributors. 2023. ‘The COMET Project: Creating Online Materials for Econometric Teachings’. https://comet.arts.ubc.ca/.\n\n\n\n\n\nHowever, some notebooks have an additional suggested attribution (especially those in GEOG 374). Check the authors on the notebook page!\nFurther, some of the data used in the project has different attribution requirements. You can find details about the licensing on our copyright page."
  },
  {
    "objectID": "index.html#get-involved",
    "href": "index.html#get-involved",
    "title": "Welcome to COMET!",
    "section": "",
    "text": "COMET is proudly and continually open-source and community driven. We welcome and encourage contributions from students, educators, and the public regardless of what area or field you call home.\n\nLearn how to get involved with COMET!\n\nSome partners from other fields have made substantial contributions to this project. We call these people part of  COMET+. These materials are particularly relevant for interdisciplinary students in economics or other fields who need to combine insights from econometrics with other statistical approaches.\n\n\n\n\nUBC’s Department of Geography: Hewitt Lab - Nina Hewitt, Mike Jerowsky, and Kevin Pierce\nUBC’s Department of Sociology and the Center for Computational Social Science: Laura K. Nelson\n\n\n\n\n\n\n\nLand Acknowledgement\n\n\n\nThe COMET Project and the UBC Vancouver School of Economics are located on the traditional, ancestral and unceded territory of the xʷməθkʷəy̓əm (Musqueam) and Sḵwx̱wú7mesh (Squamish) peoples who have stewarded this land, water and air for time immemorial. We hope that this project will make learning more open, inclusive, and accessible for people whichever land they call home."
  }
]