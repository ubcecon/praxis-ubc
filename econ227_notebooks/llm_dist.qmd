---
title: "LLM Probability Distribution"
author: "Yash Mali, Kaiyan Zhang"
format:
  html:
    embed-resources: true
  markdown:
    html: true
---
## What do Large Language Models do?

Large language models like ChatGPT do something that seems very simple: **Next word prediction.**

What does that mean? It means that given a sequence of words, the model predicts the next word in the sequence. For example, if the input is "The cat sat on the", the model might predict "mat" as the next word.


```{mermaid}
flowchart LR
    THE["the"]
    CAT["cat"]
    SAT["sat"]
    ON["on"]
    MAT["mat"]
    MODEL["LLM"]

    THE --> CAT --> SAT --> ON --> MODEL --> MAT

    style MAT fill:#FFD700,stroke:#333,stroke-width:2px,font-weight:bold
    style MODEL fill:#ADD8E6,stroke:#333,stroke-width:2px
```

We saw an example of predicting just one word. These models predict only one word at a time, but they can do this for very long sequences of words.

For example: "bob went to the store"  to buy some milk.

```{mermaid}
flowchart TB

  subgraph S1["bob went to"]
    direction LR
    B1[bob] --> W1[went] --> T1[to] --> M1[LLM] --> P1[buy]
    style M1 fill:#ADD8E6,stroke:#333,stroke-width:2px
    style P1 fill:#FFD700,stroke:#333,stroke-width:2px,font-weight:bold
  end

  subgraph S2["bob went to buy"]
    direction LR
    B2[bob] --> W2[went] --> T2[to] --> B2b[buy] --> M2[LLM] --> P2[some]
    style M2 fill:#ADD8E6,stroke:#333,stroke-width:2px
    style P2 fill:#FFD700,stroke:#333,stroke-width:2px,font-weight:bold
  end

  subgraph S3["bob went to buy some"]
    direction LR
    B3[bob] --> W3[went] --> T3[to] --> B3b[buy] --> S3b[some] --> M3[LLM] --> P3[milk]
    style M3 fill:#ADD8E6,stroke:#333,stroke-width:2px
    style P3 fill:#FFD700,stroke:#333,stroke-width:2px,font-weight:bold
  end

  %% Connect subgraphs to stack vertically
  S1 --> S2
  S2 --> S3

  style S1 fill:none,stroke:none
  style S2 fill:none,stroke:none
  style S3 fill:none,stroke:none
```

What the model is doing is learning the probability distribution of the next word given the previous words.

The probability of predicting the next word $w_{t}$ given the previous words $w_1, w_2, \ldots, w_{t-1}$ is:

$$
P(w_t \mid w_1, w_2, \ldots, w_{t-1}) = \frac{P(w_1, w_2, \ldots, w_{t-1}, w_t)}{P(w_1, w_2, \ldots, w_{t-1})}
$$

LLMs approximate this probability:

$$
P(w_t \mid w_1, w_2, \ldots, w_{t-1})
$$

The model predicts the next word by selecting the word $w_t$ that maximizes this conditional probability:

$$
\hat{w}_t = \arg\max_{w} P(w \mid w_1, w_2, \ldots, w_{t-1})
$$

These models give a probability distribution over the entire vocabulary (all the words the model was trained on). We can then pick the word with the highest probability as the next word or we can sample from this distribution to get more varied (creative) outputs.

Lets look at an example of how this works in practice:

```{python}
#| echo: false
import warnings
warnings.filterwarnings("ignore")
import matplotlib.pyplot as plt
import seaborn as sns

# Example vocabulary
vocab = ['buy', 'some', 'milk', 'other1', 'other2', 'other3']

# Probabilities at each step (toy example)
probs_step1 = [0.8, 0.05, 0.05, 0.03, 0.04, 0.03]  # 'buy' high
probs_step2 = [0.05, 0.7, 0.1, 0.05, 0.05, 0.05]   # 'some' high
probs_step3 = [0.05, 0.05, 0.75, 0.05, 0.05, 0.05] # 'milk' high

prob_distributions = [probs_step1, probs_step2, probs_step3]
step_labels = ['Step 1: Predict "buy"', 'Step 2: Predict "some"', 'Step 3: Predict "milk"']

fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharey=True)

for i, ax in enumerate(axes):
    sns.barplot(x=vocab, y=prob_distributions[i], palette='muted', ax=ax)
    ax.set_title(step_labels[i])
    ax.set_ylim(0, 1)
    ax.set_ylabel('Probability' if i == 0 else '')
    ax.set_xlabel('Vocabulary')
    # Highlight the max prob bar in gold
    max_idx = prob_distributions[i].index(max(prob_distributions[i]))
    ax.bar(max_idx, prob_distributions[i][max_idx], color='gold')

plt.tight_layout()
plt.show()
```

To get more creative responses you change the distribution at the output where you pick the next word. Very simply this involves making the distribution sharper or flatter.
If you make the distribution sharper, you are more likely to pick the word with the highest probability. If you make it flatter, you are more likely to pick a word that is not the most probable one.

This is called **temperature**. A higher temperature makes the distribution flatter, while a lower temperature makes it sharper. You would want to use a temperature of more than 1 $(1.2-1.5)$ for creative responses, and a temperature of less than 1 $(0.1 - 0.5)$ for more focused responses. For a balanced response, you can use a temperature of $0.7-1$.
Another set of parameters are called top-p and top-k sampling.

```{python}
#| echo: false
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

np.random.seed(42)
vocab_size = 20
vocab = [f'token{i}' for i in range(1, vocab_size+1)]
base_probs = np.random.rand(vocab_size)
base_probs /= base_probs.sum()  # Normalize to sum to 1

def apply_temperature(probs, temp):
    logits = np.log(probs + 1e-20)
    scaled_logits = logits / temp
    exp_logits = np.exp(scaled_logits)
    return exp_logits / exp_logits.sum()

temperatures = [0.5, 1.0, 1.5]
distributions = [apply_temperature(base_probs, t) for t in temperatures]

fig, axes = plt.subplots(1, 3, figsize=(20, 5), sharey=True)
for i, (ax, dist, temp) in enumerate(zip(axes, distributions, temperatures)):
    sns.barplot(x=vocab, y=dist, palette='muted', ax=ax)
    ax.set_title(f'Temperature = {temp}')
    ax.set_ylim(0, 0.25)
    ax.set_ylabel('Probability' if i == 0 else '')
    ax.set_xlabel('Vocabulary')
    ax.tick_params(axis='x', rotation=90) # Rotate x-axis labels
    max_idx = dist.argmax()
    ax.bar(max_idx, dist[max_idx], color='gold')

plt.tight_layout()
plt.show()
```

In the example above, we see how the probability distribution changes with different temperatures. A high temperature (1.5) results in a flatter distribution, meaning the model is more likely to sample from less probable tokens, while a low temperature (0.5) results in a sharper distribution, favoring the most probable tokens.

Note that while this is with words and language, the same idea applies to any sequential data, like stock prices, weather data, etc. The model learns the probability distribution of the next value given the previous values.

```{python}
#| echo: false

import numpy as np
import matplotlib.pyplot as plt

n = 10
m = 9

# Generate actual data: random walk + small trend
actual_data = np.cumsum(np.random.normal(0, 1, n)) + 50

# Predictor approximates entire data closely with small noise everywhere
predicted = actual_data + np.random.normal(0, 0.2, n)

# Posterior uncertainty: low and roughly constant over entire period
posterior_std = np.full(n, 0.3)

upper = predicted + posterior_std
lower = predicted - posterior_std

plt.figure(figsize=(6,6))
plt.plot(range(n), actual_data, label="Actual Data", color='blue')
plt.plot(range(n), predicted, label="Predicted", color='orange')
plt.axvline(x=m-1, color='black', linestyle='--', label="Observed / Future Split")

plt.xlabel("Time")
plt.ylabel("Value")
plt.title("Stock price over time.")
plt.legend()
plt.show()
```

Typically, you would use more inputs to the model than the price history. This can public sentiment, news, or other features that might affect the price. The model learns the joint distribution of these features and the price, allowing it to make predictions about future prices.