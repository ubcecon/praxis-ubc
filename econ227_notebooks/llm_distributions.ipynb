{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: ECON 227 - How Do Large Language Models Predict?\n",
        "date: '2025-08-04'\n",
        "author: prAxIs UBC Team <br> Krishaant Pathman, Yash Mali, Kaiyan Zhang\n",
        "---\n",
        "\n",
        "### Prerequisite\n",
        "\n",
        "Before you start, make sure you have the required libraries installed, if not, **uncomment the lines below (i.e. remove the #) and run the cell to install them**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install yfinance finvizfinance transformers pandas numpy statsmodels holidays plotly ipywidgets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important:** Run this cell to load the libraries we need for running this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load libraries we need to run this notebook\n",
        "import yfinance as yf\n",
        "from finvizfinance.quote import finvizfinance\n",
        "from transformers import pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import pandas as pd\n",
        "import holidays\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import seaborn as sns \n",
        "import warnings\n",
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime, timedelta\n",
        "from math import sqrt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction: Distribution of LLM Predictions\n",
        "\n",
        "Large language models like ChatGPT do something that seems very simple: **Next word prediction**.\n",
        "\n",
        "What does that mean? It means that given a sequence of words, the model predicts the next word in the sequence. For example, if the input is \"The cat sat on the\", the model might predict \"mat\" as the next word.\n",
        "\n",
        "![](media/next_word_prediction.png)\n",
        "\n",
        "We saw an example of predicting just one word. These models predict only one word at a time, but they can do this for very long sequences of words.\n",
        "\n",
        "For example: \"bob went to the store\" to buy some milk.\n",
        "\n",
        "![](media/sequence_words.png)\n",
        "\n",
        "What the model is doing is learning the probability distribution of the next word given the previous words.\n",
        "\n",
        "The probability of predicting the next word $w_{t}$ given the previous words $w_1, w_2, \\ldots, w_{t-1}$ is:\n",
        "\n",
        "$$\n",
        "P(w_t \\mid w_1, w_2, \\ldots, w_{t-1}) = \\frac{P(w_1, w_2, \\ldots, w_{t-1}, w_t)}{P(w_1, w_2, \\ldots, w_{t-1})}\n",
        "$$\n",
        "\n",
        "LLMs approximate this probability:\n",
        "\n",
        "$$\n",
        "P(w_t \\mid w_1, w_2, \\ldots, w_{t-1})\n",
        "$$\n",
        "\n",
        "The model predicts the next word by choosing the word that, based on everything so far, seems the most likely to come next. The word $w_t$ that maximizes this conditional probability:\n",
        "\n",
        "$$\n",
        "\\hat{w}_t = \\arg\\max_{w} P(w \\mid w_1, w_2, \\ldots, w_{t-1})\n",
        "$$\n",
        "\n",
        "These models give a probability distribution over the entire vocabulary (all the words the model was trained on). We can then pick the word with the highest probability as the next word or we can sample from this distribution to get more varied (creative) outputs.\n",
        "\n",
        "Lets look at an example of how this works in practice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Example vocabulary\n",
        "vocab = ['buy', 'some', 'milk', 'along', 'the', 'way']\n",
        "\n",
        "# Probabilities at each step (toy example)\n",
        "probs_step1 = [0.8, 0.05, 0.05, 0.03, 0.04, 0.03]  # 'buy' high\n",
        "probs_step2 = [0.05, 0.7, 0.1, 0.05, 0.05, 0.05]   # 'some' high\n",
        "probs_step3 = [0.05, 0.05, 0.75, 0.05, 0.05, 0.05] # 'milk' high\n",
        "\n",
        "prob_distributions = [probs_step1, probs_step2, probs_step3]\n",
        "step_labels = ['Step 1: Predict \"buy\"', 'Step 2: Predict \"some\"', 'Step 3: Predict \"milk\"']\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(8, 8), sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes):\n",
        "    sns.barplot(x=vocab, y=prob_distributions[i], palette='muted', ax=ax)\n",
        "    ax.set_title(step_labels[i])\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_ylabel('Probability' if i == 0 else '')\n",
        "    ax.set_xlabel('Vocabulary')\n",
        "    # Highlight the max prob bar in gold\n",
        "    max_idx = prob_distributions[i].index(max(prob_distributions[i]))\n",
        "    ax.bar(max_idx, prob_distributions[i][max_idx], color='gold')\n",
        "\n",
        "    for patch, token, prob in zip(ax.patches, vocab, prob_distributions[i]):\n",
        "        height = patch.get_height()\n",
        "        ax.annotate(\n",
        "            f\"{prob:.2f}\",                            \n",
        "            xy=(patch.get_x() + patch.get_width() / 2, height),  \n",
        "            xytext=(0, 3),       \n",
        "            textcoords=\"offset points\",\n",
        "            ha='center', va='bottom',\n",
        "            fontsize=9\n",
        "        )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get more creative responses you change the distribution at the output where you pick the next word. Very simply this involves making the distribution sharper or flatter.\n",
        "If you make the distribution sharper, you are more likely to pick the word with the highest probability. If you make it flatter, you are more likely to pick a word that is not the most probable one.\n",
        "\n",
        "This is called **temperature**. A higher temperature makes the distribution flatter, while a lower temperature makes it sharper. You would want to use a temperature of more than 1 $(1.2-1.5)$ for creative responses, and a temperature of less than 1 $(0.1 - 0.5)$ for more focused responses. For a balanced response, you can use a temperature of $0.7-1$.\n",
        "Another set of parameters are called top-p and top-k sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Example vocabulary\n",
        "vocab = [\n",
        "    'apple', 'banana', 'cherry', 'date', 'elderberry',\n",
        "    'fig', 'grape', 'honeydew', 'kiwi', 'lemon',\n",
        "    'mango', 'nectarine', 'orange', 'papaya', 'quince',\n",
        "    'raspberry', 'strawberry', 'tangerine', 'ugli', 'watermelon'\n",
        "]\n",
        "\n",
        "# Normalize to sum to 1\n",
        "vocab_size = len(vocab)\n",
        "base_probs = np.random.rand(vocab_size)\n",
        "base_probs /= base_probs.sum()  \n",
        "\n",
        "def apply_temperature(probs, temp):\n",
        "    logits = np.log(probs + 1e-20)\n",
        "    scaled_logits = logits / temp\n",
        "    exp_logits = np.exp(scaled_logits)\n",
        "    return exp_logits / exp_logits.sum()\n",
        "\n",
        "temperatures = [0.5, 1.0, 1.5]\n",
        "distributions = [apply_temperature(base_probs, t) for t in temperatures]\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(8, 8), sharey=True)\n",
        "for i, (ax, dist, temp) in enumerate(zip(axes, distributions, temperatures)):\n",
        "    sns.barplot(x=vocab, y=dist, palette='muted', ax=ax)\n",
        "    ax.set_title(f'Temperature = {temp}')\n",
        "    ax.set_ylim(0, 0.25)\n",
        "    ax.set_ylabel('Probability' if i == 0 else '')\n",
        "    ax.set_xlabel('Vocabulary')\n",
        "    ax.tick_params(axis='x', rotation=90)\n",
        "\n",
        "    # Highlight the max-prob bar in gold\n",
        "    max_idx = dist.argmax()\n",
        "    ax.patches[max_idx].set_color('gold')\n",
        "\n",
        "    # Annotate each bar with its probability\n",
        "    for patch, prob in zip(ax.patches, dist):\n",
        "        height = patch.get_height()\n",
        "        ax.annotate(\n",
        "            f\"{prob:.2f}\",\n",
        "            xy=(patch.get_x() + patch.get_width() / 2, height),\n",
        "            xytext=(0, 3),            # 3 points vertical offset\n",
        "            textcoords=\"offset points\",\n",
        "            ha='center', va='bottom',\n",
        "            fontsize=9\n",
        "        )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the example above, we see how the probability distribution changes with different temperatures. A high temperature (1.5) results in a flatter distribution, meaning the model is more likely to sample from less probable tokens, while a low temperature (0.5) results in a sharper distribution, favoring the most probable tokens.\n",
        "\n",
        "Note that while this is with words and language, the same idea applies to any sequential data, like stock prices, weather data, etc. The model looks at what happened before and tries to guess what comes next.\n",
        "\n",
        "If you are interested in understanding the inner workings of these models, take a look at the interactive visualization at [The Illustrated Transformer](https://poloclub.github.io/transformer-explainer/) . It provides an excellent, hands-on way to explore the core ideas behind modern language models.\n",
        "\n",
        "\n",
        "So, just like it guesses the next word in a sentence, it can guess the next day's temperature or the next movement in a stock price, based on the pattern it sees in the earlier numbers. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 10\n",
        "m = 9\n",
        "\n",
        "# Generate actual data: random walk + small trend\n",
        "actual_data = np.cumsum(np.random.normal(0, 1, n)) + 50\n",
        "\n",
        "# Predictor approximates entire data closely with small noise everywhere\n",
        "predicted = actual_data + np.random.normal(0, 0.2, n)\n",
        "\n",
        "# Posterior uncertainty: low and roughly constant over entire period\n",
        "posterior_std = np.full(n, 0.3)\n",
        "\n",
        "upper = predicted + posterior_std\n",
        "lower = predicted - posterior_std\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.plot(range(n), actual_data, label=\"Actual Data\", color='blue')\n",
        "plt.plot(range(n), predicted, label=\"Predicted\", color='orange')\n",
        "plt.axvline(x=m-1, color='black', linestyle='--', label=\"Observed / Future Split\")\n",
        "\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Stock price over time.\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Typically, when building a model to predict stock prices, you would use more information than just the past prices. For example, you might include things like public sentiment (how people feel about the stock), news headlines, or other features that could influence the price.\n",
        "\n",
        "In the examples below, that's exactly what we're going to try! We'll see how adding these extra features can help the model make better predictions about what happens next.\n",
        "\n",
        "### Predicting Stock Prices from News Headlines with AI\n",
        "\n",
        "Just like LLMs predict the next word based on the context of prior words:\n",
        "\n",
        "$$\n",
        "P(w_t \\mid w_1, w_2, \\ldots, w_{t-1})\n",
        "$$\n",
        "\n",
        "We can use similar models to predict the next value in a time series, like **stock prices** or **percentage changes** in returns.\n",
        "\n",
        "While prediction of next word in language models is inherently **univariate** that the model predicts the next word based solely on the sequence of previous words, predicting daily stock returns is often a **multivariate** problem as more exogenous factors must be taken into account. Here, we don't just use past stock prices (returns) as context, but also incorporate **additional features** such as public sentiment from news headlines.\n",
        "\n",
        "In other words, instead of predicting the next token from a single stream (words), we predict the next value in a time series using **multiple sources of information**: historical price data and external signals like news sentiment. This richer, multivariate context allows the model to capture more complex relationships and potentially make more accurate forecasts.\n",
        "\n",
        "The comparison of word prediction and stock price prediction is given as follows:\n",
        "\n",
        "| Word Prediction                 | Stock Price Prediction           |\n",
        "|-------------------------------------|------------------------------------------------|\n",
        "| Previous words                      | Past daily returns + aggregated news sentiment |\n",
        "| Next word prediction                | Future return prediction                       |\n",
        "| Attention to important words        | Feature weights on returns **and** sentiment       |   \n",
        "| Temperature to control randomness   | Confidence or prediction intervals in forecasts|\n",
        "| Word probability distribution       | Forecasted return distribution                 |\n",
        "\n",
        "In this case study, we use a language model to analyze real-time news headlines alongside historical stock prices in order to forecast short-term changes in stock value.\n",
        "\n",
        "You will:\n",
        "\n",
        "- Collect news headlines about real companies (like Amazon or Starbucks)\n",
        "- Use a pre-trained AI model to classify the **sentiment** (positive or negative) of these news headlines\n",
        "- Combine that with stock prices\n",
        "- Use a forecasting model to predict future price changes\n",
        "- Visualize your results interactively\n",
        "\n",
        "\n",
        "### Preview the News Data\n",
        "\n",
        "Here we use the `finvizfinance` packages to retrieve real-time news headlines for companies like Starbucks (SBUX)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The code below will give us a snapshot of the 100 most recent news headlines for a particular stock in the last 30 days.  \n",
        "\n",
        "def get_news_data(ticker):\n",
        "    stock = finvizfinance(ticker)\n",
        "    news_df = stock.ticker_news()\n",
        "    news_df = pd.DataFrame(news_df)\n",
        "\n",
        "    # Drop NaN and clean whitespace\n",
        "    news_df = news_df.dropna(subset=[\"Title\"])\n",
        "    news_df = news_df[news_df[\"Title\"].str.strip() != \"\"]\n",
        "    news_df['Title'] = news_df['Title'].str.lower()\n",
        "    news_df['Date'] = pd.to_datetime(news_df['Date'])\n",
        "    news_df['DateOnly'] = news_df['Date'].dt.date\n",
        "    news_df[\"Ticker\"] = ticker.upper()\n",
        "    # Remove the 'Date' column\n",
        "    news_df = news_df.drop(columns=['Date'])\n",
        "\n",
        "    return news_df.reset_index(drop=True)\n",
        "\n",
        "# For the sake of reproducibility (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n",
        "\n",
        "# Uncomment the line below get a more recent snapshot of the data! \n",
        "# SBUX_news_df = get_news_data(\"SBUX\")\n",
        "# SBUX_news_df.to_csv(\"data/SBUX_news.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at what the cleaned news data looks like. We'll start with Starbucks (`SBUX`).\n",
        "\n",
        "Each row is a headline, what website it was from, the date it was published and a ticker indicating what stock it is for. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SBUX_news_df = pd.read_csv(\"data/SBUX_news.csv\")\n",
        "SBUX_news_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a closer look at the news titles. What would you say about their sentiment?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titles = SBUX_news_df['Title'].tolist()\n",
        "print(\"News on Starbucks:\\n\")\n",
        "for i in range(5):\n",
        "    print(titles[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Classifying Headline Sentiment with LLM\n",
        "\n",
        "Earlier, we explored how **large language models (LLMs)** predict the *next word* by learning the probability distribution of possible outcomes based on context.\n",
        "\n",
        "Now, we apply a similar idea to **entire sentences**: in this case, financial news headlines. Instead of predicting the next word, the model assigns a probability to each **sentiment category** (e.g. POSITIVE, NEGATIVE, or NEUTRAL).\n",
        "\n",
        "#### How it works:\n",
        "- A pre-trained model reads the headline.\n",
        "- It assigns probabilities to the sentiment labels.\n",
        "- We keep only **positive** or **negative** headlines, since those are more likely to affect stock prices.\n",
        "  \n",
        "This is like asking:\n",
        "> *Given the words in this sentence, what is the most likely emotion behind it?*\n",
        "\n",
        "We use Hugging Face's `pipeline()` function to load a **DistilBERT** model: a lighter version of the BERT model trained to understand the tone of text. Although it was originally trained on movie reviews, it generalizes well and works surprisingly well on financial headlines too!\n",
        "\n",
        "This builds directly on our earlier discussion of LLMs predicting **probability distributions**, but here, the prediction is over **sentiment classes** rather than words.\n",
        "\n",
        "We now apply a pre-trained **large language model** to each headline.\n",
        "\n",
        "It returns:\n",
        "\n",
        "- `POSITIVE`: news that sounds good (e.g., \"profits surge\")\n",
        "- `NEGATIVE`: news that sounds bad (e.g., \"lawsuit filed\")\n",
        "\n",
        "> *Can you think of a positive and negative news headline?*\n",
        "\n",
        "We skip `NEUTRAL` news to focus on strong market signals.\n",
        "\n",
        "In this case, we apply a pre-trained LLM called **Twitter-RoBERTa**, specifically trained on tweets and social media text. It's well-suited to handling short, informal writing like news headlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")\n",
        "# Here we are just specifying the classifier (AI model) that decides on a sentiment \n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\", device=-1)\n",
        "\n",
        "# define a function to classify sentiment of each text \n",
        "def classify_sentiment(text):\n",
        "    return classifier(text)[0][\"label\"].upper()\n",
        "\n",
        "# define classify_sentiment to entire dataframe\n",
        "def apply_sentiment(news_df):\n",
        "    news_df[\"Sentiment\"] = news_df[\"Title\"].apply(classify_sentiment)\n",
        "    return news_df # [news_df[\"Sentiment\"] != \"NEUTRAL\"] # remove neutral headlines\n",
        "\n",
        "def process_sentiment(news_df):\n",
        "    grouped = news_df.groupby([\"DateOnly\", \"Sentiment\"]).size().unstack(fill_value=0)\n",
        "    grouped = grouped.reindex(columns=[\"POSITIVE\", \"NEGATIVE\"], fill_value=0)\n",
        "    # Calculate a rolling 7-day total of positive headlines\n",
        "    grouped[\"7day_avg_positive\"] = grouped[\"POSITIVE\"].rolling(window=7, min_periods=1).sum()\n",
        "    # Calculate a rolling 7-day total of negative headlines\n",
        "    grouped[\"7day_avg_negative\"] = grouped[\"NEGATIVE\"].rolling(window=7, min_periods=1).sum()\n",
        "    # Calculate the percentage of positive headlines each day (out of total positive + negative)\n",
        "    grouped[\"7day_pct_positive\"] = grouped[\"POSITIVE\"] / (grouped[\"POSITIVE\"] + grouped[\"NEGATIVE\"])\n",
        "\n",
        "    return grouped.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets use our model to see wether the sentence \"I hate bananas\" is negative or positive \n",
        "\n",
        ">*Try changing the words inside classify sentiment to see wether its is classified positive or negative*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classify_sentiment(\"I hate bananas\")        # You can change the words inside the function to test anything you want! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's apply this to our entire `SBUX_news_df` and see how each news headline is classified. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "news_df = apply_sentiment(SBUX_news_df)       # Classify sentiment of each Starbucks news headline\n",
        "news_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"News Titles and Sentiments\\n\")\n",
        "\n",
        "for i, row in news_df.iterrows():\n",
        "    if i < 5:\n",
        "        print(f\"Title: {row['Title']}\\nSentiment: {row['Sentiment']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lets summarize the sentiment results by date. For each day we count the number of positive and negative headlines, then calculate 7-day moving averages and the daily percentage of positive news. This gives us a quick overview of news sentiment trends over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentiment_df = process_sentiment(news_df)       # Summarize daily sentiment statistics\n",
        "sentiment_df.head()                             # Display the first 5 rows of the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  What this table shows?\n",
        "This table is the output of a function that summarizes news sentiment over time.\n",
        "\n",
        "Each row corresponds to a specific date and gives us a snapshot of how positive or negative the news headlines were for that day and the surrounding week.\n",
        "\n",
        "Markets move not just based on today's headlines, but on **short-term trends** in public sentiment.  \n",
        "This table lets us track how optimism or pessimism is **building up over time**, which we can later use to help predict stock price movements.\n",
        "\n",
        "####  Why is this useful?\n",
        "- We know that in the last 100 news stories about NVIDIA 24 have been positive and 5 have been negative. \n",
        "- If the `7day_pct_positive` is rising, the overall tone of news is getting more optimistic.\n",
        "- If it's dropping, it could mean public or investor concern is growing.\n",
        "- We can later plot this and compare it against stock price to see if sentiment influences market behavior.\n",
        "\n",
        "### Getting Stock Price Data \n",
        "\n",
        "We are using the `yfinance` package to get real stock price data directly from Yahoo Finance. The function below helps us download historical stock prices and compute the daily percentage change in the stock's closing price. To make sure our results are replicable, this data has been saved as a `.csv` file \"data/NVDA_snapshot.csv\"\n",
        "\n",
        "This allows us to analyze how stock prices change over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The code below will give us a snapshot of stock prices for the duration in we have news headlines for.  \n",
        "\n",
        "def get_stock_data(ticker, start, end):\n",
        "    stock = yf.download(ticker, start=start, end=end)\n",
        "\n",
        "    # Flatten columns if multi-indexed (e.g., multiple tickers)\n",
        "    if isinstance(stock.columns, pd.MultiIndex):\n",
        "        stock.columns = ['_'.join(col).strip() for col in stock.columns]  # \"Close_SBUX\", etc.\n",
        "        close_col = f\"Close_{ticker}\"\n",
        "    else:\n",
        "        close_col = \"Close\"\n",
        "\n",
        "    stock[\"Pct_Change\"] = stock[close_col].pct_change() * 100\n",
        "    stock.reset_index(inplace=True)\n",
        "    stock[\"DateOnly\"] = pd.to_datetime(stock[\"Date\"])\n",
        "    return stock[[\"DateOnly\", \"Pct_Change\"]]\n",
        "\n",
        "# merges sentiment and stock data by date, and lags sentiment by one day to align with price changes.\n",
        "def combine_data(sent_df, stock_df):\n",
        "    sent_df = sent_df.reset_index(drop=True)\n",
        "    stock_df = stock_df.reset_index(drop=True)\n",
        "\n",
        "    sent_df[\"DateOnly\"] = pd.to_datetime(sent_df[\"DateOnly\"])\n",
        "    stock_df[\"DateOnly\"] = pd.to_datetime(stock_df[\"DateOnly\"])\n",
        "\n",
        "    return (\n",
        "        pd.merge(sent_df, stock_df, on=\"DateOnly\", how=\"inner\")\n",
        "          .assign(lagged_sentiment=lambda df: df[\"7day_pct_positive\"].shift(1))\n",
        "    )\n",
        "\n",
        "\n",
        "# For the sake of reproducibility (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n",
        "\n",
        "# Uncomment the line below get a more recent snapshot of the data ! \n",
        "\n",
        "SBUX_news_df[\"DateOnly\"] = pd.to_datetime(SBUX_news_df[\"DateOnly\"])\n",
        "start_date = SBUX_news_df[\"DateOnly\"].min() - pd.Timedelta(days=1) \n",
        "end_date = SBUX_news_df[\"DateOnly\"].max() + pd.Timedelta(days=1) \n",
        "stock_df = get_stock_data(\"SBUX\", start_date, end_date)\n",
        "stock_df.to_csv(\"data/SBUX_price.csv\", index=False)\n",
        "\n",
        "SBUX_price_df = pd.read_csv(\"data/SBUX_price.csv\")\n",
        "SBUX_price_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we bring together the sentiment summary data and the stock price changes. By merging these two datasets, we can analyze how changes in news sentiment might be related to changes in Starbucks' stock price. This combined dataset will help us answer the question: \"Does positive news sentiment lead to an increase in stock price?\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_df = combine_data(sentiment_df, stock_df)\n",
        "combined_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Forecasting Future Stock Changes with Sentiment\n",
        "\n",
        "In this notebook, we will try to predict future stock price changes using the **SARIMAX model**, a powerful forecasting model that allows us to include external information, in our case, **public sentiment**. For those who are interested in learning more about the SARIMAX model and its implementation in Python, see this [comprehensive guide](https://www.geeksforgeeks.org/python/complete-guide-to-sarimax-in-python/) on GeeksforGeeks.\n",
        "\n",
        "Below, we define three key functions for our forecasting workflow:\n",
        "\n",
        "- `get_future_dates()`: Returns the next business days for which we want to make predictions.\n",
        "- `fit_and_forecast()`: Uses both historical stock prices and recent news sentiment to predict how Starbucks' (SBUX) stock price might change over the next few days. This function fits a SARIMAX model, which incorporates both past price data and the influence of news sentiment, and then generates forecasts along with confidence intervals.\n",
        "- `create_plot()`: Produces an interactive line chart that visualizes trends in both stock price percentage changes and sentiment, allowing us to explore their relationship over time.\n",
        "\n",
        "By combining these functions, we can see how shifts in news sentiment may impact SBUX's future stock movements. The forecasting approach essentially answers the question: \"Given recent sentiment, what does the model predict for this stock's price in the coming days?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# business days we should forecast for \n",
        "def get_future_dates(start_date, num_days):\n",
        "    if not isinstance(start_date, pd.Timestamp):\n",
        "        start_date = pd.to_datetime(start_date)\n",
        "\n",
        "    us_holidays = holidays.US()\n",
        "    future_dates = []\n",
        "    current_date = start_date + pd.Timedelta(days=1)\n",
        "\n",
        "    while len(future_dates) < num_days:\n",
        "        if current_date.weekday() < 5 and current_date.date() not in us_holidays:\n",
        "            future_dates.append(current_date)\n",
        "        current_date += pd.Timedelta(days=1)\n",
        "\n",
        "    return future_dates\n",
        "\n",
        "# prediction model\n",
        "def fit_and_forecast(combined_df, forecast_steps=3):\n",
        "    combined_df = combined_df.dropna(subset=['Pct_Change', 'lagged_sentiment'])\n",
        "\n",
        "    endog = combined_df['Pct_Change']\n",
        "    exog = combined_df['lagged_sentiment']\n",
        "\n",
        "    model = SARIMAX(endog, exog=exog, order=(1, 1, 1))\n",
        "    fit = model.fit(disp=False)\n",
        "\n",
        "    future_dates = get_future_dates(combined_df.index[-1], forecast_steps)\n",
        "    future_exog = np.tile(combined_df['lagged_sentiment'].iloc[-1], forecast_steps).reshape(-1, 1)\n",
        "\n",
        "    forecast = fit.get_forecast(steps=forecast_steps, exog=future_exog)\n",
        "    return forecast.predicted_mean, forecast.conf_int(), future_dates\n",
        "\n",
        "# Create a plot with the forecasted data, historical sentiment, and stock price changes\n",
        "def create_plot(combined_df, forecast_mean, forecast_ci, forecast_index, actuals_df=None):\n",
        "    combined_df = combined_df.copy()\n",
        "    \n",
        "    # Aggregate data by date to show one clear point per day\n",
        "    daily_agg = combined_df.groupby(combined_df.index).agg({\n",
        "        'lagged_sentiment': 'mean',\n",
        "        'Pct_Change': 'mean'\n",
        "    }).reset_index()\n",
        "    daily_agg.set_index('DateOnly', inplace=True)\n",
        "    \n",
        "    sentiment_std = (daily_agg['lagged_sentiment'] - daily_agg['lagged_sentiment'].mean()) \\\n",
        "                    / daily_agg['lagged_sentiment'].std()\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Historical sentiment\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=daily_agg.index,\n",
        "        y=sentiment_std,\n",
        "        name='Standardized Sentiment',\n",
        "        mode='lines+markers',\n",
        "        line=dict(color='royalblue', width=2),\n",
        "        marker=dict(color='royalblue', size=4),\n",
        "        hovertemplate='Date: %{x}<br>Sentiment: %{y:.2f}<extra></extra>'\n",
        "    ))\n",
        "\n",
        "    # Historical % change\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=daily_agg.index,\n",
        "        y=daily_agg['Pct_Change'],\n",
        "        name='Stock % Change',\n",
        "        yaxis='y2',\n",
        "        mode='lines+markers',\n",
        "        line=dict(color='limegreen', width=2),\n",
        "        marker=dict(color='limegreen', size=4),\n",
        "        hovertemplate='Date: %{x}<br>% Change: %{y:.2f}%<extra></extra>'\n",
        "    ))\n",
        "\n",
        "    # Forecast mean (NOW on y2)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=forecast_index,\n",
        "        y=forecast_mean,\n",
        "        name='Forecasted % Change',\n",
        "        yaxis='y2',\n",
        "        line=dict(color='tomato', width=2, dash='dash'),\n",
        "        hovertemplate='Date: %{x}<br>Forecast: %{y:.2f}%<extra></extra>'\n",
        "    ))\n",
        "\n",
        "    # Forecast confidence interval (NOW on y2)\n",
        "    fig.add_trace(go.Scatter(\n",
        "        x=np.concatenate([forecast_index, forecast_index[::-1]]),\n",
        "        y=np.concatenate([forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1][::-1]]),\n",
        "        name='95% CI',\n",
        "        yaxis='y2',\n",
        "        fill='toself',\n",
        "        fillcolor='rgba(255,99,71,0.2)',  # softened tomato\n",
        "        line=dict(color='rgba(255,255,255,0)'),\n",
        "        hoverinfo=\"skip\",\n",
        "        showlegend=True\n",
        "    ))\n",
        "\n",
        "    # Add actual data if provided\n",
        "    if actuals_df is not None and not actuals_df.empty:\n",
        "        # If actuals_df has multiple tickers, aggregate by date (take mean)\n",
        "        if 'Ticker' in actuals_df.columns and len(actuals_df['Ticker'].unique()) > 1:\n",
        "            actual_agg = actuals_df.groupby('DateOnly')['Actual_Pct_Change'].mean().reset_index()\n",
        "            actual_dates = actual_agg['DateOnly']\n",
        "            actual_values = actual_agg['Actual_Pct_Change']\n",
        "        else:\n",
        "            actual_dates = actuals_df['DateOnly']\n",
        "            actual_values = actuals_df['Actual_Pct_Change']\n",
        "        \n",
        "        # Actual data points\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=actual_dates,\n",
        "            y=actual_values,\n",
        "            name='Actual % Change',\n",
        "            yaxis='y2',\n",
        "            mode='markers',\n",
        "            marker=dict(color='gold', size=8, symbol='circle'),\n",
        "            hovertemplate='Date: %{x}<br>Actual: %{y:.2f}%<extra></extra>'\n",
        "        ))\n",
        "        \n",
        "        # Calculate and display gaps (forecast vs actual) where dates overlap\n",
        "        forecast_df = pd.DataFrame({\n",
        "            'DateOnly': pd.to_datetime(forecast_index).normalize(),\n",
        "            'Forecast': forecast_mean\n",
        "        })\n",
        "        \n",
        "        # Prepare actual data for merging\n",
        "        if 'Ticker' in actuals_df.columns and len(actuals_df['Ticker'].unique()) > 1:\n",
        "            actual_for_gap = actual_agg.copy()\n",
        "            actual_for_gap.rename(columns={'Actual_Pct_Change': 'Actual'}, inplace=True)\n",
        "        else:\n",
        "            actual_for_gap = actuals_df[['DateOnly', 'Actual_Pct_Change']].copy()\n",
        "            actual_for_gap.rename(columns={'Actual_Pct_Change': 'Actual'}, inplace=True)\n",
        "        \n",
        "        # Merge to find overlapping dates\n",
        "        gap_df = pd.merge(forecast_df, actual_for_gap, on='DateOnly', how='inner')\n",
        "        \n",
        "        if not gap_df.empty:\n",
        "            gap_df['Gap'] = gap_df['Forecast'] - gap_df['Actual']\n",
        "            \n",
        "            # Add gap visualization (error bars or connecting lines)\n",
        "            for _, row in gap_df.iterrows():\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=[row['DateOnly'], row['DateOnly']],\n",
        "                    y=[row['Forecast'], row['Actual']],\n",
        "                    name='Forecast Gap',\n",
        "                    yaxis='y2',\n",
        "                    mode='lines',\n",
        "                    line=dict(color='orange', width=2, dash='dot'),\n",
        "                    showlegend=False,\n",
        "                    hoverinfo=\"none\"\n",
        "                ))\n",
        "            \n",
        "            # Add invisible points at gap midpoints for targeted hover\n",
        "            for _, row in gap_df.iterrows():\n",
        "                midpoint = (row['Forecast'] + row['Actual']) / 2\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=[row['DateOnly']],\n",
        "                    y=[midpoint],\n",
        "                    name='Gap Info',\n",
        "                    yaxis='y2',\n",
        "                    mode='markers',\n",
        "                    marker=dict(color='orange', size=8, opacity=0),\n",
        "                    showlegend=False,\n",
        "                    hovertemplate=f'Date: %{{x}}<br>Gap: {row[\"Gap\"]:.2f}%<extra></extra>'\n",
        "                ))\n",
        "            \n",
        "            # Add a single legend entry for gaps\n",
        "            if len(gap_df) > 0:\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=[gap_df.iloc[0]['DateOnly']],\n",
        "                    y=[gap_df.iloc[0]['Forecast']],\n",
        "                    name='Forecast Gap',\n",
        "                    yaxis='y2',\n",
        "                    mode='lines',\n",
        "                    line=dict(color='orange', width=2, dash='dot'),\n",
        "                    showlegend=True,\n",
        "                    hoverinfo=\"skip\"\n",
        "                ))\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=dict(\n",
        "            text='Sentiment vs Stock Change Forecast',\n",
        "            font=dict(size=20),\n",
        "            y=0.95,\n",
        "            x=0.5,\n",
        "            xanchor='center',\n",
        "            yanchor='top'\n",
        "        ),\n",
        "        hovermode='x unified',\n",
        "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1),\n",
        "        margin=dict(l=60, r=60, t=140, b=60),\n",
        "        template='plotly_dark',\n",
        "        xaxis=dict(\n",
        "            title='Date',\n",
        "            showgrid=False,\n",
        "            tickangle=-45,\n",
        "            tickfont=dict(size=10)\n",
        "        ),\n",
        "        yaxis=dict(\n",
        "            title='Standardized Sentiment',\n",
        "            title_font=dict(color='royalblue'),\n",
        "            tickfont=dict(color='royalblue'),\n",
        "            range=[-2, 2]   # sentiment fixed range\n",
        "        ),\n",
        "        yaxis2=dict(\n",
        "            title='Stock % Change',\n",
        "            title_font=dict(color='limegreen'),\n",
        "            tickfont=dict(color='limegreen'),\n",
        "            overlaying='y',\n",
        "            side='right'\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Plot: Sentiment vs Stock % Change Forecast\n",
        "\n",
        "This chart shows how news sentiment about a company relates to its stock price changes over time, and how we can use this relationship to make simple predictions.\n",
        "\n",
        "- The blue line shows the standardized 7-day average of positive sentiment extracted from financial news headlines. A higher value means news sentiment was more positive.\n",
        "- The green line shows the actual daily percentage change in the company's stock price.\n",
        "- The red line shows our simple **forecast** of future stock movement based on past sentiment trends. The shaded red area represents uncertainty around the forecast (a 95% confidence interval).\n",
        "\n",
        "We want to see whether the emotions in the news (blue) can help us predict price changes (green and red). If they move together, it suggests that public mood might influence investor behavior.\n",
        "\n",
        "> This plot helps us visualize correlations and test basic forecasting using real-world data like stock prices and media sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_df['DateOnly'] = pd.to_datetime(combined_df['DateOnly'])  # convert to datetime\n",
        "combined_df.set_index('DateOnly', inplace=True)  # use as index\n",
        "combined_df.sort_index(inplace=True)  # ensure time order\n",
        "\n",
        "forecast_mean, forecast_ci, forecast_index = fit_and_forecast(combined_df)\n",
        "create_plot(combined_df, forecast_mean, forecast_ci, forecast_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">  **Disclaimer**: This is a simplified model. In reality, stock prices are influenced by many factors, such as interest rates, earnings reports, geopolitical events, and investor speculation. This chart only considers one variable: news sentiment. It should not be used for actual trading decisions.\n",
        "\n",
        "> *Think about what you would include in a model other than news to help us predict how a stock price might change ?*\n",
        "\n",
        "### How Do AIs Feel About AI?\n",
        "\n",
        "In this section of the notebook, we explore how our AI Sentiment Analysis model** feels about AI-related stocks. That's a mouthful! \n",
        "\n",
        "The goal is to see if public sentiment (as captured by the headlines) is generally optimistic or pessimistic toward leading AI companies — as interpreted by another AI (we are using BERT here!).\n",
        "\n",
        "We'll start by selecting the [top 10 AI stocks in 2025](https://www.forbes.com/advisor/investing/best-ai-stocks/) as suggested by financial news magazine, Forbes\n",
        "- `ACN` (Accenture)\n",
        "- `ADBE` (Adobe)\n",
        "- `AMD` (Advanced Micro Devices)\n",
        "- `APP` (Applovin)\n",
        "- `AVGO` (Broadcom)\n",
        "- `CRM` (Salesforce)\n",
        "- `MRVL` (Marvell Technology)\n",
        "- `MU` (Micron Technology)\n",
        "- `NVDA` (NVIDIA)\n",
        "- `QCOM` (Qualcomm)\n",
        "\n",
        "For each company, we will:\n",
        "2. Classify the sentiment using our `RoBERTa`-based model\n",
        "1. Pull recent news headlines\n",
        "3. Analyze the **7-day rolling trends** in public sentiment\n",
        "4. Compare results across companies\n",
        "\n",
        "Let's find out if the machines love themselves ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This code collects recent news headlines for each company in our AI stock list. It uses our  `get_news_data()` which we defined above. \n",
        "\n",
        "ai_tickers = [\"ACN\", \"ADBE\", \"AMD\", \"APP\", \"AVGO\", \"CRM\", \"MRVL\", \"MU\", \"NVDA\", \"QCOM\"] # Top 10 AI stocks \n",
        "\n",
        "all_news = []\n",
        "\n",
        "def fetch_all_news(ticker_list):\n",
        "    all_news = []\n",
        "    for ticker in ticker_list:\n",
        "        try:\n",
        "            news = get_news_data(ticker)\n",
        "            all_news.append(news)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to get news for {ticker}: {e}\")\n",
        "    if all_news:\n",
        "        return pd.concat(all_news, ignore_index=True)\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# For the sake of reproducability (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n",
        "\n",
        "# Uncomment the line below get a more recent snapshot of the data !\n",
        "\n",
        "# Usage :\n",
        "AI_combined_news_df = fetch_all_news(ai_tickers)\n",
        "#AI_combined_news_df = pd.concat(all_news, ignore_index=True)\n",
        "AI_combined_news_df.to_csv(\"data/AI_news_snapshot.csv\", index=False)\n",
        "AI_sentiment_news_df = apply_sentiment(AI_combined_news_df)         \n",
        "AI_sentiment_news_df.to_csv(\"data/AI_news_sentiment.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "AI_sentiment_news_df = pd.read_csv(\"data/AI_news_sentiment.csv\")\n",
        "AI_sentiment_news_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This`AI_process_sentiment` does for a bunch of companies what we did for SBUX. It automatically collect all their news headlines and put them in one place for analysis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def AI_process_sentiment(news_df):\n",
        "    filtered = news_df[news_df[\"Sentiment\"].isin([\"POSITIVE\", \"NEGATIVE\"])].copy()\n",
        "\n",
        "    # Group by DateOnly, Ticker, Sentiment → count headlines\n",
        "    grouped = (\n",
        "        filtered\n",
        "        .groupby([\"DateOnly\", \"Ticker\", \"Sentiment\"])\n",
        "        .size()\n",
        "        .unstack(fill_value=0)\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Ensure both sentiment columns exist\n",
        "    if \"POSITIVE\" not in grouped.columns:\n",
        "        grouped[\"POSITIVE\"] = 0\n",
        "    if \"NEGATIVE\" not in grouped.columns:\n",
        "        grouped[\"NEGATIVE\"] = 0\n",
        "\n",
        "    # Sort for rolling computation\n",
        "    grouped = grouped.sort_values([\"Ticker\", \"DateOnly\"])\n",
        "\n",
        "    # 7-day rolling sums by ticker\n",
        "    grouped[\"7day_avg_positive\"] = (\n",
        "        grouped.groupby(\"Ticker\")[\"POSITIVE\"]\n",
        "        .transform(lambda x: x.rolling(window=7, min_periods=1).sum())\n",
        "    )\n",
        "    grouped[\"7day_avg_negative\"] = (\n",
        "        grouped.groupby(\"Ticker\")[\"NEGATIVE\"]\n",
        "        .transform(lambda x: x.rolling(window=7, min_periods=1).sum())\n",
        "    )\n",
        "\n",
        "    # Compute % positive\n",
        "    grouped[\"7day_pct_positive\"] = grouped[\"7day_avg_positive\"] / (\n",
        "        grouped[\"7day_avg_positive\"] + grouped[\"7day_avg_negative\"]\n",
        "    )\n",
        "\n",
        "    return grouped[[\"DateOnly\", \"Ticker\", \"7day_pct_positive\"]]\n",
        "\n",
        "\n",
        "AI_sentiment_df = AI_process_sentiment(AI_sentiment_news_df)\n",
        "AI_sentiment_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Like we did with the `SBUX` stock above lets get stock prices for all of our AI stocks now.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Match date range to your sentiment dataset\n",
        "start_date = pd.to_datetime(AI_combined_news_df[\"DateOnly\"]).min() - pd.Timedelta(days=1)\n",
        "end_date = pd.to_datetime(AI_combined_news_df[\"DateOnly\"]).max()\n",
        "\n",
        "def fetch_and_save_stock_data(ticker_list, start_date, end_date, save_dir=\"data/ai_prices\"):\n",
        "    \"\"\"\n",
        "    Fetches stock price data for each ticker in the list and saves it as a CSV file.\n",
        "    Prints a success or error message for each ticker.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    \n",
        "    for ticker in ticker_list:\n",
        "        try:\n",
        "            stock_df = get_stock_data(ticker, start_date, end_date)\n",
        "            stock_df[\"Ticker\"] = ticker\n",
        "            file_path = f\"{save_dir}/{ticker}_price.csv\"\n",
        "            stock_df.to_csv(file_path, index=False)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to get stock data for {ticker}: {e}\")\n",
        "\n",
        "\n",
        "# For the sake of reproducability (so we always get the same results), we will use a a dataset that was scrapped using the same method at an earlier time (July - 2025)\n",
        "\n",
        "# Uncomment the line below get a more recent snapshot of the data !\n",
        "\n",
        "# Usage :\n",
        "# fetch_and_save_stock_data(ai_tickers, \"start_date\", \"end_date\")\n",
        "# price_files = glob.glob(\"data/ai_prices/*.csv\")\n",
        "# price_dfs = [pd.read_csv(f) for f in price_files]\n",
        "# AI_combined_price_df = pd.concat(price_dfs, ignore_index=True)\n",
        "# AI_combined_price_df.to_csv(\"data/AI_top10_price_snapshot.csv\", index=False)\n",
        "\n",
        "AI_stock_prices = pd.read_csv(\"data/AI_top10_price_snapshot.csv\")\n",
        "AI_stock_prices.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we did before lets merge everything into one dataframe to analyse it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure both DateOnly columns are datetime\n",
        "AI_sentiment_df['DateOnly'] = pd.to_datetime(AI_sentiment_df['DateOnly'])\n",
        "AI_stock_prices['DateOnly'] = pd.to_datetime(AI_stock_prices['DateOnly'])\n",
        "\n",
        "# Merge on DateOnly and Ticker\n",
        "final_df = pd.merge(\n",
        "    AI_stock_prices,\n",
        "    AI_sentiment_df,\n",
        "    on=['DateOnly', 'Ticker'],\n",
        "    how='inner'                  \n",
        ")\n",
        "\n",
        "final_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similar to our approach with the `SBUX` stock price, we can fit a SARIMAX model to this combined dataset and forecast the percentage change in AI stock prices for the next 7 business days, using both historical price trends and recent news sentiment as inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_and_forecast(final_df, forecast_steps=7):\n",
        "    from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    # Drop missing values\n",
        "    final_df = final_df.dropna(subset=['Pct_Change', 'lagged_sentiment'])\n",
        "\n",
        "    # Define endogenous and exogenous variables\n",
        "    endog = final_df['Pct_Change']\n",
        "    exog = final_df['lagged_sentiment']\n",
        "\n",
        "    # Fit SARIMAX\n",
        "    model = SARIMAX(endog, exog=exog, order=(1, 1, 1), enforce_stationarity=False, enforce_invertibility=False)\n",
        "    fit = model.fit(disp=False)\n",
        "\n",
        "    # Future exog (use last known lagged sentiment)\n",
        "    last_sentiment = exog.iloc[-1]\n",
        "    future_exog = np.full(shape=(forecast_steps,), fill_value=last_sentiment)\n",
        "\n",
        "    # Forecast\n",
        "    forecast = fit.get_forecast(steps=forecast_steps, exog=future_exog)\n",
        "    forecast_mean = forecast.predicted_mean\n",
        "    forecast_ci = forecast.conf_int()\n",
        "\n",
        "    # Create future dates\n",
        "    last_date = final_df.index[-1]\n",
        "    forecast_index = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=forecast_steps, freq='B')\n",
        "\n",
        "    return forecast_mean, forecast_ci, forecast_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_df = final_df.copy()\n",
        "\n",
        "# Sort and index\n",
        "final_df['DateOnly'] = pd.to_datetime(final_df['DateOnly'])\n",
        "final_df.set_index('DateOnly', inplace=True)\n",
        "final_df.sort_index(inplace=True)\n",
        "\n",
        "# Create lagged and standardized sentiment\n",
        "final_df['sentiment_std'] = (\n",
        "    final_df['7day_pct_positive'] - final_df['7day_pct_positive'].mean()\n",
        ") / final_df['7day_pct_positive'].std()\n",
        "\n",
        "final_df['lagged_sentiment'] = final_df['sentiment_std'].shift(1)\n",
        "final_df.dropna(subset=['lagged_sentiment', 'Pct_Change'], inplace=True)\n",
        "\n",
        "# Now call\n",
        "forecast_mean, forecast_ci, forecast_index = fit_and_forecast(final_df)\n",
        "create_plot(final_df, forecast_mean, forecast_ci, forecast_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How well does the model predict?\n",
        "\n",
        "We can fetch the actual data to see how well our model predicts the AI stocks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create function to generate the compare window\n",
        "def get_compare_window(compare_start, compare_target_end):\n",
        "\n",
        "    # Define today to avoid fetching beyond range\n",
        "    today = pd.Timestamp(datetime.utcnow().date())  # use UTC date\n",
        "\n",
        "    compare_end = min(today, compare_target_end)\n",
        "    return compare_start, compare_end\n",
        "\n",
        "# Create function to fetch the actual percentage change\n",
        "def fetch_actual_pct_changes(ticker_list, start_date, end_date, buffer_days=7):\n",
        "    # This function fetches the pct change of the given list of tickers and return as a DataFrame\n",
        "    rows = []\n",
        "    # fetch one ticker at a time to avoid group_by complexity\n",
        "    start_fetch = (start_date - pd.Timedelta(days=buffer_days)).strftime('%Y-%m-%d')\n",
        "    end_fetch = (end_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
        "\n",
        "    for t in ticker_list:\n",
        "        try:\n",
        "            df = yf.download(t, start=start_fetch, end=end_fetch, progress=False, interval='1d', auto_adjust=False)\n",
        "            if df.empty:\n",
        "                # no data for ticker in window\n",
        "                continue\n",
        "            df.index = pd.to_datetime(df.index).normalize()\n",
        "            # compute percent change on Close\n",
        "            if 'Adj Close' in df.columns:\n",
        "                price_col = 'Adj Close'\n",
        "            else:\n",
        "                price_col = 'Close'\n",
        "            df['pct_change'] = df[price_col].pct_change() * 100.0\n",
        "            # select only rows in [start_date, end_date]\n",
        "            sel = df.loc[(df.index >= start_date) & (df.index <= end_date)]\n",
        "            # Fix: Properly select the pct_change column\n",
        "            sel = sel[['pct_change']].dropna()\n",
        "            for idx, r in sel.iterrows():\n",
        "                rows.append({'DateOnly': idx, 'Ticker': t, 'Actual_Pct_Change': r['pct_change']})\n",
        "        except Exception as e:\n",
        "            print(f\"yfinance fetch failed for {t}: {e}\")\n",
        "\n",
        "    actuals_df = pd.DataFrame(rows)\n",
        "    if not actuals_df.empty:\n",
        "        actuals_df['DateOnly'] = pd.to_datetime(actuals_df['DateOnly']).dt.normalize()\n",
        "    return actuals_df\n",
        "\n",
        "# Function to compare predictions with actuals\n",
        "def compare_predictions(pred_mean, pred_index, actuals_df, ticker=None):\n",
        "\n",
        "    pred_df = pd.DataFrame({\n",
        "        'DateOnly': pd.to_datetime(pred_index).normalize(),\n",
        "        'Predicted_Pct_Change': np.asarray(pred_mean).astype(float)\n",
        "    })\n",
        "    if ticker is not None:\n",
        "        actuals_df = actuals_df[actuals_df['Ticker'] == ticker].copy()\n",
        "    # If actuals contain multiple tickers and ticker=None, will compare using all actual rows\n",
        "    merged = pd.merge(pred_df, actuals_df, on='DateOnly', how='inner')\n",
        "\n",
        "    if merged.empty:\n",
        "        print(\"No overlapping observed days to compare (maybe market closed or today < start).\")\n",
        "        return merged, {}\n",
        "\n",
        "    # If actuals has multiple tickers for same DateOnly (if ticker=None), aggregated handling:\n",
        "    if 'Ticker' in merged.columns and ticker is None:\n",
        "        # average actuals across tickers for that day\n",
        "        aggregated = merged.groupby('DateOnly').agg({\n",
        "            'Predicted_Pct_Change': 'first',  \n",
        "            'Actual_Pct_Change': 'mean'\n",
        "        }).reset_index()\n",
        "        mdf = aggregated\n",
        "    else:\n",
        "        mdf = merged[['DateOnly', 'Predicted_Pct_Change', 'Actual_Pct_Change']].copy()\n",
        "\n",
        "    # metrics\n",
        "    mae = mean_absolute_error(mdf['Actual_Pct_Change'], mdf['Predicted_Pct_Change'])\n",
        "    rmse = sqrt(mean_squared_error(mdf['Actual_Pct_Change'], mdf['Predicted_Pct_Change']))\n",
        "    metrics = {'count': len(mdf), 'MAE': mae, 'RMSE': rmse}\n",
        "\n",
        "    return mdf, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The evaluation metrics we use are the **Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)**, which measure how far off our predictions are from the actual stock price changes. \n",
        "\n",
        "The MAE is the average of the absolute differences between predicted and actual values, while the RMSE is the square root of the average of squared differences. Usually, lower values of these metrics indicate better model performance.\n",
        "\n",
        "The MAE is calculated as follows:\n",
        "$$\n",
        "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
        "$$\n",
        "And the RMSE is calculated as follows:\n",
        "$$\n",
        "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
        "$$\n",
        "\n",
        "However, we must note that these metrics are not unitless, meaning they depend on the scale of the data. In our case, since we are predicting percentage changes in stock prices, the MAE and RMSE will be in percentage points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fit the previous example for comparison\n",
        "compare_start = forecast_index[0]\n",
        "compare_target_end = forecast_index[-1]\n",
        "\n",
        "compare_start, compare_end = get_compare_window(compare_start, compare_target_end)\n",
        "\n",
        "# Create the actual comparison DataFrame\n",
        "actuals_df = fetch_actual_pct_changes(ai_tickers, compare_start, compare_end, buffer_days=7)\n",
        "\n",
        "# Calculate the metrics\n",
        "pred_mean_values = forecast_mean.values  # array-like\n",
        "mdf, metrics = compare_predictions(pred_mean_values, forecast_index, actuals_df)\n",
        "\n",
        "print(\"Comparison metrics:\")\n",
        "print(f\"Count: {metrics['count']}\\nMAE: {metrics['MAE']:.2f}\\nRMSE: {metrics['RMSE']:.2f}\\n\")\n",
        "\n",
        "if not mdf.empty:\n",
        "    print(mdf.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, with the actual stock price data, let's visualize it in the plot alongside our predictions to see if our confidence intervals capture the actual movements well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the comparison and evaluate the confidence intervals\n",
        "create_plot(final_df, forecast_mean, forecast_ci, forecast_index, actuals_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ">\n",
        "> - *Do you think the model's predictions are accurate enough for practical use?*  \n",
        "> - *What strategies could you use to:*  \n",
        ">   1. *Reduce the width of the confidence intervals?*  \n",
        ">   2. *Improve the accuracy of the predictions?*  \n",
        ">   3. *Increase the model's robustness to outliers?*  \n",
        ">\n",
        ">   *Share your ideas with your classmates!*\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "This notebook provides an overview of how large language models (LLMs) make predictions, typically with an example showing the distribution of a LLM's prediction and an example of stock price forecasting based on historical data and news sentiment. It demonstrates how LLMs work and how they can be applied to real-world problems like stock price prediction. The use of LLM and sentiment analysis allows us to incorporate additional qualitative information into our predictions, potentially allowing for better inference and forecasting of economic variables like stock prices.\n",
        "\n",
        "However, it is important to note that the model's predictions are not perfect and should be used with caution. The notebook also highlights the importance of evaluating the model's performance using metrics like MAE and RMSE, and encourages users to think critically about how to improve the model's accuracy and robustness.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- **Large language models (LLMs)** predict the next word in a sequence by learning the probability distribution of possible outcomes based on context.\n",
        "- LLMs can also be used to predict future values in a time series, such as stock prices, by incorporating additional features like public sentiment.\n",
        "- **The SARIMAX model** is a powerful forecasting model that allows us to include external information, such as news sentiment, in our predictions.\n",
        "- We must evaluate the model's performance using metrics like **MAE** and **RMSE** to ensure its predictions are reliable.\n",
        "\n",
        "### Glossary\n",
        "\n",
        "- **Large Language Model (LLM)**: A type of AI model that predicts the next word in a sequence based on the context of previous words.\n",
        "- **Next Word Prediction**: The task of predicting the next word in a sequence given the previous words.\n",
        "- **Sentiment Analysis**: The process of determining the emotional tone behind a series of words, used to understand the sentiment expressed in text.\n",
        "- **SARIMAX Model**: A statistical model used for forecasting time series data that can incorporate external variables.\n",
        "- **Mean Absolute Error (MAE)**: A measure of prediction accuracy that calculates the average absolute difference between predicted and actual values.\n",
        "- **Root Mean Squared Error (RMSE)**: A measure of prediction accuracy that calculates the square root of the average of squared differences between predicted and actual values.\n",
        "\n",
        "### References\n",
        "\n",
        "- Kirsch, N. (2024, April 29). 10 Best AI Stocks Of August 2024. Forbes Advisor. [https://www.forbes.com/advisor/investing/best-ai-stocks/](https://www.forbes.com/advisor/investing/best-ai-stocks/)\n",
        "- Chen, J. (2023, October 6). *Efficient Market Hypothesis (EMH): Forms and criticisms*. Investopedia. [https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp](https://www.investopedia.com/terms/e/efficientmarkethypothesis.asp)\n",
        "- Yahoo Finance. (n.d.). Yahoo Finance — Stocks, financial news, quotes, and market data. Retrieved August 1, 2025, from [https://ca.finance.yahoo.com/](https://ca.finance.yahoo.com/)\n",
        "- Aroussi, R. (n.d.). yfinance [Python package]. GitHub. Retrieved August 1, 2025, from [https://github.com/ranaroussi/yfinance](https://github.com/ranaroussi/yfinance)\n",
        "- Li, T. (2025). finvizfinance (Version 1.1.1) [Python package]. PyPI. Retrieved August 1, 2025, from [https://pypi.org/project/finvizfinance/](https://pypi.org/project/finvizfinance/)\n",
        "- GeeksforGeeks. (2023, September 27). Complete guide to SARIMAX in Python. [https://www.geeksforgeeks.org/python/complete-guide-to-sarimax-in-python/](https://www.geeksforgeeks.org/python/complete-guide-to-sarimax-in-python/)\n",
        "- Sprenger, T. O., Tumasjan, A., Sandner, P. G., & Welpe, I. M. (2014, February 5). *Twitter sentiment and stock market movements: The predictive power of social media*. VoxEU. [https://cepr.org/voxeu/columns/twitter-sentiment-and-stock-market-movements-predictive-power-social-media](https://cepr.org/voxeu/columns/twitter-sentiment-and-stock-market-movements-predictive-power-social-media)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3",
      "path": "C:\\Users\\Kaiyan Zhang\\AppData\\Local\\Programs\\Python\\Python312\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
