{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"AMNE 376: A Study of Richter's Kouroi Through Image Embedding\"\n",
        "author: prAxIs UBC Team <br> Kaiyan Zhang, Yash Mali, Krishaant Pathman\n",
        "date: 2025-07-25\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Outcomes\n",
        "\n",
        "In the notebook you will\n",
        "\n",
        "1. Familiarize yourself with concepts such as computer vision, convolution, convolutional neural network (CNN) and image embeddings.\n",
        "2. Understand how computers \"see\" and distinguish between different images by identifying unique visual elements and quantifying their similarity.\n",
        "3. Explore photographs of Kouroi from Richter's book *Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942)* and create image embeddings for these photographs using pre-trained models.\n",
        "4. Learn how to cluster and classify Kouroi based solely on photographs, and critically analyze the advantages and limitations of these techniques and their potential applications.\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before you start, make sure you\n",
        "\n",
        "1. Have at least 200 MB storage available on your device.\n",
        "2. Download the 2 folders from [SharePoint](https://ubcca.sharepoint.com/:f:/t/ubcARTS-gr-TLEFAIProjectUnpacking/EvoKvjaVo9NJkddmJATgdMMBYC7womvIzj5k96AUcF00_A?e=sqVtpd) by hovering around the three points next to the folder name, then click download (You will only have access if you are an enrolled UBC student with an activated student email. If you are a UBC student but don't have a student email, please follow this [link](https://it.ubc.ca/services/email-voice-internet/ubc-student-email-service) to activate it). They will be saved locally as `.zip` files.\n",
        "3. Find the downloaded zipped files in your device and upload them to Jupyter in the same directory as this notebook.\n",
        "4. Have the required libraries installed, if not, **uncomment the lines below (i.e. remove the #) and run the cell to install them**: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install ipywidgets plotly pillow matplotlib numpy==2.1.3 pandas opencv-python scikit-learn torch torchvision transformers datasets grad-cam tensorflow keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important:** Please run the following cells before continuing, as it sets up the libraries and image folders required for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Standard library\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "# Imaging / computer vision\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.colors import ListedColormap\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "\n",
        "# Data handling\n",
        "import pandas as pd\n",
        "\n",
        "# SciPy / numerical utilities\n",
        "from scipy.ndimage import convolve\n",
        "\n",
        "# scikit-learn\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# TensorFlow / Keras (VGG16)\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing import image as keras_image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "# PyTorch / torchvision / transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.models import convnext_base, ConvNeXt_Base_Weights\n",
        "from transformers import AutoImageProcessor, ConvNextV2Model\n",
        "\n",
        "# Misc\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Local helpers\n",
        "from zip_extractor import ZipExtractor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Don't change this cell\n",
        "\n",
        "# Define the source path and the destination\n",
        "sources = [\"example_images.zip\", \"richter_kouroi_complete_front_only.zip\"]\n",
        "dest = \"../data/\"\n",
        "for src in sources:\n",
        "    extractor = ZipExtractor(src, dest)\n",
        "    extractor.extract_all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Introduction: How Do Computers \"See\" Visual Art?\n",
        "\n",
        "In this notebook, we explore a dataset of photographs collected from Gisela Richter's ***Kouroi: Archaic Greek Youths: a Study of the Development of the Kouros Type in Greek Sculpture (1942)***.\n",
        "\n",
        "Gisela Richter's 1942 book was one of the first systematic efforts to catalog and classify kouroi based on their stylistic evolution. Her work combined archaeological evidence with visual comparison, laying the foundation for how we study ancient sculpture today.\n",
        "\n",
        "In this project, we aim to apply computer vision techniques to digitally analyze and group images from this dataset. Just as Richter used her trained eye to identify patterns and typologies, we'll explore how machines can \"see\" these sculptures through their eyes (image embeddings, clustering, and convolutional neural networks).\n",
        "\n",
        "Have you ever wondered how images are stored in computers, how computers see them and distinguish the difference between them? \n",
        "\n",
        "Many of you probably know that digital images are stored based on pixels as a grid of figures, but when we are doing image searches using a search engine or uploading them to a Generative AI model, how exactly do computers interpret, distinguish, and process them? Here is a brief introduction that introduces you to some of the basic forms and methods.\n",
        "\n",
        "#### 1.1 Digital Representations of Images\n",
        "\n",
        "Have you ever heard of the RGB primary colour model? For those who are unfamiliar with the concept, the model uses numbers in a range 0 ~ 255 to represent the colour intensity of red, green and blue and add up the three colour channels to generate any colour that's visible to human. In a colorful digital image, each pixel is characterized by its colour stored in the form (R, G, B), so knowing the distribution of colour intensity gives you a lot of information about the image. \n",
        "\n",
        "However, for monochrome images, there is only one colour channel, the grayscale. We can still use the distribution of grayscale intensities to represent the image. Since all of our images (cropped from scanned pdf books) are printed in monochrome, we can represent them using a **grayscale colour histogram**. \n",
        "\n",
        "Let's start with a three-view of the New York Kouros, here we read in the images and present them together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the folder path where the images are stored\n",
        "image_path = '../data/example_images' \n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(8, 5))\n",
        "\n",
        "# List of specific image filenames\n",
        "image_names = {'page188_img01_photo12.jpg': \"Left\", 'page188_img01_photo13.jpg': \"Front\", 'page189_img01_photo3.jpg': \"Back\"}\n",
        "\n",
        "# Display the images side by side\n",
        "axes = axes.flatten()\n",
        "for i, img_name in enumerate(image_names):\n",
        "    img_path = f\"{image_path}/{img_name}\"\n",
        "    image = Image.open(img_path)\n",
        "    axes[i].imshow(image)\n",
        "    axes[i].set_title(image_names[img_name])\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle(\"Selected Images from Richter's Kouroi Dataset\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each of these images, when loaded into the computer, becomes a 2D array of numbers representing **intensity values**. We then plot the colour histogram for each image representing the distribution of **grayscale intensity**. \n",
        "\n",
        "> **Discussion:** What do you notice by looking at the three histograms?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate and plot greyscale histograms for the selected images\n",
        "fig, axes = plt.subplots(1, 3, figsize=(7, 4))\n",
        "\n",
        "for i, img_name in enumerate(image_names):\n",
        "    img_path = f\"{image_path}/{img_name}\"\n",
        "    image = Image.open(img_path)\n",
        "    histogram = image.histogram()\n",
        "\n",
        "    axes[i].plot(histogram, color='black')\n",
        "    axes[i].set_title(f'{image_names[img_name]}')\n",
        "    axes[i].set_xlim([0, 255])\n",
        "    axes[i].set_xlabel(\"Intensity\")\n",
        "    if i == 0:\n",
        "        axes[i].set_ylabel(\"Frequency\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They look very similar! This result is not surprising given that the three images were taken at the same time with the same equipment of the same Kouros. The above example shows us that comparing the similarity of colour distributions is one way that computers understand the similarity of images.\n",
        "\n",
        "However, one can quickly realize the drawbacks of this approach. First, it relies on the correct representation of colour, so two identical images with color differences may not be recognized as similar. Second, since it focuses only on colour, it ignores the fundamental information for object recognition such as spatial, shape and texture in the image. Last but not least, there may exist two completely different images with exactly the same color distribution. Therefore, we need better methods to consider the similarity between images.\n",
        "\n",
        "**Bag of Visual Words (BoVW)** is a more practical method for recognizing similarity. The rationale behind this is very complicated, but to put it simply, it treats a “feature” in an image as a “word” (a set of numbers containing information about the feature) and calculates how often each word appears in the image. Here, we created a visual vocabulary containing 20 “words” using three-view photos of the New York Kouros, and visualized what a visual word represents on the left-view image. Here, we pick the visual word with ID 6: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the number of clusters for KMeans\n",
        "n_clusters   = 20\n",
        "word_to_show = 6\n",
        "max_patches  = 30\n",
        "\n",
        "# Initialize ORB detector\n",
        "orb = cv2.ORB_create(nfeatures=500)\n",
        "all_descriptors = []      # for stacking\n",
        "image_data      = []      # (img_name, kps, descs)\n",
        "\n",
        "# Detect and describe all images\n",
        "for img_name in image_names:\n",
        "    img_path = os.path.join(image_path, img_name)\n",
        "    img      = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    keypoints, descriptors = orb.detectAndCompute(img, None)\n",
        "\n",
        "    if descriptors is None:\n",
        "        descriptors = np.zeros((0, orb.descriptorSize()), dtype=np.uint8)\n",
        "\n",
        "    all_descriptors.append(descriptors)\n",
        "    image_data.append((img_name, keypoints, descriptors))\n",
        "\n",
        "# Build the visual vocabulary\n",
        "all_descriptors_stacked = np.vstack(all_descriptors)\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "kmeans.fit(all_descriptors_stacked)\n",
        "\n",
        "# Compute BoVW histograms\n",
        "histograms = []\n",
        "for img_name, _, descriptors in image_data:\n",
        "    if descriptors.shape[0] > 0:\n",
        "        words = kmeans.predict(descriptors)\n",
        "        hist, _ = np.histogram(words, bins=np.arange(n_clusters + 1))\n",
        "    else:\n",
        "        hist = np.zeros(n_clusters, dtype=int)\n",
        "    histograms.append((img_name, hist))\n",
        "\n",
        "# Find the locations matching visual word ID = 6\n",
        "locations = []\n",
        "for img_idx, (_, keypoints, descriptors) in enumerate(image_data):\n",
        "    if descriptors.shape[0] == 0:\n",
        "        continue\n",
        "    assignments = kmeans.predict(descriptors)\n",
        "    for kp, w in zip(keypoints, assignments):\n",
        "        if w == word_to_show:\n",
        "            x, y = map(int, kp.pt)\n",
        "            locations.append((img_idx, x, y))\n",
        "            if len(locations) >= max_patches:\n",
        "                break\n",
        "    if len(locations) >= max_patches:\n",
        "        break\n",
        "\n",
        "# Group by image and visualize\n",
        "imgs = defaultdict(list)\n",
        "for idx, x, y in locations:\n",
        "    imgs[idx].append((x, y))\n",
        "\n",
        "if imgs:\n",
        "    img_idx, pts = next(iter(imgs.items()))\n",
        "    fname = image_data[img_idx][0]\n",
        "    img   = cv2.imread(os.path.join(image_path, fname), cv2.IMREAD_GRAYSCALE)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
        "    for x, y in pts:\n",
        "        cv2.circle(img_rgb, (x, y), radius=25, color=(0,255,0), thickness=2)\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(f\"Word {word_to_show} Keypoints on the Left-View Photo\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We note that the word \"6\" could represent the beads in the beadwork worn by the Kouros. \n",
        "\n",
        "> **Discussion:** Based on your knowledge of the various Kouros, do you think this visual word can be the key to differentiating between different Kouros, or even different sculptural subjects?\n",
        "\n",
        "\n",
        "#### 1.2 Measurement of Similarity\n",
        "\n",
        "As you may have realized, the visual word frequency distributions of different images are not exactly the same, so how can we determine if these images are similar? More importantly, what can we use as a criterion to categorize different images based on visual words? Here, we will use something called **cosine similarity** to make a measurement.\n",
        "<!-- ```{=html}\n",
        "<iframe src=\"../data/cosine_demo.html\" width=\"600\" height=\"600\" style=\"border:none;\"></iframe>\n",
        "``` -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Academic style settings\n",
        "plt.rc('font', family='serif', size=12)\n",
        "plt.rc('axes', titlesize=14, labelsize=12)\n",
        "plt.rc('legend', fontsize=10)\n",
        "plt.rc('figure', titlesize=16)\n",
        "\n",
        "# Define vectors for each scenario\n",
        "# (a) general example\n",
        "v1_a = np.array([1, 0])\n",
        "v2_a = np.array([0.5, 0.8])\n",
        "# (b) same direction, different norms\n",
        "v1_b = np.array([1, 1])\n",
        "v2_b = np.array([2, 2])\n",
        "# (c) orthogonal vectors\n",
        "v1_c = np.array([1, 0])\n",
        "v2_c = np.array([0, 1])\n",
        "# (d) 3D example\n",
        "v1_d = np.array([-1, 1, 1])\n",
        "v2_d = np.array([1, -1, 1])\n",
        "\n",
        "# Function to compute cosine similarity\n",
        "def cosine_sim(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "\n",
        "# Set up figure and subplots\n",
        "grid = plt.figure(constrained_layout=True, figsize=(8, 8))\n",
        "sub_a = grid.add_subplot(2, 2, 1)\n",
        "sub_b = grid.add_subplot(2, 2, 2)\n",
        "sub_c = grid.add_subplot(2, 2, 3)\n",
        "sub_d = grid.add_subplot(2, 2, 4, projection='3d')\n",
        "\n",
        "# Plot helper for 2D vectors\n",
        "def plot_2d(ax, u, v, title, color_u='C0', color_v='C1'):\n",
        "    # Plot vectors\n",
        "    ax.quiver(0, 0, u[0], u[1], angles='xy', scale_units='xy', scale=1,\n",
        "              color=color_u, label=r'$\\mathbf{{u}}={} $'.format(u))\n",
        "    ax.quiver(0, 0, v[0], v[1], angles='xy', scale_units='xy', scale=1,\n",
        "              color=color_v, label=r'$\\mathbf{{v}}={} $'.format(v))\n",
        "    # Compute cosine similarity\n",
        "    cos_sim = cosine_sim(u, v)\n",
        "    # Title and labels\n",
        "    ax.set_title(f\"{title}\\nCosine similarity = {cos_sim:.2f}\")\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$y$')\n",
        "    ax.grid(True)\n",
        "    ax.legend(loc='lower left')\n",
        "    # Set axis limits to keep vectors in view\n",
        "    max_val = max(np.linalg.norm(u), np.linalg.norm(v)) * 1.2\n",
        "    ax.set_xlim(-max_val + 0.5*max_val, max_val)\n",
        "    ax.set_ylim(-max_val + 0.5*max_val, max_val)\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "# (a) General example\n",
        "plot_2d(sub_a, v1_a, v2_a, '(a) General example')\n",
        "# (b) Same direction, different norms\n",
        "plot_2d(sub_b, v1_b, v2_b, '(b) Same direction, different norms')\n",
        "# (c) Orthogonal vectors\n",
        "plot_2d(sub_c, v1_c, v2_c, '(c) Orthogonal vectors')\n",
        "\n",
        "# (d) 3D static visualization\n",
        "ux, uy, uz = v1_d\n",
        "vx, vy, vz = v2_d\n",
        "sub_d.quiver(0, 0, 0, ux, uy, uz, length=1.0, normalize=True, color='C0',\n",
        "             label=r'$\\mathbf{{u}}={} $'.format(v1_d))\n",
        "sub_d.quiver(0, 0, 0, vx, vy, vz, length=1.0, normalize=True, color='C1',\n",
        "             label=r'$\\mathbf{{v}}={} $'.format(v2_d))\n",
        "cos_sim_d = cosine_sim(v1_d, v2_d)\n",
        "sub_d.set_title(f\"(d) 3D example\\nCosine similarity = {cos_sim_d:.2f}\")\n",
        "sub_d.set_xlabel('$x$')\n",
        "sub_d.set_ylabel('$y$')\n",
        "sub_d.set_zlabel('$z$')\n",
        "sub_d.legend(loc='best')\n",
        "\n",
        "# Adjust 3D axis limits symmetrically around origin\n",
        "def set_symmetric_limits(ax, vectors, margin=1.2):\n",
        "    norms = [np.linalg.norm(vec) for vec in vectors]\n",
        "    max_norm = max(norms) * margin\n",
        "    ax.set_xlim(-0.02, 1.5)\n",
        "    ax.set_ylim(-0.02, 1.5)\n",
        "    ax.set_zlim(-0.02, 1.5)\n",
        "\n",
        "set_symmetric_limits(sub_d, [v1_d, v2_d])\n",
        "\n",
        "# Modify viewing angle so origin appears at back\n",
        "sub_d.view_init(elev=30, azim=45)\n",
        "\n",
        "# Show all plots\n",
        "plt.suptitle(\"Examples of Cosine Similarity\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can think of the visual word frequency histogram for each image as an arrow in space, and cosine similarity is a measure of how much those arrows are pointing in the same direction. The criterion is very intuitive: the closer the cosine similarity of two images is to 1, the more similar the two images are; the closer the cosine similarity is to 0, the less similar the two images are.\n",
        "\n",
        "Here, we perform pairwise cosine similarity measurements on left-view, front-view, and back-view photographs of New York Kouros, and show the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hist_list = []\n",
        "for img_name, _, descriptors in image_data:\n",
        "    if descriptors.shape[0] > 0:\n",
        "        words = kmeans.predict(descriptors)\n",
        "        hist, _ = np.histogram(words, bins=np.arange(n_clusters + 1))\n",
        "    else:\n",
        "        hist = np.zeros(n_clusters, dtype=int)\n",
        "    hist = hist.astype(float)\n",
        "    if hist.sum() > 0:\n",
        "        hist /= hist.sum()\n",
        "    hist_list.append(hist)\n",
        "    \n",
        "histograms = np.array(hist_list)\n",
        "\n",
        "sim_matrix = cosine_similarity(histograms)\n",
        "\n",
        "image_keys = list(image_names.keys())\n",
        "image_labels = list(image_names.values())\n",
        "\n",
        "# Display the similarity matrix\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "cax = ax.imshow(sim_matrix, interpolation='nearest', cmap='viridis')\n",
        "ax.set_title('BoVW Cosine Similarity between Images')\n",
        "ax.set_xticks(np.arange(len(image_labels)))\n",
        "ax.set_yticks(np.arange(len(image_labels)))\n",
        "ax.set_xticklabels(image_labels, rotation=45, ha='right')\n",
        "ax.set_yticklabels(image_labels)\n",
        "fig.colorbar(cax, ax=ax, label='Cosine Similarity')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Pairwise Cosine Similarity Matrix:\")\n",
        "for i in range(len(image_labels)):\n",
        "    for j in range(i + 1, len(image_labels)):\n",
        "        print(f\"{image_labels[i]} vs {image_labels[j]}: {sim_matrix[i, j]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the pairwise cosine similarities are all very high, even though the BoVW histograms look very different! This is good evidence that they are photographs of the same object, and computers can understand this by setting the appropriate threshold. \n",
        "\n",
        "However, would this also work for photos of different objects? Let's find out by calculating the cosine similarity between existing images and another Kouros currently exhibited in the Piraeus Archaeological Museum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a new image to compare with the existing ones\n",
        "new_image_path = '../data/richter_kouroi_complete_front_only/page312_img01_photo4.jpg'\n",
        "new_image_label = 'A Kouros from Athens' # Suppose this is a new artifact we just discovered\n",
        "\n",
        "img_new = cv2.imread(new_image_path, cv2.IMREAD_GRAYSCALE)\n",
        "orb = cv2.ORB_create(nfeatures=500)\n",
        "kp_new, desc_new = orb.detectAndCompute(img_new, None)\n",
        "\n",
        "if desc_new is not None and len(desc_new) > 0:\n",
        "    words_new = kmeans.predict(desc_new)\n",
        "    hist_new, _ = np.histogram(words_new, bins=np.arange(kmeans.n_clusters + 1))\n",
        "else:\n",
        "    hist_new = np.zeros(kmeans.n_clusters, dtype=int)\n",
        "    \n",
        "hist_new = hist_new.astype(float)\n",
        "if hist_new.sum() > 0:\n",
        "    hist_new /= hist_new.sum()\n",
        "\n",
        "sims = cosine_similarity(histograms, hist_new.reshape(1, -1)).flatten() \n",
        "\n",
        "# Print the cosine similarity of the new image with existing images\n",
        "print(f\"\\nCosine Similarity of '{new_image_label}' with existing images:\")\n",
        "for i, label in enumerate(image_labels):\n",
        "    print(f\"{label} vs {new_image_label}: {sims[i]:.3f}\")\n",
        "\n",
        "# Show the new image\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(img_new, cmap='gray')\n",
        "plt.title(f\"{new_image_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By looking at the results, we see that it has a lower but still relatively high cosine similarity to the previous images, albeit with different textures and poses. Although computers do not understand what \"Kouroi\" are simply by collecting visual words, they can still see the similarity! To support this view, let's look at an example that is also a standing figure, but from a different culture (China, Sanxingdui). If our conjecture is correct, its cosine similarity to the previous images will decrease significantly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a new image to compare with the existing ones\n",
        "new_image_path2 = '../data/example_images/sanxingdui.jpeg'\n",
        "new_image_label2 = 'A Bronze Figure from Sanxingdui' # Suppose this is a new artifact we just discovered\n",
        "\n",
        "img_new2 = cv2.imread(new_image_path2, cv2.IMREAD_GRAYSCALE)\n",
        "orb = cv2.ORB_create(nfeatures=500)\n",
        "kp_new, desc_new = orb.detectAndCompute(img_new2, None)\n",
        "\n",
        "if desc_new is not None and len(desc_new) > 0:\n",
        "    words_new = kmeans.predict(desc_new)\n",
        "    hist_new, _ = np.histogram(words_new, bins=np.arange(kmeans.n_clusters + 1))\n",
        "else:\n",
        "    hist_new = np.zeros(kmeans.n_clusters, dtype=int)\n",
        "    \n",
        "hist_new = hist_new.astype(float)\n",
        "if hist_new.sum() > 0:\n",
        "    hist_new /= hist_new.sum()\n",
        "\n",
        "sims = cosine_similarity(histograms, hist_new.reshape(1, -1)).flatten()  \n",
        "\n",
        "# Print the cosine similarity of the new image with existing images\n",
        "print(f\"\\nCosine Similarity of '{new_image_label2}' with existing images:\")\n",
        "for i, label in enumerate(image_labels):\n",
        "    print(f\"{label} vs {new_image_label2}: {sims[i]:.3f}\")\n",
        "\n",
        "# Show the new image\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.imshow(img_new2, cmap='gray')\n",
        "plt.title(f\"{new_image_label2}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results were exactly as we expected. The cosine similarity measured for different artistic styles, different poses and different angles of the Sanxingdui sculpture is significantly lower.\n",
        "\n",
        "This provides us with a hint on how to build an automatic image-based classifier for art and artifacts of different genres, cultures, and textures. Although BoVW also has some obvious limitations (lack of spatial relationships, lack of ability to detect specific objects in complex images), the examples above demonstrate the fundamentals of computer vision, and with the help of more advanced techniques we can do much more in analyzing artwork based on digitized images.\n",
        "\n",
        "### 2. Convolutions on Images\n",
        "\n",
        "#### 2.1 What are convolutions?\n",
        "Before diving into applying a **convolutional neural network**, let's first make an intuitive introduction to the concept **convolution**.\n",
        "\n",
        "Imagine sliding a tiny image over an image as a filter to make the actual image appear the same as the filter. Convolution is the mathematical operation to achieve such an effect.\n",
        "\n",
        "Below is one of such filters, or its professional term, a kernel, how do you think it will filter an image to make the image look like it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kernel\n",
        "kernel = np.array([\n",
        "    [-1, -1, -1],\n",
        "    [ 0,  0,  0],\n",
        "    [ 1,  1,  1]\n",
        "])\n",
        "\n",
        "# Map: 1 -> 1.0 (white), 0 -> 0.0 (black), -1 -> 1.0 (white)\n",
        "display_kernel = np.where(kernel == 0, 0, 1)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "cax = ax.matshow(display_kernel, cmap='gray', vmin=0, vmax=1)\n",
        "plt.colorbar(cax)\n",
        "\n",
        "# Annotate the kernel values\n",
        "\n",
        "ax.set_title('Example of a Kernel')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is how an actual image **Convolved** with the filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the image as grayscale\n",
        "img_path = \"../data/example_images/page300_img01_photo8.jpg\"\n",
        "image = Image.open(img_path).convert('L')\n",
        "img_array = np.array(image)\n",
        "\n",
        "# Define the horizontal edge detection kernel\n",
        "kernel = np.array([\n",
        "    [-1, -1, -1],\n",
        "    [ 0,  0,  0],\n",
        "    [ 1,  1,  1]\n",
        "])\n",
        "\n",
        "# Convolve the image with the kernel\n",
        "convolved = convolve(img_array, kernel, mode='reflect')\n",
        "\n",
        "# Display the original and convolved images\n",
        "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
        "ax[0].imshow(img_array, cmap='gray')\n",
        "ax[0].set_title(\"Original Image\")\n",
        "ax[0].axis('off')\n",
        "ax[1].imshow(convolved, cmap='gray')\n",
        "ax[1].set_title(\"Convolved with Kernel\")\n",
        "ax[1].axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above is just one example of a convolutional kernel that extracts horizontal edges in an image. In fact, there are many different kernels with different effects. For example, here is a filter that blurs all images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = np.array(Image.open(img_path).convert('L'))\n",
        "\n",
        "def gaussian_kernel(size=21, sigma=5):\n",
        "    ax = np.linspace(-(size-1)//2, (size-1)//2, size)\n",
        "    xx, yy = np.meshgrid(ax, ax)\n",
        "    kernel = np.exp(-(xx**2 + yy**2) / (2. * sigma**2))\n",
        "    return kernel / np.sum(kernel)\n",
        "\n",
        "kernel = gaussian_kernel(21, 5)\n",
        "conv = convolve(img, kernel)\n",
        "fig, ax = plt.subplots(1, 3, figsize=(8, 3))\n",
        "ax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\n",
        "ax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"21x21 Gaussian Kernel\"); ax[1].axis('off')\n",
        "ax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Heavily Blurred\"); ax[2].axis('off')\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is a kernel that preserves the input image as it is; it is also known as the identity kernel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = np.array(Image.open(img_path).convert('L'))\n",
        "\n",
        "# Identity kernel (3x3)\n",
        "kernel = np.zeros((3, 3))\n",
        "kernel[1, 1] = 1\n",
        "\n",
        "conv = convolve(img, kernel)\n",
        "fig, ax = plt.subplots(1, 3, figsize=(8, 3))\n",
        "ax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\n",
        "ax[1].imshow(kernel, cmap='gray', vmin=0, vmax=1); ax[1].set_title(\"Identity Kernel\"); ax[1].axis('off')\n",
        "ax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There is also a kernel that sharpens the images, known as the sharpening filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = np.array(Image.open(img_path).convert('L'))\n",
        "\n",
        "# Sharpen kernel\n",
        "kernel = np.array([[ 0, -1,  0],\n",
        "                   [-1,  5, -1],\n",
        "                   [ 0, -1,  0]])\n",
        "\n",
        "conv = convolve(img, kernel)\n",
        "fig, ax = plt.subplots(1, 3, figsize=(8, 3))\n",
        "ax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\n",
        "ax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"Sharpen Kernel\"); ax[1].axis('off')\n",
        "ax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other than sharpening, there is even a filter to emboss the image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = np.array(Image.open(img_path).convert('L'))\n",
        "\n",
        "# Emboss kernel\n",
        "kernel = np.array([[-2, -1, 0],\n",
        "                   [-1,  1, 1],\n",
        "                   [ 0,  1, 2]])\n",
        "\n",
        "conv = convolve(img, kernel)\n",
        "fig, ax = plt.subplots(1, 3, figsize=(8, 3))\n",
        "ax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off')\n",
        "ax[1].imshow(kernel, cmap='gray'); ax[1].set_title(\"Emboss Kernel\"); ax[1].axis('off')\n",
        "ax[2].imshow(conv, cmap='gray'); ax[2].set_title(\"Convolved\"); ax[2].axis('off')\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other than only detecting vertical or horizontal edges, a filter named after Laplace was discovered to detect all edges:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = np.array(Image.open(img_path).convert('L'))\n",
        "\n",
        "laplacian = np.array([[0,-1,0],[-1,8,-1],[0,-1,0]])\n",
        "edge = convolve(img, laplacian)\n",
        "\n",
        "fig, ax = plt.subplots(1, 3, figsize=(8, 3))\n",
        "ax[0].imshow(img, cmap='gray'); ax[0].set_title(\"Original\"); ax[0].axis('off'\n",
        ")\n",
        "ax[1].imshow(laplacian, cmap='gray'); ax[1].set_title(\"Laplacian Kernel\"); ax[1].axis('off')\n",
        "ax[2].imshow(edge, cmap='gray'); ax[2].set_title(\"Edges\"); ax[2].axis('off')\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2 What can Machine Learning do?\n",
        "\n",
        "Over the years, people have discovered these tiny images or \"kernels\" or \"filters\". In machine learning, we discover or learn these potentially useful filters directly from the data, rather than through mathematical derivation. In a word, machine learning can \"learn\" the filters from data what would be useful for downstream tasks like classifying images or identifying things in an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Load a pretrained conv model (VGG16 without top)\n",
        "model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "# 2. Choose three conv layers: early, middle, late\n",
        "early_layer = model.get_layer('block1_conv2')\n",
        "mid_layer   = model.get_layer('block3_conv3')\n",
        "late_layer  = model.get_layer('block5_conv3')\n",
        "\n",
        "# 3. Extract one kernel from each layer\n",
        "# Each kernel has shape (k, k, in_channels, out_channels)\n",
        "kernel_early = early_layer.get_weights()[0][:, :, 0, 0]\n",
        "kernel_mid   = mid_layer.get_weights()[0][:, :, 0, 0]\n",
        "kernel_late  = late_layer.get_weights()[0][:, :, 0, 0]\n",
        "\n",
        "# 4. Load and preprocess an image\n",
        "def load_and_gray(path, target_size=(224,224)):\n",
        "    img = keras_image.load_img(path, target_size=target_size)\n",
        "    img_arr = keras_image.img_to_array(img)\n",
        "    # convert to grayscale\n",
        "    gray = cv2.cvtColor(img_arr.astype('uint8'), cv2.COLOR_RGB2GRAY)\n",
        "    # normalize\n",
        "    gray = gray.astype('float32') / 255.0\n",
        "    return gray\n",
        "\n",
        "gray = load_and_gray(img_path)\n",
        "\n",
        "# 5. Convolve the image with each kernel\n",
        "def apply_filter(img, kernel):\n",
        "    # Flip kernel for convolution\n",
        "    k = kernel.shape[0]\n",
        "    # OpenCV uses correlation; flip kernel to perform convolution\n",
        "    flipped = np.flipud(np.fliplr(kernel))\n",
        "    filtered = cv2.filter2D(img, -1, flipped)\n",
        "    return filtered\n",
        "\n",
        "out_early = apply_filter(gray, kernel_early)\n",
        "out_mid   = apply_filter(gray, kernel_mid)\n",
        "out_late  = apply_filter(gray, kernel_late)\n",
        "\n",
        "# 6. Visualize\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title('Original Gray')\n",
        "plt.imshow(gray, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title('Early Layer Kernel')\n",
        "plt.imshow(out_early, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title('Mid Layer Kernel')\n",
        "plt.imshow(out_mid, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.title('Late Layer Kernel')\n",
        "plt.imshow(out_late, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  # In Colab this will display inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Different parts of the model highlight different things. The important thing to note is that no one wrote the filters themselves. The network learned that the features highlighted by these filters are useful.**We simply wrote the learning algorithm, then the model learned from data by itself.**\n",
        "\n",
        "Typically, a model used for image classification can (on its own) learn filters to highlight things the model needs, such as edges and lines, and more importantly, in addition to these simple features, the model can learn filters to detect heads, eyes, ears, and other abstract concepts, and this is exactly how convolution makes it possible to detect, characterize, and categorize complex objects in complex images. In order to achieve these amazing features, we usually need to employ models such as **Convolutional Neural Networks**.\n",
        "\n",
        "### 3. Data Exploration\n",
        "\n",
        "#### 3.1 Exploring the Metadata\n",
        "\n",
        "For the rest of the notebook, we will use a small selection of photographs from Richter's Kouroi (1942), which contain frontal shots of Kouroi with a full torso and recognizable facial features. We have also prepared a labeled metadata that shows information about which group and era these Kouroi belong to and what materials they are made of. We can begin by looking at some basic information from the metadata:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the metadata CSV file\n",
        "# Note that we are only going to investigate a subset of the full dataset\n",
        "df = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')\n",
        "\n",
        "df = df.drop(columns = 'page')\n",
        "\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Information of the dataset:\")\n",
        "print(f\"Number of images: {df.shape[0]}\")\n",
        "print(f\"Number of distinct eras: {df['era'].nunique()}\")\n",
        "print(f\"Number of distinct materials: {df['material'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see the distribution of each label by plotting histograms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bar_plot(df, column1, column2):\n",
        "    # Calculate counts of each value in the specified columns\n",
        "    label_counts1 = df[column1].value_counts()\n",
        "    label_counts2 = df[column2].value_counts()\n",
        "\n",
        "    # Create a figure with a fixed size\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    \n",
        "    # Plot the bar chart\n",
        "    label_counts1.plot(kind='bar', ax=axes[0], color='steelblue')\n",
        "    axes[0].set_title(f'Distribution of {column1.capitalize()}')\n",
        "    axes[0].set_xlabel(column1.capitalize())\n",
        "    axes[0].set_ylabel('Count')\n",
        "    axes[0].tick_params(axis='x', rotation=90)\n",
        "    label_counts2.plot(kind='bar', ax=axes[1], color='darkorange')\n",
        "    axes[1].set_title(f'Distribution of {column2.capitalize()}')\n",
        "    axes[1].set_xlabel(column2.capitalize())\n",
        "    axes[1].set_ylabel('Count')\n",
        "    axes[1].tick_params(axis='x', rotation=90)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the distribution of labels in the dataset\n",
        "bar_plot(df, 'era', 'material')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2 Exploring the Images\n",
        "\n",
        "To get a direct idea of the general characteristics of this subset of photographs, we read the photographs from the image directory and show the first 4 images in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the images as a list \n",
        "\n",
        "data_dir = Path(\"../data/richter_kouroi_complete_front_only\")\n",
        "image_paths = sorted(data_dir.glob(\"*.jpg\"))\n",
        "\n",
        "images = []\n",
        "for p in image_paths:\n",
        "    img = Image.open(p).convert(\"RGB\")   # ensure 3‑channel\n",
        "    img_arr = np.array(img)\n",
        "    images.append(img_arr)\n",
        "    \n",
        "fig, axes = plt.subplots(1, 4, figsize=(6, 4))\n",
        "for ax, img in zip(axes, images[:4]):\n",
        "    ax.imshow(img)\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"First 4 images in the dataset\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Image Embedding Using ConvNeXt V2\n",
        "\n",
        "#### 4.1 CNN and Models See Images\n",
        "\n",
        "What we'll do next is we will bring in ConvNeXt V2, it is a CNN model trained on millions of images based on ImageNet. \n",
        "\n",
        "We're sure everyone has used some **Large Language Models (LLMs)** and gotten a feel for how these models mimic the way humans think. This imitation of the human way of thinking comes from **Artificial Neural Networks (ANN)**. Just as **LLMs** process and generate natural language, **CNN models** process visual images, examining the features in those images through **convolutional layers** and generating their own digital representations of the images. \n",
        "\n",
        "Here, we will create a visualization that shows the early, middle, and late feature layers of the 4 images after convolution, what commonality and difference did you notice from the extracted features in these layers?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = VGG16(weights='imagenet', include_top=False)\n",
        "\n",
        "# Grab kernels from the first and last conv layers\n",
        "early_layer = model.get_layer('block1_conv2')\n",
        "mid_layer = model.get_layer('block3_conv3')\n",
        "late_layer  = model.get_layer('block5_conv3')\n",
        "\n",
        "# choose the (0,0) filter for each\n",
        "kernel_early = early_layer.get_weights()[0][:, :, 0, 0]\n",
        "kernel_mid  = mid_layer.get_weights()[0][:, :, 0, 0]\n",
        "kernel_late  = late_layer.get_weights()[0][:, :, 0, 0]\n",
        "\n",
        "# Convolution helper (flip kernel for true conv)\n",
        "def apply_filter(img, kernel):\n",
        "    flipped = np.flipud(np.fliplr(kernel))\n",
        "    return cv2.filter2D(img, -1, flipped)\n",
        "\n",
        "fig, axes = plt.subplots(3, 4, figsize=(8, 8))\n",
        "\n",
        "for col, img in enumerate(images[:4]):\n",
        "    gray = (\n",
        "        cv2.cvtColor(img.astype('uint8'), cv2.COLOR_RGB2GRAY)\n",
        "          .astype('float32') / 255.0\n",
        "    )\n",
        "    out_early = apply_filter(gray, kernel_early)\n",
        "    out_early = (out_early - out_early.min()) / (out_early.max() - out_early.min())\n",
        "\n",
        "    out_mid = apply_filter(gray, kernel_mid)\n",
        "    out_mid = (out_mid - out_mid.min()) / (out_mid.max() - out_mid.min())\n",
        "    \n",
        "    out_late = apply_filter(gray, kernel_late)\n",
        "    out_late = (out_late - out_late.min()) / (out_late.max() - out_late.min())\n",
        "\n",
        "    # plot\n",
        "    axes[0, col].imshow(out_early, cmap='gray')\n",
        "    axes[0, col].set_title(f'Early Layer #{col+1}')\n",
        "    axes[0, col].axis('off')\n",
        "\n",
        "    axes[1, col].imshow(out_mid, cmap='gray')\n",
        "    axes[1, col].set_title(f'Mid Layer #{col+1}')\n",
        "    axes[1, col].axis('off')\n",
        "\n",
        "    axes[2, col].imshow(out_late, cmap='gray')\n",
        "    axes[2, col].set_title(f'Late Layer #{col+1}')\n",
        "    axes[2, col].axis('off')\n",
        "\n",
        "fig.suptitle('First, Middle vs. Last Conv Layer Responses', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4.2 Creating Image Embeddings\n",
        "\n",
        "**Image embedding** is a process where images are transformed into numerical representations, specifically, lists of numbers that carry informations about the images. While this sounds somewhat similar to the idea of visual words, they are not the same. Think of BoVW as counting how many times specific words appear in a book without caring about grammar or sentence structure-- this can identify simple patterns, but cannot summarize the big picture of the book. Image embeddings, on the other hand, are like reading the entire book and summarizing its meaning in a well crafted passage, they capture the bigger picture, context, and nuance.\n",
        "\n",
        "We can build a vocabulary of visual words quite easily, but creating image embeddings usually require using deep neural networks pretrained on millions of images. These networks process the entire image and learn hierarchical, abstract features that are more semantically meaningful.  \n",
        "\n",
        "Here, we will load the pre-trained ConvNeXt V2 model, pass the image folder to generate embeddings, and save the embeddings in a grid of numbers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read in the pre-trained ConvNeXtV2 model\n",
        "# Load the pre-trained ConvNeXtV2 model and image processor\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-base-22k-224\") \n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = ConvNextV2Model.from_pretrained(\"facebook/convnextv2-base-22k-224\")\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "_ = model.to(device)\n",
        "\n",
        "# Define the image directory for later use\n",
        "image_directory = \"../data/richter_kouroi_complete_front_only\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** The code below is commented out to avoid running it by accident, you can uncomment it and run it if you want to generate the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# batch_size = 16\n",
        "# embeddings = []\n",
        "# valid_filenames = []\n",
        "\n",
        "# filenames = df['filename'].tolist()\n",
        "\n",
        "# for i in tqdm(range(0, len(filenames), batch_size), desc=\"Processing Images in Batches\"):\n",
        "#     batch_filenames = filenames[i : i + batch_size]\n",
        "#     images = []\n",
        "#     for filename in batch_filenames:\n",
        "#         path = os.path.join(image_directory, filename)\n",
        "#         try:\n",
        "#             img = Image.open(path).convert(\"RGB\")\n",
        "#             images.append(img)\n",
        "#         except FileNotFoundError:\n",
        "#             print(f\"Missing: {path}\")\n",
        "#         except Exception as e:\n",
        "#             print(f\"Error with {filename}: {e}\")\n",
        "\n",
        "#     if not images:\n",
        "#         continue\n",
        "\n",
        "#     # Prepare inputs\n",
        "#     inputs = processor(images=images, return_tensors=\"pt\")\n",
        "#     pixel_values = inputs[\"pixel_values\"].to(device)\n",
        "\n",
        "#     # Forward through feature extractor\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(pixel_values)   \n",
        "#     # Global average pool\n",
        "#     if isinstance(outputs, torch.Tensor):\n",
        "#         hidden = outputs           \n",
        "#     else:\n",
        "#         hidden = outputs.last_hidden_state \n",
        "\n",
        "#     batch_emb = hidden.mean(dim=(2, 3)).cpu().numpy()\n",
        "\n",
        "#     embeddings.extend(batch_emb)   \n",
        "    \n",
        "# embeddings = np.stack(embeddings, axis=0) \n",
        "\n",
        "# np.save('../data/embeddings/convnextv2_image_embeddings.npy', embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load embeddings from the saved file\n",
        "embeddings = np.load('../data/embeddings/convnextv2_image_embeddings.npy')\n",
        "\n",
        "print(embeddings)\n",
        "\n",
        "print(f\"The embedding has {embeddings.shape[0]} rows and {embeddings.shape[1]} numbers each row.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We printed the embedded results above to see what they look like, and we also displayed the shape of the grid. As you can see, it contains 62 rows representing the 62 images in the dataset, and each row has 1024 numbers representing all the information extracted from each image. From now on, we will use this embedded data instead of the original image data.\n",
        "\n",
        "### 5. Analysis of Image Embeddings\n",
        "\n",
        "1024 is way too many numbers for us to examine and understand with our brains. So, we use something techniques called **dimensionality reduction** to squish the data down to just 2 or 3 dimensions, making it easy to visualize in 2D. \n",
        "\n",
        "**Principal Component Analysis (PCA)** is one of such techniques, it is essentially finding the two or three major axes through our huge data along with which the data has the most variations. By PCA we decompose our data with 1024 dimensions (number of cells in each row) to 2 dimensions and represent each image as a point on our scatterplots. We then colour the data with \"era\" and \"material\" respectively. \n",
        "\n",
        "Here, we will use plotly to create interactive visualizations, feel free to play with it and discuss patterns that you notice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
        "\n",
        "# load metadata\n",
        "df = pd.read_csv('../data/complete_sculpture_dataset_labeled.csv')   \n",
        "\n",
        "# PCA to 2 components\n",
        "pca = PCA(n_components=2)\n",
        "pc2 = pca.fit_transform(embeddings)\n",
        "\n",
        "# build DataFrame\n",
        "pc_df = pd.DataFrame(pc2, columns=['PC1','PC2'])\n",
        "pc_df['filename'] = df['filename'].values\n",
        "pc_df['era'] = df['era'].values\n",
        "\n",
        "# interactive scatter\n",
        "fig = px.scatter(\n",
        "        pc_df,\n",
        "        x='PC1',\n",
        "        y='PC2',\n",
        "        color='era',\n",
        "        hover_data=['filename'], \n",
        "        title='Interactive PCA of Image Embeddings Colored by Era',\n",
        "        width=700, height=500\n",
        "    )\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
        "\n",
        "pc_df['material'] = df['material'].values\n",
        "\n",
        "fig = px.scatter(\n",
        "        pc_df,\n",
        "        x='PC1',\n",
        "        y='PC2',\n",
        "        color='material',\n",
        "        hover_data=['filename'], \n",
        "        title='Interactive PCA of Image Embeddings Colored by Material',\n",
        "        width=700, height=500\n",
        "    )\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Discussion:** Did you see any clear patterns of distributions by looking at the visualizations above? How can you interpret the results? What primary \"features\" do you think the PCA embedding captured in the first two dimensions?\n",
        "\n",
        "### 6. Classification of Kouroi \n",
        "\n",
        "**Archaeological classification** has always been an important issue in archaeology and artifact research. This problem is especially challenging when faced with a large amount of artifact data, or when faced with new artifacts with insufficient information. With the development of machine learning and image recognition technology, the use of computer technology to assist classification has become a trend in the new era of information archaeology. In this section, we would like to provide an example of classifying Kouroi by visual element for your reference.\n",
        "\n",
        "#### 6.1 Traditional Approach\n",
        "\n",
        "In addition to observing how the labels are clustered based on the embeddings, we can train classifiers to categorize objects into appropriate labels based on the image embeddings directly. A traditional approach is through a technique called **logistic regression**. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = embeddings\n",
        "y1 = pc_df['era'].tolist()\n",
        "y2 = pc_df['material'].tolist()\n",
        "\n",
        "y1 = np.array(y1)\n",
        "y2 = np.array(y2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that here we use image embeddings and labels as training data and test data respectively, this is because we want to evaluate the effectiveness of the classifier when dealing with unseen data. After training the classifier for eras, we use it to predict the labels of the test data and compare the results with the real labels. The report is printed below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform the classification of eras\n",
        "# Split the data into training and testing sets\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y1, test_size=0.25, random_state=42, stratify=y1)\n",
        "\n",
        "# Create a logistic regression model\n",
        "clf1 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "clf1.fit(X_train1, y_train1)\n",
        "\n",
        "y_pred1 = clf1.predict(X_test1)\n",
        "\n",
        "# Calculate classification metrics\n",
        "classification_report1 = metrics.classification_report(y_test1, y_pred1, zero_division=0)\n",
        "\n",
        "# Print the classification report \n",
        "print(\"Classification Report for Eras:\")\n",
        "print(classification_report1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The key metrics we especially care about here is the accuracy of our classifier, it is defined by\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{True Predictions}}{\\text{True Predictions} + \\text{Flase Predictions}}\n",
        "$$\n",
        "\n",
        "It reflects the proportion of true predictions out of all predictions made using the classifier. However, as shown above in the report, the accuracy of predicting era based on image embedding is not satisfactory. Still, we can visualize the decision boundary of this classifier on our 2D PCA of image embeddings to see what went wrong:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare your 2D data + labels\n",
        "X_pca = pc_df[['PC1','PC2']].values\n",
        "y_era = pc_df['era'].values\n",
        "\n",
        "# Encode eras as integers\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y_era)\n",
        "\n",
        "# Train the logistic on the encoded labels\n",
        "clf_2d = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_2d.fit(X_pca, y_enc)\n",
        "\n",
        "# Build a mesh grid over the plotting area\n",
        "x_min, x_max = X_pca[:,0].min() - 1, X_pca[:,0].max() + 1\n",
        "y_min, y_max = X_pca[:,1].min() - 1, X_pca[:,1].max() + 1\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 300),\n",
        "    np.linspace(y_min, y_max, 300)\n",
        ")\n",
        "\n",
        "# Predict integer labels on the mesh\n",
        "Z = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "# Number of classes\n",
        "n_classes = len(le.classes_)\n",
        "\n",
        "# Pick a sequential colormap name:\n",
        "base_cmap_red = plt.cm.Reds\n",
        "base_cmap_blue = plt.cm.Blues\n",
        "\n",
        "# Sample 7 colors evenly from the light part of the cmap for regions\n",
        "# and the dark part for points.\n",
        "region_colors = base_cmap_red(np.linspace(0.1, 0.6, n_classes))\n",
        "point_colors  = base_cmap_blue(np.linspace(0.1, 1.0, n_classes))\n",
        "\n",
        "cmap_light = ListedColormap(region_colors)\n",
        "cmap_bold  = ListedColormap(point_colors)\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n",
        "\n",
        "# scatter original points, mapping back to string labels in the legend\n",
        "for class_int, era_label in enumerate(le.classes_):\n",
        "    mask = (y_enc == class_int)\n",
        "    plt.scatter(\n",
        "        X_pca[mask,0], X_pca[mask,1],\n",
        "        color=cmap_bold(class_int),\n",
        "        label=era_label,\n",
        "        edgecolor='k', s=40\n",
        "    )\n",
        "\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Logistic Decision Boundary on 2D PCA Embedding\\n(colored by era)')\n",
        "plt.legend(title='Era')\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Discussion:** What can you say about this decision boundary?\n",
        "\n",
        "Similarly, we can use the same approach to classify material of Kouroi. We first perform a train-test split, then train the classifier, use the trained classifier to predict the labels of the test set, and print out the classification report for quality evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform the classification of materials\n",
        "# Split the data into training and testing sets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y2, test_size=0.25, random_state=42, stratify=y2)\n",
        "\n",
        "# Create a logistic regression model\n",
        "clf2 = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "clf2.fit(X_train2, y_train2)\n",
        "\n",
        "y_pred2 = clf2.predict(X_test2)\n",
        "\n",
        "# Calculate classification metrics\n",
        "classification_report2 = metrics.classification_report(y_test2, y_pred2, zero_division=0)\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report for Materials:\")\n",
        "print(classification_report2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the accuracy is much higher now, but does that mean the classifier is good? You may have noticed that bronze and marble are classified almost perfectly, but other materials are not. This means that the classifier, while having a high accuracy, may have low precision or recall, as defined below:\n",
        "\n",
        "- **Precision**: The ratio of the number of true positives to the number of positive predictions. Precision tells us how often the model predicts correctly. \n",
        "- **Recall**: The ratio of the number of true positives to the number of actual positives. Recall answers the question, \"What percentage of positive results did we correctly predict?\"\n",
        "\n",
        "We can also visualize the decision boundary on the 2D PCA of materials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare 2D data and labels\n",
        "X_pca = pc_df[['PC1','PC2']].values\n",
        "y_era = pc_df['material'].values\n",
        "\n",
        "# Encode eras as integers\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y_era)\n",
        "\n",
        "# Train the logistic on the encoded labels\n",
        "clf_2d = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf_2d.fit(X_pca, y_enc)\n",
        "\n",
        "# Build a mesh grid over the plotting area\n",
        "x_min, x_max = X_pca[:,0].min() - 1, X_pca[:,0].max() + 1\n",
        "y_min, y_max = X_pca[:,1].min() - 1, X_pca[:,1].max() + 1\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 300),\n",
        "    np.linspace(y_min, y_max, 300)\n",
        ")\n",
        "\n",
        "# Predict integer labels on the mesh\n",
        "Z = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "# light colors for regions\n",
        "cmap_light = ListedColormap(['#FFCCCC','#CCFFCC','#CCCCFF','#FFE5CC','#E5CCFF'][:len(le.classes_)])\n",
        "# bold colors for points\n",
        "cmap_bold  = ListedColormap(['#FF0000','#00AA00','#0000FF','#FF8000','#8000FF'][:len(le.classes_)])\n",
        "\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=cmap_light)\n",
        "\n",
        "# Scatter original points, mapping back to string labels in the legend\n",
        "for class_int, era_label in enumerate(le.classes_):\n",
        "    mask = (y_enc == class_int)\n",
        "    plt.scatter(\n",
        "        X_pca[mask,0], X_pca[mask,1],\n",
        "        color=cmap_bold(class_int),\n",
        "        label=era_label,\n",
        "        edgecolor='k', s=40\n",
        "    )\n",
        "\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.title('Logistic Decision Boundary on 2D PCA Embedding\\n(colored by material)')\n",
        "plt.legend(title='Era')\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Discussion:** Now, going back to the classification reports shown above, do you think the classifier trained on era is a good chronological classifier? What about materials? \n",
        "\n",
        "#### 6.2 CNN Classification of Materials\n",
        "\n",
        "The last classifier we're going to visit today is a neural network classifier, using a **Multi-layer Perceptron (MLP)** network architecture, which means we're going to add a classification layer to the ConvNeXt V2 model to classify the material. All the model does here is act as a **backbone** to observe and extract features of interest in the input image. The MLP process, on the other hand, can be visualized as a number of experts examining different features on an image, then discussing them with each other, and finally voting to reach a final conclusion.\n",
        "\n",
        "We begin by creating the data loader and load the Kouroi data directly from the folder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map the materials to integers\n",
        "MAT2IDX = {\n",
        "    'Marble': 0,\n",
        "    'Bronze': 1,\n",
        "    'Other': 2\n",
        "}\n",
        "\n",
        "# Create a custom dataset class for the Kouroi dataset\n",
        "class KouroiDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, processor, mat2idx):\n",
        "        self.df = df\n",
        "        self.img_dir = image_directory\n",
        "        self.processor = processor\n",
        "        self.mat2idx = MAT2IDX\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img = Image.open(os.path.join(self.img_dir, row.filename)).convert(\"RGB\")\n",
        "\n",
        "        inputs = self.processor(images=img, return_tensors=\"pt\")\n",
        "\n",
        "        for k,v in inputs.items():\n",
        "            inputs[k] = v.squeeze(0)\n",
        "        label = self.mat2idx[row.material]\n",
        "        return inputs, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tain-test split\n",
        "\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.25,                 # 20% held out for testing\n",
        "    stratify=df[\"material\"],       # preserve class proportions\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize the dataset and dataloader\n",
        "train_ds = KouroiDataset(\n",
        "    df=train_df,\n",
        "    img_dir=image_directory,\n",
        "    processor=processor,\n",
        "    mat2idx=MAT2IDX\n",
        ")\n",
        "test_ds  = KouroiDataset(\n",
        "    df=test_df,\n",
        "    img_dir=image_directory,\n",
        "    processor=processor,\n",
        "    mat2idx=MAT2IDX\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As mentioned above, here we freeze the model so that training does not change the way it understands the input image, but we are going to add a new classification layer on top of the network so that it can now use the additional knowledge about Kouroi for classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the model with convnextv2 as the backbone and a linear layer for classification\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, backbone_name, num_classes):\n",
        "        super().__init__()\n",
        "        # Load the ConvNeXtV2 backbone correctly and freeze it\n",
        "        self.backbone = ConvNextV2Model.from_pretrained(\n",
        "            backbone_name,\n",
        "            output_hidden_states=False,\n",
        "            output_attentions=False\n",
        "        )\n",
        "        for p in self.backbone.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        embed_dim = self.backbone.config.hidden_sizes[-1]\n",
        "\n",
        "        # Build a simple 2-layer MLP head\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim // 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(embed_dim // 2, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        # Forward through the frozen backbone\n",
        "        outputs = self.backbone(pixel_values=pixel_values)\n",
        "        x = outputs.pooler_output\n",
        "\n",
        "        # Classification head\n",
        "        logits = self.head(x)\n",
        "        return logits\n",
        "\n",
        "# Instantiate and move to device\n",
        "model = Classifier(\n",
        "    backbone_name=\"facebook/convnextv2-base-22k-224\",\n",
        "    num_classes=len(MAT2IDX)\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, after setting up the new model, we will define the training loop and train the model. Please note that this process may take some time, especially when running on devices without a GPU. This also suggests that the high computational power requirement is a drawback when using CNNs for classification.\n",
        "\n",
        "**Note:** The training loop is commented out to avoid running it by accident, you can uncomment it and run it if you want to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the loss function and optimizer\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.head.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "# epochs = 20  \n",
        "\n",
        "# for epoch in range(1, epochs+1):\n",
        "#     model.train()\n",
        "#     total_loss = 0\n",
        "#     for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
        "#         inputs, labels = batch\n",
        "#         # move to device\n",
        "#         inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         logits = model(**inputs)\n",
        "#         loss   = criterion(logits, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "#     avg_loss = total_loss / len(train_ds)\n",
        "#     print(f\" Epoch {epoch} avg loss: {avg_loss:.4f}\")\n",
        "\n",
        "# torch.save(model.state_dict(), \"../data/models/mlp_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training, we set the model to evaluation mode and assessed the classification quality by printing the confusion matrix and classification report. It is clear that the accuracy does improve with the MLP architecture. However, we still lacked samples of materials other than bronze and marble, which undoubtedly harmed the quality of our training. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the trained model\n",
        "model.load_state_dict(torch.load(\"../data/models/mlp_model.pth\"))\n",
        "model.to(device)  # move to the right device \n",
        "\n",
        "# Run one pass over your data in eval mode\n",
        "model.eval()\n",
        "\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        if inputs is None: continue\n",
        "        inputs = {k:v.to(device) for k,v in inputs.items()}\n",
        "        logits = model(**inputs)\n",
        "        all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Print the classification report\n",
        "report = classification_report(all_labels, all_preds, target_names=list(MAT2IDX.keys()), zero_division=0)\n",
        "print(\"Classification Report for Materials:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Discussion:** What difference did you notice in the result? Does this mean that MLP is not applicable to classifying materials?\n",
        "\n",
        "While the MLP classifier via CNN has advantages in dealing with more complex data structures (especially high-dimension nonlinear data), we note that it has two serious drawbacks: 1. it is more susceptible to the randomness of the training-testing split, with greater model variability; and 2. it is more susceptible to **overfitting**, whereas logistic regression is more susceptible to **underfitting** (for more detailed information on these concepts discussion can be found in Appendix B). This tells us that no one model is perfect for all situations. We must remain cautious in our choice of models.\n",
        "\n",
        "#### 6.3 Example: Predicting the Material of Unseen Kouroi Photos\n",
        "\n",
        "Now, let's imagine a scenario where we find a new Kouros, but we are not sure what it is made of, and we want to use our trained classifier to classify it based on its image features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a new image to compare with the existing ones\n",
        "new_artifact_path = '../data/example_images/NAMA_3938_Aristodikos_Kouros.jpeg'\n",
        "new_artifact_label = 'A Kouros from Athens' # Suppose this is a new artifact we just discovered\n",
        "\n",
        "art_new = cv2.imread(new_artifact_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Show the new image\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(art_new, cmap='gray')\n",
        "plt.title(f\"{new_artifact_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Discussion:** Looking at this photo, how would you classify the era and material of this Kouros? \n",
        "\n",
        "We pass it into our trained logistic regression classifier and see how would its era be classified:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reload the processor and the model\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-base-22k-224\") \n",
        "\n",
        "model = ConvNextV2Model.from_pretrained(\"facebook/convnextv2-base-22k-224\")\n",
        "\n",
        "# Open as new img\n",
        "img_new = Image.open(new_artifact_path).convert(\"RGB\")\n",
        "\n",
        "inputs = processor(images=img_new, return_tensors=\"pt\")\n",
        "pixel_values = inputs[\"pixel_values\"].to(device)\n",
        "\n",
        "# Forward through feature extractor\n",
        "with torch.no_grad():\n",
        "    outputs = model(pixel_values) \n",
        "\n",
        "# Global average pool\n",
        "if isinstance(outputs, torch.Tensor):\n",
        "    hidden = outputs           \n",
        "else:\n",
        "    hidden = outputs.last_hidden_state \n",
        "\n",
        "emb_new = hidden.mean(dim=(2, 3)).cpu().numpy()\n",
        "\n",
        "\n",
        "pred_era = clf1.predict(emb_new)[0]\n",
        "\n",
        "print(\"Predicted Era:\", pred_era)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then pass it into our trained CNN classifier and see how its material would be classified:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reload the model\n",
        "model = Classifier(\n",
        "    backbone_name=\"facebook/convnextv2-base-22k-224\",\n",
        "    num_classes=len(MAT2IDX)\n",
        ").to(device)\n",
        "model.load_state_dict(torch.load(\"../data/models/mlp_model.pth\"))\n",
        "model.to(device) \n",
        "\n",
        "# Resize the model to an appropriate size\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),                \n",
        "    transforms.CenterCrop(224),            \n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(                   \n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std= [0.229, 0.224, 0.225]\n",
        "    ),\n",
        "])\n",
        "\n",
        "idx2mat = {idx: mat for mat, idx in MAT2IDX.items()}\n",
        "\n",
        "def predict_image(image_path, model, device):\n",
        "    # Load\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    # Preprocess\n",
        "    x = preprocess(img)\n",
        "    x = x.unsqueeze(0).to(device)\n",
        "    # Inference\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits = model(**{\"pixel_values\": x}      \n",
        "                       if isinstance(x, torch.Tensor) else x)\n",
        "        pred_idx = logits.argmax(dim=1).item()\n",
        "\n",
        "    return idx2mat[pred_idx]\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "predicted_material = predict_image(new_artifact_path, model, device)\n",
        "print(\"Predicted Material:\", predicted_material)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Discussion:** Do you think the predictions made above are correct? What is your evidence?\n",
        "\n",
        "#### 6.4 Additional Note on Fine-tuning\n",
        "\n",
        "What we didn't really include here are the more advanced applications of fine-tuning. **Fine-tuning** refers to the process of taking a pre-trained model (like ConvNeXt V2) and continuing its training on your specific dataset, allowing the model to adapt its learned features to better suit your task. Why we did not cover fine-tuning because it requires more computational resources, careful hyperparameter tuning, and a larger dataset to avoid overfitting. Additionally, fine-tuning can be time-consuming and is often not practical in an introductory or resource-limited setting.\n",
        "\n",
        "However, fine-tuning has the potential to significantly improve classification quality, especially for irregular data. By allowing the model to update its internal representations based on the unique characteristics of your images, it can learn more relevant features for distinguishing between subtle differences in style, era, or material. For research or production applications with sufficient data and compute, fine-tuning is a powerful next step to achieve higher accuracy and more robust results.\n",
        "\n",
        "### 7. Conclusion\n",
        "\n",
        "Through this notebook, you've taken a journey with the example of Richter's Kouroi from the basics of how computers \"see\" images to advanced techniques for analyzing and classifying images. You've explored how simple pixel values can be transformed into powerful representations using convolution, image embeddings, and neural networks. Along the way, you learned to visualize, cluster, and classify artworks...... these are all skills that are at the heart of modern computer vision.\n",
        "\n",
        "Remember, the tools and concepts you've practiced here are not just limited to art history or archaeology: they are widely used in fields ranging from medicine to astronomy, and beyond. As you continue your studies, keep experimenting, stay curious, and don't be afraid to explore new datasets or try more advanced models. The intersection of technology and the humanities is full of exciting possibilities, and your creativity is the key to unlocking them.\n",
        "\n",
        "Congratulations on completing this exploration, and we hope you feel inspired to keep learning and discovering the field of Machine Learning!\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- There are multiple different ways for computers to represent and understand content in images, including **intensity histograms**, **BoVW distribution** and **image embeddings**.\n",
        "- **Convolution** is a common technique to process and extract different features in input images. \n",
        "- **Convolutional Neural Networks (CNN)** are trained models that mimic the way people view images and understand them through convolutional layers.\n",
        "- **Image embeddings** produced by a pretrained CNN map each kouros image into a high‑dimensional feature space. We can then apply **dimensionality‑reduction techniques** such as **Principal Component Analysis** to visualize clusters among different archaic sculptures.\n",
        "- For formal classification of kouroi, such as distinguishing groups/eras or identifying materials, both **logistic regression** and **neural-network classification** provide robust methods to assign style labels based on extracted image features, but we should always be aware of issues such as **underfitting** and **overfitting**.\n",
        "- Different models may yield different results for image embedding, and sometimes the features recognized from an image are not exactly what we want. We should always be cautious about our results.\n",
        "\n",
        "### Glossary\n",
        "\n",
        "- **Computer Vision**: Computer Vision is a field of artificial intelligence that enables computers to \"see\" and interpret images and videos, mimicking human vision.\n",
        "- **Convolution**: In the context of Computer Vision, Convolution is an operation that applies a filter (kernel) to an image, transforming it in a way that enhances certain features or extracts useful information. It is like using a brush to color a painting or a pen to sketch a line art.\n",
        "- **Convolutional Neural Network (CNN)**: Convolutional Neural Network is a type of feedforward neural network that learns features via filter (or kernel) optimization. It is distinguished from other neural networks by its superior performance with image, speech or audio signal inputs.\n",
        "- **Image Embedding**: Image Embedding is a process where images are transformed into numerical representations, called vectors, that capture the semantic meaning of the image. \n",
        "- **Principal Component Analysis (PCA)**: Principal Component Analysis is a statistical technique that simplifies complex data sets by reducing the number of variables while retaining key information. It does so by finding the major axes where the data sets vary the most.\n",
        "- **Logistic Regression**: Logistic Regression is a statistical model used for binary or multiclass classification tasks. It estimates the probability that an input belongs to a particular class by applying the logistic (sigmoid) function to a weighted sum of the input features, making it well-suited for problems where outputs are discrete categories.\n",
        "- **Multilayer Perceptron (MLP)**: A Multilayer Perceptron is a class of feedforward artificial neural network composed of an input layer, one or more hidden layers of nonlinear activation units, and an output layer. It learns complex patterns by adjusting the weights of connections through backpropagation and is versatile for both classification and regression tasks.\n",
        "- **Underfitting**: Underfitting in machine learning occurs when a model is too simple to capture the underlying patterns in the training data.\n",
        "- **Overfitting**: Overfitting in machine learning occurs when a model learns the training data too well, including its noise and random fluctuations, leading to poor performance on new, unseen data.\n",
        "\n",
        "\n",
        "### Appendix A: Image Data Collection and Preprocessing from `.pdf` Files\n",
        "\n",
        "This part provides a brief overview of how the data was collected and preprocessed for the analysis, typically how we cropped the images and prepared the metadata.\n",
        "\n",
        "We used the following python script to convert a scanned pdf of Richter (1942) to image files in `.jpg` format.\n",
        "\n",
        "```python\n",
        "import fitz  \n",
        "\n",
        "# Change the filename here if you want to reuse the script for your own project\n",
        "doc = fitz.open(\"kouroiarchaicgre0000rich_1.pdf\") \n",
        "\n",
        "import os\n",
        "out_dir = \"extracted_images\"\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "# Iterate pages\n",
        "for page_index in range(len(doc)):\n",
        "    page = doc[page_index]\n",
        "    image_list = page.get_images(full=True)  # get all images on this page\n",
        "\n",
        "    # Skip pages without images\n",
        "    if not image_list:\n",
        "        continue\n",
        "\n",
        "    # Extract each image\n",
        "    for img_index, img_info in enumerate(image_list, start=1):\n",
        "        xref = img_info[0]                   \n",
        "        base_image = doc.extract_image(xref)  \n",
        "        image_bytes = base_image[\"image\"]     \n",
        "        image_ext   = base_image[\"ext\"]      \n",
        "\n",
        "        # Write to file\n",
        "        out_path = os.path.join(\n",
        "            out_dir,\n",
        "            f\"page{page_index+1:03d}_img{img_index:02d}.{image_ext}\"\n",
        "        )\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            f.write(image_bytes)\n",
        "\n",
        "print(f\"Saved all images to {out_dir}\")\n",
        "```\n",
        "\n",
        "The following script cropped the photos by applying convolution.\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Folder containing your page images\n",
        "input_folder = \"extracted_images\"\n",
        "output_folder = \"cropped_photos\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "def extract_photos_from_page(image_path, min_area=5000):\n",
        "    img = cv2.imread(image_path)\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # Blur and threshold to get binary image\n",
        "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
        "    _, thresh = cv2.threshold(blurred, 200, 255, cv2.THRESH_BINARY_INV)\n",
        "    \n",
        "    # Dilate to merge photo regions\n",
        "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (15, 15))\n",
        "    dilated = cv2.dilate(thresh, kernel, iterations=2)\n",
        "    \n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    \n",
        "    crops = []\n",
        "    for cnt in contours:\n",
        "        x, y, w, h = cv2.boundingRect(cnt)\n",
        "        area = w * h\n",
        "        # Filter by area to remove small artifacts\n",
        "        if area > min_area:\n",
        "            crop = img[y:y+h, x:x+w]\n",
        "            crops.append((crop, (x, y, w, h)))\n",
        "    return crops\n",
        "\n",
        "# Process all pages\n",
        "for img_path in glob.glob(os.path.join(input_folder, \"*.*\")):\n",
        "    base = os.path.splitext(os.path.basename(img_path))[0]\n",
        "    photos = extract_photos_from_page(img_path)\n",
        "    for idx, (crop, (x, y, w, h)) in enumerate(photos, start=1):\n",
        "        out_file = os.path.join(output_folder, f\"{base}_photo{idx}.jpg\")\n",
        "        cv2.imwrite(out_file, crop)\n",
        "\n",
        "print(f\"Saved all images at {output_folder}\")\n",
        "```\n",
        "The following script employed `tesseract` OCR engine to detect pure text images and filter out photos of Kouros. You may want to visit the GitHub repository <https://github.com/tesseract-ocr/tesseract> to see how to install and setup appropriately.\n",
        "\n",
        "```python\n",
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "# Ensure you have Tesseract installed and pytesseract configured correctly.\n",
        "# On Windows, you might need:\n",
        "# pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
        "\n",
        "# Folders\n",
        "input_folder = \"cropped_photos\"\n",
        "text_folder = \"text_crops\"\n",
        "photo_folder = \"filtered_photos\"\n",
        "os.makedirs(text_folder, exist_ok=True)\n",
        "os.makedirs(photo_folder, exist_ok=True)\n",
        "\n",
        "# Threshold for text length to consider as \"text-only\"\n",
        "# You can also adjust this threshold based on your specific needs.\n",
        "TEXT_CHAR_THRESHOLD = 2 # Be careful with this threshold, do remember to check the results manually\n",
        "\n",
        "\n",
        "for filename in os.listdir(input_folder):\n",
        "    path = os.path.join(input_folder, filename)\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # Perform OCR to extract text\n",
        "    extracted_text = pytesseract.image_to_string(img)\n",
        "\n",
        "    # Classify based on length of extracted text\n",
        "    if len(extracted_text.strip()) >= TEXT_CHAR_THRESHOLD:\n",
        "        dest = os.path.join(text_folder, filename)\n",
        "    else:\n",
        "        dest = os.path.join(photo_folder, filename)\n",
        "\n",
        "    shutil.move(path, dest)\n",
        "    print(f\"Moved {filename} -> {os.path.basename(dest)}\")\n",
        "\n",
        "print(\"Filtering complete\")\n",
        "```\n",
        "This script creates a `.csv` file for mannual labelling.\n",
        "\n",
        "```python\n",
        "import os, re\n",
        "import pandas as pd\n",
        "\n",
        "# Scan your filtered_photos folder\n",
        "records = []\n",
        "\n",
        "# Updated regex to match \"page<number>_img<number>_photo<number>.<ext>\"\n",
        "pattern = re.compile(r\"page(\\d+)_img\\d+_photo(\\d+)\\.(?:png|jpe?g)\", re.IGNORECASE)\n",
        "\n",
        "for fn in os.listdir(\"richter_kouroi_head_front_only\"):\n",
        "    m = pattern.match(fn)\n",
        "    if not m:\n",
        "        continue\n",
        "    page = int(m.group(1))\n",
        "    photo_idx = int(m.group(2))\n",
        "    records.append({\n",
        "        \"filename\": fn,\n",
        "        \"page\": page,\n",
        "        \"group\": \"\",    # blank for manual entry\n",
        "        \"era\": \"\",  # blank for manual entry\n",
        "        \"material\": \"\"  # blank for manual entry\n",
        "    })\n",
        "\n",
        "# Build DataFrame\n",
        "df = pd.DataFrame(records)\n",
        "\n",
        "df.sort_values([\"page\", \"filename\"], inplace=True)\n",
        "\n",
        "# Save out to CSV for manual labeling\n",
        "df.to_csv(\"label_template.csv\", index=False)\n",
        "```\n",
        "You can try out the scripts with your interested pdf files yourself by running them in a python environment.\n",
        "\n",
        "### Appendix B: The Risk of Underfitting and Overfitting\n",
        "\n",
        "In the context of machine learning, two common pitfalls are **underfitting** and **overfitting**. \n",
        "\n",
        "**Underfitting** occurs when a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test sets. This is often seen when using models like logistic regression on complex, non-linear datasets, as shown in the left panel above, where the decision boundary fails to separate the classes effectively. On the other hand, **overfitting** happens when a model is excessively complex, such as a deep neural network with many layers, and learns not only the true patterns but also the noise in the training data. This leads to excellent performance on the training set but poor generalization to new, unseen data, as illustrated in the right panel where the decision boundary is overly intricate. \n",
        "\n",
        "Let's examine the two problems with a simulated two-class data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate a simulater two-class data\n",
        "X, y = make_moons(n_samples=500, noise=0.40, random_state=0)\n",
        "\n",
        "# Visualize the simulated data\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolor='k')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Simulated Two-Class Data')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can clearly see that the data is very non-linear, which gives us a hint that the right model should be able to account for non-linear relationships. However, for demonstration purposes, I will be using logistic regression to generate an underfitting classifier; and while MLP is suitable for use here, I will let it generate an overfitting classifier by significantly oversizing the neurons and iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=1\n",
        ")\n",
        "under = LogisticRegression()\n",
        "\n",
        "over  = MLPClassifier(hidden_layer_sizes=(200,200,200),\n",
        "                      max_iter=2000,\n",
        "                      random_state=1)\n",
        "\n",
        "# Train both models\n",
        "under.fit(X_train, y_train)\n",
        "over.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we get the decision boundaries of the two classifiers and visualize them with the test data, what do you find about their accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build grid\n",
        "x_min, x_max = X[:,0].min() - .5, X[:,0].max() + .5\n",
        "y_min, y_max = X[:,1].min() - .5, X[:,1].max() + .5\n",
        "xx, yy = np.meshgrid(\n",
        "    np.linspace(x_min, x_max, 300),\n",
        "    np.linspace(y_min, y_max, 300)\n",
        ")\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "\n",
        "# Get decision boundary\n",
        "Zu = under.predict_proba(grid)[:,1].reshape(xx.shape)\n",
        "Zo = over.predict_proba(grid)[:,1].reshape(xx.shape)\n",
        "\n",
        "# Plot side by side\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8,4))\n",
        "\n",
        "for ax, Z, title in [\n",
        "    (ax1, Zu, 'Extreme Underfit'),\n",
        "    (ax2, Zo, 'Extreme Overfit')\n",
        "]:\n",
        "    ax.contourf(xx, yy, Z>0.5, alpha=0.3)\n",
        "    # draw precise decision boundary P=0.5\n",
        "    ax.contour(   xx, yy, Z, levels=[0.5], colors='k', linewidths=1.5)\n",
        "\n",
        "    ax.scatter(X_test[:,0], X_test[:,1], c=y_test, edgecolor='k')\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel('Feature 1')\n",
        "    ax.set_ylabel('Feature 2')\n",
        "\n",
        "fig.suptitle('Underfitting vs. Overfitting in Classification', fontsize=16)\n",
        "plt.tight_layout(rect=[0,0.03,1,0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can tell easily from the above visualization that both scenarios are harmful: underfitting prevents the model from making meaningful predictions, while overfitting results in unreliable predictions on real-world data. The example above also highlights the necessity to validate classifier quality using test data.\n",
        "\n",
        "While underfitting and overfitting can seem scary, there are many tools that have been developed to address this issue. We can adjust the complexity of the model, use regularization techniques, collect more data, or employ cross-validation to find a balance that generalizes well to new examples. These are left for you to explore on your own.\n",
        "\n",
        "### References\n",
        "1. Richter, G. M. A. (1970). *Kouroi: Archaic Greek youths: A study of the development of the Kouros type in Greek sculpture.* Phaidon. Accessed through Internet Archive <https://archive.org/details/kouroiarchaicgre0000rich>.\n",
        "2. Pinecone. *Embedding Methods for Image Search.* Accessed through Pinecone <https://www.pinecone.io/learn/series/image-search/>.\n",
        "3. IBM. What are convolutional neural networks? <https://www.ibm.com/think/topics/convolutional-neural-networks>\n",
        "4. Hugging Face. Image Classification. <https://huggingface.co/docs/transformers/tasks/image_classification>\n",
        "5. Coleman, C., Lyon, S., & Perla, J. (2020). Introduction to Economic Modeling and Data Science. QuantEcon. Retrieved from <https://datascience.quantecon.org/> \n",
        "6. Woo, S., Debnath, S., Hu, R., Chen, X., Liu, Z., Kweon, I. S., & Xie, S. (2024). ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders. arXiv preprint arXiv:2301.00808v1.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
